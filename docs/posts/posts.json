[
  {
    "path": "posts/welcome/",
    "title": "Welcome to Simply Statistics",
    "description": {},
    "author": [
      {
        "name": "Nora Jones",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-11-11",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T11:46:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-10-thinking-about-failure-in-data-analysis/",
    "title": "Thinking About Failure in Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2021-11-10",
    "categories": [],
    "contents": "\nWhat does it mean for a data analysis to fail? I’ve come to feel that this is an important question because considering the ways that data analyses can fail is a good way to prevent an analysis from actually failing. However, thinking about the ways an analysis can fail is easier said than done. It requires an active imagination, an ability to think about what might happen or what might have happened. It requires thinking about what was not observed rather than what was observed.\nOne thing to clarify here is that when I think of failure, I think of something that is more or less immediately observable. So once you’ve done the analysis, you know whether you’ve failed or not. This rules out a number of possibilities that I think people are used to thinking about. For example, it rules out the question of whether the claims of an analysis are true or not. Often, it is not possible to determine the truth of an analysis until much later. It also rules out considerations of replication or reproducibility because both of those require future analyses to be done. Therefore, I’m not going to consider these ideas for now and focus on what we as analysts can do in the moment.\nAn analysis failure, broadly speaking, is any outcome that does not meet our expectations. This might sound like a bit of an anti-climactic definition of “failure” but ultimately such an outcome represents a failure of our understanding about something: the data (and data generation process), the methods, or the science. As a result, a success is any outcome that is as-expected. That also seems like a bit of low bar for success, but we can make other distinctions later regarding whether a success is good or not.\nTypes of Failure\nMy taxonomy of failure falls into two large bins:\nVerification failure: This is a narrow class a failure when an analysis produces an outcome that is counter to our expectation in any way. If we are expecting a correlation coefficient to be between 0.4 and 0.6 and we compute a correlation coefficient that is actually 0.1, then that is a verification failure. This failure indicates that there is something we don’t quite understand about the underlying correlation or the data.\nValidation failure: This is a broader class of failure that includes considerations that are outside the data. Validation failure is typically a result of a poor design process for the analysis that results in flawed requirements. For example, if we are interested in the association between predictor X and outcome Y adjusted for Z, but we build a prediction model for Y that omits Z, then this is a validation failure. The prediction model may operate as-expected and do a good job of predicting Y, but it doesn’t answer the question of whether X and Y are associated adjusted for Z.\nI find a simple non-data related example can be helpful. A person can ask an architect to design a house with no roof. If the architect builds the house, then the finished house meets expectations from a verification standpoint. It is a verification success. However, when it rains and the inside of the house gets wet, that is a validation failure.\nThis failure can be traced back to the design process and the setting of the requirements for the project. The actual building of the house went without a hitch. So how could the failure be prevented?\nOne possibility is the architect could have pushed back and argued that a house with no roof is a bad idea and is therefore a bad requirement (and probably against local code). The architect might have even refused to build the house. Or the architect could simply ask why the person doesn’t want a roof. If it turned out that the person simply wanted a lot of light in the house, then a different requirement could be specified, like a glass roof, that might satisfy everyone’s needs.\nIn general, I think the things we teach about statistics and data analysis in the classroom (textbook knowledge, so to speak) are focused around preventing verification failure. Given a set of requirements, we’d like people to know how to build the widget with the right tools and to be knowledgeable about the assumptions they are making. In my experience, the things that are taught “outside the classroom” are focused on preventing validation failure. Here, we want people to know whether they are building something useful and not simply building something that is “correct”.\nPotential Outcomes for Analysis\nStatisticians like to talk about data analysis failure because it’s a little like Monday morning quarterbacking. It’s often easy to recognize a failure after the analysis is done. But what about before the analysis is done?\nCatching a failure before it happens requires an understanding of the potential outcomes of an analysis. Most data analysis problems will admit a range of possible analysis plans and it is up to the analyst to choose one. Given an analysis plan, there is then a set of potential outcomes that this plan can produce before it is applied to the data. Once we apply the plan to the data, we will observe one of these potential outcomes.\nFor example, if our analysis is to take the arithmetic mean of a set of numbers, the set of potential outcomes is some subset of the real line. If we are making a scatter plot of two continuous variables, then our set of potential outcomes is the set of all possible scatter plots of two variables. Clearly, the second example with the scatter plot is harder to characterize than the first. But most analysts will intuitively understand what this set looks like.\nOnce we can characterize the set of potential outcomes, we can divide that set into two broad regions: expected outcomes and unexpected outcomes. When we apply our analysis plan to the data, the outcome will fall in one of these two regions. Outcomes that fall into the “unexpected” region are what I am characterizing as failures here.\nContinuing our two examples from above, for the mean we might expect that the outcome will fall into the interval [3, 7]. If the observed mean ended up being 10, that would be unexpected. If it were 4 then that would be as-expected. For the scatter plot, if we believed the two variables were positively correlated, then we might expect the scatter plot to look like a nice cluster of dots shaped like a “football” (American, that is) leaning at roughly a 45 degree angle. If the actual scatter plot looked like a circle (i.e. no correlation) or maybe a circle with a large outlier to the right, that would be unexpected.\nData analysis potential outcome set.Going From Success to Failure\nThe traditional notions of success and failure would seem to suggest that we should favor success over failure. But in the data analysis context, what we need to consider is how an analysis can go from success to failure and vice versa. If an analysis outcome is a success and is as-expected, it is important to ask “How could this have failed?” If an analysis outcome is a failure and is unexpected, it is important to ask “How could this have succeeded?”\nThere is therefore a common task when it comes to the observed output of a data analysis, regardless of whether it could be considered a success or a failure. That task is to consider the entire potential outcome set (given the chosen plan) and determine what could cause one to observe a different outcome than what was actually observed.\nIn the case of failure, this scenario is a bit easier to understand. When one observes an unexpected outcome, usually one is highly incentivized to get to the bottom of what caused that to occur. It might have been an error in the dataset, or a problem in the data wrangling, or a misunderstanding of how the methods or software work. Finally, there might have been a misunderstanding in our expectation (i.e. in the underlying science) and that perhaps this outcome should have been expected.\nIn the case of success, it is critical that we apply essentially the same thinking, especially in the early stages of analysis. We should be getting to the bottom of what caused this (success) to occur. In this case, it is still possible to ask whether their might have been an error in the dataset, or a problem in the wrangling, or a misunderstanding of the methods, software or underlying science. In the case of success, it is sometimes valuable to ask what would happen if we induced some sort of problem, like an error in the dataset, or an outlier, or a mis-applied statistical model (sometimes this is called sensitivity analysis).\nIn both success and failure it is valuable to consider the unobserved outcomes in the potential outcome set and ask whether we actually should be observing one of those other outcomes, perhaps because there exists a better model or because the data should be processed in a different way. It is this consideration of the potential outcomes of an analysis, as well as the alternating between success and failure, that drives the iterative nature of data analysis. Ultimately, we want to come to a place where we feel we understand the data, the data generation process, and the analytic process that leads to our result.\nGeneralizing From Failure\nWhen I hear people (including myself) say that data analysis is learned through experience, I realize now that what we mean is that experience is what allows one to build up that active imagination of what could happen. Producing the observed data analytic outcome requires skills-—fitting statistical models, data wrangling, visualization—-that can largely be taught in the classroom. But building the set of potential outcomes becomes easier and faster with experience as we observe more outcomes in other data analyses.\nThe way that we generalize our experience across different data analyses is to enrich and expand our set of potential outcomes for use in future analyses. What was once unexpected now becomes somewhat as-expected. And because such outcomes are expected, we know to watch out for them and we learn the techniques for checking on them.\nBut there is another way that we can “learn from experience” that has the potential to take a lot less time——collaborating with other people. Other people may have more experience or they may have different experience. Combining people with different experiences on the same analysis can produce a similar effect to a single person having more experience. Each person has seen different outcomes in their past and together they can produce a potential outcome set that is much larger than they could produce on their own. In this way, people can gain “experience” by working together. And the more diverse the experiences of the individual collaborators, the richer and larger the potential outcome set will be that they can construct and imagine.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T11:47:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-31-streamline-tidy-data-as-a-service/",
    "title": "Streamline - tidy data as a service",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2021-03-31",
    "categories": [],
    "contents": "\nTldr: We started a company called Streamline Data Science https://streamlinedatascience.io/ that offers tidy data as a service. We are looking for customers, partnerships and employees as we scale up after closing our funding round!\nMost of my career, I have worked in the muck of data cleaning. In the world of genomics, a lot of my research has focused on batch effects, synthesizing big genomic data into usable formats, and generally making data easier to use. A couple of years ago, we also started a company called Problem Forward Data Science. Problem Forward offered fractional data science services to a variety of businesses around the country, from the very big corporations to startups just getting started. We were asked to do a lot of different types of data work, everything from turning spreadsheets into dashboards to building complicated forecasting models. But no matter the project, whether in government, academia, or industry, we always ended up with the same problem.\n\nWe needed to clean the data before we could do the data science.\n\nThis will be no surprise to anyone who has worked in data science or analytics, but the data almost always led to setbacks and frustration when we were working with our clients. Customers wanted complex AI, insightful dashboards, or easy reports, but the data just weren’t ready for that yet. And we wasted a huge amount of time cleaning the data over and over again.\nWe realized that the most common challenge companies have is that their data processing and management pipelines aren’t ready for analytics. Or as Google so eloquently puts it:\n\n“Everyone wants to do the model work, not the data work”\n\nWe realized that this was a service that many businesses needed. They needed someone who could come in and set up a data processing pipeline for them, manage it, and make sure the data were up to date. Some people call this Extract Load Transform (ELT), but we found it goes a bit beyond that. It is figuring out what format is most useful for the people who rely on data and working backward to create a customized and unique data pipeline that gets the data ready to use.\nThe ELT pipeline we set up is designed to consistently output “tidy data” that makes it easy for our customers to use BI tools like Tableau or Looker and to ingest their data without having to do all the ugly data work that is painful and time-consuming.\nWe piloted this for one of our startup customers - we built their data pipeline and provided ongoing management, maintenance, and upkeep. When they hired their first data scientist, they were able to quickly create dashboards for their whole business because they already had easy-to-use, tidy data.\nWe got so excited about this data dry cleaning idea that we started a new company called Streamline Data Science that solely focuses on tidy data as a service. We just closed our seed round and are now working with our first set of customers to set up their data pipelines. The cool thing is we found that our most excited customers were the ones that already had a data scientist on the team. This seems a little counter-intuitive until you realize that we handle the painful/boring bits of data management so they can focus on the fun part.\nThe interesting thing about Streamline is that it isn’t a product. There are a ton of complicated tools out there that you can use to set up your own data pipeline. Streamline is a service that handles all your data issues for you so the data “just works”. It can often be a lot cheaper than building out a full stack data engineering team in house.\nIf you are a company that is worried about the state of your data - they are difficult to share, to manage, and to quality control - we’d love to hear from you! We would also love to hear from you if you are a data scientist or analyst at a company that is frustrated about how much time you are spending on data management and cleaning.\nI’ll write more in the future about how we figured out setting up a data pipeline efficiently and the problems Streamline solves. We will also be releasing our first public data Streamlines that you can play with. In the meantime, I wanted to share how excited I am to finally be working on solving the first mile of data science and building a company that can help Baltimore grow its data science community.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T11:53:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-24-the-four-jobs-of-the-data-scientist/",
    "title": "The Four Jobs of the Data Scientist",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2020-11-24",
    "categories": [],
    "contents": "\nIn 2019 I wrote a post about The Tentpoles of Data Science that tried to distill the key skills of the data scientist. In the post I wrote:\n\nWhen I ask myself the question “What is data science?” I tend to think of the following five components. Data science is (1) the application of design thinking to data problems; (2) the creation and management of workflows for transforming and processing data; (3) the negotiation of human relationships to identify context, allocate resources, and characterize audiences for data analysis products; (4) the application of statistical methods to quantify evidence; and (5) the transformation of data analytic information into coherent narratives and stories.\n\n\nMy contention is that if you are a good data scientist, then you are good at all five of the tentpoles of data science. Conversely, if you are good at all five tentpoles, then you’ll likely be a good data scientist.\n\nI still feel the same way about these skills but my feeling now is that actually that post made the job of the data scientist seem easier than it is. This is because it wrapped all of these skills into a single job when in reality data science requires being good at four jobs. In order to explain what I mean by this, we have to step back and ask a much more fundamental question.\nWhat is the Core of Data Science?\nThis is a question that everyone is asking and I think struggling to answer. With any field there’s always a distinction between the questions of\nWhat is the core of the field?\nWhat do people in that field do on a regular basis?\nIn case it’s not clear, these are not the same question. For example, in Statistics, based on the curriculum from most PhD program, the core of the field involves statistical methods, statistical theory, probability, and maybe some computing. Data analysis is generally not formally taught (i.e. in the classroom), but rather picked up as part of a thesis or research project. Many classes labeled “Data Science” or “Data Analysis” simply teach more methods like machine learning, clustering, or dimension reduction. Formal software engineering techniques are also not generally taught, but in practice are often used.\nOne could argue that data analysis and software engineering is something that statisticians do but it’s not the core of the field. Whether that is correct or incorrect is not my point. I’m only saying that a distinction has to be made somewhere. Statisticians will always do more than what would be considered the core of the field.\nWith data science, I think we are collectively taking inventory of what data scientists tend to do. The problem is that at the moment it seems to be all over the map. Traditional statistics does tend to be central to the activity, but so does computer science, software engineering, cognitive science, ethics, communication, etc. This is hardly a definition of the core of a field but rather an enumeration of activities.\nThe question then is can we define something that all data scientists do? If we had to teach something to all data science students without knowing where they might end up afterwards, what would it be? My opinion is that at some point, all data scientists have to engage in the basic data analytic iteration.\nData Analytic Iteration\nThe basic data analytic iteration comes in four parts. Once a question has been established and a plan for obtaining or collecting data is available, we can do the following:\nConstruct a set of expected outcomes\nDesign/Build/Apply a data analytic system to the data\nDiagnose any anomalies in the analytic system output\nMake a decision about what to do next\nWhile this iteration might be familiar or obvious to many, its familiarity masks the complexity involved. In particular, each step of the iteration requires that the data scientist play a different role involving very different skills. It’s like a one-person play where the data scientist has to change costumes when going from one step to the next. This is what I refer to as the the four jobs of the data scientist.\nThe Four Jobs\nEach of the steps in the basic data analytic iteration requires the data scientist to be four different people: (1) Scientist; (2) Statistician; (3) Systems Engineer; and (4) Politician. Let’s take a look at each job in greater detail.\nScientist\nThe scientist develops and asks the question and is responsible for knowing the state of the science and what the key gaps are. The scientist also designs any experiments for collecting new data and executes the data collection. The scientist must work with the statistician to design a system for analyzing the data and ultimately construct a set of expected outcomes from any analysis of the data being collected.\nThe scientist plays a key role in developing the system that results in our set of expected outcomes. Components of this system might include a literature review, meta-analysis, preliminary data, or anecdotal data from colleagues. I use the term “Scientist” broadly here. In other settings this could be a policy-maker or product manager.\nStatistician\nThe statistician, in concert with the scientist, designs the analytic system that will analyze the data generated by any data collection efforts. They specify how the system will operate, what outputs it will generate, and obtain any resources needed to implement the system. The statistician draws on statistical theory and personal experience to choose the different components of the analytic system that will be applied.\nThe statistician’s role here is to apply the data analytic system to the data and to produce the data analytic output. This output could be a regression coefficient, a mean, a plot, or a prediction. But there must be something produced that we can compare to our set of expected outcomes. If the output deviates from our set of expected outcomes, then the next task is to identify the reasons for that deviation.\nSystems Engineer\nOnce the analytic system is applied to the data there are only two possible outcomes:\nThe outputs meet our expectations, or\nThe output does not meet our expectations (an anomaly).\nIn the case of an anomaly, the systems engineer’s responsibility is to diagnose the potential root causes of the anomaly, based on knowledge of the data collection process, the analytic system, and the state of scientific knowledge.\nRecently, Emma Vitz wrote on Twitter:\n\nHow do you teach debugging to people who are more junior? I feel like it’s such an important skill and yet we seem to have no structured framework for teaching it\n\nFor software and for data analysis alike, the challenge is that bugs or unexpected behavior can originate from anywhere. Any complex system is composed of multiple components, some of which may be your responsibility and many of which are someone else’s. But bugs and anomalies do not respect those boundaries! There may be an issue that occurs in one component that only becomes known when you see the data analytic output.\nSo if you are responsible for diagnosing a problem, it is your responsibility to investigate the behavior of each component of the system. If it is something you are not that familiar with, then you need to become familiar with it, either by learning on your own or (more likely) talking to the person who is in fact responsible.\nA common source of unexpected behavior in data analytic output is the data collection process, but the statistician who analyzes the data may not be responsible for that aspect of the project. Nevertheless, the systems engineer who identifies an anomaly has to go back through and talk to the statistician and the scientist to figure out exactly how each component works.\nUltimately, the systems engineer is tasked with taking a broad view of all the activities that affect the output from a data analysis in order to identify any deviations from what we would expect. Once those root causes have been explained, we can then move on to decide how we should act on this new information.\nPolitician\nThe politician’s job is to make decisions while balancing the needs of the various constituents to achieve a reasonable outcome. Most statisticians and scientist that I know would recoil at the idea of being considered a politician or that politics in any form would play a role in doing any sort of science. However, my thinking here is a bit more basic: In any data analysis iteration, we are constantly making decisions about what to do, keeping in mind a variety of conflicting factors. In order to resolve these conflicts and come to a reasonable agreement, one has to engage a key skill, which is negotiation.\nAt various stages of the data analytic iteration the politician must negotiate about (1) the definition of success in the analysis; (2) resources for executing the analysis; and (3) the decision for what to do after we have seen the output from the analytic system and have diagnosed the root causes of any anomalies. Decisions about what to do next fundamentally involve factors outside the data and the science.\nPoliticians have to identify who the stakeholders of the problem are and what is it that they ultimately want (as opposed to what their position is). For example, an investigator might say “We need a p-value less than 0.05”. That’s their position. But what they want is more likely “a publication in a high profile journal”. Another example might be an investigator who needs to meet a tight publication deadline while another investigator who wants to run a time-consuming (but more robust) analysis. Clearly, the positions conflict but arguably both investigators share the same goal, which is a rigorous high-impact publication.\nIdentifying positions versus underlying needs is a key task in negotiating a reasonable outcome for everyone involved. Rarely, in my experience, does this process have to do with the data (although data may be used to make certain arguments). The dominating elements of this process tend to be the nature of relationships between each constituent and the constraints on resources (such as time).\nApplying the Iteration\nIf you’re reading this and find yourself saying “I’m not an X” where X is either scientist, statistician, systems engineer, or politician, then chances are that is where you are weak at data science. I think a good data scientist has to have some skill in each of these domains in order to be able to complete the basic data analytic iteration.\nIn any given analysis, the iteration may be applied anywhere from once to dozens if not hundreds of times. If you’ve ever made two plots of the same dataset, you’ve likely done two iterations. While the exact details and frequency of the iterations may vary widely across applications, the core nature and the skills involved do not change much.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T11:51:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-26-palantir-shows-its-cards/",
    "title": "Palantir Shows Its Cards",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2020-08-26",
    "categories": [],
    "contents": "\nFile this under long-term followup, but just about four years ago I wrote about Palantir, the previously secretive but now soon to be public data science company, and how its valuation was a commentary on the value of data science more generally. Well, just recently Palantir filed to go public and therefore submitted a registration statement (S-1) describing its business. It’s a fascinating read, if you’re into that kind of stuff.\nBut the important thing is that Palantir itself summarized the question I asked more than 4 years ago. In their enumeration of risk factors, one risk factor they highlight is\n\nIf our customers are not able or willing to accept our product-based business model, instead of a labor-based business model, our business and results of operations could be negatively impacted. [emphasis added]\n\nIn my original post I wrote about the “Data Science Spectrum”, which has consulting on one end and software on the other.\nData Science SpectrumThe point of the diagram was that businesses on the right hand side have huge valuations while businesses on the left side have merely large valuations. The people running Palantir clearly understand this and are trying to push the company in a software-based productized direction.\nHere’s the rest of their summary of this risk factor:\n\nOur platforms are generally offered on a productized basis to minimize our customers’ overall cost of acquisition, maintenance, and deployment time of our platforms. Many of our customers and potential customers are instead generally familiar with the practice of purchasing or licensing software through labor contracts, where custom software is written for specific applications, the intellectual property in such software is often owned by the customer, and the software typically requires additional labor contracts for modifications, updates, and services during the life of that specific software. Customers may be unable or unwilling to accept our model of commercial software procurement. Should our customers be unable or unwilling to accept this model of commercial software procurement, our growth could be materially diminished, which could adversely impact our business, financial condition, results of operations, and growth prospects.\n\nThose of us who do data analysis for a living already know this to be true. Custom consulting is not scalable, and therefore, not as valuable as a piece of boxed up software, which is infinitely scalable.\nShow Me The Numbers\nSo, how is Palantir doing?\nAt first glance it seems their doing pretty well. Their gross profit (Revenue - Cost of Revenue) suggests about a 72% gross margin percentage for 2018 and 67% for 2019, which both seem high. These gross margin percentages are in software company territory. (For comparison, Facebook’s percentage runs around 80%.) This suggests that each dollar of Palantir’s revenue does not have a lot of direct costs associated with it.\nBut ulimately, Palantir has posted net losses every year, indicating there are significant indirect costs to generating that revenue. In particular, their Sales and marketing costs almost equal their entire gross profit. Reading through the S-1 this ultimately is not surprising. Palantir itself admits that\n\nOur sales efforts involve considerable time and expense and our sales cycle is often long and unpredictable.\n\nAlas, there is some consulting to do after all. My guess is that much of the up front “sales” work comes down to Palantir having to\nFigure out a customer’s problem and what question they’re asking;\nFigure out how a customer’s data are organized;\nFigure out how to their existing software products to the customer’s specific situation.\nThis should sound familiar to seasoned data scientists. Indeed, this is almost all the work of the data scientist. This is expensive because it requires humans to do it and there’s typically not much to generalize from customer to customer. Implementing the software and deploying it is work too, but is often more straightforward and their are often existing solutions that can be employed.\nThe Road to Profits\nSo here’s the problem: Palantir’s route to profitability involves making these costs go down…a lot. Maybe not to zero, but substantially, because each new customer—with their different problems and different data—costs a lot to acquire. If they can do this, they’ve cracked the nut of data science scalability.\nAnother big expense is Research and Development, which the company describes as developing new methods for data analysis (machine learning tools, etc.). While it’s nice to have room to do open-ended research on new data science tools, my guess is that this line item goes down a lot in the near future, as it often does at companies that start off with large R&D budgets. Ultimately, it would save Palantir ~$300 million a year.\nSee you in another four years?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T11:55:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-01-examplepost/",
    "title": "Example post",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2011-09-01",
    "categories": [],
    "contents": "\nWrite your text here in Markdown. Be aware that our blog runs with Jekyll\nDo codeblocks like this https://help.github.com/articles/creating-and-highlighting-code-blocks/\nPut all images in the public/ directory or point to them on a website where they are permanent\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T11:57:16-05:00",
    "input_file": {}
  }
]
