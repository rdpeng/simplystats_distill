[
  {
    "path": "posts/2022-07-29-is-code-the-best-way-to-represent-a-data-analysis/",
    "title": "Is Code the Best Way to Represent a Data Analysis?",
    "description": "Code is a useful representation of a data analysis for the purposes of transparency and opennness. But code alone is often insufficient for evaluating the quality of a data analysis and for determining why certain outputs differ from what was expected. Is there a better way to represent a data analysis that helps to resolve some of these questions?",
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2022-07-29",
    "categories": [],
    "contents": "\nI’ve spent the better part of my career advocating for the increased\npublication of code that is used in data analysis. This effort to make\ndata analysis more reproducible is largely focused around principles of\ntransparency and openness, the idea being that scientific investigation\ncannot be fully evaluated unless you knew what was done.\nOver the past 20 years, the principles of transparency and openness\nhave not changed. But I have changed in the sense that I’ve found myself\nwanting more. While transparency has inherent value, I have found that\nit’s not exactly what people want when they see a data\nanalysis. What they want is an answer to the question, “Is this data\nanalysis any good?”\nLooking at code usually does not tell me anything about the quality\nof the analysis because in the best case scenario, the code matches what\nwas written in the description of the analysis, which is what I would\nexpect. In my experience, most people get value out of the code (and the\ndata) when they can go into the code, make modifications, and run\nalternate analyses on the data. They may be interested in the\nsensitivity to a certain parameter or certain assumption. These things\ncannot be evaluated without doing a new analysis that differs\nfrom the original published analysis.\nThe need for people to actively modify code and re-run analyses in\norder to assess the quality of an analysis implies that a\nstatic analysis of the code itself is not necessarily\nsufficient for this purpose. In other words, we cannot in general simply\nlook at the unmodified published code for an analysis and determine\nwhether it is a good analysis or not.\nThe fundamental problem with code as a representation for data\nanalysis is that code only shows what were the observed\nresults. But when evaluating the quality of a data analysis, I\nthink it is equally important to know what were the expected\nresults and why the observed results differ from the expected\nresults (if they do). Information about the expected results is\nusually not directly embedded in the code. Sometimes this information is\npresented in a written summary (i.e. “these results were surprising\nbecause…”) but sometimes not. Figuring out why observed results differ\nfrom expectations can usually be assessed by modifying the original code\n(if one is so skilled) and re-running the analysis to see alternate\nresults. Essentially, this is the process of “debugging” the data\nanalysis.\nAnalysis Conundrum\nWith software, for example, it is possible to say “This software was\ndesigned to produce output X.” Then, if the software produces output X,\nthat is as-expected. If the software produces output Y, we know because\nof the design specification that this is unexpected and we can engage in\nthe debugging process to determine the cause. Other engineers may\ndisagree with the design and believe that producing output X is a bad\nidea, but given the design specification, everyone can agree that output\nY is incorrect.\nWith a data analysis, it is possible for an analyst to say “I expect\nthe output of this data analysis to be in the set X.” However, because\nthe thing we are ultimately investigating is nature itself, it’s not\nlike we can design nature to produce a specific output. Therefore,\nanother analyst could say, “Well, I expect the output of this analysis\nto be in the set Y.”\nNow, no matter what the output of this data analysis, there will be\nsome disagreement. Output X will be as-expected to the first analyst and\nunexpected for the second analyst. Output Y would be unexpected for the\nfirst analyst and as-expected for the second. If the analysis produces\nsome other output in the set Z, then both analysts will be surprised.\nTherefore, no matter the output of the analysis, one or both of the\nanalysts will demand an explanation.\nThe question here is how does looking at the code resolve any of\nthis? Even in the best of circumstances, there isn’t really a debate\nabout the code itself because both analysts are observing the same\nanalysis with the same data and are in total agreement about what\nwas done. The disagreement stems from differences in the analysts’\nexpectations and the uncertainty about what caused the observed results\nto deviate from their expectations. Simply looking at the analytic code\ndoes not necessarily resolve this uncertainty.\nThere can be many reasons why an observed result differs from\nexpectations, but I tend to group them at a high level into three\ncategories:\nScience - While it can be difficult to admit, sometimes we are just\nwrong about the science and therefore our original expectations are\nmisplaced. Perhaps we misinterpreted the literature or indexed too\nheavily on our own personal experience or something else.\nData - The data collection/generation process may be misunderstood\nor mischaracterized, leading to unexpected results at the end of the\nanalysis. Distributions of the data may be different from what was\nexpected, or perhaps there were missing data where we didn’t expect\nthere to be. There are many things that can fall in this category and\nexperienced analyst tend to have a feel for what is possible and what is\nmost likely.\nAnalysis - Data analyses these days tend to have relatively complex\ncode bases and long pipelines that process the data. It’s always\npossible that there is a problem somewhere in this code that will need\nto be debugged. Often these issues can be caught before publication, but\nnot always.\nA challenge, of course, is that the data analyst does not immediately\nknow which category is to blame when an observed result differs from\nexpectations. Therefore, the analyst has to consider all three sources\nof possible problems to narrow down the answer. This is not an easy\ntask!\nA Better Way?\nIs it possible to develop a representation for a data analysis that\nallows for some amount of “static analysis” (i.e. not having to re-run\nthe analysis) that can explain why a result differs from one’s\nexpectations? To be honest, I don’t know, but here are some thoughts in\nthat direction.\nEmbedding expectations about observed results - this is something\nI’ve already mentioned and is useful to allow others to interpret your\ninterpretation of the results. Many people do this already (perhaps\nindirectly), but not always. Sometimes a literature review serves this\nrole. But it would be good to do it more explicitly so that we have a\nconcrete thing to compare the results to. That said, everyone has\ndifferent expectations so it’s not really that important that people\nknow yours specifically. But it is a useful starting point. (Also note\nthat this is not so much about pre-registration because pre-registration\ntends to be more about hypotheses and analytic plans, not so much about\nobserved outputs and results.)\nDescribing the unexpected - This is obviously the other side of the\ncoin of specifying expectations, but is equally important. Describing\nthe set of unexpected outcomes is part of the design of the\nanalysis and is something that could be debated.\nSpecifying operating conditions - Many methods have assumptions\nunder which they admit certain properties. These assumptions, however,\nhave to be translated into specific operating conditions that\nare functions of the observed data. For example, if we assume the data\nare Normally distributed, we have to translate that assumption into some\nfunction of the data (e.g. a histogram or Q-Q plot). Although it is\ngenerally considered good practice to “check the assumptions”, not\neveryone does it and even if they do, the checking process is usually\nnot well-specified. Specifying these operating conditions is valuable\nbecause it may be possible to trace an unexpected result to a violation\nof one of these conditions.\nFailure modes - there are some conditions that are simply not\ncheckable with the observed data but can nevertheless cause an\nunexpected outcome. These failure modes cannot always be\nprevented in the context of the data analysis, but can at least be\ndescribed or perhaps mitigated against. For example, an unobserved\nconfounder may be driving the results or lack of independence between\nobservations may be causing large variability than expected.\nThe increasing publication of computer code for data analyses is a\nwelcome development for the sake of transparency and openness. However,\nthe code alone often does not address some key questions that others may\nhave about why an analysis produced results that were perhaps\ndifferent from what was expected. I think it would be useful to consider\nthe possible alternatives or additions to representing data analyses via\ncode that would give us the ability to evaluate a data analysis without\nhaving to re-run it.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-29T20:23:05-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-13-narrative-failure-in-data-analysis/",
    "title": "Narrative Failure in Data Analysis",
    "description": "A data analysis can fail if it doesn't present a coherent story and \"close all the doors\". Such a failure is not simply a problem with communication, but often indicates a problem with the details of the analysis itself.",
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2022-01-13",
    "categories": [],
    "contents": "\nIn data analysis there is often a distinction between doing the data analysis and communicating the data analysis. The idea is to analyze the data and then come up with some sort of narrative that explains what is going on with the data. There is a sense in which these are distinct steps of a process.\nHowever, considering the narrative or story of a data analysis may contribute to improving the quality of a data analysis and so we should re-consider whether the idea of story and the idea of analysis are unrelated.\nWhat were are not talking about here is fitting the data analysis to tell a specific story. The danger that most people fear is that we come up with a story first and the pick and choose the elements of a data analysis that are aligned with that story (and remove the elements that contradict the story).\nThe kind of story we are talking about here involves the explanation for the sequence of steps that are taken as part of the analysis. If one were to examine the steps taken in a data analysis, one could come up with a kind of “story” or narrative that could be told about those steps.\nFor example, a short data analysis might read as:\n\nWe read the data in, then we made a histogram of x, and then we ran a linear regression of y on x.\n\nThat is the “story” of the analysis. Any real data analysis will likely have a more complex story but the idea of a story or narrative remains. It’s valuable to think about the narrative you are building as you are analyzing the data. It can serve as a way to organize your steps and can guide you to planning future steps.\nLike with any story, the story or narrative of the data analysis has to make sense. The problem with the story I just told above is that it makes no sense. Why did we make a histogram? Why not a boxplot or a scatterplot? Why did we run a regression? Why not use an approach that is more flexible? These questions are not about the story of how the results are interpreted, they are about the story of the analysis itself.\nClosing All the Doors\nIn the documentary Showrunners: The Art of Running a TV Show, Bill Prady, creator of the long-running comedy TV show The Big Bang Theory talks about the challenges of writing comedy versus writing drama. He claims writing comedy is harder than drama because in comedy you’re often moving people into ridiculous situations. But simply putting someone in a ridiculous situation isn’t inherently funny. Rather, you have to write the story so that the character must be put into that situation. Prady says:\n\nOur slang in the [writer’s] room is you have to close all the other doors, so that the only door available to this character is the door that leads to the big block comedy scene you want to do…. You have to get to the point where the audience would say, “You know what, if I were in that situation, damn it if I wouldn’t do the exact same thing!”\n\nSuppose you’re watching a movie about a bank robbery at night and the robbers are having all kinds of problems because it’s dark. Maybe that’s funny, but not if you’re thinking to yourself “Why didn’t they just bring some flashlights?” If you find yourself asking that question, then this is a narrative failure. The writers of the story didn’t close all of the doors. They didn’t come up with a reason to explain why the robbers didn’t have flashlights to begin with.\nNarrative failures in a movie or TV show represent gaps in logic that the viewer has to either impute or just be confused about. In either case, it’s not something you want the viewer to do. You want the story to just make sense and for each sequence in the story to follow from previous events.\nThe same is true in data analysis. If you’ve ever seen a presentation of a data analysis and thought, “Why didn’t they just…” then the presenter failed to close a door. For example, how many fancy machine learning presentations have you seen where you thought, “Why didn’t they just run a logistic regression?”\nTypes of Data Analysis Failure\nThere is no perfect door.A narrative failure in a data analysis is a failure to justify the path chosen in the analysis vis a vis any number of plausible alternative paths that could have been taken. If those alternatives have not been ruled out, then a gap remains and the audience will be confused about why the alternative was not tried. It is the job of the analyst to either execute some of these alternate analyses or present some evidence regarding why they are not preferable. Ruling out these alternative analyses is part of building a successful data analysis narrative.\nThe idea of narrative failure gets to a difficult aspect of data analysis that students have trouble understanding, which is that rarely is there a single “right” or “best” way to do an analysis. In other words, rarely is there a single “door” to walk through. Often, there are many doors to walk through, and it almost doesn’t matter which door you choose, just so long as you manage to close all of the other ones along the way. If you choose door 1 then people might ask “Why didn’t you choose door 2?”. And if you choose door 2 then people will ask about door 1. There’s no perfect door to walk through, you just have to close one and walk through the other.\n\n\n\n",
    "preview": "posts/2022-01-13-narrative-failure-in-data-analysis/images/doors.png",
    "last_modified": "2022-01-13T14:06:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-10-thinking-about-failure-in-data-analysis/",
    "title": "Thinking About Failure in Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2021-11-10",
    "categories": [],
    "contents": "\nWhat does it mean for a data analysis to fail? I’ve come to feel that this is an important question because considering the ways that data analyses can fail is a good way to prevent an analysis from actually failing. However, thinking about the ways an analysis can fail is easier said than done. It requires an active imagination, an ability to think about what might happen or what might have happened. It requires thinking about what was not observed rather than what was observed.\nOne thing to clarify here is that when I think of failure, I think of something that is more or less immediately observable. So once you’ve done the analysis, you know whether you’ve failed or not. This rules out a number of possibilities that I think people are used to thinking about. For example, it rules out the question of whether the claims of an analysis are true or not. Often, it is not possible to determine the truth of an analysis until much later. It also rules out considerations of replication or reproducibility because both of those require future analyses to be done. Therefore, I’m not going to consider these ideas for now and focus on what we as analysts can do in the moment.\nTypes of Failure\nAn analysis failure, broadly speaking, is any outcome that does not meet our expectations. This might sound like a bit of an anti-climactic definition of “failure” but ultimately such an outcome represents a failure of our understanding about something: the data (and data generation process), the methods, or the science. As a result, a success is any outcome that is as-expected. That also seems like a bit of low bar for success, but we can make other distinctions later regarding whether a success is good or not.\nMy taxonomy of failure falls into two large bins:\nVerification failure: This is a narrow class a failure when an analysis produces an outcome that is counter to our expectation in any way. If we are expecting a correlation coefficient to be between 0.4 and 0.6 and we compute a correlation coefficient that is actually 0.1, then that is a verification failure. This failure indicates that there is something we don’t quite understand about the underlying correlation or the data.\nValidation failure: This is a broader class of failure that includes considerations that are outside the data. Validation failure is typically a result of a poor design process for the analysis that results in flawed requirements. For example, if we are interested in the association between predictor X and outcome Y adjusted for Z, but we build a prediction model for Y that omits Z, then this is a validation failure. The prediction model may operate as-expected and do a good job of predicting Y, but it doesn’t answer the question of whether X and Y are associated adjusted for Z.\nI find a simple non-data related example can be helpful. A person can ask an architect to design a house with no roof. If the architect builds the house, then the finished house meets expectations from a verification standpoint. It is a verification success. However, when it rains and the inside of the house gets wet, that is a validation failure.\nThis failure can be traced back to the design process and the setting of the requirements for the project. The actual building of the house went without a hitch. So how could the failure be prevented?\nOne possibility is the architect could have pushed back and argued that a house with no roof is a bad idea and is therefore a bad requirement (and probably against local code). The architect might have even refused to build the house. Or the architect could simply ask why the person doesn’t want a roof. If it turned out that the person simply wanted a lot of light in the house, then a different requirement could be specified, like a glass roof, that might satisfy everyone’s needs.\nIn general, I think the things we teach about statistics and data analysis in the classroom (textbook knowledge, so to speak) are focused around preventing verification failure. Given a set of requirements, we’d like people to know how to build the widget with the right tools and to be knowledgeable about the assumptions they are making. In my experience, the things that are taught “outside the classroom” are focused on preventing validation failure. Here, we want people to know whether they are building something useful and not simply building something that is “correct”.\nPotential Outcomes for Analysis\nStatisticians like to talk about data analysis failure because it’s a little like Monday morning quarterbacking. It’s often easy to recognize a failure after the analysis is done. But what about before the analysis is done?\nCatching a failure before it happens requires an understanding of the potential outcomes of an analysis. Most data analysis problems will admit a range of possible analysis plans and it is up to the analyst to choose one. Given an analysis plan, there is then a set of potential outcomes that this plan can produce before it is applied to the data. Once we apply the plan to the data, we will observe one of these potential outcomes.\nFor example, if our analysis is to take the arithmetic mean of a set of numbers, the set of potential outcomes is some subset of the real line. If we are making a scatter plot of two continuous variables, then our set of potential outcomes is the set of all possible scatter plots of two variables. Clearly, the second example with the scatter plot is harder to characterize than the first. But most analysts will intuitively understand what this set looks like.\nOnce we can characterize the set of potential outcomes, we can divide that set into two broad regions: expected outcomes and unexpected outcomes. When we apply our analysis plan to the data, the outcome will fall in one of these two regions. Outcomes that fall into the “unexpected” region are what I am characterizing as failures here.\nContinuing our two examples from above, for the mean we might expect that the outcome will fall into the interval [3, 7]. If the observed mean ended up being 10, that would be unexpected. If it were 4 then that would be as-expected. For the scatter plot, if we believed the two variables were positively correlated, then we might expect the scatter plot to look like a nice cluster of dots shaped like a “football” (American, that is) leaning at roughly a 45 degree angle. If the actual scatter plot looked like a circle (i.e. no correlation) or maybe a circle with a large outlier to the right, that would be unexpected.\nData analysis potential outcome set.Going From Success to Failure\nThe traditional notions of success and failure would seem to suggest that we should favor success over failure. But in the data analysis context, what we need to consider is how an analysis can go from success to failure and vice versa. If an analysis outcome is a success and is as-expected, it is important to ask “How could this have failed?” If an analysis outcome is a failure and is unexpected, it is important to ask “How could this have succeeded?”\nThere is therefore a common task when it comes to the observed output of a data analysis, regardless of whether it could be considered a success or a failure. That task is to consider the entire potential outcome set (given the chosen plan) and determine what could cause one to observe a different outcome than what was actually observed.\nIn the case of failure, this scenario is a bit easier to understand. When one observes an unexpected outcome, usually one is highly incentivized to get to the bottom of what caused that to occur. It might have been an error in the dataset, or a problem in the data wrangling, or a misunderstanding of how the methods or software work. Finally, there might have been a misunderstanding in our expectation (i.e. in the underlying science) and that perhaps this outcome should have been expected.\nIn the case of success, it is critical that we apply essentially the same thinking, especially in the early stages of analysis. We should be getting to the bottom of what caused this (success) to occur. In this case, it is still possible to ask whether their might have been an error in the dataset, or a problem in the wrangling, or a misunderstanding of the methods, software or underlying science. In the case of success, it is sometimes valuable to ask what would happen if we induced some sort of problem, like an error in the dataset, or an outlier, or a mis-applied statistical model (sometimes this is called sensitivity analysis).\nIn both success and failure it is valuable to consider the unobserved outcomes in the potential outcome set and ask whether we actually should be observing one of those other outcomes, perhaps because there exists a better model or because the data should be processed in a different way. It is this consideration of the potential outcomes of an analysis, as well as the alternating between success and failure, that drives the iterative nature of data analysis. Ultimately, we want to come to a place where we feel we understand the data, the data generation process, and the analytic process that leads to our result.\nGeneralizing From Failure\nWhen I hear people (including myself) say that data analysis is learned through experience, I realize now that what we mean is that experience is what allows one to build up that active imagination of what could happen. Producing the observed data analytic outcome requires skills-—fitting statistical models, data wrangling, visualization—-that can largely be taught in the classroom. But building the set of potential outcomes becomes easier and faster with experience as we observe more outcomes in other data analyses.\nThe way that we generalize our experience across different data analyses is to enrich and expand our set of potential outcomes for use in future analyses. What was once unexpected now becomes somewhat as-expected. And because such outcomes are expected, we know to watch out for them and we learn the techniques for checking on them.\nBut there is another way that we can “learn from experience” that has the potential to take a lot less time——collaborating with other people. Other people may have more experience or they may have different experience. Combining people with different experiences on the same analysis can produce a similar effect to a single person having more experience. Each person has seen different outcomes in their past and together they can produce a potential outcome set that is much larger than they could produce on their own. In this way, people can gain “experience” by working together. And the more diverse the experiences of the individual collaborators, the richer and larger the potential outcome set will be that they can construct and imagine.\n\n\n\n",
    "preview": "posts/2021-11-10-thinking-about-failure-in-data-analysis/images/potentialoutcomeset.png",
    "last_modified": "2021-11-12T15:06:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-31-streamline-tidy-data-as-a-service/",
    "title": "Streamline - tidy data as a service",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2021-03-31",
    "categories": [],
    "contents": "\nTldr: We started a company called Streamline Data Science https://streamlinedatascience.io/ that offers tidy data as a service. We are looking for customers, partnerships and employees as we scale up after closing our funding round!\nMost of my career, I have worked in the muck of data cleaning. In the world of genomics, a lot of my research has focused on batch effects, synthesizing big genomic data into usable formats, and generally making data easier to use. A couple of years ago, we also started a company called Problem Forward Data Science. Problem Forward offered fractional data science services to a variety of businesses around the country, from the very big corporations to startups just getting started. We were asked to do a lot of different types of data work, everything from turning spreadsheets into dashboards to building complicated forecasting models. But no matter the project, whether in government, academia, or industry, we always ended up with the same problem.\n\nWe needed to clean the data before we could do the data science.\n\nThis will be no surprise to anyone who has worked in data science or analytics, but the data almost always led to setbacks and frustration when we were working with our clients. Customers wanted complex AI, insightful dashboards, or easy reports, but the data just weren’t ready for that yet. And we wasted a huge amount of time cleaning the data over and over again.\nWe realized that the most common challenge companies have is that their data processing and management pipelines aren’t ready for analytics. Or as Google so eloquently puts it:\n\n“Everyone wants to do the model work, not the data work”\n\nWe realized that this was a service that many businesses needed. They needed someone who could come in and set up a data processing pipeline for them, manage it, and make sure the data were up to date. Some people call this Extract Load Transform (ELT), but we found it goes a bit beyond that. It is figuring out what format is most useful for the people who rely on data and working backward to create a customized and unique data pipeline that gets the data ready to use.\nThe ELT pipeline we set up is designed to consistently output “tidy data” that makes it easy for our customers to use BI tools like Tableau or Looker and to ingest their data without having to do all the ugly data work that is painful and time-consuming.\nWe piloted this for one of our startup customers - we built their data pipeline and provided ongoing management, maintenance, and upkeep. When they hired their first data scientist, they were able to quickly create dashboards for their whole business because they already had easy-to-use, tidy data.\nWe got so excited about this data dry cleaning idea that we started a new company called Streamline Data Science that solely focuses on tidy data as a service. We just closed our seed round and are now working with our first set of customers to set up their data pipelines. The cool thing is we found that our most excited customers were the ones that already had a data scientist on the team. This seems a little counter-intuitive until you realize that we handle the painful/boring bits of data management so they can focus on the fun part.\nThe interesting thing about Streamline is that it isn’t a product. There are a ton of complicated tools out there that you can use to set up your own data pipeline. Streamline is a service that handles all your data issues for you so the data “just works”. It can often be a lot cheaper than building out a full stack data engineering team in house.\nIf you are a company that is worried about the state of your data - they are difficult to share, to manage, and to quality control - we’d love to hear from you! We would also love to hear from you if you are a data scientist or analyst at a company that is frustrated about how much time you are spending on data management and cleaning.\nI’ll write more in the future about how we figured out setting up a data pipeline efficiently and the problems Streamline solves. We will also be releasing our first public data Streamlines that you can play with. In the meantime, I wanted to share how excited I am to finally be working on solving the first mile of data science and building a company that can help Baltimore grow its data science community.\n\n\n\n",
    "preview": "posts/2021-03-31-streamline-tidy-data-as-a-service/images/streamline.png",
    "last_modified": "2021-11-12T15:17:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-24-the-four-jobs-of-the-data-scientist/",
    "title": "The Four Jobs of the Data Scientist",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2020-11-24",
    "categories": [],
    "contents": "\nIn 2019 I wrote a post about The Tentpoles of Data Science that tried to distill the key skills of the data scientist. In the post I wrote:\n\nWhen I ask myself the question “What is data science?” I tend to think of the following five components. Data science is (1) the application of design thinking to data problems; (2) the creation and management of workflows for transforming and processing data; (3) the negotiation of human relationships to identify context, allocate resources, and characterize audiences for data analysis products; (4) the application of statistical methods to quantify evidence; and (5) the transformation of data analytic information into coherent narratives and stories.\n\n\nMy contention is that if you are a good data scientist, then you are good at all five of the tentpoles of data science. Conversely, if you are good at all five tentpoles, then you’ll likely be a good data scientist.\n\nI still feel the same way about these skills but my feeling now is that actually that post made the job of the data scientist seem easier than it is. This is because it wrapped all of these skills into a single job when in reality data science requires being good at four jobs. In order to explain what I mean by this, we have to step back and ask a much more fundamental question.\nWhat is the Core of Data Science?\nThis is a question that everyone is asking and I think struggling to answer. With any field there’s always a distinction between the questions of\nWhat is the core of the field?\nWhat do people in that field do on a regular basis?\nIn case it’s not clear, these are not the same question. For example, in Statistics, based on the curriculum from most PhD program, the core of the field involves statistical methods, statistical theory, probability, and maybe some computing. Data analysis is generally not formally taught (i.e. in the classroom), but rather picked up as part of a thesis or research project. Many classes labeled “Data Science” or “Data Analysis” simply teach more methods like machine learning, clustering, or dimension reduction. Formal software engineering techniques are also not generally taught, but in practice are often used.\nOne could argue that data analysis and software engineering is something that statisticians do but it’s not the core of the field. Whether that is correct or incorrect is not my point. I’m only saying that a distinction has to be made somewhere. Statisticians will always do more than what would be considered the core of the field.\nWith data science, I think we are collectively taking inventory of what data scientists tend to do. The problem is that at the moment it seems to be all over the map. Traditional statistics does tend to be central to the activity, but so does computer science, software engineering, cognitive science, ethics, communication, etc. This is hardly a definition of the core of a field but rather an enumeration of activities.\nThe question then is can we define something that all data scientists do? If we had to teach something to all data science students without knowing where they might end up afterwards, what would it be? My opinion is that at some point, all data scientists have to engage in the basic data analytic iteration.\nData Analytic Iteration\nThe basic data analytic iteration comes in four parts. Once a question has been established and a plan for obtaining or collecting data is available, we can do the following:\nConstruct a set of expected outcomes\nDesign/Build/Apply a data analytic system to the data\nDiagnose any anomalies in the analytic system output\nMake a decision about what to do next\nWhile this iteration might be familiar or obvious to many, its familiarity masks the complexity involved. In particular, each step of the iteration requires that the data scientist play a different role involving very different skills. It’s like a one-person play where the data scientist has to change costumes when going from one step to the next. This is what I refer to as the the four jobs of the data scientist.\nThe Four Jobs\nEach of the steps in the basic data analytic iteration requires the data scientist to be four different people: (1) Scientist; (2) Statistician; (3) Systems Engineer; and (4) Politician. Let’s take a look at each job in greater detail.\nScientist\nThe scientist develops and asks the question and is responsible for knowing the state of the science and what the key gaps are. The scientist also designs any experiments for collecting new data and executes the data collection. The scientist must work with the statistician to design a system for analyzing the data and ultimately construct a set of expected outcomes from any analysis of the data being collected.\nThe scientist plays a key role in developing the system that results in our set of expected outcomes. Components of this system might include a literature review, meta-analysis, preliminary data, or anecdotal data from colleagues. I use the term “Scientist” broadly here. In other settings this could be a policy-maker or product manager.\nStatistician\nThe statistician, in concert with the scientist, designs the analytic system that will analyze the data generated by any data collection efforts. They specify how the system will operate, what outputs it will generate, and obtain any resources needed to implement the system. The statistician draws on statistical theory and personal experience to choose the different components of the analytic system that will be applied.\nThe statistician’s role here is to apply the data analytic system to the data and to produce the data analytic output. This output could be a regression coefficient, a mean, a plot, or a prediction. But there must be something produced that we can compare to our set of expected outcomes. If the output deviates from our set of expected outcomes, then the next task is to identify the reasons for that deviation.\nSystems Engineer\nOnce the analytic system is applied to the data there are only two possible outcomes:\nThe outputs meet our expectations, or\nThe output does not meet our expectations (an anomaly).\nIn the case of an anomaly, the systems engineer’s responsibility is to diagnose the potential root causes of the anomaly, based on knowledge of the data collection process, the analytic system, and the state of scientific knowledge.\nRecently, Emma Vitz wrote on Twitter:\n\nHow do you teach debugging to people who are more junior? I feel like it’s such an important skill and yet we seem to have no structured framework for teaching it\n\nFor software and for data analysis alike, the challenge is that bugs or unexpected behavior can originate from anywhere. Any complex system is composed of multiple components, some of which may be your responsibility and many of which are someone else’s. But bugs and anomalies do not respect those boundaries! There may be an issue that occurs in one component that only becomes known when you see the data analytic output.\nSo if you are responsible for diagnosing a problem, it is your responsibility to investigate the behavior of each component of the system. If it is something you are not that familiar with, then you need to become familiar with it, either by learning on your own or (more likely) talking to the person who is in fact responsible.\nA common source of unexpected behavior in data analytic output is the data collection process, but the statistician who analyzes the data may not be responsible for that aspect of the project. Nevertheless, the systems engineer who identifies an anomaly has to go back through and talk to the statistician and the scientist to figure out exactly how each component works.\nUltimately, the systems engineer is tasked with taking a broad view of all the activities that affect the output from a data analysis in order to identify any deviations from what we would expect. Once those root causes have been explained, we can then move on to decide how we should act on this new information.\nPolitician\nThe politician’s job is to make decisions while balancing the needs of the various constituents to achieve a reasonable outcome. Most statisticians and scientist that I know would recoil at the idea of being considered a politician or that politics in any form would play a role in doing any sort of science. However, my thinking here is a bit more basic: In any data analysis iteration, we are constantly making decisions about what to do, keeping in mind a variety of conflicting factors. In order to resolve these conflicts and come to a reasonable agreement, one has to engage a key skill, which is negotiation.\nAt various stages of the data analytic iteration the politician must negotiate about (1) the definition of success in the analysis; (2) resources for executing the analysis; and (3) the decision for what to do after we have seen the output from the analytic system and have diagnosed the root causes of any anomalies. Decisions about what to do next fundamentally involve factors outside the data and the science.\nPoliticians have to identify who the stakeholders of the problem are and what is it that they ultimately want (as opposed to what their position is). For example, an investigator might say “We need a p-value less than 0.05”. That’s their position. But what they want is more likely “a publication in a high profile journal”. Another example might be an investigator who needs to meet a tight publication deadline while another investigator who wants to run a time-consuming (but more robust) analysis. Clearly, the positions conflict but arguably both investigators share the same goal, which is a rigorous high-impact publication.\nIdentifying positions versus underlying needs is a key task in negotiating a reasonable outcome for everyone involved. Rarely, in my experience, does this process have to do with the data (although data may be used to make certain arguments). The dominating elements of this process tend to be the nature of relationships between each constituent and the constraints on resources (such as time).\nApplying the Iteration\nIf you’re reading this and find yourself saying “I’m not an X” where X is either scientist, statistician, systems engineer, or politician, then chances are that is where you are weak at data science. I think a good data scientist has to have some skill in each of these domains in order to be able to complete the basic data analytic iteration.\nIn any given analysis, the iteration may be applied anywhere from once to dozens if not hundreds of times. If you’ve ever made two plots of the same dataset, you’ve likely done two iterations. While the exact details and frequency of the iterations may vary widely across applications, the core nature and the skills involved do not change much.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:34:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-26-palantir-shows-its-cards/",
    "title": "Palantir Shows Its Cards",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2020-08-26",
    "categories": [],
    "contents": "\nFile this under long-term followup, but just about four years ago I wrote about Palantir, the previously secretive but now soon to be public data science company, and how its valuation was a commentary on the value of data science more generally. Well, just recently Palantir filed to go public and therefore submitted a registration statement (S-1) describing its business. It’s a fascinating read, if you’re into that kind of stuff.\nBut the important thing is that Palantir itself summarized the question I asked more than 4 years ago. In their enumeration of risk factors, one risk factor they highlight is\n\nIf our customers are not able or willing to accept our product-based business model, instead of a labor-based business model, our business and results of operations could be negatively impacted. [emphasis added]\n\nIn my original post I wrote about the “Data Science Spectrum”, which has consulting on one end and software on the other.\nData Science SpectrumThe point of the diagram was that businesses on the right hand side have huge valuations while businesses on the left side have merely large valuations. The people running Palantir clearly understand this and are trying to push the company in a software-based productized direction.\nHere’s the rest of their summary of this risk factor:\n\nOur platforms are generally offered on a productized basis to minimize our customers’ overall cost of acquisition, maintenance, and deployment time of our platforms. Many of our customers and potential customers are instead generally familiar with the practice of purchasing or licensing software through labor contracts, where custom software is written for specific applications, the intellectual property in such software is often owned by the customer, and the software typically requires additional labor contracts for modifications, updates, and services during the life of that specific software. Customers may be unable or unwilling to accept our model of commercial software procurement. Should our customers be unable or unwilling to accept this model of commercial software procurement, our growth could be materially diminished, which could adversely impact our business, financial condition, results of operations, and growth prospects.\n\nThose of us who do data analysis for a living already know this to be true. Custom consulting is not scalable, and therefore, not as valuable as a piece of boxed up software, which is infinitely scalable.\nShow Me The Numbers\nSo, how is Palantir doing?\nAt first glance it seems their doing pretty well. Their gross profit (Revenue - Cost of Revenue) suggests about a 72% gross margin percentage for 2018 and 67% for 2019, which both seem high. These gross margin percentages are in software company territory. (For comparison, Facebook’s percentage runs around 80%.) This suggests that each dollar of Palantir’s revenue does not have a lot of direct costs associated with it.\nBut ulimately, Palantir has posted net losses every year, indicating there are significant indirect costs to generating that revenue. In particular, their Sales and marketing costs almost equal their entire gross profit. Reading through the S-1 this ultimately is not surprising. Palantir itself admits that\n\nOur sales efforts involve considerable time and expense and our sales cycle is often long and unpredictable.\n\nAlas, there is some consulting to do after all. My guess is that much of the up front “sales” work comes down to Palantir having to\nFigure out a customer’s problem and what question they’re asking;\nFigure out how a customer’s data are organized;\nFigure out how to their existing software products to the customer’s specific situation.\nThis should sound familiar to seasoned data scientists. Indeed, this is almost all the work of the data scientist. This is expensive because it requires humans to do it and there’s typically not much to generalize from customer to customer. Implementing the software and deploying it is work too, but is often more straightforward and their are often existing solutions that can be employed.\nThe Road to Profits\nSo here’s the problem: Palantir’s route to profitability involves making these costs go down…a lot. Maybe not to zero, but substantially, because each new customer—with their different problems and different data—costs a lot to acquire. If they can do this, they’ve cracked the nut of data science scalability.\nAnother big expense is Research and Development, which the company describes as developing new methods for data analysis (machine learning tools, etc.). While it’s nice to have room to do open-ended research on new data science tools, my guess is that this line item goes down a lot in the near future, as it often does at companies that start off with large R&D budgets. Ultimately, it would save Palantir ~$300 million a year.\nSee you in another four years?\n\n\n\n",
    "preview": "posts/2020-08-26-palantir-shows-its-cards/images/DS_Spectrum2.png",
    "last_modified": "2021-11-11T16:59:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-30-asymptotics-of-reproducibility/",
    "title": "Asymptotics of Reproducibility",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2020-04-30",
    "categories": [],
    "contents": "\nEvery once in a while, I see a tweet or post that asks whether one should use tool X or software Y in order to “make their data analysis reproducible”. I think this is a reasonable question because, in part, there are so many good tools out there! This is undeniably a good thing and quite a contrast to just 10 years ago when there were comparatively few choices.\nThe question of toolset though is not a question worth focusing on too much because it’s the wrong question to ask. Of course, you should choose a tool/software package that is reasonably usable by a large percentage of your audience. But the toolset you use will not determine whether your analysis is reproducible in the long-run.\nI think of the choice of toolset as kind of like asking “Should I use wood or concrete to build my house?” Regardless of what you choose, once the house is built, it will degrade over time without any deliberate maintenance. Just ask any homeowner! Sure, some materials will degrade slower than others, but the slope is definitely down.\nDiscussions about tooling around reproducibility often sound a lot like “What material should I use to build my house so that it never degrades*?” Such materials do not exist and similarly, toolsets do not exist to make your analysis permanently reproducible.\nI’ve been reading some of the old web sites from Jon Claerbout’s group at Stanford (thanks to the Internet Archive), the home of some of the original writings about reproducible research. At the time (early 90s), the work was distributed on CD-ROMs, which totally makes sense given that CDs could store lots of data, were relatively compact and durable, and could be mailed or given to other people without much concern about compatibility. The internet was not quite a thing yet, but it was clearly on the horizon.\nBut ask yourself this: If you held one of those CD-ROMs in your hand right now, would you consider that work reproducible? Technically, yes, but I don’t even have a CD-ROM reader in my house, so I couldn’t actually read the data. And a larger problem is that a CD from the 90s probably degraded to the point where it is likely unreadable anyway.\nClaerbout’s group obviously knew about the web and were transitioning in that direction, but such a transition costs money. As does keeping a keen eye on emerging trends and technology usage.\nHilary Parker and I recently discussed the how the economics of academic research are not well-suited to support the reproducibility of scientific results. The traditional model is that a research grant pays for the conduct of research over a 3-5 year period, after which the grant is finished and there is no more funding. During (or after) that time, scientific results are published. While the funding can be used to prepare materials (data, software, and code) to make the published findings reproducible at the instant of publication, there is no funding afterwards for dealing with two key tasks:\nEnsuring that the work continues to be reproducible given changes to the software and computing environment (maintenance)\nFielding questions or inquiries from others interested in reproducing the results or in building upon the published work (support)\nThese two activities (maintenance and support) can continue to be necessary in perpetuity for every study that an investigator publishes. The mismatch between how the grant funding system works and the requirements of reproducible research is depicted in the diagram below.\nAnalysis DepreciationWhen I say “value” in the drawing above, what I really mean is the “reproducibility value”. In the old model of publishing science, there was no reproducibility value because the work was generally not reproducible in the sense that data and code were available. Hence, this whole discussion would be moot.\nTraditional paper publications held their value because the text on the page did not generally degrade much over time and copies could easily be made. Scientists did have to field the occasional question about the results but it was not the same as maintaining access to software and datasets and answering technical questions therein. As a result, the traditional economic model for funding academic research really did match the manner in which research was conducted and then published. Once the results were published, the maintenance and support costs were nominal and did not really need to be paid for explicitly.\nFast forward to today and the economic model has not changed but the “business” of academic research has. Now, every publication has data and code/software attached to it which come with maintenance and support costs that can extend for a substantial period into the future. While any given publication may not require significant maintenance and support, the costs for an investigator’s publications in aggregate can add up very quickly. Even a single paper that turns out to be popular can take up a lot of time and energy.\nIf you play this movie to the end, it becomes soberingly clear that reproducible research, from an economic stand point, is not really sustainable. To see this, it might help to use an analogy from the business world. Most businesses have capital costs, where they buy large expensive things – machinery, buildings, etc. These things have a long life, but are thought to degrade over time (accountants call it depreciation). As a result, most businesses have “maintenance capital expenditure” costs that they report to show how much money they are investing every quarter to keep their equipment/buildings/etc. up to shape. In this context, the capital expenditure is worth it because every new building or machine that is purchased is designed to ultimately produce more revenue. As long as the revenue generated exceeds the cost of maintenance, the capital costs are worth it (not to oversimplify or anything!).\nIn academia, each new publications incurs some maintenance and support costs to ensure reproducibility (the “capital expenditure” here) but it’s unclear how much each new publication brings in more “revenue” to offset those costs. Sure, more publications allow one to expand the lab or get more grant funding or hire more students/postdocs, but I wouldn’t say that’s universally true. Some fields are just constrained by how much total funding there is and so the available funding cannot really be increased by “reaching more customers”. Given that the budgets for funding agencies (at least in the U.S.) have barely kept up with inflation and the number of publications increases every year, it seems the goal of making all research reproducible is simply not economically supportable.\nI think we have to concede that at any given moment in time, there will always be some fraction of published research for which there is no maintenance or support for reproducibility. Note that this doesn’t mean that people don’t publish their data and code (they should still do that!), it just means they don’t support or maintain it. The only question is which fraction should not be supported or maintained? Most likely, it will be older results where the investigators simply cannot keep up with maintenance and support. However, it might be worth coming up with a more systematic approach to determining which publications need to maintain their reproducibility and which don’t.\nFor example, it might be more important to maintain the reproducibility of results from huge studies that cannot be easily replicated independently. However, for a small study conducted a decade ago that has subsequently been replicated many times, we can probably let that one go. But this isn’t the only approach. We might want to preserve the reproducibility of studies that collect unique datasets that are difficult to re-collect. Or we might want to consider term-limits on reproducibility, so an investigator commits to maintaining and supporting the reproducibility of a finding for say, 5 years, after which either the maintenance and support is dropped or longer-term funding is obtained. This doesn’t necessarily mean that the data and code suddenly disappear from the world; it just means the investigator is no longer committed to supporting the effort.\nReproducibility of scientific research is of critical importance, perhaps now more than ever. However, we need to think harder about how we can support it in both the short- and long-term. Just assuming that the maintenance and support costs of reproducibility for every study are merely nominal is not realistic and simply leads to investigators not supporting reproducibility as a default.\n\n\n\n",
    "preview": "posts/2020-04-30-asymptotics-of-reproducibility/images/Drawings-1.jpg",
    "last_modified": "2021-11-11T17:00:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-29-amplifying-people-i-trust-on-covid-19/",
    "title": "Amplifying people I trust on COVID-19",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2020-04-29",
    "categories": [],
    "contents": "\nLike a lot of people, I’ve been glued to various media channels trying to learn about the latest with what is going on with COVID-19. I have also been frustrated - like a lot of people - with misinformation and the deluge of preprints and peer reviewed material. Some of this information is critically important and some is hard to trust.\nAs a biostatistician at a very visible school of public health I have also had a number of media outreaches, but I’ve been hesitant to do any interviews or talk about COVID-19. The reason is that even thought I have a PhD in Biostatistics and I work in a School of Public Health I actually know very little about infectious disease modeling and response. I think if you aren’t really deep in the field, its difficult to know the difference between someone like me and someone with real expertise.\nWhile I’m not an expert in the area, I know many of the real experts professionally or by reputation. So I thought I’d make a brief list of people and organizations I find credible and have been following for good information in case it is helpful to others. Many of these folks have already been found by audiences much bigger than ours but I just thought it would be useful to amplify further their work.\nPaper review\nJHU Novel Coronavirus Research Compendium - Hopkins experts rapidly reviewing preprints and peer reviewed literature to find the gems.\nInfectious disease modeling\nTrevor Bedford - Fred Hutchinson Cancer Research center expert in phylogenetic modeling of infectious disease, his viz work and sober analysis is one of my go-tos.\nJustin Lessler - infectious disease professor and epidemiologist at Hopkins who did some of the earliest studies of contact tracing in China.\nKate Grabowski - infectious disease professor and epidemiologist at Hopkins\nNicholas Reich - UMass expert in infectious disease modeling, doing a great job of aggregating and evaluating disease models.\nNatalie Dean - University of Florida expert statistician in vaccine clinical trials - also one of my favorite pragmatic reviewers of big papers.\nVaccine development\nDerek Lowe - drug discovery chemist and blogger who is one of the best out there at distilling progress on vaccines.\nScicom and public outreach\nEllie Murray - Boston University expert epidemiologist professor and communicator, providing clear understandable breakdowns of the best practices.\nLucy D’Agostino McGowan - Vanderbilt statistics professor and communicator who does an amazing job of breaking down difficult stats and causal inference issues.\nCarl Bergstrom - UW Biology Professor and infectious disease expert, providing sober reviews and interactions around many of the papers coming out.\nPolicy\nTom Ingelsby - Professor and director of Johns Hopkins Center for Health Security has been producing solid analysis and policy recommendations on when to re-open.\nCaitlin Rivers - Professor at the Hopkins Center for Health Security, outbreak specialist, also producing solid analysis and policy recommendations.\nAndy Slavitt - Ex-Obama health care head and providing solid policy reviews and ideas.\nJosh Sharfstein - Professor of the Practice at Johns Hopkins Bloomberg School of Public Health, has a great public health podcast with lots of experts on it.\nKeshia Pollack-Porter - Professor of Health Policy and Management at Johns Hopkins Bloomberg School of Public Health who has a great take on mobility issues associated with Covid-19.\nLisa Cooper - Bloomberg Professor at the Johns Hopkins Bloomberg School of Public Health who has great content on inequality of impact.\nI’m sure I’ve missed great people to mention as I’ve dashed this off pretty quickly so apologies if I missed you!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:34:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-04-is-artificial-intelligence-revolutionizing-environmental-health/",
    "title": "Is Artificial Intelligence Revolutionizing Environmental Health?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2019-12-04",
    "categories": [],
    "contents": "\nNOTE: This post was written by Kevin Elliott, Michigan State University; Nicole Kleinstreuer, National Institutes of Health; Patrick McMullen, ScitoVation; Gary Miller, Columbia University; Bhramar Mukherjee, University of Michigan; Roger D. Peng, Johns Hopkins University; Melissa Perry, The George Washington University; Reza Rasoulpour, Corteva Agriscience, and Elizabeth Boyle, National Academies of Sciences, Engineering, and Medicine. The full summary for the workshop on which this post is based can be obtained here.\nOn June 6 and 7, 2019, the National Academy of Sciences, Engineering, and Medicine (NASEM), hosted a workshop on the use of artificial intelligence (AI) in the field of Environmental Health. Rapid advances in machine learning are demonstrating the ability of machines to carry out repetitive “smart” tasks requiring discreet judgments. Machine learning algorithms are now being used to analyze large volumes of complex data to find patterns and make predictions, often exceeding the accuracy and efficiency of people attempting the same task. Driven by tremendous growth in data availability as well as computing power and accessibility, artificial intelligence and machine learning applications are rapidly growing in various sectors of society including retail, such as predicting consumer purchases; the automotive industry as demonstrated by self-driving cars, and in health care with advances in automated medical diagnoses.\nBuilding upon the major themes of the NASEM workshop, in this blog post we address the following questions:\nHow might AI advance environmental health?\nDoes AI change the standards used for conducting environmental health research?\nDoes the use of AI allow us to change our established research principles?\nHow does AI impact our training programs for the next generation of environmental health scientists?\nAre there barriers within the current academic incentive structures that are hindering the full potential of AI, and how might those barriers be overcome?\nHow might AI advance environmental health?\nEnvironmental health is the study of how the environment affects human health. Due to the complexity of both human biology and the multiplicity of environmental factors that we encounter daily, studying environmental impacts on human health presents many data challenges. Due to the data boom we have seen in recent years we now have a multitude of individualized data including genetic sequencing and wearable health and activity monitors. We have also seen exponential growth in the availability of data on individual environmental exposures. Wearable sensors and personal chemical samplers are allowing for more detailed exposure models, whereas advancements in exposure biomonitoring in a variety of matrices including blood and urine is giving more granular detail about actual chemical body burdens. We have also seen an increase in available population level data on dietary factors, the social and built environment, climate, and many other variables affected by environmental and genetic factors. Concurrently, while population data are booming, toxicology is creating a variety of experimental models to advance our understanding of how chemicals and environmental exposures may pose risks to human health. Large-scale high-throughput chemical safety screening efforts can now generate data on tens of thousands of chemicals in thousands of biological targets. Integrating these diverse data streams represents a new level of complexity.\nAI and machine learning provide many opportunities to make this complexity more manageable, such as highly accurate prediction methods to better assess exposures and flexible approaches to allow incorporation of exposure to complex mixtures in population health analyses. Incorporating artificial intelligence and machine learning methods in environmental health research offers the potential to transform how we analyze environmental exposures and our understanding of how these myriad factors influence our health and contribute to disease.\nDoes AI change the standards used for conducting environmental health research?\nWhile we think the use of AI and machine learning techniques clearly hold great promise for the advancement of environmental health research, we also believe such techniques introduce new challenges and magnify existing ones. While the major standards by which we conduct scientific research do not change, our ability to adhere to them will require some adaptation. Transparency and repeatability are key. We must ensure that the computational reproducibility and replicability of our scientific findings do not suffer at the hands of complex algorithms and poorly assembled data pipelines. Complex data analyses that incorporate more diverse data types from varied sources stretch our ability to track, curate, and validate these data without robust data curation tools. Although some data curation tools that establish standard approaches for creating, managing, and maintaining data are available, they are usually field-specific, and currently there are no incentives or strict requirements to ensure that investigators use them.\nMachine learning and artificial intelligence algorithms have demonstrated themselves to be very powerful. At the same time, we also recognize their complexity and general opacity can be cause for concern. While investigators may be willing to overlook the opacity of these algorithms when predictions are highly accurate and precise, all is well until it isn’t. When an algorithm does not work as expected, it is critical to know why it didn’t work. With transparency and reproducibility of utmost importance, machine learning algorithms must ensure that investigators and data analysts have accountability in their analyses and that regulators have confidence in applying AI generated results to inform public health decisions.\nDoes the use of AI allow us to change our established research principles?\nAI does not change established research principles such as sound study designs and understanding threats of bias. However, there is a need to create updated guidelines and implement best practices for choosing, cleaning, structuring, and sharing the data used in AI applications. Creating appropriate training datasets, engaging in ongoing processes of validation, and assessing the domain of applicability for the models that are generated are also important. As in all areas of science, it is crucial to clarify whether models solely provide accurate predictions or whether they also provide understanding of relevant mechanisms. The current Open Science movement’s emphasis on transparency is particularly relevant to the use of AI and machine learning. Users of these methods in environmental health should be looking for ways to be open about the model training data, to clarify validation methods, to create interpretable “models of the models” where possible, and to clarify their domains of applicability. Recent innovations like model cards, or short documents that go alongside machine learning models to share information that everyone impacted by the model should know, is one example of a way model developers can communicate their models’ strengths and weaknesses in a way that is accessible.\nHow does AI impact our training programs for the next generation of environmental health scientists?\nAs complex AI methods are increasingly applied to environmental health research, it is important to consider effective training of the workforce and its future leaders. Currently, training in the application of data science is unstandardized, as trainees learn how to apply methods to a specific research application through an apprenticeship type model, where a trainee works with a mentor. Classroom training standardizes theory and methods, but the mentor teaches the fine details of analyzing data in a specific research area, which introduces heterogeneity into the ways in which scientists analyze data. The lack of training standards leads to a worry that analysts may apply cutting-edge computational/algorithmic approaches to data analysis, without consideration of fundamental biostatistical and epidemiologic principles, such as statistical design, sampling, and inference. Fundamental questions taught in biostatistics and epidemiology courses, such as “Who is in my sample?” and “What is my target population of inference?” are even more relevant in our current era of algorithms and machine learning. Now analysts are agnostically querying databases not designed for population-based research such electronic health records, medical claims, Twitter, Facebook, and Google searches, for new discoveries in environmental health. It is important to recognize that a lack of proper consideration of issues related to sampling, selection bias, correlation of multiple exposures, exposure and outcome misclassification could lead to erroneous results and false conclusions. Training programs will need to evolve so that we do not just teach scientists and analysts how to program models and interpret their results, but also emphasize how to recognize human biases that can be inadvertently built into the data and model approaches, and the continuous need for rigor, responsibility, and reproducibility.\nAn increased focus on mathematical theory may also improve training in the application of AI to environmental health. A greater effort in developing standardized theory about how and why a specific research area analyses data in a certain way may help adapt approaches from one research area to another. In addition, deeper mathematical exploration of AI methods will help data scientists understand when and why AI methods work well, and when they don’t.\nAre there barriers within the current academic incentive structures that are hindering the full potential of AI, and how might those barriers be overcome?\nRigorous data science requires a team science approach to achieve a variety of functions such as developing algorithms, formalizing common data platforms and testing protocols, and properly maintaining and curating data sources. Over recent decades, we have witnessed how the power of team science has improved the understanding of critical health problems of our time such as in unlocking the human genome and achieving major advancements in cancer treatment. These advances have demonstrated the payoff of interdisciplinary, transdisciplinary, and multidisciplinary investigations. Despite these successes, there are still barriers to large team science projects, because these projects often have goals that do not sit precisely within a single funding agency. In order for AI to truly advance environmental health, federal agencies and institutions that fund environmental health research need to create pathways to support large multi-disciplinary and multi-institutional teams that are conducting this research. An example could be a multi-agency/multi-institute funding consortia. A ten-year investment in a well-coordinated initiative that harnesses AI data opportunities could accelerate new findings in not only the environmental causes of disease, but also in informing interventions that can prevent environmentally mediated disease and improve population health.\nFinal thoughts\nWe believe machine learning and AI methods have tremendous potential but we also believe they cannot be used in a way that overlooks limitations or relaxes data integrity standards. With these considerations in mind, we have tempered enthusiasm for the promises of these approaches. We have to make sure that environmental health scientists stay out in front of these considerations to avoid potential pitfalls such as the allure of hype or chasing after the next new thing because it is novel rather than truly meaningful. We can do this by fostering ongoing conversations about the challenges and opportunities AI provides for environmental health research. An intentional union of the two cultures of careful (and often overly cautious) stochastic and bold (and often overly optimistic) algorithmic modeling can help to ensuring we are not abandoning principles of proper study design when a new technology comes along, but explore how to use the new technology to better understand the myriad ways the environment affects health and disease.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:34:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-08-28-you-can-replicate-almost-any-plot-with-ggplot2/",
    "title": "You can replicate almost any plot with R",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2019-08-28",
    "categories": [],
    "contents": "\nAlthough R is great for quickly turning data into plots, it is not widely used for making publication ready figures. But, with enough tinkering you can make almost any plot in R. For examples check out the flowingdata blog or the Fundamentals of Data Visualization book.\nHere I show five charts from the lay press that I use as examples in my data science courses. In the past I would show the originals, but I decided to replicate them in R to make it possible to generate class notes with just R code (there was a lot of googling involved).\nBelow I show the original figures followed by R code and the version of the plot it produces. I used the ggplot2 package but you can achieve similar results using other packages or even just with R-base. Any recommendations on how to improve the code or links to other good examples are welcomed. Please at to the comments or @ me on twitter: @rafalab.\nExample 1\nThe first example is from this ABC news article. Here is the original:\n\nHere is the R code for my version. Note that I copied the values by hand.\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(ggflags)\nlibrary(countrycode)\n\ndat <- tibble(country = toupper(c(\"US\", \"Italy\", \"Canada\", \"UK\", \"Japan\", \"Germany\", \"France\", \"Russia\")),\n              count = c(3.2, 0.71, 0.5, 0.1, 0, 0.2, 0.1, 0),\n              label = c(as.character(c(3.2, 0.71, 0.5, 0.1, 0, 0.2, 0.1)), \"No Data\"),\n              code = c(\"us\", \"it\", \"ca\", \"gb\", \"jp\", \"de\", \"fr\", \"ru\"))\n\ndat %>% mutate(country = reorder(country, -count)) %>%\n  ggplot(aes(country, count, label = label)) +\n  geom_bar(stat = \"identity\", fill = \"darkred\") +\n  geom_text(nudge_y = 0.2, color = \"darkred\", size = 5) +\n  geom_flag(y = -.5, aes(country = code), size = 12) +\n  scale_y_continuous(breaks = c(0, 1, 2, 3, 4), limits = c(0,4)) +   \n  geom_text(aes(6.25, 3.8, label = \"Source UNODC Homicide Statistics\")) + \n  ggtitle(toupper(\"Homicide Per 100,000 in G-8 Countries\")) + \n  xlab(\"\") + \n  ylab(\"# of gun-related homicides\\nper 100,000 people\") +\n  ggthemes::theme_economist() +\n  theme(axis.text.x = element_text(size = 8, vjust = -16),\n        axis.ticks.x = element_blank(),\n        axis.line.x = element_blank(),\n        plot.margin = unit(c(1,1,1,1), \"cm\")) \n\n\n\nExample 2\nThe second example from everytown.org. Here is the original:\n\nHere is the R code for my version. As in the previous example I copied the values by hand.\n\n\ndat <- tibble(country = toupper(c(\"United States\", \"Canada\", \"Portugal\", \"Ireland\", \"Italy\", \"Belgium\", \"Finland\", \"France\", \"Netherlands\", \"Denmark\", \"Sweden\", \"Slovakia\", \"Austria\", \"New Zealand\", \"Australia\", \"Spain\", \"Czech Republic\", \"Hungry\", \"Germany\", \"United Kingdom\", \"Norway\", \"Japan\", \"Republic of Korea\")),\n              count = c(3.61, 0.5, 0.48, 0.35, 0.35, 0.33, 0.26, 0.20, 0.20, 0.20, 0.19, 0.19, 0.18, 0.16,\n                        0.16, 0.15, 0.12, 0.10, 0.06, 0.04, 0.04, 0.01, 0.01))\n\ndat %>% \n  mutate(country = reorder(country, count)) %>%\n  ggplot(aes(country, count, label = count)) +   \n  geom_bar(stat = \"identity\", fill = \"darkred\", width = 0.5) +\n  geom_text(nudge_y = 0.2,  size = 3) +\n  xlab(\"\") + ylab(\"\") + \n  ggtitle(toupper(\"Gun Murders per 100,000 residents\")) + \n  theme_minimal() +\n  theme(panel.grid.major =element_blank(), panel.grid.minor = element_blank(), \n        axis.text.x = element_blank(),\n        axis.ticks.length = unit(-0.4, \"cm\")) + \n  coord_flip() \n\n\n\nExample 3\nThe next example is from the Wall Street Journal. The original is interactive but here is a screenshot:\n\nHere is the R code for my version. Note I matched the colors by hand as the original does not seem to follow a standard palette.\n\n\nlibrary(dslabs)\ndata(us_contagious_diseases)\nthe_disease <- \"Measles\"\ndat <- us_contagious_diseases %>%\n  filter(!state%in%c(\"Hawaii\",\"Alaska\") & disease == the_disease) %>%\n  mutate(rate = count / population * 10000 * 52 / weeks_reporting) \n\njet.colors <- colorRampPalette(c(\"#F0FFFF\", \"cyan\", \"#007FFF\", \"yellow\", \"#FFBF00\", \"orange\", \"red\", \"#7F0000\"), bias = 2.25)\n\ndat %>% mutate(state = reorder(state, desc(state))) %>%\n  ggplot(aes(year, state, fill = rate)) +\n  geom_tile(color = \"white\", size = 0.35) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_fill_gradientn(colors = jet.colors(16), na.value = 'white') +\n  geom_vline(xintercept = 1963, col = \"black\") +\n  theme_minimal() + \n  theme(panel.grid = element_blank()) +\n        coord_cartesian(clip = 'off') +\n        ggtitle(the_disease) +\n        ylab(\"\") +\n        xlab(\"\") +  \n        theme(legend.position = \"bottom\", text = element_text(size = 8)) + \n        annotate(geom = \"text\", x = 1963, y = 50.5, label = \"Vaccine introduced\", size = 3, hjust = 0)\n\n\n\nExample 4\nThe next example is from the New York Times. Here is the original:\n\nHere is the R code for my version:\n\n\ndata(\"nyc_regents_scores\")\nnyc_regents_scores$total <- rowSums(nyc_regents_scores[,-1], na.rm=TRUE)\nnyc_regents_scores %>% \n  filter(!is.na(score)) %>%\n  ggplot(aes(score, total)) + \n  annotate(\"rect\", xmin = 65, xmax = 99, ymin = 0, ymax = 35000, alpha = .5) +\n  geom_bar(stat = \"identity\", color = \"black\", fill = \"#C4843C\") + \n  annotate(\"text\", x = 66, y = 28000, label = \"MINIMUM\\nREGENTS DIPLOMA\\nSCORE IS 65\", hjust = 0, size = 3) +\n  annotate(\"text\", x = 0, y = 12000, label = \"2010 Regents scores on\\nthe five most common tests\", hjust = 0, size = 3) +\n  scale_x_continuous(breaks = seq(5, 95, 5), limit = c(0,99)) + \n  scale_y_continuous(position = \"right\") +\n  ggtitle(\"Scraping By\") + \n  xlab(\"\") + ylab(\"Number of tests\") + \n  theme_minimal() + \n  theme(panel.grid.major.x = element_blank(), \n        panel.grid.minor.x = element_blank(),\n        axis.ticks.length = unit(-0.2, \"cm\"),\n        plot.title = element_text(face = \"bold\"))\n\n\n\nExample 5\nThis last one is from fivethirtyeight.\n\nBelow is the R code for my version. Note that in this example I am essentially just drawing as I don’t estimate the distributions myself. I simply estimated parameters “by eye” and used a bit of trial and error.\n\n\nmy_dgamma <- function(x, mean = 1, sd = 1){\n  shape = mean^2/sd^2\n  scale = sd^2 / mean\n  dgamma(x, shape = shape, scale = scale)\n}\n\nmy_qgamma <- function(mean = 1, sd = 1){\n  shape = mean^2/sd^2\n  scale = sd^2 / mean\n  qgamma(c(0.1,0.9), shape = shape, scale = scale)\n}\n\ntmp <- tibble(candidate = c(\"Clinton\", \"Trump\", \"Johnson\"), \n              avg = c(48.5, 44.9, 5.0), \n              avg_txt = c(\"48.5%\", \"44.9%\", \"5.0%\"), \n              sd = rep(2, 3), \n              m = my_dgamma(avg, avg, sd)) %>%\n  mutate(candidate = reorder(candidate, -avg))\n\nxx <- seq(0, 75, len = 300)\n\ntmp_2 <- map_df(1:3, function(i){\n  tibble(candidate = tmp$candidate[i],\n         avg = tmp$avg[i],\n         sd = tmp$sd[i],\n         x = xx,\n         y = my_dgamma(xx, tmp$avg[i], tmp$sd[i]))\n})\n\ntmp_3 <- map_df(1:3, function(i){\n  qq <- my_qgamma(tmp$avg[i], tmp$sd[i])\n  xx <- seq(qq[1], qq[2], len = 200)\n  tibble(candidate = tmp$candidate[i],\n         avg = tmp$avg[i],\n         sd = tmp$sd[i],\n         x = xx,\n         y = my_dgamma(xx, tmp$avg[i], tmp$sd[i]))\n})\n         \ntmp_2 %>% \n  ggplot(aes(x, ymax = y, ymin = 0)) +\n  geom_ribbon(fill = \"grey\") + \n  facet_grid(candidate~., switch = \"y\") +\n  scale_x_continuous(breaks = seq(0, 75, 25), position = \"top\",\n                     label = paste0(seq(0, 75, 25), \"%\")) +\n  geom_abline(intercept = 0, slope = 0) +\n  xlab(\"\") + ylab(\"\") + \n  theme_minimal() + \n  theme(panel.grid.major.y = element_blank(), \n        panel.grid.minor.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        strip.text.y = element_text(angle = 180, size = 11, vjust = 0.2)) + \n  geom_ribbon(data = tmp_3, mapping = aes(x = x, ymax = y, ymin = 0, fill = candidate), inherit.aes = FALSE, show.legend = FALSE) +\n  scale_fill_manual(values = c(\"#3cace4\", \"#fc5c34\", \"#fccc2c\")) +\n  geom_point(data = tmp, mapping = aes(x = avg, y = m), inherit.aes = FALSE) + \n  geom_text(data = tmp, mapping = aes(x = avg, y = m, label = avg_txt), inherit.aes = FALSE, hjust = 0, nudge_x = 1) \n\n\n\n\n\n\n",
    "preview": "posts/2019-08-28-you-can-replicate-almost-any-plot-with-ggplot2/index_files/figure-html5/murder-rate-example-1-1.png",
    "last_modified": "2022-11-11T12:11:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-08-27-so-you-want-to-start-a-podcast/",
    "title": "So You Want to Start a Podcast",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2019-08-27",
    "categories": [],
    "contents": "\nPodcasting has gotten quite a bit easier over the past 10 years, due in part to improvements to hardware and software. I wrote about both how I edit and record both of my podcasts about 2 years ago and, while not much has changed since then, I thought it might be helpful if I organized the information in a better way for people just starting out with a new podcast.\nOne frustrating problem that I find with podcasting is that the easy methods are indeed easy, and the difficult methods are indeed difficult, but the methods that are just above easy, which other markets might label as “prosumer” or something like that, are…kind of hard. One of the reasons is that once you start buying better hardware, everything kind of snowballs because the hardware becomes more modular. So instead of just using your phone headphones to record, you might buy a microphone, that connects to a stand, that connects to a USB interface using an XLR cable, that connects to your computer. Similarly, on the software side, there’s really not much out there that’s free. As a result of both phenomena, costs start to go up pretty quickly as soon as you step up just a little bit.\nI can’t do anything about costs, but I thought I could help a little bit on sorting out what’s out there and what’s genuinely valuable. There are two versions here: the free and easy plan if you’re just starting out and the next level up, which is basically what I use.\nThe three things I’ll cover here that you need for podcasting are:\nHardware - this includes all recording equipment like microphones, stands, cables, etc.\nRecording Software - Unless you live in a recording booth you’ll need some software for your computer (which I assume you have!)\nEditing Software - the more complicated your podcast gets the more you’ll need to edit (beyond just trimming the beginning and end of the audio files)\nHosting - Unless you plan on running your own server (which is an option but I don’t recommend it) you’ll need someone to host your audio files.\nFree and Easy\nThere are in fact ways to podcast for free and many people stay at this level for a long time because the quality is acceptable and cost is zero. If you want to just get started quickly here’s what you can do:\nHardware - just use the headphones/microphone that came with your mobile phone.\nRecording Software - If you are doing a podcast by yourself, you can just use whatever app your phone has to record things like voice memos. On your computer, there should be a built-in app that just lets you record sound through the headphones.\nEditing Software - For editing I recommend either not editing (simpler!) or using something like Audacity to just trim the beginning and the end.\nHosting - SoundCloud offers free hosting for up to 3 hours of content. This is plenty for just starting out and seeing if you like it, but you will likely use it up.\nIf you are working with a partner, it gets a little more complicated and there are some additional notes on the recording software. My go-to recommendation for recording with a partner is to use Zencastr. Zencastr has a free plan that lets you record high-quality audio for a max of 2 people. (If you need to record more than 2 people, you can’t use the free option.) The nice thing about Zencastr is that it uses WebRTC to record directly off your microphone, so you don’t need to worry too much about the quality of your internet connection. What you get is separate audio files, one for each speaker, that are synched together. Occasionally, there are some synching glitches, but usually it works out. The files are automatically uploaded to a Dropbox account, so you’ll need one of those. Because Zencastr automatically goes to MP3 format, the files are relatively small. Also, if you have a guest who is less familiar with audio hardware/software, you can just send them a link that they can click on and they’re recording.\nNote that even if your partner is sitting right next to you, it’s often simpler to just go to separate spaces and record “remotely”. The primary benefit of doing this is that you can cleanly record separate/independent audio tracks. This can be useful in the editing process.\nIf you prefer an all-in-one solution, there are services like Cast and Anchor that offer recording, hosting, and distribution. Cast only has a free 1-month trial and so you have to pay eventually. Anchor appears to be free (I’ve never used it), but it was recently purchased by Spotify so it’s not immediately clear to me if anything will change. My guess is they’ll likely stay free because they want as many people making podcasts as possible. Anchor didn’t exist when I started podcasting but if it had I might have used it first. But it always makes me a little nervous when I can’t figure out how a company makes money.\nTo summarize, here’s the “free and easy” workflow that I recommend:\nRecord your podcast using Zencastr (especially if you have a partner), which then puts audio files on Dropbox\nTrim beginning/ending of audio file with Audacity\nUpload audio to SoundCloud and add episode metadata\nAnd here are the pros and cons:\nPros\nIt’s free\nCons\nAudio quality is acceptable but not great. Earbud type microphones are not designed for high quality and you can usually tell when someone has used them to record. Given that podcasts are all about audio, it’s hard for me to trade off audio quality.\nHosting limitations mean you can only get a few episodes up. But that’s a problem for down the road, right?\nEditing is generally a third-order issue, but there is one scenario where it can be critical—when you have a bad internet connection. Bad internet connections can introduce delays and cross-talk. These problems can be mitigated when editing (I give an example here) but only with better software.\nBeyond Free\nBeyond the free workflow, there are a number of upgrades that you can make and you can easily start spending a lot of money. But the only real upgrade that I think you need to make is to buy a good microphone. Surprisingly, this does not need to cost much money. The best podcasting microphone for the money out there is the Audio Technica ATR2100 USB micrphone. This is the microphone that Elizabeth uses on the The Effort Report and Hilary uses on Not So Standard Deviations. As of this writing it’s $65 on Amazon, but I’ve seen it for as low as $40. The benefits of this microphone are:\nThe audio quality is high\nIt isolates vocal audio really well and doesn’t pick up a lot of background audio (good for noisy rooms like my office).\nIt connects directly to a computer via USB so you don’t need to buy a separate USB interface.\nIt’s cheap\nThe problem with getting “better” (i.e. more expensive) microphones as that they tend to be more sensitive, which means they pick up more high-frequency background noise. Professional microphones are designed for you to be working in a sound-proof recording studio environment in which you want to pick up as much sound as possible. But podcasting, in general, tends to take place wherever. So you want a microphone that will only pick up your voice right in front of it. Technically, you lose a little quality this way, but it’s equally annoying to have a lot of background noise.\nNow that you’ve got a microphone, you need to stick it somewhere. While you can always just hold the microphone, I’d recommend an adjustable stand of some sort. Desk stands like this one are nice because they’re adjustable but they do require you to have a semi-permanent office where you can just keep it. The main point here is that podcasting requires you to sit still and talk for a while, and you don’t want to be uncomfortable while you’re doing it.\nThe last upgrade you’ll likely need to make is the hosting provider. SoundCloud itself offers an unlimited plan but I don’t recommend it as it’s not really designed for podcasting. I use Libsyn, which has a $5 a month plan that should be enough for a monthly podcast. They also provide some decent analytics that you can download and read into R. What I like about Libsyn is that they do one job and they do it really well. I give them money, and they provide me a service in return. How simple is that?\nThat’s it for now. I’m happy to make more recommendations regarding software and hardware (feel free to tweet me @rdpeng), but I think what I’ve got here should get you 99% of the way there.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:33:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-07-23-the-data-deluge-means-no-reasonable-expectation-of-privacy-now-what/",
    "title": "The data deluge means no reasonable expectation of privacy - now what?",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2019-07-23",
    "categories": [],
    "contents": "\nToday a couple of different things reminded me about something that I suppose many people are talking about but has been on my mind as well.\nThe idea is that many of our societies social norms are based on the reasonable expectation of privacy. But the reasonable expectation of privacy is increasingly a thing of the past. Three types of data I’ve been thinking about are:\nObviously identifying data: Data like cellphone GPS traces and public social media posts are obviously information that is indentifiable and reduce privacy.\nData that can be inferred from public data: We can also now infer a lot about people given the data that is public. For example a couple of years ago I challenged the students in my advanced data science class to predict the Gail score - one of the most widely used measures of breast cancer risk - using only the information available from a person’s public Facebook profile. While not all of the information was available, a good fraction of it was. This is an example of something you might not think that posting pictures of your family, your birthday celebrations, and family life events could enable. I was reminded of this when hearing about this paper that claims to be able to deidentify up to 99.98% of Americans using only 15 pieces of demographic information.\nData other people share about us: The stories around the capture of the Golden Gate Killer using genealogy data make it clear that even when you personally don’t share your data, someone else may be sharing it for you. The same can be said of photos of you that were tagged on Facebook even if you aren’t on the platform.\nI don’t think these types of data are going to magically disappear. So like a lot of other people I’ve been wondering how we should individually and as a society adapt to the world where privacy is no longer an expectation.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:33:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-07-19-more-datasets-for-teaching-data-science-the-expanded-dslabs-package/",
    "title": "More datasets for teaching data science: The expanded dslabs package",
    "description": {},
    "author": [
      {
        "name": "Amy Gill",
        "url": {}
      }
    ],
    "date": "2019-07-19",
    "categories": [],
    "contents": "\nIntroduction\nWe have expanded the dslabs package, which we previously introduced as a package containing realistic, interesting and approachable datasets that can be used in introductory data science courses.\nThis release adds 7 new datasets on climate change, astronomy, life expectancy, and breast cancer diagnosis. They are used in improved problem sets and new projects within the HarvardX Data Science Professional Certificate Program, which teaches beginning R programming, data visualization, data wrangling, statistics, and machine learning for students with no prior coding background.\nYou can install the dslabs package from CRAN:\n\n\ninstall.packages(\"dslabs\")\n\n\nIf you already have the package installed, you can add the new datasets by updating the package:\n\n\nupdate.packages(\"dslabs\")\n\n\nYou can load the package into your workspace normally:\n\n\nlibrary(dslabs)\n\n\nLet’s preview these new datasets! To code along, use the following libraries and options:\n\n\n# install packages if necessary\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"ggrepel\")) install.packages(\"ggrepel\")\nif(!require(\"matrixStats\")) install.packages(\"matrixStats\")\n\n\n# load libraries\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(matrixStats)\n\n# set colorblind-friendly color palette\ncolorblind_palette <- c(\"black\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\n                        \"#CC79A7\", \"#F0E442\", \"#0072B2\", \"#D55E00\")\n\n\nClimate change\nThree datasets related to climate change are used to teach data visualization and data wrangling. These data produce clear plots that demonstrate an increase in temperature, greenhouse gas levels, and carbon emissions from 800,000 years ago to modern times. Students can create their own impactful visualizations with real atmospheric and ice core measurements.\nModern temperature anomaly and carbon dioxide data: temp_carbon\nThe temp_carbon dataset includes annual global temperature anomaly measurements in degrees Celsius relative to the 20th century mean temperature from 1880-2018. The temperature anomalies over land and over ocean are reported also. In addition, it includes annual carbon emissions (in millions of metric tons) from 1751-2014. Temperature anomalies are from NOAA and carbon emissions are from Boden et al., 2017 via CDIAC.\n\n\ndata(temp_carbon)\n\n# line plot of annual global, land and ocean temperature anomalies since 1880\ntemp_carbon %>%\n    select(Year = year, Global = temp_anomaly, Land = land_anomaly, Ocean = ocean_anomaly) %>%\n    gather(Region, Temp_anomaly, Global:Ocean) %>%\n    ggplot(aes(Year, Temp_anomaly, col = Region)) +\n    geom_line(size = 1) +\n    geom_hline(aes(yintercept = 0), col = colorblind_palette[8], lty = 2) +\n    geom_label(aes(x = 2005, y = -.08), col = colorblind_palette[8], \n               label = \"20th century mean\", size = 4) +\n    ylab(\"Temperature anomaly (degrees C)\") +\n    xlim(c(1880, 2018)) +\n    scale_color_manual(values = colorblind_palette) +\n    ggtitle(\"Temperature anomaly relative to 20th century mean, 1880-2018\")\n\n\n\nGreenhouse gas concentrations over 2000 years: greenhouse_gases\nThe greenhouse_gases data frame contains carbon dioxide (\\(\\mbox{CO}_2\\), ppm), methane (\\(\\mbox{CO}_2\\), ppb) and nitrous oxide (\\(\\mbox{N}_2\\mbox{O}\\), ppb) concentrations every 20 years from 0-2000 CE. The data are a subset of ice core measurements from MacFarling Meure et al., 2006 via NOAA. There is a clear increase in all 3 gases starting around the time of the Industrial Revolution.\n\n\ndata(greenhouse_gases)\n\n# line plots of atmospheric concentrations of the three major greenhouse gases since 0 CE\ngreenhouse_gases %>%\n    ggplot(aes(year, concentration)) +\n    geom_line() +\n    facet_grid(gas ~ ., scales = \"free\") +\n    xlab(\"Year\") +\n    ylab(\"Concentration (CH4/N2O ppb, CO2 ppm)\") +\n    ggtitle(\"Atmospheric greenhouse gas concentration by year, 0-2000 CE\")\n\n\n\nCompare this pattern with manmade carbon emissions since 1751 from temp_carbon, which have risen in a similar way:\n\n\n# line plot of anthropogenic carbon emissions over 250+ years\ntemp_carbon %>%\n    ggplot(aes(year, carbon_emissions)) +\n    geom_line() +\n    xlab(\"Year\") +\n    ylab(\"Carbon emissions (metric tons)\") +\n    ggtitle(\"Annual global carbon emissions, 1751-2014\")\n\n\n\nCarbon dioxide levels over the last 800,000 years, historic_co2\nA common argument against the existence of anthropogenic climate change is that the Earth naturally undergoes cycles of warming and cooling governed by natural changes beyond human control. \\(\\mbox{CO}_2\\) levels from ice cores and modern atmospheric measurements at the Mauna Loa observatory demonstrate that the speed and magnitude of natural variations in greenhouse gases pale in comparison to the rapid changes in modern industrial times. While the planet has been hotter and had higher \\(\\mbox{CO}_2\\) levels in the distant past (data not shown), the current unprecedented rate of change leaves little time for planetary systems to adapt.\n\n\ndata(historic_co2)\n\n# line plot of atmospheric CO2 concentration over 800K years, colored by data source\nhistoric_co2 %>%\n    ggplot(aes(year, co2, col = source)) +\n    geom_line() +\n    ylab(\"CO2 (ppm)\") +\n    scale_color_manual(values = colorblind_palette[7:8]) +\n    ggtitle(\"Atmospheric CO2 concentration, -800,000 BCE to today\")\n\n\n\nProperties of stars for making an H-R diagram: stars\nIn astronomy, stars are classified by several key features, including temperature, spectral class (color) and luminosity (brightness). A common plot for demonstrating the different groups of stars and their propreties is the Hertzsprung-Russell diagram, or H-R diagram. The stars data frame compiles information for making an H-R diagram with about approximately 100 named stars, including their temperature, spectral class and magnitude (which is inversely proportional to luminosity).\nThe H-R diagram has the hottest, brightest stars in the upper left and coldest, dimmest stars in the lower right. Main sequence stars are along the main diagonal, while giants are in the upper right and dwarfs are in the lower left. Several aspects of data visualization can be practiced with these data.\n\n\ndata(stars)\n\n# H-R diagram color-coded by spectral class\nstars %>%\n    mutate(type = factor(type, levels = c(\"O\", \"B\", \"DB\", \"A\", \"DA\", \"DF\", \"F\", \"G\", \"K\", \"M\")),\n           star = ifelse(star %in% c(\"Sun\", \"Polaris\", \"Betelgeuse\", \"Deneb\",\n                                     \"Regulus\", \"*SiriusB\", \"Alnitak\", \"*ProximaCentauri\"),\n                         as.character(star), NA)) %>%\n    ggplot(aes(log10(temp), magnitude, col = type)) +\n    geom_point() +\n    geom_label_repel(aes(label = star)) +\n    scale_x_reverse() +\n    scale_y_reverse() +\n    xlab(\"Temperature (log10 degrees K)\") +\n    ylab(\"Magnitude\") +\n    labs(color = \"Spectral class\") +\n    ggtitle(\"H-R diagram of selected stars\")\n\nWarning: Removed 88 rows containing missing values (geom_label_repel).\n\n\nUnited States period life tables: death_prob\nObtained from the US Social Security Administration, the 2015 period life table lists the probability of death within one year at every age and for both sexes. These values are commonly used to calculate life insurance premiums. They can be used for exercises on probability and random variables. For example, the premiums can be calculated with a similar approach to that used for interest rates in this case study on The Big Short in Rafael Irizarry’s Introduction to Data Science textbook.\nBrexit polling data: brexit_polls\nbrexit_polls contains vote percentages and spreads from the six months prior to the Brexit EU membership referendum in 2016 compiled from Wikipedia. These can be used to practice a variety of inference and modeling concepts, including confidence intervals, p-values, hierarchical models and forecasting.\n\n\ndata(brexit_polls)\n\n# plot of Brexit referendum polling spread between \"Remain\" and \"Leave\" over time\nbrexit_polls %>%\n    ggplot(aes(enddate, spread, color = poll_type)) +\n    geom_hline(aes(yintercept = -.038, color = \"Actual spread\")) +\n    geom_smooth(method = \"loess\", span = 0.4) +\n    geom_point() +\n    scale_color_manual(values = colorblind_palette[1:3]) +\n    xlab(\"Poll end date (2016)\") +\n    ylab(\"Spread (Proportion Remain - Proportion Leave)\") +\n    labs(color = \"Poll type\") +\n    ggtitle(\"Spread of Brexit referendum online and telephone polls\")\n\n\n\nBreast cancer diagnosis prediction: brca\nThis is the Breast Cancer Wisconsin (Diagnostic) Dataset, a classic machine learning dataset that allows classification of breast lesion biopsies as malignant or benign based on cell nucleus characteristics extracted from digitized images of fine needle aspirate cytology slides. The data are appropriate for principal component analysis and a variety of machine learning algorithms. Models can be trained to a predictive accuracy of over 95%.\n\n\n# scale x values\nx_centered <- sweep(brca$x, 2, colMeans(brca$x))\nx_scaled <- sweep(x_centered, 2, colSds(brca$x), FUN = \"/\")\n\n# principal component analysis\npca <- prcomp(x_scaled) \n\n# scatterplot of PC2 versus PC1 with an ellipse to show the cluster regions\ndata.frame(pca$x[,1:2], type = ifelse(brca$y == \"B\", \"Benign\", \"Malignant\")) %>%\n    ggplot(aes(PC1, PC2, color = type)) +\n    geom_point() +\n    stat_ellipse() +\n    ggtitle(\"PCA separates breast biospies into benign and malignant clusters\")\n\n\n\nConclusion\nWe hope that these additional datasets make the dslabs package even more useful for teaching data science through real-world case studies and motivating examples.\nAre you an R programming novice but want to learn how to do all of this and more? Check out the Data Science Professional Certificate Program from HarvardX on edX, taught by Rafael Irizarry!\n\n\n\n",
    "preview": "posts/2019-07-19-more-datasets-for-teaching-data-science-the-expanded-dslabs-package/index_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-12-27T15:39:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-05-29-research-quality-data-and-research-quality-databases/",
    "title": "Research quality data and research quality databases",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2019-05-29",
    "categories": [],
    "contents": "\nWhen you are doing data science, you are doing research. You want to use data to answer a question, identify a new pattern, improve a current product, or come up with a new product. The common factor underlying each of these tasks is that you want to use the data to answer a question that you haven’t answered before. The most effective process we have come up for getting those answers is the scientific research process. That is why the key word in data science is not data, it is science.\nNo matter where you are doing data science - in academia, in a non-profit, or in a company - you are doing research. The data is the substrate you use to get the answers you care about. The first step most people take when using data is to collect the data and store it. This is a data engineering problem and is a necessary first step before you can do data science. But the state and quality of the data you have can make a huge amount of difference in how fast and accurately you can get answers. If the data is structured for analysis - if it is research quality - then it makes getting answers dramatically faster.\nA common analogy says that data is the new oil. Using this analogy pulling the data from all of the different available sources is like mining and extracting the oil. Putting it in a data lake or warehouse is like storing the crude oil for use in different products. In this analogy research is like getting the cars to go using the oil. Crude oil extracted from the ground can be used for a lot of different products, but to make it really useful for cars you need to refine the oil into gas. Creating research quality data is the way that you refine and structure data to make it conducive to doing science. It means that the data is no longer as general purpose, but it means you can use it much, much more efficiently for the purpose you care about - getting answers to your questions.\nResearch quality data is data that:\nIs summarized the right amount\nIs formatted to work with the tools you are going to use\nIs easy to manipulate and use\nIs valid and accurately reflects the underlying data collection\nHas potential biases clearly documented.\nCombines all the relevant data types you need to answer questions\nLet’s use an example to make this concrete. Suppose that you want to analyze data from an electronic health record. You want to do this to identify new potential efficiencies, find new therapies, and understand variation in prescribing within your medical system. The data that you have collected is in the form of billing records. They might be stored in a large database for a health system, where each record looks something like this:\n\n_An example electronic health record. Source: http://healthdesignchallenge.com/_\nThese data are collected incidentally during the health process and are designed for billing, not for research. Often they contain information about what treatments patients received and were billed for, but they might not include information on the health of the patient and whether they had any health complications or relapses they weren’t billed for.\nThese data are great, but they aren’t research grade. They aren’t summarized in any meaningful way, can’t be manipulated with visualization or machine learning tools, are unwieldy and contain a lot of information we don’t need, are subject to all sorts of strange sampling biases, and aren’t merged with any of the health outcome data you might care about.\nSo let’s talk about how we would turn this pile of crude data into research quality data.\n\nTurning raw data into research quality data.\nSummarizing the data the right amount\nTo know how to summarize the data we need to know what are the most common types of questions we want to answer and what resolution we need to answer them. A good idea is to summarize things at the finest unit of analysis you think you will need - it is always easier to aggregate than disaggregate at the analysis level. So we might summarize at the patient and visit level. This would give us a data set where everything is indexed by patient and visit. If we want to answer something at a clinic, physician, or hospital level we can always aggregate there.\nWe also need to choose what to quantify. We might record for each visit the date, prescriptions with standardized codes, tests, and other metrics. Depending on the application we may store the free text of the physician notes as a text string - for potential later processing into specific tokens or words. Or if we already have a system for aggregating physicians notes we could apply it at this stage.\nIs formatted to work with the tools you are going to use\nResearch quality data is organized so the most frequent tasks can be completed quickly and without large amounts of data processing and reformatting. Each data analytic tool has different requirements on the type of data you need to input. For example, many statistical modeling tools use “tidy data” so you might store the summarized data in a single tidy data set or a set of tidy data tables linked by a common set of indicators. Some software (for example in the analysis of human genomic data) require inputs in different formats - say as a set of objects in the R programming language. Others, like software to fit a convolutional neural network to a set of images, might require a set of image files organized in a directory in a particular way along with a metadata file providing information about each set of images. Or we might need to one hot encode categories that need to be classified.\nIn the case of our EHR data we might store everything in a set of tidy tables that can be used to quickly correlate different measurements. If we are going to integrate imaging, lab reports, and other documents we might store those in different formats to make integration with downstream tools easier.\nIs easy to manipulate and use\nThis seems like it is just a re-hash of formatting the data to work with the tools you care about, but there are some subtle nuances. For example, if you have a huge amount of data (petabyes of images, for example) you might not want to do research on all of those data at once. It will be inefficient and expensive. So you might use sampling to get a smaller data set for your research quality data that is easier to use and manipulate. The data will also be easier to use if they are (a) stored in an easy to access database with security systems well documented, (b) have a data dictionary that makes it clear what the data are and where they come from, or (c) have a clear set of tutorials on how to perform common tasks on the data.\nIn our EHR example you might include a data dictionary that describes the dates of the data pull, the types of data pulled, the type of processing performed, and pointers to the scripts that pulled the data.\nIs valid and accurately reflects the underlying data collection\nData can be invalid for a whole host of reasons. The data could be incorrectly formatted, input with error, could change over time, could be mislabeled, and more. All of these problems can occur on the original data pull or over time. Data can also be out of date as new data becomes available.\nThe research quality database should include only data that has been checked, validated, cleaned and QA’d so that it reflects the real state of the world. This process is not a one time effort, but an ongoing set of code, scripts, and processes that ensure the data you use for research are as accurate as possible.\nIn the EHR example there would be a series of data pulls, code to perform checks, and comparisons to additional data sources to validate the values, levels, variables, and other components of the research quality database.\nHas potential biases clearly documented\nA research quality data set is by definition a derived data set. So there is a danger that problems with the data will be glossed over since it has been processed and easy to use. To avoid this problem, there has to be documentation on where the data came from, what happened to them during processing, and any potential problems with the data.\nWith our EHR example this could include issues about how patients come into the system, what procedures can be billed (or not), what data was ignored in the research quality database, what are the time periods the data were collected, and more.\nCombines all the relevant data types you need to answer questions\nOne big difference between a research quality data set/database and a raw database or even a general purpose tidy data set, is that it merges all of the relevant data you need to answer specific questions, even if they come from distinct sources. Research quality data pulls together and makes easy to access, all the information you need to answer your questions. This could still be in the form of a relational database - but the databases organization is driven by the research question, rather than driven by other purposes.\nFor example, EHR data may already be stored in a relational database. But it is stored in a way that makes it easy to understand billing and patient flow in a clinic. To answer a research question you might need to combine the billing data, with patient outcome data, and prescription fulfillment data, all processed and indexed so they are either already merged or can be easily merged.\nWhy do this?\nSo why build a research quality data set? It sure seems like a lot of work (and it is!). The reason is that this work will always be done, one way or the other. If you don’t invest in making a research quality data set up front, you will do it as a thousand papercuts over time. Each time you need to answer a new question or try a different model you’ll be slowed down by the friction of identifying, creating, and checking a new cleaned up data set. On the one hand this amortizes the work over the course of many projects. But by doing it piecemeal you also dramatically increase the chance of an error in processing, reduce answer time, slow down the research process, and make the investment for any individual project much higher.\nProblem Forward Data Science\nIf you want help planning or building a research quality data set or database, we can help at Problem Forward Data Science. Get in touch here: https://problemforward.typeform.com/to/L4h89P\n\n\n\n",
    "preview": "https://user-images.githubusercontent.com/1571674/58572594-f77d2080-8209-11e9-87a2-0621a13eeb03.png",
    "last_modified": "2021-11-12T15:19:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-05-20-i-co-founded-a-company-meet-problem-forward-data-science/",
    "title": "I co-founded a company! Meet Problem Forward Data Science",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2019-05-20",
    "categories": [],
    "contents": "\nI have some exciting news about something I’ve been working on for the last year or so. I started a company! It’s called Problem Forward data science. I’m pumped about this new startup for a lot of reasons.\nMy co-founder is one of my families closest friends, Jamie McGovern, who has more than 2 decades of experience in the consulting world and who I’ve known for 15 years.\nWe are creating a cool new model of “data scientist as a service” (more on that below)\nWe have a problem forward, not solution backward approach to data science that grew out of the Hopkins philosophy of data science.\nWe are headquartered in East Baltimore and are creating awesome new tech jobs in a place where they haven’t been historically.\nProblem Forward, Not Solution Backward\nWe have always had a “problem forward, not solution backward” approach to statistics, machine learning and data here at Simply Stats. This has grown out of the Johns Hopkins Biostatistics philosophy of starting with the public health or medical problem you care about and working back to the statistical models, software, and tools you need to solve it.\nThis idea is so important to us, it is in the name of the company. When we work with people our first goal is to find out the problems and questions that they genuinely care about, then work backward to figure out how to solve them. We don’t come in with a particular predetermined algorithm or strategy. One of the first questions we ask people isn’t about data at all, it is:\n\nWhat question do you wish you could answer about your business (ignoring if you have the data or not to answer it yet)?\n\nMy favorite example of this is Moneyball. This is one of the classic stories about how the Oakland A’s used data to gain a unique advantage. But one of the key messages about this story that often gets missed is that the data weren’t unique to the A’s! Everyone had the same data, the A’s just started with a problem that they needed to solve. They needed to find a unique way to win games that wasn’t as expensive. Then they moved forward to looking at the data and realized that on base percentage was cheaper than home runs. So the A’s used a “problem forward, not solution backward” approach to data analysis.\nUsing this approach we have worked with companies with a wide variety of needs. Our main capabilities are in data strategy, data cleaning and research quality database generation, modeling and machine learning, and data views through dashboards, reports, and presentations.\n\nData Scientist as a Service\nThere are a huge number of data science platform companies out there. Some of them are producing awesome tools, but as any serious data analyst will tell you we are years from automating real data science. We are only very recently seeing formal definitions of what success of a data analysis even means! So it isn’t surprising when general purpose platforms like IBM Watson struggle with specific problems - the problem isn’t specified clearly enough for a platform to solve it yet..\nThe reason there are so many platforms is that its easy to sell the “cool” part of the problem - say building an AI to classify images or drive a car. But often the deeper problem is (a) figuring out what you even want to or can say with a set of data set, (b) collecting a set of disorganized data, (c) getting buy in from groups with different motivations and data sets, (d) organizing ugly data from different sources or finding new data you might need, and (e) putting your answers in context. These problems are more like “glue” that comes between each of the platforms. We have a phrase we like to use:\n\nTo solve your data problem you need a person, not a platform\n\nSo we have set up a “platform” that lets you scale up and down the number team members you have to solve data problems, just like you would scale up and down the number of servers or tools that you use on AWS.\n\nThis means if you are an early stage startup we can help you scale data science before you can afford to hire a whole team. Even if you are a non-profit or a small academic group we can scale up or down to suit your needs. And if you are a big company we can provide utility data science for projects with tight deadlines.\nWorking with friends and building East Baltimore\nThe thing that gets me most excited about this new adventure is working with my really close friend Jamie. It’s been huge for me to learn about the ins and outs of starting and running a business with someone who has decades of experience in the consulting industry.\nIt’s also exciting to be able to headquarter the company right in East Baltimore and to work to upskill and develop talent here in a neighborhood I care about.\nLike what you hear? Get in touch\nIf you are looking for data science work we’d love to hear from you! Whether you are an academic, a non-profit, a small startup, or a big business our utility model means we can work with you.\nIf you are interested in working with us contact us here:\nhttps://problemforward.typeform.com/to/L4h89P\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:33:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-04-29-generative-and-analytical-models-for-data-analysis/",
    "title": "Generative and Analytical Models for Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2019-04-29",
    "categories": [],
    "contents": "\nDescribing how a data analysis is created is a topic of keen interest to me and there are a few different ways to think about it. Two different ways of thinking about data analysis are what I call the “generative” approach and the “analytical” approach. Another, more informal, way that I like to think about these approaches is as the “biological” model and the “physician” model. Reading through the literature on the process of data analysis, I’ve noticed that many seem to focus on the former rather than the latter and I think that presents an opportunity for new and interesting work.\nGenerative Model\nThe generative approach to thinking about data analysis focuses on the process by which an analysis is created. Developing an understanding of the decisions that are made to move from step one to step two to step three, etc. can help us recreate or reconstruct a data analysis. While reconstruction may not exactly be the goal of studying data analysis in this manner, having a better understanding of the process can open doors with respect to improving the process.\nA key feature of the data analytic process is that it typically takes place inside the data analyst’s head, making it impossible to directly observe. Measurements can be taken by asking analysts what they were thinking at a given time, but that can be subject to a variety of measurement errors, as with any data that depend on a subject’s recall. In some situations, partial information is available, for example if the analyst writes down the thinking process through a series of reports or if a team is involved and there is a record of communication about the process. From this type of information, it is possible to gather a reasonable picture of “how things happen” and to describe the process for generating a data analysis.\nThis model is useful for understanding the “biological process”, i.e. the underlying mechanisms for how data analyses are created, sometimes referred to as “statistical thinking”. There is no doubt that this process has inherent interest for both teaching purposes and for understanding applied work. But there is a key ingredient that is lacking and I will talk about that more below.\nAnalytical Model\nA second approach to thinking about data analysis ignores the underlying processes that serve to generate the data analysis and instead looks at the observable outputs of the analysis. Such outputs might be an R markdown document, a PDF report, or even a slide deck (Stephanie Hicks and I refer to this as the analytic container). The advantage of this approach is that the analytic outputs are real and can be directly observed. Of course, what an analyst puts into a report or a slide deck typically only represents a fraction of what might have been produced in the course of a full data analysis. However, it’s worth noting that the elements placed in the report are the cumulative result of all the decisions made through the course of a data analysis.\nI’ve used music theory as an analogy for data analysis many times before, mostly because…it’s all I know, but also because it really works! When we listen to or examine a piece of music, we have essentially no knowledge of how that music came to be. We can no longer interview Mozart or Beethoven about how they wrote their music. And yet we are still able to do a few important things:\nAnalyze and Theorize. We can analyze the music that we hear (and their written representation, if available) and talk about how different pieces of music differ from each other or share similarities. We might develop a sense of what is commonly done by a given composer, or across many composers, and evaluate what outputs are more successful or less successful. It’s even possible to draw connections between different kinds of music separated by centuries. None of this requires knowledge of the underlying processes.\nGive Feedback. When students are learning to compose music, an essential part of that training is the play the music in front of others. The audience can then give feedback about what worked and what didn’t. Occasionally, someone might ask “What were you thinking?” but for the most part, that isn’t necessary. If something is truly broken, it’s sometimes possible to prescribe some corrective action (e.g. “make this a C chord instead of a D chord”).\nThere are even two whole podcasts dedicated to analyzing music—Sticky Notes and Switched on Pop—and they generally do not interview the artists involved (this would be particularly hard for Sticky Notes). By contrast, the Song Exploder podcast takes a more “generative approach” by having the artist talk about the creative process.\nI referred to this analytical model for data analysis as the “physician” approach because it mirrors, in a basic sense, the problem that a physician confronts. When a patient arrives, there is a set of symptoms and the patient’s own report/history. Based on that information, the physician has to prescribe a course of action (usually, to collect more data). There is often little detailed understanding of the biological processes underlying a disease, but they physician may have a wealth of personal experience, as well as a literature of clinical trials comparing various treatments from which to draw. In human medicine, knowledge of biological processes is critical for designing new interventions, but may not play as large a role in prescribing specific treatments.\nWhen I see a data analysis, as a teacher, a peer reviewer, or just a colleague down the hall, it is usually my job to give feedback in a timely manner. In such situations there usually isn’t time for extensive interviews about the development process of the analysis, even though that might in fact be useful. Rather, I need to make a judgment based on the observed outputs and perhaps some brief follow-up questions. To the extent that I can provide feedback that I think will improve the quality of the analysis, it is because I have a sense of what makes for a successful analysis.\nThe Missing Ingredient\nStephanie Hicks and I have discussed what are the elements of a data analysis as well as what might be the principles that guide the development of an analysis. In a new paper, we describe and characterize the success of a data analysis, based on a matching of principles between the analyst and the audience. This is something I have touched on previously, both in this blog and on my podcast with Hilary Parker, but in a generally more hand-wavey fashion. Developing a more formal model, as Stephanie and I have done here, has been useful and has provided some additional insights.\nFor both the generative model and the analytical model of data analysis, the missing ingredient was a clear definition of what made a data analysis successful. The other side of that coin, of course, is knowing when a data analysis has failed. The analytical approach is useful because it allows us to separate the analysis from the analyst and to categorize analyses according to their observed features. But the categorization is “unordered” unless we have some notion of success. Without a definition of success, we are unable to formally criticize analyses and explain our reasoning in a logical manner.\nThe generative approach is useful because it reveals potential targets of intervention, especially from a teaching perspective, in order to improve data analysis (just like understanding a biological process). However, without a concrete definition of success, we don’t have a target to strive for and we do not know how to intervene in order to make genuine improvement. In other words, there is no outcome on which we can “train our model” for data analysis.\nI mentioned above that there is a lot of focus on developing the generative model for data analysis, but comparatively little work developing the analytical model. Yet, both models are fundamental to improving the quality of data analyses and learning from previous work. I think this presents an important opportunity for statisticians, data scientists, and others to study how we can characterize data analyses based on observed outputs and how we can draw connections between analyses.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:33:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-04-17-tukey-design-thinking-and-better-questions/",
    "title": "Tukey, Design Thinking, and Better Questions",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2019-04-17",
    "categories": [],
    "contents": "\nRoughly once a year, I read John Tukey’s paper “The Future of Data Analysis”, originally published in 1962 in the Annals of Mathematical Statistics. I’ve been doing this for the past 17 years, each time hoping to really understand what it was he was talking about. Thankfully, each time I read it I seem to get something new out of it. For example, in 2017 I wrote a whole talk around some of the basic ideas.\nWell, it’s that time of year again, and I’ve been doing some reading.\nProbably the most famous line from this paper is\n\nFar better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.\n\nThe underlying idea in this sentence arises in at least two ways in Tukey’s paper. First is his warning that statisticians should not be called upon to produce the “right” answers. He argues that the idea that statistics is a “monolithic, authoritarian structure designed to produce the ‘official’ results” presents a “real danger to data analysis”. Second, Tukey criticizes the idea that much of statistical practice centers around optimizing statistical methods around precise (and inadequate) criteria. One can feel free to identify a method that minimizes mean squared error, but that should not be viewed as the goal of data analysis.\nBut that got me thinking—what is the ultimate goal of data analysis? In 64 pages of writing, I’ve found it difficult to identify a sentence or two where Tukey describes the ultimate goal, why it is we’re bothering to analyze all this data. It occurred to me in this year’s reading of the paper, that maybe the reason Tukey’s writing about data analysis is often so confusing to me is because his goal is actually quite different from that of the rest of us.\nMore Questions, Better Questions\nMost of the time in data analysis, we are trying to answer a question with data. I don’t think it’s controversial to say that, but maybe that’s the wrong approach? Or maybe, that’s what we’re not trying to do at first. Maybe what we spend most of our time doing is figuring out a better question.\nHilary Parker and I have discussed at length the idea of design thinking on our podcast. One of the fundamental ideas from design thinking involves identifying the problem. It’s the first “diamond” in the “double diamond” approach to design.\nTukey describes the first three steps in a data analysis as:\nRecognition of problem\nOne technique used\nCompeting techniques used\nIn other words, try one approach, then try a bunch of other approaches! You might be thinking, why not just try the best approach (or perhaps the right approach) and save yourself all that work? Well, that’s the kind of path you go down when you’re trying to answer the question. Stop doing that! There are two reasons why you should stop thinking about answering the question:\nYou’re probably asking the wrong question anyway, so don’t take yourself too seriously;\nThe “best” approach is only defined as “best” according to some arbitrary criterion that probably isn’t suitable for your problem/question.\nAfter thinking about all this I was inspired to draw the following diagram.\nStrength of Evidence vs. Quality of QuestionThe goal in this picture is to get to the upper right corner, where you have a high quality question and very strong evidence. In my experience, most people assume that they are starting in the bottom right corner, where the quality of the question is at its highest. In that case, the only thing left to do is to choose the optimal procedure so that you can squeeze as much information out of your data. The reality is that we almost always start in the bottom left corner, with a vague and poorly defined question and a similarly vague sense of what procedure to use. In that case, what’s a data scientist to do?\nIn my view, the most useful thing a data scientist can do is to devote serious effort towards improving the quality and sharpness of the question being asked. On the diagram, the goal is to move us as much as possible to the right hand side. Along the way, we will look at data, we will consider things outside the data like context, resources and subject matter expertise, and we will try a bunch of different procedures (some optimal, some less so).\nUltimately, we will develop some of idea of what the data tell us, but more importantly we will have a better sense of what kinds of questions we can ask of the data and what kinds of questions we actually want to have answered. In other words, we can learn more about ourselves by looking at the data.\nExploring the Data\nIt would seem that the message here is that the goal of data analysis is to explore the data. In other words, data analysis is exploratory data analysis. Maybe this shouldn’t be so surprising given that Tukey wrote the book on exploratory data analysis. In this paper, at least, he essentially dismisses other goals as overly optimistic or not really meaningful.\nFor the most part I agree with that sentiment, in the sense that looking for “the answer” in a single set of data is going to result in disappointment. At best, you will accumulate evidence that will point you in a new and promising direction. Then you can iterate, perhaps by collecting new data, or by asking different questions. At worst, you will conclude that you’ve “figured it out” and then be shocked when someone else, looking at another dataset, concludes something completely different. In light of this, discussions about p-values and statistical significance are very much beside the point.\nThe following is from the very opening of Tukey’s book *Exploratory Data Analysis:\n\nIt is important to understand what you CAN DO before you learn to measure how WELL you seem to have DONE it\n\n(Note that the all caps are originally his!) Given this, it’s not too surprising that Tukey seems to equate exploratory data analysis with essentially all of data analysis.\nBetter Questions\nThere’s one story that, for me, totally captures the spirit of exploratory data analysis. Legend has it that Tukey once asked a student what were the benefits of the median polish technique, a technique he invented to analyze two-way tabular data. The student dutifully answered that the benefit of the technique is that it provided summaries of the rows and columns via the row- and column-medians. In other words, like any good statistical technique, it summarized the data by reducing it in some way. Tukey fired back, saying that this was incorrect—the benefit was that the technique created more data. That “more data” was the residuals that are leftover in the table itself after running the median polish. It is the residuals that really let you learn about the data, discover whether there is anything unusual, whether your question is well-formulated, and how you might move on to the next step. So in the end, you got row medians, column medians, and residuals, i.e. more data.\nIf a good exploratory technique gives you more data, then maybe good exploratory data analysis gives you more questions, or better questions. More refined, more focused, and with a sharper point. The benefit of developing a sharper question is that it has a greater potential to provide discriminating information. With a vague question, the best you can hope for is a vague answer that may not lead to any useful decisions. Exploratory data analysis (or maybe just data analysis) gives you the tools that let the data guide you towards a better question.\n\n\n\n",
    "preview": "posts/2019-04-17-tukey-design-thinking-and-better-questions/images/question_evidence.png",
    "last_modified": "2021-11-11T17:03:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-04-01-interview-with-abhi-datta/",
    "title": "Interview with Abhi Datta",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2019-04-01",
    "categories": [],
    "contents": "\nEditor’s note: This is the next in our series of interviews with early career statisticians and data scientists. Today we are talking to Abhi Datta about his work in large scale spatial analysis and his interest in soccer! Follow him on Twitter at [@datta_science](https://twitter.com/datta_science). If you have recommendations of an (early career) person in academics or industry you would like to see promoted, reach out to Jeff (@jtleek) on Twitter!\nSS: Do you consider yourself a statistician, biostatistician, data scientist, or something else?\nAD: That is a difficult question for me, as I enjoy working on theory, methods and data analysis and have co-authored diverse papers ranging from theoretical expositions to being primarily centered around a complex data analysis. My research interests also span a wide range of areas. A lot of my work on spatial statistics is driven by applications in environmental health and air pollution. Another significant area of my research is developing Bayesian models for epidemiological applications using survey data.\nI would say what I enjoy most is developing statistical methodology motivated by a complex application where current methods fall short, applying the method for analysis of the motivating data, and trying to see if it is possible to establish some guarantees about the method through a combination of theoretical studies and empirical experiments that will help to generalize applicability of the method for other datasets. Of course, not all projects involve all the steps, but that is my ideal workflow. Not sure what that classifies me as.\nSS: How did you get into statistics? What was your path to ending up at Hopkins?\nAD: I was born and grew up in Kolkata, India. I had the option of going for engineering, medical or statistics undergrad. I chose statistics persuaded by my appreciation for mathematics and the reputation of the statistics program at Indian Statistical Institute (ISI), Kolkata. I completed my undergrad (BStat) and Masters (MStat) in Statistics from ISI and I’m thankful I made that choice as those 5 years at ISI played a pivotal role in my life. Besides getting rigorous training in the foundations of statistics, most importantly, I met my wife Dr. Debashree Ray at ISI.\nAfter my Masters, I had a brief stint in the finance industry, working for 2 years at Morgan Stanley (in Mumbai and then in New York City) before I joined the PhD program at the Division of Biostatistics at University of Minnesota (UMN) in 2012 where Debashree was pursuing her PhD in Biostatistics. I had initially planned to work in Statistical Genetics as I had done a research project in that area in my Master’s. However, I explored other research areas in my first year and ended up working on spatial statistics under the supervision of my advisor Dr. Sudipto Banerjee, and on high-dimensional data with my co-advisorDr. Hui Zou from the Department of Statistics in Minnesota. I graduated from Minnesota in 2016 and joined Hopkins Biostat as an Assistant Professor in the Fall of 2016.\nSS: You work on large scale spatio-temporal modeling - how do you speed up computations for the bootstrap when the data are very large?\nAD: A main computational roadblock in spatio-temporal statistics is working with very big covariance matrices that strain memory and computing resources typically available in personal computers. Previously, I have developed nearest neighbor Gaussian Processes (NNGP) – a Bayesian hierarchical model for inference in massive geospatial datasets. One issue with hierarchical Bayesian models is their reliance on long sequential MCMC runs. Bootstrap, unlike MCMC, can be implemented in an embarrassingly parallel fashion. However, for geospatial data, all observations are correlated across space prohibiting direct resampling for bootstrap.\nIn a recent work with my student Arkajyoti Saha, we proposed a semi-parametric bootstrap for inference on large spatial covariance matrices. We use sparse Cholesky factors of spatial covariance matrices to approximately decorrelate the data before resampling for bootstrap. Arkajyoti has implemented this in an R-package BRISC: Bootstrap for rapid inference on spatial covariances. BRISC is extremely fast and at the time of publication, to my knowledge, it was the only R-package that offered inference on all the spatial covariance parameters without using MCMC. The package can also be used simply for super-fast estimation and prediction in geo-statistics.\nSS: You have a cool paper on mapping local and global trait variation in plant distributions, how did you get involved in that collaboration? Does your modeling have implications for people studying the impacts of climate change?\nAD: In my final year of PhD at UMN, I was awarded the Inter-Disciplinary Doctoral Fellowship – a fantastic initiative by the graduate school at UMN providing research and travel funding, and office space to work with an inter-disciplinary team of researchers on a collaborative project. In my IDF, mentored by Dr. Arindam Banerjee and Dr. Peter Reich, I worked with a group of climate modelers, ecologists and computer scientists from several institutions on a project whose eventual goal is to improve carbon projections from climate models.\nThe paper you mention was aimed at improving the global characterization of plant traits (measurements). This is important as plant trait values are critical inputs to climate model. Even the largest plant trait database TRY offers poor geographical coverage with little or no data across many large geographical regions. We used the fast NNGP approach I had been developing in my PhD to spatially gap-fill the plant trait data to create a global map of important plant traits with proper uncertainty quantification. The collaboration was a great learning experience for me on how to conduct a complex data analysis, and how to communicate with scientists.\nCurrently, we are looking at ways to incorporate the uncertainty quantified trait values as inputs to Earth System Models (ESMs) – the land component of climate models. We hope that replacing single trait values with entire trait distributions as inputs to these models will help to better propagate the uncertainty and improve the final model projections.\nSS: What project has you most excited at the moment?\nAD: There are two. I have been working with Dr. Scott Zeger on a project lead by Dr. Agbessi Amouzou in the Department of International Health at Hopkins aiming to estimate the cause-specific fractions (CSMF) of child mortality in Mozambique using family questionnaire data (verbal autopsy). Verbal autopsies are often used as a surrogate to full autopsy in many countries and there exists software that use these questionnaire data to predict a cause for every death. However, these software are usually trained on some standard training data and yield inaccurate predictions in local context. This problem is a special case of transfer learning where a model trained using data representing a standard population offers poor predictive accuracy when specific populations are of interest. We have developed a general approach for transfer learning of classifiers that uses the predictions from these verbal autopsy software and limited full autopsy data from the local population to provide improved estimates of cause-specific mortality fractions. The approach is very general and offers a parsimonious model-based solution to transfer learning and can be used in any other classification-based application.\nThe second project involves creating high-resolution space-time maps of particulate matter (PM2.5) in Baltimore. Currently a network of low-cost air pollution monitors is being deployed in Baltimore that promises to offer air pollution measurements at a much higher geospatial resolution than what is provided by EPA’s sparse regulatory monitoring network. I was awarded a Bloomberg American Health Initiative Spark award for working with Dr. Kirsten Koehler in the Department of Environmental Health and Engineering to combine the low-cost network data, the sparse EPA data and other land-use covariates to create uncertainty quantified maps of PM2.5 at an unprecedented spatial resolution. We have just started analyzing the first two months of data and I’m really looking forward to help create the end-product and understand how PM2.5 levels vary across the different neighborhoods in Baltimore.\nSS: You have an interest in soccer and spatio temporal models have played an increasing role in soccer analytics. Have you thought about using your statistics skills to study soccer or do you try to avoid mixing professional work and being a fan?\nAD: Yes, I’m an avid soccer fan. I have travelled to Brazil in 2014 and Russia in 2018 to watch live games in the world cups. It also unfortunately means that I set my alarm to earlier times on weekends than on weekdays as the European league games start pretty early in US time.\nHowever, until recent times, I’ve been largely ignorant of applications of spatio-temporal statistics in soccer analytics. I just finished teaching a Spatial Statistics course and one of the students presented a fascinating work he has done on predicting player’s scoring abilities using spatial statistics. I certainly plan to read more literature on this and maybe one day can contribute. Till then I remain a fan.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:33:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-03-13-10-things-r-can-do-that-might-surprise-you/",
    "title": "10 things R can do that might surprise you",
    "description": {},
    "author": [],
    "date": "2019-03-13",
    "categories": [],
    "contents": "\nOver the last few weeks I’ve had a couple of interactions with folks from the computer science world who were pretty disparaging of the R programming language. A lot of the critism focused on perceived limitations of R to statistical analysis.\nIt’s true, R does have a hugely comprehensive list of analysis packages on CRAN, Bioconductor, Neuroconductor, and ROpenSci as well as great package management. As I was having these conversations I realized that R has grown into a multi-purpose connective language for things beyond just data analysis. But that the functionality isn’t always as well known outside of the R community. So this post is about some of the ridiculously awesome features of R that may or may not be as widely known. Here are 10 things R can do you might not have known about, building on Kara’s great tweet thread about lighthearted things to do with R.\n1. You can write reproducible Word or Powerpoint documents from R markdown\nThe rmarkdown package lets you create reproducible Word documents and reproducible Powerpoint Presentations from your R markdown code just by changing one line in the YAML!\n2. You can build and host interactive web apps in just a few lines of code\nIn just a few lines of code you can create interactive web apps in R. For example, in just 36 lines of code you can create an interactive dashboard to explore your BMI in relation to the NHANES sample using the flexdashboard package.\n3. You can host your web apps in one more line of R code\nThe other cool thing about building web apps in R is that you can get them up on the web with just another line or two of R code using the rsconnect package. You can put them up on your own server or, even easier, host them on a cloud server like shinyapps.io.\n4. You can connect to almost any database under the sun and pull data with dplyr/dbplyr\nIt is really easy to connect to almost any database (local or remote) using the dbplyr package. This makes it possible for an R user to work independently pulling data from almost all common database types. You can also use specialized packages like bigrquery to work directly with BigQuery and other high performance data stores.\n5. You can use the same dplyr grammar locally or on data on multiple different data stores\nOnce you learn how to do basic data tranforms with dplyr, you can apply the same code to analyze data locally on your computer or remotely on any of the above databases or data stores. This simplifies and unifies data manipulation across multiple different databases and languages.\n6. You can fit deep learning models with keras and Tensorflow\nThe keras package allows you to fit both pre-trained and denovo deep learning models directly from R. You can also work with the direct TensorFlow interface to fit the same kind of models.\n7. You can build APIs and serve them from R\nThe plumbr R package lets you convert R functions to web APIs that can be integrated into downstream applications. If you have Rstudio Connect you can also deploy them as easily as you deploy web apps.\n8. You can make video game interfaces with R\nNot only can you deploy web apps, you can make them into awesome video games in R. The nessy package lets you create NES looking Shiny apps and deploy them just like you would any other Shiny app.\n9. You can analyze data using Spark clusters right from R\nWant to fit big, gnarly machine learning models on huge data sets? You can do that right from R using the sparklyr package. You can use spark on your Desktop or a monster Spark cluster.\n10. You can build and learn R interactively in R\nThe swirl package is an R package that lets you build interactive tutorials for R, right inside R.\nThis is by no means a comprehensive list. You can also connect to AWS Polly and write text to speech synthesis software or build Shiny apps that respond to voice commands or build apps that let you combine deep learning and accelerometry data to cast Harry Potter spells. The point is that R has become much more than just a data analysis language (although its still good at that!) and being good at R opens the door to lots of practical and cool applications.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:33:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-02-21-dynamite-plots-must-die/",
    "title": "Open letter to journal editors: dynamite plots must die",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2019-02-21",
    "categories": [],
    "contents": "\nStatisticians have been pointing out the problem with dynamite plots,\nalso known as bar and line graphs, for years. Karl Broman lists them as\none of the top ten\nworst graphs. The problem has even been documented in the peer\nreviewed literature. For example, this\nBritish Journal of Pharmacology paper titled Show the data,\ndon’t conceal them was published in 2011.\nHowever, despite all these efforts, dynamite plots continue to be\nubiquitous in the scientific literature. Just open the latest issue of\nNature, Science or Cell and you will likely see a few. In fact, in this\nPLOS\nBiology paper, Tracey Weissgerber and co-authors perform a systmetic\nreview of “top physiology journals” and find that “85.6% of papers\nincluded at least one bar graph”. They go on to recommend “training\ninvestigators in data presentation, encouraging a more complete\npresentation of data, and changing journal editorial policies”. In my\nview, the training will be accelerated if editors implement a policy\nthat requires authors to show the data or, if the dataset is too large,\nshow the distribution of the data with boxplots, histograms or smooth\ndensity estimates.\nWhat’s wrong with dynamite\nplots\nDynamite plots are used to compare measurements from two or more\ngroups: cases and controls, for example. In a two group comparison, the\nplots are graphical representations of a grand total of 4 numbers,\nregardless of the sample size. The four numbers are the average and the\nstandard error (or the standard deviation, it’s not always clear) for\neach group. Here is a simulated example comparing diastolic blood\npressure for patients on a drug and placebo:\n\n\n\nStars are often added to point out that the differences are\nstatistically significant.\nSo what is the problem with these plots? First, if you have a print\nedition of your journal you are wasting ink. No need to waste all that\ntoner just to show these four summaries:\n\n         x average  se\n1 Controls      60 2.3\n2    Cases      81 9.7\n\nFrom these numbers you compute the p-value, which in this case is\njust below 0.05.\nSecond, the dynamite plot makes it appear as if there is a clear\ndifference between the two groups. Showing the data\nreveals more information. In our example, showing the data reveals that\nthe lowest blood pressure is actually in the treatment group. It also\nreveals the presence of one somewhat extreme value of 150. This might\nrepresent a data entry mistake. Perhaps systolic pressure was recorded\nby accident? Note that without that data point, the difference is no\nlonger significant at the 0.05 level.\n\n\n\nNote also that, as pointed out by Weissgerber, data that look quite\ndifferent can result in exactly the same barplot. For instance, the two\ndatasets below would produce the same barplot as the one shown\nabove.\n\n\n\nWhat should we do instead?\nFirst, let’s generate the data that we will use in the example R code\nshown below.\n\n\nlibrary(tidyverse)\nset.seed(0)\nn <- 10\ncases <- rnorm(n, log2(64), 0.25)\ncontrols <- rnorm(n, log2(64), 0.25)\ncases <- 2^(cases)\ncontrols <- 2^(controls)\ncases[1:2] <- c(110, 150) #introduce outliers\ndat <- data.frame(x = factor(rep(c(\"Controls\", \"Cases\"), each = n), \n                             levels = c(\"Controls\", \"Cases\")),\n                             Outcome = c(controls, cases))\n\n\n\nOne option is simply to show the data points, which you can do like\nthis:\n\n\ndat %>% ggplot(aes(x, Outcome)) + \n        geom_jitter(width = 0.05)\n\n\n\n\nIn this case we see that the data is right skewed so we might want to\nremake the plot in the log scale\n\n\ndat %>% ggplot(aes(x, Outcome)) + \n        geom_jitter(width = 0.05) + \n        scale_y_log10()\n\n\n\n\nIf we want to show summary statistics for the data, we can\nsuperimpose a boxplot:\n\n\ndat %>% ggplot(aes(x, Outcome)) + \n        geom_boxplot() +\n        geom_jitter(width = 0.05) + \n        scale_y_log10()\n\n\n\n\nAlthough not the case here, if there are too many points, we can\nsimply show the boxplot.\n\n\ndat %>% ggplot(aes(x, Outcome)) + \n        geom_boxplot() +\n        scale_y_log10()\n\n\n\n\nAnd if we are worried that five summary statistics might be hiding\nimportant characteristics of the data, we can use ridge plots.\n\n\nlibrary(ggridges)\ndat %>% ggplot(aes(Outcome, x)) + \n        scale_x_log10() +\n        geom_density_ridges(scale = 0.9) \n\n\n\n\nIf of manageable size, you should show the data points as well:\n\n\nlibrary(ggridges)\ndat %>% ggplot(aes(Outcome, x)) + \n        scale_x_log10() +\n        geom_density_ridges(scale = 0.9,\n                            jittered_points = TRUE, \n                            position = position_points_jitter(width = 0.05,\n                                                              height = 0),\n                            point_shape = '|', point_size = 3, \n                            point_alpha = 1, alpha = 0.7) \n\n\n\n\nFor more recommendation and Excel templates please consult Weissgerber\net al. or this\nthread.\n\n\n\n",
    "preview": "posts/2019-02-21-dynamite-plots-must-die/index_files/figure-html5/dynamite-1.png",
    "last_modified": "2022-02-05T17:03:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-02-18-interview-with-stephanie-hicks/",
    "title": "Interview with Stephanie Hicks",
    "description": {},
    "author": [],
    "date": "2019-02-18",
    "categories": [],
    "contents": "\nEditor’s note: For a while we ran an interview series for statisticians and data scientists, but things have gotten a little hectic around here so we’ve dropped the ball! But we are re-introducing the series, starting with Stephanie Hicks. If you have recommendations of a (junior) person in academics or industry you would like to see promoted, reach out to Jeff (@jtleek) on Twitter!\nStephanie Hicks received her PhD in statistics in 2013 at Rice University and has already made major contributions to the analysis of single cell sequencing data and the theory and practice of teaching data science.\nSS: Do you consider yourself a statistician, biostatistician, data scientist or something else?\nSH: Fantastic question! I’m a statistician by training, and I work in a department of biostatistics, so I would be remiss if I didn’t answer a statistician. However my interests are at the intersection of statistics, data science, genomics and education. Broadly, my research interests are to leverage statistical methods and computational algorithms to effectively derive knowledge from data. I’m also very interested in identifying better ways to teach students how to do that. I work a lot with genomics data, but I also analyze data from many other areas. You might think of this as data science, so I could easily imagine someone classifying myself as an ‘academic data scientist’ if such a thing exists?\nSS: How did you end up at Johns Hopkins (i.e. your history)?\nSH: I received my B.S. in Mathematics in 2007 rom LSU and my M.A. and Ph.D. in 2013 from the Department of Statistics at Rice University under the direction of Marek Kimmel and Sharon Plon (@splon). I completed my postdoctoral training with Rafael Irizarry (@rafalab) in the Department of Data Sciences at Dana-Farber Cancer Institute and Department of Biostatistics at Harvard T.H. Chan School of Public Health. While I was a postdoc, I had the opportunity to meet many students from the Johns Hopkins Biostatistics Department at the Women in Statistics Conference in Cary, North Carolina in 2014. The following year, I attended the ROpenSci Unconference and teamed up with Roger Peng, Hilary Parker and David Robinson to work on the explainr and catsplainr R packages. Given this department been a pioneer in developing statistical methods for the analysis of genomics data and in data science education, I couldn’t think of a better department to join.\nSS: What are the problems that most excite you right now?\nSH: Within the world of genomics, I’m most excited about open challenges and problems in what’s often referred to as “single-cell genomics”. There is a great potential for using this data that measure features or traits (such as measuring what genes are expressed) in individual cells to help find better diagnoses, prognoses and treatments for many diseases. There are not only open statistical challenges to this data, but also computational challenges. For example, datasets being generated are frequently so large that they cannot even be read into memory. Therefore, one of my projects is to make unsupervised learning methods, such as k-means, scalable to millions of observations (or cells) by combining on-disk data representations (such as HDF5 files) and performing computations on small, random batches of observations so they can be stored in memory and analyzed in a scalable manner.\nSS: Are you working on any non-research data science projects you are excited about? What are they?\nSH: So many! Let’s see, I just focus on one, but happy to talk about more.\nIf you close your eyes and say the word ’teacher’, ’doctor or nurse’, ‘pilot’, ‘chef’: these are all jobs that most people, including children, can easily visualize and conceptualize. I have two young children who easily get inspired from the books that we read. Last year, I went to look for children’s book featuring women in statistics or data science and couldn’t find any. To address this, I’m working with a team of awesome individuals to create a children’s book featuring women in statistics and data science. My goal with this book is to allow for young children to visualize and conceptualize what it means to be a statistician or data scientist. And even more importantly, highlight women to have been trailblazers in these fields, so little girls reading this book may one day be inspired to learn more about statistics and data science and may even choose this career.\nSS: What do you see as the most exciting ways to incorporate data science into academia?\nWithin the world of data science, I’m super interested in finding new ways to teach students how to analyze data in a format that is efficient, effective and scalable. There is a ton of exciting curriculum development happening across the world, and I hope to contribute to that as well. For example, I’m interested in teaching students how to analyze data using the case-study based approach that was pioneered by Deb Nolan and Terry Speed in 1999. My postdoc advisor and I wrote a guide for teaching data science courses based on our teaching experiences. This fall, Roger Peng and I used this approach to teach an Introduction to Data Science course offered in our department at Hopkins (https://jhu-advdatasci.github.io/2018/).\nSS: You started R ladies Baltimore, what is it about that community that inspired you to create the Baltimore branch?\nR-Ladies is an amazing global organization that has a goal of achieving proportionate representation by encouraging, inspiring, and empowering people of genders currently underrepresented in the R community. It was started by Gabriela de Queiroz in San Francisco in October 2012 who wanted to give back to her local community after going to several meetups and learning a lot for free, but saw disparity in gender diversity. Since then R-Ladies has grown to 138 chapters in 44 countries and 39000 members. It is also now funded by the R-Consortium.\nAs an active user and developer of R packages, I know first hand how intimidating it can be to get started using R as both a new user and a gender minority in the R community. I started R-Ladies Baltimore (https://rladies-baltimore.github.io) to create a community in Baltimore for underrepresented minorities to grow their knowledge, experience and skills in R, to confidently ask questions, to learn and support each other, and to contribute new R code and packages for their own projects. We have meetups every two months and try to think creatively about how we can inspire our attendees to learn and use R. One of my favorite meetups was in the fall where we met to make holiday card designs in R (https://rladies-baltimore.github.io/post/making-holiday-cards-in-r-2018/).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:33:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-18-the-tentpoles-of-data-science/",
    "title": "The Tentpoles of Data Science",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2019-01-18",
    "categories": [],
    "contents": "\nWhat makes for a good data scientist? This is a question I asked a long time ago and am still trying to figure out the answer. Seven years ago, I wrote:\n\nI was thinking about the people who I think are really good at data analysis and it occurred to me that they were all people I knew. So I started thinking about people that I don’t know (and there are many) but are equally good at data analysis. This turned out to be much harder than I thought. And I’m sure it’s not because they don’t exist, it’s just because I think good data analysis chops are hard to evaluate from afar using the standard methods by which we evaluate people.\n\nNow that time has passed and I’ve had an opportunity to see what’s going on in the world of data science, what I think about good data scientists, and what seems to make for good data analysis, I have a few more ideas on what makes for a good data scientist. In particular, I think there are broadly five “tentpoles” for a good data scientist. Each tentpole represents a major area of activity that will to some extent be applied in any given data analysis.\nWhen I ask myself the question “What is data science?” I tend to think of the following five components. Data science is\nthe application of design thinking to data problems;\nthe creation and management of workflows for transforming and processing data;\nthe negotiation of human relationships to identify context, allocate resources, and characterize audiences for data analysis products;\nthe application of statistical methods to quantify evidence; and\nthe transformation of data analytic information into coherent narratives and stories\nMy contention is that if you are a good data scientist, then you are good at all five of the tentpoles of data science. Conversely, if you are good at all five tentpoles, then you’ll likely be a good data scientist.\nDesign Thinking\nListeners of my podcast know that Hilary Parker and I are fans of design thinking. Having recently spent eight episodes discussing Nigel Cross’s book Design Thinking, it’s clear I think this is a major component of good data analysis.\nThe main focus here is developing a proper framing of a problem and homing in on the most appropriate question to ask. Many good data scientists are distinguished by their ability to think of a problem in a new way. Figuring out the best way to ask a question requires knowledge and consideration of the audience and what it is they need. I think it’s also important to frame the problem in a way that is personally interesting (if possible) so that you, as the analyst, are encouraged to look at the data analysis as a systems problem. This requires digging into all the details and looking into areas that others who are less interested might overlook. Finally, alternating between divergent and convergent thinking is useful for exploring the problem space via potential solutions (rough sketches), but also synthesizing many ideas and bringing oneself to focus on a specific question.\nAnother important area that design thinking touches is the solicitation of domain knowledge. Many would argue that having domain knowledge is a key part of developing a good data science solution. But I don’t think being a good data scientist is about having specific knowledge of biology, web site traffic, environmental health, or clothing styles. Rather, if you want to have an impact in any of those areas, it’s important to be able to solicit the relevant information—including domain knowledge—for solving the problem at hand. I don’t have a PhD in environmental health sciences, and my knowledge of that area is not at the level of someone who does. But I believe that over my career, I have solicited the relevant information from experts and have learned the key facts that are needed to conduct data science research in this area.\nWorkflows\nOver the past 15 years or so, there has been a growing discussion of the importance of good workflows in the data analysis community. At this point, I’d say a critical job of a data scientist is to develop and manage the workflows for a given data problem. Most likely, it is the data scientist who will be in a position to observe how the data flows through a team or across different pieces of software, and so the data scientist will know how best to manage these transitions. If a data science problem is a systems problem, then the workflow indicates how different pieces of the system talk to each other. While the tools of data analytic workflow management are constantly changing, the importance of the idea persists and staying up-to-date with the best tools is a key part of the job.\nIn the scientific arena the end goal of good workflow management is often reproducibility of the scientific analysis. But good workflow can also be critical for collaboration, team management, and producing good science (as opposed to merely reproducible science). Having a good workflow can also facilitate sharing of data or results, whether it’s with another team at the company or with the public more generally, as in the case of scientific results. Finally, being able to understand and communicate how a given result has been generated through the workflow can be of great importance when problems occur and need to be debugged.\nHuman Relationships\nIn previous posts I’ve discussed the importance of context, resources, and audience for producing a successful data analysis. Being able to grasp all of these things typically involves having good relationships with other people, either within a data science team or outside it. In my experience, poor relationships can often lead to poor work.\nIt’s a rare situation where a data scientist works completely alone, accountable to no one, only presenting to themselves. Usually, resources must be obtained to do the analysis in the first place and the audience (i.e. users, customers, viewers, scientists) must be characterized to understand how a problem should be framed or a question should be asked. All of this will require having relationships with people who can provide the resources or the information that a data scientist needs.\nFailures in data analysis can often be traced back to a breakdown in human relationships and in communication between team members. As the Duke Saga showed us, dramatic failures do not occur because someone didn’t know what a p-value was or how to fit a linear regression. In that particular case, knowledgeable people reviewed the analysis, identified exactly all the serious the problems, raised the issues with the right people, and…were ignored. There is no statistical method that I know of that can prevent disaster from occurring under this circumstance. Unfortunately, for outside observers, it’s usually impossible to see this process happening, and so we tend to attribute failures to the parts that we can see.\nStatistical Methods\nApplying statistical methods is obviously essential to the job of a data scientist. In particular, knowing what methods are most appropriate for different situations and different kinds of data, and which methods are best-suited to answer different kinds of questions. Proper application of statistical methods is clearly important to doing good data analysis, but it’s also important for data scientists to know what methods can be reasonably applied given the constraint on resources. If an analysis must be done by tomorrow, one cannot apply a method that requires two days to complete. However, if the method that requires two days is the only appropriate method, then additional time or resources must be negotiated (thus necessitating good relationships with others).\nI don’t think much more needs to be said here as I think most assume that knowledge of statistical methods is critical to being a good data scientist. That said, one important aspect that falls into this category is the implementation of statistical methods, which can be more or less complex depending on the size of the data. Sophisticated computational algorithms and methods may need to be applied or developed from scratch if a problem is too big to work on off-the-shelf software. In such cases, a good data scientist will need to know how to implement these methods so that the problem can be solved. While it is sometimes necessary to collaborate with an expert in this area who can implement a complex algorithm, this creates a new layer of communication and another relationship that must be properly managed.\nNarratives and Stories\nEven the simplest of analyses can produce an overwhelming amount of results and being able to distill that information into a coherent narrative or story is critical to the success of an analysis. If a great analysis is done, but no one can understand it, did it really happen? Narratives and stories serve as dimension reduction for results and allow an audience to navigate a specified path through the sea of information.\nData scientists have to prioritize what is important and what is not and present things that are relevant to the audience. Part of building a good narrative is choosing the right presentation materials to tell the story, whether they be plots, tables, charts, or text. There is rarely an optimal choice that serves all situations because what works best will be highly audience- and context-dependent. Data scientists need to be able to “read the room”, so to speak, and make the appropriate choices. Many times, when I’ve seen critiques of data analyses, it’s not the analysis that is being criticized but rather the choice of narrative. If the data scientist chooses to emphasize one aspect but the audience thinks another aspect is more important, the analysis will seem “wrong” even though the application of the methods to the data is correct.\nA hallmark of good communication about a data analysis is providing a way for the audience to reason about the data and to understand how the data are tied to the result. This is a data analysis after all, and we should be able to see for ourselves how the data inform the conclusion. As an audience member in this situation, I’m not as interested in just trusting the presenter and their conclusions.\nDescribing a Good Data Scientist\nWhen thinking of some of the best data scientists I’ve known over the years, I think they are all good at the five tentpoles I’ve described above. However, what about the converse? If you met someone who demonstrated that they were good at these five tentpoles, would you think they were a good data scientist? I think the answer is yes, and to get a sense of this, one need look no further than a typical job advertisement for a data science position.\nI recently saw this job ad from my Johns Hopkins colleague Elana Fertig. She works in the area of computational biology and her work involves analyzing large quantities of data to draw connections between people’s genes and cancer (if I may make a gross oversimplification). She is looking for a postdoctoral fellow to join her lab and the requirements listed for the position are typical of many ads of this type:\nPhD in computational biology, biostatistics, biomedical engineering, applied mathematics, or a related field.\nProficiency in programming with R/Bioconductor and/or python for genomics analysis.\nExperience with high-performance computing clusters and LINUX scripting.\nTechniques for reproducible research and version control, including but not limited to experience generating knitr reports, GitHub repositories, and R package development.\nProblem-solving skills and independence.\nThe ability to work as part of a multidisciplinary team.\nExcellent written and verbal communication skills.\nThis is a job where complex statistical methods will be applied to large biological datasets. As a result, knowledge of the methods or the biology will be useful, and knowing how to implement these methods on a large scale (i.e. via cluster computing) will be important. Knowing techniques for reproducible research requires knowledge of the proper workflows and how to manage them throughout an analysis. Problem-solving skills is practically synonymous with design thinking; working as part of a multidisciplinary team requires negotiating human relationships; and developing narratives and stories requires excellent written and verbal communication skills.\nSummary\nA good data scientist can be hard to find, and part of the reason is because being a good data scientist requires mastering skills in a wide range of areas. However, these five tentpoles are not haphazardly chosen; rather they reflect the interwoven set of skills that are needed to solve complex data problems. Focusing on being good at these five tentpoles means sacrificing time spent studying other things. To the extent that we can coalesce around the idea of convincing people to do exactly that, data science will become a distinct field with its own identity and vision.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:33:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-09-how-data-scientists-think-a-mini-case-study/",
    "title": "How Data Scientists Think - A Mini Case Study",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2019-01-09",
    "categories": [],
    "contents": "\nIn episode 71 of Not So Standard Deviations, Hilary Parker and I inaugurated our first “Data Science Design Challenge” segment where we discussed how we would solve a given problem using data science. The idea with calling it a “design challenge” was to contrast it with common “hackathon” type models where you are presented with an already-collected dataset and then challenged to find something interesting in the data. Here, we wanted to start with a problem and then talk about how data might be collected and analyzed to address the problem. While both approaches might result in the same end-product, they address the various problems you encounter in a data analysis in a different order.\nIn this post, I want to break down our discussion of the challenge and highlight some of the issues that were discussed in framing the problem and in designing the data collection and analysis. I’ll end with some thoughts about generalizing this approach to other problems.\nYou can download an MP3 of this segment of the episode (it is about 45 minutes long) or you can read the transcript of the segment. If you’d prefer to stream the segment you can start listening here.\nThe Brief\nThe general goal was to learn more about the time it takes for each of us to commute to work. Hilary lives in San Francisco and I live in Baltimore, so the characteristics of our commutes are very different. She walks and takes public transit; I drive most days. We also wanted to discuss how we might collect data on our commute times in a systematic, but not intrusive, manner. When we originally discussed having this segment, this vague description was about the level of specification that we started with, so an initial major task was to\nDevelop a better understanding of what question each of us was trying to answer;\nFrame the problem in a manner that could be translated into a data collection task; and\nSketch out a feasible statistical analysis.\nFraming the Problem\nHilary and I go through a few rounds of discussion on the topic of how to think about this problem and the questions that we’re trying to answer. Early in the discussion Hilary mentions that this problem was “pressing on my mind” and that she took a particular interest in seeing the data and acting on it. Her intense interest in the problem potentially drove part of her creativity in developing solutions.\nHilary initially mentions that the goal is to understand the variation in commute times (i.e. estimate the variance), but then quickly shifts to the problem of estimating average commute times for the two different commute methods that she uses.\n\nHILARY: you…maybe only have one commute method and you want to understand the variance of that. So…what range of times does it usually take for me to get to work, or…I have two alternatives for my commute methods. So it might be like how long does it take me in this way versus that way? And for me the motivation is that I want to make sure I know when to leave so that I make it to meetings on time….\n\nIn her mind, the question being answered by this data collection is, “When should I leave home to get to meetings on time?” At this point she mentions two possible ways to think about addressing this question.\nEstimate the variability of commute times and leave the house accordingly; or\nCompare two different commute methods and then choose a method on any given day.\nRight off the bat, Hilary notes that she doesn’t actually do this commute as often as she’d thought. Between working from home, taking care of chores in the morning, making stops on the way, and walking/talking with friends, a lot of variation can be introduced in to the data.\nI mention that “going to work” and “going home”, while both can be thought of as commutes, are not the same thing and that we might be interested in one more than the other. Hilary agrees that they are different problems but they are both potentially of interest.\nQuestion/Intervention Duality\nAt this point I mention that my commuting is also affected by various other factors and that on different days of the week, I have a different commute pattern. On days where I drop my son off at school, I have less control over when I leave compared to days when I drive straight to work. Here, we realize a fundamental issue, which is that different days of the week indicate somewhat different interventions to take:\nOn days where I drive straight to work, the question is “When should I leave to arrive on time for the first meeting?”\nOn days where I drop my son off at school, the question is “When is the earliest time that I can schedule the first meeting of the day?”\nIn the former situation I have control over, and could potentially intervene on, when I leave the house, whereas in the latter situation I have control over when I schedule the first meeting. While these are distinct questions with different implications, at this point they may both require collecting travel time data in the same manner.\nEarlier in this section I mention that on days when I drop my son off at school it can take 45 minutes to get to work. Hilary challenges this observation and mentions that “Baltimore is not that big”. She makes use of her knowledge of Baltimore geography to suggest that this is unexpected. However, I mention that the need to use local roads exclusively for this particular commute route makes it indeed take longer than one might expect.\nDesigning the Data Collection System\nIn discussing the design of her data collection system, Hilary first mentions that a podcast listener had emailed in and mentioned his use of Google Maps to predict travel times based on phone location data. While this seemed like a reasonable idea, it ultimately was not the direction she took.\n\nHILARY: At first I was thinking about that because I have location history on and I look at it a lot, but there’s also a fair degree of uncertainty there. Like sometimes it just puts me in these really weird spots or…I lose GPS signal when I go underground and also I do not know how to get data in an API sense from that. So I knew it would be manual data. In order to analyze the data, I would have to go back and be like let me go and collect the data from this measurement device. So I was trying to figure out what I could use instead.\n\nThen she describes how she can use Wi-Fi connections (and dis-connections) to serve as surrogates for leaving and arriving.\n\nAnd at some point I realized that two things that reliably happen every time I do a commute is that my phone disconnects from my home Wi-Fi. And then it reconnects to my work Wi-Fi. And so I spent some time trying to figure out if I could log that information, like if there’s an app that logged that, and there is not. But, there is a program called If This, Then That, or an app. And so with that you can say “when my phone disconnects from Wi-Fi do something”, and you can set it to a specific Wi-Fi. So that was exciting.\n\nOther problems that needed solving were:\nWhere to store the data. Hilary mentions that a colleague was using Airtable (a kind of cloud-based spreadsheet/database) and decided to give it a try.\nIndicating commute method. Hilary created a system where she could send a text message containing a keyword about her commute method to a service that would then log the information to the table collecting the travel time data.\nMultiple Wi-Fi connects. Because her phone was constantly connecting and disconnecting from Wi-Fi at work, she had to define the “first connection to Wi-Fi at work” as meaning that she had arrived at work.\nSensing a Wi-Fi disconnect. Hilary’s phone had to be “awake” in order to sense a Wi-Fi disconnect, which was generally the case, but not always. There was no way to force her phone to always be awake, but she knew that the system would send her a push notification when it had been triggered. Therefore, she would at least know that if she didn’t receive a push notification, then something had gone wrong.\nHilary mentions that much of the up front effort is important in order to avoid messy data manipulations later on.\n\nHILARY: I think I’ll end up—I did not do the analysis yet but I’ll end up having to scrub the data. So I was trying to avoid manual data scrubbing, but I think I’m going to have to do it anyway.\n\nUltimately, it is impossible to avoid all data manipulation problems.\nSpecifying the Data\nWhat exactly are the data that we will be collecting? What are the covariates that we need to help us understand and model the commute times? Obvious candidates are\nThe start time for the commute (date/time format, but see below)\nThe end time for the commute (date/time)\nIndicator of whether we are going to work or going home (categorical)\nCommute method (categorical)\nHilary notes that from the start/end times we can get things like day of the week and time of day (e.g. via the lubridate package). She also notes that her system doesn’t exactly produce date/time data, but rather a text sentence that includes the date/time embedded within. Thankfully, that can be systematically dealt with using simple string processing functions.\nA question arises about whether a separate variable should be created to capture “special circumstances” while commuting. In the data analysis, we may want to exclude days where we know something special happened to make the commute much longer than we might have expected (e.g. we happened to see a friend along the way or we decided to stop at Walgreens). The question here is\n\nAre these special circumstances part of the natural variation in the commute time that we want to capture, or are they “one-time events” that are in some sense predictable?\n\nA more statistical way of asking the question might be, do these special circumstances represent fixed or random variation? If they are random and essentially uncontrollable events, then we would want to include that in the random portion of any model. However, if they are predictable (and perhaps controllable) events, then we might want to think of them as another covariate.\nWhile Hilary believes that she ultimately does have control over whether these time-consuming detours occur or not, she decides to model them as essentially random variation and that these events should be lumped in with the natural variation in the data.\nSpecifying the Treatment\nAt this point in the discussion there is a question regarding what effect we are trying to learn about. The issue is that sometimes changes to a commute have to be made on the fly to respond to unexpected events. For example, if the public transportation system breaks down, you might have to go on foot.\n\nROGER: Well it becomes like a compliance situation, right? Like you can say, do you want to know how long does it take when you take MUNI or how long does it take when you intend to take MUNI?\n\nIn this section I mention that it’s like a “compliance problem”. In clinical trials, for example when testing a new drug versus a placebo, it is possible to have a situation where people in the treatment group of the study are given the new drug but do not actually take it. Maybe the drug has side effects or is inconvenient to take. Whatever the reason, they are not complying with the protocol of the study, which states that everyone in the treatment group takes the new drug. The question then is whether you want to use the data from the clinical trial to understand the actual effect of the drug or if you want to understand the effect of telling someone to take the drug. The latter effect is often referred to as the intention to treat effect while the former is sometimes called the complier average effect. Both are valid effects to estimate and have different implications in terms of next steps.\nIn the context of Hilary’s problem, we want to estimate the average commute time for each commute method. However, what happens if Muni experiences some failure that requires altering the commute method? The potential “compliance issue” here is whether Muni works properly or not. If it does not, then Hilary may take some alternate route to work, even though she intended to take Muni. Whether Muni works properly or not is a kind of “post-treatment variable” because it’s not under the direct control of Hilary and its outcome is only known after she decides on which commute method she is going to take (i.e. the “treatment”). Now a choice must be made: Do we estimate the average commute time when taking Muni or the average commute time when she intends to take Muni, even if she has to divert to an alternate route?\nHilary and I both seem to agree that the intention to treat effect is the one we want to estimate in the commute time problem. The reason is that the estimation of this effect has direct implications for the thing that we have control over: choosing which commute method to use. While it might be interesting from a scientific perspective to know the average commute time when taking Muni, regardless of whether we intended to take it or now, we have no control over the operation of Muni on any given day.\nStarting from the End\nI ask Hilary, suppose we have the data, what might we do with it? Specifically, suppose that we estimate for a given commute method that the average time is 20 minutes and the standard deviation is 5 minutes. What “intervention” would that lead us to take? What might we do differently from before when we had no systematically collected data?\nHilary answers by saying that we can designate a time to leave work based on the mean and standard deviation. For example, if we have to be at work at 9am we might leave at 8:35am (mean + 1 standard deviation) to ensure we’ll be arrive at 9am most of the time. In her answer, Hilary raises an important, but perhaps uncomfortable, consideration:\n\nHILARY: I think in a completely crass world for example, I would choose different cutoffs based on the importance of a meeting. And I think people do this. So if you have a super important meeting, this is like a career-making meeting, you leave like an hour early…. And so, there you’re like “I am going to do three standard deviations above the mean” so…it’s very unlikely that I’ll show up outside of the time I predicted. But then if it’s like a touch-base with someone where you have a really strong relationship and they know that you value their time, then maybe you only do like one standard deviation.\n\nLater I mention one implication for statistical modeling:\n\nRoger: Well and I feel like…the discussion of the distribution is interesting because it might come down to like, what do you think the tail of the distribution looks like? So what’s the worst case? Because if you want to minimize the worst case scenario, then you really, really need to know like what that tail looks like.\n\nThinking about what the data will ultimately be used for raises two important statistical considerations:\nWe should think about the extremes/tails of the distribution and develop cutoffs that determine what time we should leave for work.\nThe cutoffs at the tail of the distribution might be dependent on the “importance” of the first meeting of the day, suggesting the existence of a cost function that quantifies the importance of arriving on time.\nHilary raises a hard truth, which is that not everyone gets the same consideration when it comes to showing up on time. For an important meeting, we might allow for “three standard deviations” more than the mean travel time to ensure some margin of safety for arriving on time. However, for a more routine meeting, we might just provide for one standard deviation of travel time and let natural variation take its course for better or for worse.\nStatistical Modeling Considerations\nI mention that thinking about our imaginary data in terms of “mean” and “standard deviation” implies that the data have a distribution that is akin to a Normal distribution. However, given that the data will consist of travel times, which are always positive, a Normal distribution (which allows positive and negative numbers) may not be the most appropriate. Alternatives are the Gamma or the log-Normal distribution which are strictly positive. I mention that the log-Normal distribution allows for some fairly extreme events, to which Hilary responds that such behavior may in fact be appropriate for these data due to the near-catastrophic nature of Muni failures (San Francisco residents can feel free to chime in here).\nFrom the previous discussion on what we might do with this data, it’s clear that the right tail of the distribution will be important in this analysis. We want to know what the “worst case scenario” might be in terms of total commute time. However, by its very nature, extreme data are rare, and so there will be very few data points that can be used to inform the shape of the distribution in this area (as opposed to the middle of the distribution where we will have many observations). Therefore, it’s likely that our choice of model (Gamma, log-Normal, etc.) will have a big influence on the predictions that we make about commute times in the future.\nStudy Design Considerations\nTowards the end I ask Hilary how much data is needed for this project? However, before asking I should have discussed the nature of the study itself:\nIs it a fixed study designed to answer a specific question (i.e. what is the mean commute time?) within some bound of uncertainty? Or\nIs it an ongoing study where data will be continuously collected and actions will be continuously adapted as new data are collected\nHilary suggests that it is the latter and that she will simply collect data and make decisions as she goes. However, it’s clear that the time frame is not “forever” because the method of data collection is not zero cost. Therefore, at some point the costs of collecting data will likely be too great in light of any perceived benefit.\nDiscussion\nWhat have we learned from all of this? Most likely, the problem of estimating commute times is not relevant to everybody. But I think there are aspects of the process described above that illustrate how the data analytic process works before data collection begins (yes, data analysis includes parts where there are no data). These aspects can be lifted from this particular example and generalized to other data analyses. In this section I will discuss some of these aspects and describe why they may be relevant to other analyses.\nPersonal Interest and Knowledge\nHilary makes clear that she is very personally interested in this problem and in developing a solution. She wants to apply any knowledge learned from the data to her everyday life. In addition, she used her knowledge of Baltimore geography (from having lived there previously) to challenge my “mental data analysis”.\nTaking a strong personal interest in a problem is not always an option, but it can be very useful. Part of the reason is that it can allow you to see the “whole problem” and all of its facets without much additional effort. An uninterested person can certainly learn all the facets of a problem, but it will seem more laborious. If you are genuinely interested in the subject of a problem, then you will be highly motivated to learn everything about that problem, which will likely benefit you in the data analysis. To the extent that data analysis is a systems problem with many interacting parts, it helps to learn as much as possible about the system. Being interested in knowing how the system works is a key advantage you can bring to any problem.\nIn my own teaching, I have found that students who are keenly interested in the problems they’re working on often do well in the data analysis. Partly, this is because they are more willing to dig into the nitty gritty of the data and modeling and to uncover small details that others may not find. Also, students with a strong interest often have strong expectations about what the data should show. If the data turn out to be different from what they are expecting, that surprise is often an important experience, sometimes even delightful. Students with a more distant relationship with the topic or the data can never be surprised because they have little in the way of expectations.\nProblem Framing\nFrom the discussion it seems clear that we are interested in both the characteristics of different commute methods and the variability associated with individual commute methods. Statistically, these are two separate problems that can be addressed through data collection and analysis. As part of trying to frame the problem, we iterate through a few different scenarios and questions.\nOne concept that we return to periodically in the discussion is the idea that every question has associated with it a potential intervention. So when I ask “What is the variability in my commute time”, a potential intervention is changing the time when I leave home. Another potential intervention is rescheduling my first meeting of the day. Thinking about questions in terms of their potential interventions can be very useful in prioritizing which questions are most interesting to ask. If the potential intervention associated with a question is something you do not have any control over, then maybe that question is not so interesting for you to ask. For example, if you do not control your own schedule at work, then “rescheduling the first meeting of the day” is not an option for you. However, you may still be able to control when you leave home.\nWith the question “How long does it take to commute by Muni?” one might characterize the potential intervention as “Taking Muni to work or not”. However, if Muni breaks down, then that is out of your control and you simply cannot take that choice. A more useful question then is “How long does it take to commute when I choose to take Muni?” This difference may seem subtle, but it does imply a different analysis and is associated with a potential intervention that is completely controllable. I may not be able to take Muni everyday, but I can definitely choose to take it everyday.\nThe last point I want to make here is related to the concept of taking a personal interest in a problem. If a data analysis problem can be framed in such a manner that it becomes more personally interesting, then perhaps that’s the route that should be taken, if at all possible. Often, we think that there is “one true way” to approach a problem or ask a question. But usually, there are a variety of different approaches and you should try to take the one that seems most interesting you.\nFixed and Random Variation\nDeciding what is fixed variation and what is random is important at the design stage because it can have implications for data collection, data analysis, and the usefulness of the results. Sometimes this discussion can get very abstract, resulting in questions like “What is the meaning of ‘random’?”. It’s important not to get too bogged down in philosophical discussions (although the occasional one is fine). But it’s nevertheless useful to have such a discussion so that you can properly model the data later.\nClassifying everything as “random” is a common crutch that people use because it gives you an excuse to not really collect much data. This is a cheap way to do things, but it also leads to data with a lot of variability, possibility to the point of not even being useful. For example, if we only collect data on commute times, and ignored the fact that we have multiple commute methods, then we might see a bimodal distribution in the commute time data. But that mysterious bi-modality could be explained by the different commute methods, a fixed effect that is easily controlled. Taking the extra effort to track the commute method (for example, via Hilary’s text message approach) along with the commute time could dramatically reduce the residual variance in the data, making for more precise predictions.\nThat said, capturing every variable is often not feasible and so choices have to made. In this example, Hilary decided not to track whether she wandered into Walgreens or not because that event did have a random flavor to it. Practically speaking, it would be better to account for the fact that there may be an occasional random excursion into Walgreens rather than to attempt to control it every single time. This choice also simplifies the data collection system.\nSketch Models\nWhen considering what to do with the data once we had it, it turned out that mitigating the worst case scenario was a key consideration. This translated directly into a statistical model that potentially had heavy tails. At this point, it wasn’t clear what that distribution would be, and it isn’t clear whether the data would be able to accurately inform the shape of the tail’s distribution. That said, with this statistical model in mind we can keep an eye on the data as they come in and see how they shape up. Further, although we didn’t got through the exercise, it could be useful to estimate how many observations you might need in order to get a decent estimate of any model parameters. Such an exercise cannot really be done if you don’t have a specific model in mind.\nIn general, having a specific statistical model in mind is useful because it gives you a sense of what to expect. If the data come in and look substantially different from the distribution that you originally considered, then that should lead you to ask why do the data look different? Asking such a question may lead to interesting new details or uncover aspects of the data that hadn’t been considered before. For example, I originally thought the data could be modeled with a Gamma distribution. However, if the data came in and there were many long delays in Hilary’s commute, then her log-Normal distribution might seem more sensible. Her choice of that distribution from the beginning was informed by her knowledge of public transport in San Francisco, about which I know nothing.\nSummary\nI have spoken with people who argue that are little in the way of generalizable concepts in data analysis because every data analysis is uniquely different from every other. However, I think this experience of observing myself talk with Hilary about this small example suggests to me that there are some general concepts. Things like gauging your personal interest in the problem could be useful in managing potential resources dedicated to an analysis, and I think considering fixed and random variation is important aspect of any data analytic design or analysis. Finally, developing a sketch (statistical) model before the data are in hand can be useful for setting expectations and for setting a benchmark for when to be surprised or skeptical.\nOne problem with learning data analysis is that we rarely, as students, get to observe the thought process that occurs at the early stages. In part, that is why I think many call for more experiential learning in data analysis, because the only way to see the process is to do the process. But I think we could invest more time and effort into recording some of these processes, even in somewhat artificial situations like this one, in order to abstract out any generalizable concepts and advice. Such summaries and abstractions could serve as useful data analysis texts, allowing people to grasp the general concepts of analysis while using the time dedicated to experiential learning for studying the unique details of their problem.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:33:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-19-the-netflix-data-war/",
    "title": "The Netflix Data War",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-12-19",
    "categories": [],
    "contents": "\nA recent article in the Wall Street Journal, “At Netflix, Who Wins When It’s Hollywood vs. the Algorithm?” by Shalini Ramachandran and Joe Flint details some of the internal debates within Netflix between the Los Angeles-based content team, which is in charge of developing and marketing new content for the streaming service, and the data team. The initial example described is an advertising image for a new show (“Grace and Frankie”“) starring Jane Fonda and Lily Tomlin.\n\nAfter the streaming-video giant released the second season of the comedy “Grace and Frankie” in 2016, its product team put up an image to promote the show to U.S. subscribers that only included Ms. Fonda’s co-star, Lily Tomlin. Tests showed that more users clicked on the show when the photo didn’t include Ms. Fonda.\n\n\nThe decision set off a high-pitched internal debate. The Los Angeles-based content team was concerned that Netflix risked alienating Ms. Fonda, and that the move could even violate her contract, while the tech group in the Los Gatos, Calif., headquarters argued the company shouldn’t ignore the data, according to people familiar with the discussions.\n\n\nIn the end, Netflix chose to put images that included Ms. Fonda back in the mix.\n\nWith the caveat that news articles about internal workings of companies always only contain a sliver of what actually happened, I thought it was a useful place to launch a discussion about the activity of a data team and how it interfaces with other aspects of a company.\nFirst off, anytime an article comes out like this detailing internal conflict, the first question to ask is always “Why is this article appearing now?” Most likely, it’s because one side feels like they are losing and is therefore talking to the press. My guess in this case, given the above-quoted example, is that the data team is losing some battles. Why is that? Don’t the data speak the truth? Why won’t people listen??\nThe Whole Truth (or is it?)\nIt’s difficult to know what the data team did to analyze the situation here, but it probably went something like this:\n“Gracie and Frankie” ran its first season prior to 2016 and so Netflix therefore had a season’s worth of data to look at, including how it was marketed and what was the response to the marketing\nDuring that time, they likely A/B tested a number of marketing images of the show containing various combinations of the star actors.\nCustomers clicked on these different marketing images at different rates and presumably went on to watch the show at some rate.\nThe product team chose the marketing image that maximized the number of clicks.\nThis is where the story picks up. The data/product team proposed to use a marketing image that only includes Tomlin and the content team objected. What are the reasons for the objection? According to the article,\nPotential contract violation concerning not putting Jane Fonda in the marketing image; and\nDamaging a relationship with (i.e. “alienating”) a Hollywood star.\nMore guessing here, but the data team likely did not know the terms of the company’s contract with Fonda and probably did not consider the concern of maintaining a long-term relationship with an actor. The former is simply a factual issue that was likely not disclosed to the data team and the latter is vague and difficult to quantify even if it had been disclosed to the data team.\nSo, from a business perspective, what is the correct decision? What’s a data analyst at Netflix supposed to do? What is a successful data analysis in this case?\nGiven that the final marketing image did include Fonda, it would seem like the data analysis was unsuccessful because the ultimate decision went against “what the data said”. On the other hand, the data (and the analysts) didn’t know anything about the contract terms and the need for a relationship.\nThe Whole Truth (and nothing but?)\nI don’t think it’s right to declare data analytic failure when a decision seems to go against what the data recommend. In a previous article on Netflix (“Inside the Binge Factory”, Vulture reporter Josef Adalian talked to Cindy Holland and Ted Sarandos, who are in charge of content at Netflix:\n\nSarandos and Holland tell me, again, that while data is a tool for them, their various projection models and cost analyses don’t dictate their decisions. “It’s 70 percent gut and 30 percent data,” Sarandos says. “Most of it is informed hunches and intuition. Data either reinforces your worst notion or it just supports what you want to do, either way.” The company also sometimes ignores the data if executives have enough passion for a new project, Sarandos says, calling such cases “forward bets, where you go to a full season even though the model’s not quite there.”\n\nNow, you might argue that those are Hollywood people claiming that the way “they’ve always done it” is the best way. But say what you will about Holland’s and Sarandos’s breakdown, in almost any decision-making situation involving data, there is some non-zero percentage of the process that involves “gut”. The reason is because not all information about a process can be incorporated into a data analysis, and it’s important for data analysts to realize that.\nOne reaction to the example described above might be to say that the data team should be privy to more information. They should have the contract terms and they should know everything about the company’s relationship with the talent on shows. But the truth is not all information is equally well-measured, well-characterized, and well-quantified. It would be misleading for a data analyst to suggest that they could incorporate all sources of information equally well. Yes, there are methods to balance differing levels of uncertainty, but some measurements are simply not good measurements.\nIn the example above, there’s no evidence in the reporting that the content team didn’t believe the data or the analysis. It’s just that their fear of damaging a relationship with an actor overruled whatever desire they might have had to maximize clicks or views. The logic was probably along the lines of “We may take a hit in the short-run but we will benefit from this relationship in the long-run.” Whether that’s true or not is unclear, but it’s a tricky question to answer with data. It’s not even clear to me how you would formulate that question.\nUltimately, the data analysis got ignored in this case, which is something that can in fact happen. Organizations that don’t look at data can hardly be called data driven, but organizations that look at data and ignore it are basically in the same boat. How much weight should a data analysis be assigned, given that no analysis can include all the information that plays a role in a decision? Are Holland and Sarandos right that it’s 30%?\nRelationships\nIn any data analysis, there are aspects over which the analyst will not have control. In a company driven to create new content, the people in charge of making that content will likely want to have some say over how it’s done.\n\nSome shows at risk of being canceled due to poor performance have gotten a reprieve because Netflix doesn’t want to damage relationships with key producers or actors, people familiar with Netflix’s deliberations say. Stars have inserted language in their contracts giving them approval over everything from the short video that plays when users hover over a photo to the trailers promoting Netflix shows and movies. [emphasis added]\n\nShould Netflix not deal with showrunners who want to have control over their shows because their decisions might come into conflict with the data? It’s possible that Netflix is missing out on better showrunners out there who are willing to submit to such contract terms, but then it’s not clear to me that they want to work with people who don’t really care that strongly about their shows. Part of the problem is that Netflix isn’t the only game in town, and everyone is competing for good talent. Netflix’s strategy is to mop up as much good talent as possible to create as many shows as possible.\nFinally, I thought this dichotomization of the content team and the tech/data team was interesting:\n\nJosh Evans, a former Netflix technology executive, said that while the tech team is more “data-driven and analytical” and the Hollywood side more “relationship-oriented” the two sides manage to reach common ground.\n\nTo me it seems like both the tech and content teams could afford to be a bit more “relationship-driven” with each other.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:33:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-11-the-role-of-theory-in-data-analysis/",
    "title": "The Role of Theory in Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-12-11",
    "categories": [],
    "contents": "\nIn data analysis, we make use of a lot of theory, whether we like to admit it or not. In a traditional statistical training, things like the central limit theorem and the law of large numbers (and their many variations) are deeply baked into our heads. I probably use the central limit theorem everyday in my work, sometimes for the better, and sometimes for the worse. Even if I’m not directly applying a Normal approximation, knowledge of the central limit theorem will often guide my thinking and help me to decide what to do in a given data analytic situation.\nTheorems like the central limit theorem or the law of large numbers ultimately tell us something about the world around us by serving as models. When we say “Let X1, X2, … be random variables”, we’re usually thinking of X1 and X2 as abstract representations of real world phenomena. They might be people, or time points in a time series, or cars on the road. What’s nice about the theory here is that with a single statement (and a critical set of assumptions), we can make general statements about the world, regardless of whether we’re talking about people or cars. In this sense, statistical theory is analogous to scientific theory, which also tries to make general statements about the world. Statistical theory also contains statements about the tools we use, to help us understand their behavior and their properties. Most of this theory is a combination of well-understood mathematical transformations (derivatives, integrals, approximations) and models of the physical world.\nOther Theories\nThere are other kinds of theory and often their role is not to make general statements about the natural world. Rather, their goal is to provide quasi-general summaries of what is commonly done, or what might be typical. So instead of making statements along the lines of “X is true”, the aim is to make statements like “X is most common”. Often, those statements can be made because there is a written record of what was done in the past and the practitioners in the area have a collective memory of what works and what doesn’t.\nOn War\nIn his book On War, Carl von Clauswitz writes at length about the role of theory in warfare. What’s the point of discussing war in the abstract when the reality of war is complicated and highly dependent on the facts on the ground? He sees theory in warfare as a form of training, “a compression of the past transmitting experience, while making minimal claims about the future.”\nClauswitz did not envision theory as commanding a general to do something, but rather as an efficient summary of what had worked (or not worked) before. The only alternative to having a theory in this case would be for each general to re-learn the material from scratch. In the practice of warfare, such an approach could easily lead to death. The language of Clauswitz is reminiscent of John Tukey. In his 1962 paper The Future of Data Analysis, Tukey writes that theory should “guide, not command” the practice of data analysis.\nOn Music\nAnother area that contains a significant amount of theory is music. As I’m a bit more familiar with this area than with warfare, I will draw out a few more examples here. Music theory is largely a descriptive practice that attempts to draw out underlying patterns that appear in various forms and instances of music. Out of music theory we get things like “Sonata Form”, which says that a piece of music has an exposition, a development, and a recapitulation. This form is very common in western classical music and has strong ties to written work.\nWe also get tonal harmonic analysis, which provides a language for describing the harmonic transitions in a piece of music. For example, most western music has a “key signature”, which can be thought of as the primary or “tonic” chord (C-major, for example). All other chords in the scale revolve around that primary chord. These chords are usually referred to using Roman numerals, so the primary or tonic chord is denoted with Roman numeral I. In most pieces of music, the commonly used chords are the tonic (the I chord), the dominant (the V chord), and the sub-dominant (the IV chord). The harmonic pattern of I-IV-V chords is instantly recognizable in many forms of western music written across centuries. We can find chorales written by Johann Sebastian Bach that follow similar harmonic patterns as songs written by The Beatles. In this way, the tonal theory of harmony allows us to draw connections between very disparate pieces of music.\nOne thing that the tonal theory of harmony does not give us is a recipe for what to do when creating new music. Simply knowing the theory does not make one a great composer. It’s tempting to think that the theory of harmony is formulated in a manner that tells us what sounds good and what doesn’t (and sometimes it is taught this way), but this is misleading. A rote application of the theory will lead to something that is passable, but likely not very inspiring, as you will essentially end up with a reformulation of what has been done before. Part of what makes music great is that it is new and different from what came before.\nArnold Schoenberg, in his textbook Harmonielehre, argued strongly against the idea that there were certain forms of music that inherently “sounded good” versus those that “sounded bad”. His thinking was not that the theory of music tells us what sounds good versus bad but rather tells us what is commonly done versus not commonly done. One could infer that things that are commonly done are therefore good, but that would be an individual judgment and not an inherent aspect of the theory.\nKnowledge of music theory is useful if only because it provides an efficient summary of what is expected. You can’t break the rules if you don’t know what the rules are. Creating things that are surprising or unexpected or interesting relies critically on knowing what your audience is expecting to hear. The reason why Schoenberg’s “atonal” style of music sounds so different is because his audiences were expecting music written in the more common tonal style. Sometimes, we can rely on music theory to help us avoid a specific chord progression (e.g. parallel fifths) because that “sounds bad”, but what we really mean is that such a pattern is not commonly used and is perhaps unexpected. So if you’re going to do it, feel free, but it should be for a good reason.\nHumans in the Loop\nIn both examples of theory presented here–warfare and music–the theory only takes you so far, perhaps frustratingly so. Both theories leave room for a substantial human presence and for critical human decision-making. In warfare, Clauswitz acknowledges that there are significant uncertainties that every general encounters and that the role of any theory should be to “educate the mind of the future commander…not to accompany him to the battlefield.”\nSimilarly in music, theory provides an efficient way to summarize the useful aspects of what has come before in a manner that can be fit into something like a semester-long course. It also provides a language for describing characteristics of music and for communicating similarities and differences across musical pieces. But when it comes to the creation of new music, theory can only provide the foundation; the composer must ultimately build the house.\nWhat Does a Theory of Data Analysis Look Like?\nIf asked to make generally true statements about data analysis, I think most practitioners would struggle. Data analysis relies critically on the details. How could one make a statement that was true for all data analyses when the details differ so much between analyses? And yet, often one person is capable of analyzing data from vastly different domains. Two people who do data analysis in different areas can still have things to talk about related to their work. What are the things that transfer across domains?\nOne obvious candidate is the methods themselves. A linear regression in biology is the same linear regression in economics. If I am an economist, I may not know much about biology but I could still explain the concepts of linear regression. Similarly, a scatterplot is the same no matter what field it is applied to, even if the things being plotted are different. So the bread and butter of statistics, the study of methods of data analysis, is important. Yet, in my experience, people with solid training in a wide array of statistical methods can still be poor data analysts. In that case, what are they missing?\nAt this point, many would argue that what is missing is the “experience of doing data analysis”. In other words, data analysis is learned through doing it. Okay, fine, but what exactly is it that we are learning? It’s worth spending a little time asking this question and considering possible answers because any theory of data analysis would include the answers to this question.\nGeneral Statements\nThe closest thing to a general statement about data analysis that I can come up with is a successful data analysis is reproducible. (Note that I do not believe the converse is true.) The concept of reproducibility, whereby code and data accompany a data analysis so that others can re-create the results, has developed over more than 20 years and has only grown in importance. With the increase in computational power and data collection technology, it is essentially impossible to rely on written representations of data analysis. The only way to truly know what has happened to produce a result is to look at the code and perhaps run it yourself.\nWhen I wrote my first paper on reproducibility in 2006 the reaction was hardly one of universal agreement. But today, I think many would see the statement above as true. What has changed? Mostly, data analysts in the field have gained substantially more experience with complex data analyses and have increasingly been bitten by the non-reproducibility of certain analyses. With experience, both good and bad, we can come to an understanding of what works and what doesn’t. Reproducibility works as a mechanism to communicate what was done, it isn’t too burdensome if it’s considered from the beginning of an analysis, and as a by-product it can make data available to others for different uses.\nThere is no need for a new data analyst to learn about reproducibility “from experience”. We don’t need to lead a junior data analyst down a months-long winding path of non-reproducible analyses until they are finally bitten by non-reproducibility (and therefore “learn their lesson”). We can just tell them\n\nIn the past, we’ve found it useful to make our data analyses reproducible. Here’s a workflow to guide you in your own analyses.\n\nWith that one statement, we can “compress” over 20 years of experience.\nAnother statement I can think of that is applicable to most data analyses is to discover the data generating mechanism. When I talk to other data analysts, one of the favorite category of “war stories” to tell is the one where you were bitten by some detail in data collection that went unnoticed. Many data analysts are not involved in the data collection process or the experimental design and so it is important to inquire about he process by which the data came to them.\nFor example, one person told me a story of an analysis she did on a laboratory experiment that was ostensibly simple (basically, a t-test). But when she visited the lab one day to see how the experiments were done, she discovered that the experimental units were all processed in one batch and the control units were all processed in a different batch at a different time, thereby confounding any treatment effect with the batch. There’s not much a data analysis can do to rescue that situation and it’s good for the analyst to know that before spending a lot of time considering the right methodology.\nI’ve written previously about the Law & Order principle of data science where a data analyst must retrace the footsteps of the data to see how they were generated. Such an activity is time-consuming, but I’ve never come across a situation where it was actively detrimental to the analysis. At worst, it’s interesting knowledge to have but it plays no critical role in designing the ultimate analysis.\nMost analysts I know have indeed learned “through experience” the dangers of not being informed of the data generating mechanism. But it seems like a waste of time to force new analysts to go through the same experience, as if it were some sort of fraternity hazing ritual. Why not just tell them?\nTheory Principles\nAt this point, I think a theory of data analysis would look more like music than it would like physics or mathematics. Rather than produce general truths about the natural world, a theory of data analysis would provide useful summaries of what has worked and what hasn’t. A “compression of the past”, so to speak. Along those lines, I think a theory of data analysis should reflect the following principles:\nThe theory of data analysis is not a theory that instructs us what to do or tells us universal truths. Rather, it is a descriptive or constructive theory that guides analysts without tying their hands.\nA theory should speed up the training of an analyst by summarizing what is commonly done and what has been successful. It should reduce the amount of learning that must be done experientially.\nThe theory should serve the practice of data analysis and make data analysis better in order to justify its existence. This is in contrast to traditional statistical theory, whose existence could be justified by virtue of the fact that it allows us to discover truth about the natural world, not unlike with mathematics or physics. In other words, a theory of data analysis would have relatively little intrinsic value.\nThe theory should not be dependent on specific technologies or types of data. It should be reasonably applicable to a wide range of data analyses in many different subject areas. A biologist talking to an economist should be able to understand each other when discussing the theory of data analysis.\nThe theory should go far beyond the instruction of different methods of analysis, including aspects of data analysis that don’t strictly involve the data.\nThe Scope of Data Analysis\nTukey writes that data analysis should be thought of more as a scientific field, not unlike biochemistry. The key aspect of that comparison is that scientists in any field are comfortable acknowledging that there are things they don’t know. However, data analysts often feel that they have to have an answer to every question. I’ve felt this myself–when someone presents a problem to me for which there isn’t an obvious solution, I feel a bit embarrassed, as if there must be an answer and I just don’t know it.\nThe view of data analysis as a scientific field though is a bit too simplistic in that we shouldn’t view solutions to problems as either known or unknown. Often, we can design a solution to a problem even if we are unaware of the “optimal” one. Such a sub-optimal solution will likely be based on intuition, judgment, and our memory of past experience. We may not be happy about the solution, but it might nevertheless be useful. If we are unhappy about the solution developed, we may be inspired to search for the “best” or optimal procedure…or not. Whether such a best solution can be found will depend on whether a realistic optimality criterion can even be specified.\nMuch of what is written about data analysis (in particular older material) tends to be about activities involving the data. But a distinction needs to be made between what data analysis is and what a data analyst does. The theory of data analysis will largely focus on what a data analyst does, as I think this aspect is potentially more generalizable across disciplines and includes many critically important activities that we don’t often discuss.\nI think most would agree that what data analysis is includes things that involve the data (like fitting a statistical model). But many things that data analysts do include things completely outside the data. For example, refining the question to be answered and consulting with experts is a key aspect of what data analysts do, but typically does not involve the analysis of any data (the data may not even be collected yet). Experimental design is another important job where data analysts need to be involved but often does not involve data (although there is substantial theory surrounding this topic already). Allocating resources for a data analysis and building trust with collaborators are also critical things that data analysts do.\nBecause data analysis has become such a critically valuable skill in so many areas of the world, statisticians will have to think harder about what makes for a good data analyst. Further, we need to develop better ways to train analysts to do the right thing. Learning by doing will always be a critical aspect of data analytic training, if only because practice is essential (not unlike with music). But we should make sure we are not wasting time in areas where we have a rich collective experience. In other words, we need a theory of data analysis that binds together this collective experience, summarizes what has worked and what hasn’t, compresses the past, and provides useful guidance for the uninitiated.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:33:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-01-the-role-of-academia-in-data-science-education/",
    "title": "The role of academia in data science education",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2018-11-01",
    "categories": [],
    "contents": "\nI was recently asked to moderate an academic panel on the role of universities in training the data science workforce. I preceded each question with opinionated introductions which I have fused into this blog post. These are weakly held opinions so please consider commenting if you disagree with anything.\nTo discuss data science education we first need to clearly state what it means. The panel organizers defined data science as “an emerging discipline that draws upon knowledge in statistical methodology and computer science to create impactful predictions and insights for a wide range of traditional scholarly fields.” But is it an academic discipline? If so, what are the shared fundamental principles, expertise, skills, and knowledge-based shared by data scientists? Is there a core curriculum for Data Science? Providing a more detailed definition might help.\nMy attempt at defining Data Science\nThe term Data Science may have been coined in academia, but the proliferation of its use has been mostly driven by the tech industry. The term became prominent because recruiters needed to more specifically describe what they needed for data driven initiatives, a new type of project becoming more and more common. Post graduate degrees in Statistics or Computer Science did not guarantee the expertise needed to successfully complete these projects. Programming skills and experience analyzing messy, complex and large datasets were fundamental. But because you can obtain a PhD in Statistics without ever looking at a real dataset statistician was not a specific enough job title. And because you can obtain a PhD in Computer Science without ever writing one line of code computer scientist was not specific enough either. Statisticians and computer scientist could be good hires, but not always. On the other hand, some graduates from other areas, such as the social sciences and particle physics, had enough experience managing and analyzing data to be hired. So the credentials provided by universities did not provide a useful signal to these employers. The academic knowledge base offered by Statistics and Computer Science was necessary, but was not sufficient. The term data scientist therefore became useful for making the distinction between, for example, someone with experience analyzing data in all its messy glory versus someone that can prove an estimate is asymptotically normal or making the distinction between someone that knows how to write fast, efficient, interoperable, and reliable code to extract/insert data from a database versus someone that can prove if an algorithm is Np complete.\nHowever, because the challenges posed by data driven enterprises vary greatly across different organizations, and even within organization, the term remains quite vague. As a result the best definition I can provide is that data science is an umbrella term used by organizations to describe the processes used to extract value from data.\nThe data science areas of expertise\nSo what falls under the data science umbrella? First, I make one big distinction between back-end and front-end data science. I define the back-end as the part that deals with hardware, efficient computing, and data storage infrastructure. I define the front end as the part geared more towards data analysis and can be further divided into data analysts and applied machine learners. The data analysts explore, quality assess, wrangle, and fit models to data. The applied machine learners build and assess prediction algorithms. Domain knowledge is of course important for both these tasks. Often, to finish the project the front-end data scientist develops a prototype that the back-end data scientists convert into robust pipelines. As a result front-end data scientist tend to use R or Python, while back-end data scientist program in low-level languages such as C++ and database languages such as SQL.\nAnother data science area of expertise is defined by what I call the frontend software engineers. These are not necessarily involved in producing data science pipelines but instead develop the software tools that facilitate data science. They tend to have experience as front-end data scientist and use this experience to develop tools that many others find useful. Examples are the developers of Rstudio, iPython notebooks, tidyverse, Hadoop and D3 to name a few. Because academia tends to favor method developers (by methods I mean mathematical abstractions that permit data analysis ideas to be applied more widely than its original application) over software developers, this group tends to work outside of academia (with exceptions) and prefer being labeled data scientists even if they have PhDs in Statistics or Computer Science.\nThe implication for academic programs\nHaving the goal of training an individual to be an expert that can tackle all the challenges involved in the data science process is too ambitious. However, as the term Data Science became more and more fashionable, demand for Data Science education increased accordingly. Universities rushed to figure out how to meet this demand. Developing revenue generating masters programs was the first priority and, as a result, today we have dozens of universities offering these degrees. But what exactly are these students being prepared to do? What do these new programs offer that existing ones did not? Given that, with some exceptions, no new faculty were hired when creating these new programs and, in many cases, no new classes were developed, it is not clear that a masters degree in Data Science provides the signal employers are looking for.\nClearly, existing academic courses provide excellent ways of gaining some of the expertise listed above. These include courses on discrete math, probability, statistical inference an modeling, computer programming, software engineering principles, and machine learning. But this was true before Data Science programs emerged. So what can academia do to better prepare students from the data science workforce and to provide a better signal to industry. Here are my recommendations.\nRealize that Data Science is an umbrella term and offer specific tracks targeted at the different aspects of data science listed above. Three tracks might be enough but one isn’t.\nAdapting statistics and machine learning course to have applications in the forefront rather than a theoretical focus. Data scientist have to produce pipelines that work in the real-world and one needs training to learn this. Implementation is hard.\nProvide learning experiences that expose students to long-term projects like those they will be tasked to work on in industry. For this, many universities will have to invest in new faculty, with real-world experience.\nLet me know what you think.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:33:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-10-11-guest-post-galin-jones-on-criteria-for-promotion-and-tenture-in-bio-statistics-departments/",
    "title": "Guest Post: Galin Jones on criteria for promotion and tenture in (bio)statistics departments",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2018-10-11",
    "categories": [],
    "contents": "\nEditor’s Note: I attended an ASA Chair’s meeting and spoke about ways we could support junior faculty in data science. After giving my talk Galin Jones, Professor and Director of Statistics at University of Minnesota, and I had an interesting conversation about how they had changed their promotion criteria in response to a faculty candidate being unique. I asked him to write about his experience and he kindly contributed the following post.\nIf the number of looming deadlines on my calendar for recommendation letters is any indication, we are squarely in application season for tenure-track faculty positions with interviewing season soon to follow.\nWhen candidates for junior faculty positions come for an interview, the requirements for promotion are at the forefront of the discussion. This is perfectly rational. After all if the path to promotion is unclear or is going to require participation in activities that are different than what you are excited about, why take the job?\nWhat most candidates are told is essentially the same thing I was told in 2001 and others were told in 1991 and so on. Be good at research, teaching, and service. Try to get an external research grant. (In biostat departments, there is more discussion about external funding.) But the subtext of the conversation is that research is the primary concern and to be good at research the focus should be on quality, with a nod to productivity. This is often code for publishing as many articles as possible in the big four journals–JASA, Biometrika, JRSSB, and the Annals of Statistics.\nWhile this has been the traditional approach, it ignores the way the field has changed and doesn’t make any serious attempt at assessing true impact on the discipline or more broadly. Collaborative projects, refereed conference papers, software development, software vignettes, blog posts, videos, podcasts, development of novel teaching methods (MOOC anyone?), and so on are often viewed as being helpful to the P+T case only if they connect directly to a paper published in a traditional statistics journal.\nThe School of Statistics at Minnesota is no different than others in this regard, but we have at least partially realized there is an issue. A while back we interviewed a candidate for a junior faculty position. This person showed a lot of promise, had a novel research agenda, and obviously was going to impact the profession albeit in nontraditional ways. But that candidate could not be promoted under our P+T guidelines at the time so we could not in good conscience make an offer. I think it is to our credit that our faculty debated the issue and overwhelmingly agreed to modify our P+T guidelines so that things like software development, vignettes, and conference publications apply toward promotion and tenure. Our P+T guidelines are not necessarily where I’d like them to be, but this was a sort of progress at the time.\nI fear that if (bio)stat departments continue to limit evaluation criteria to the traditional ones, then it will become increasingly difficult to recruit and retain faculty since there are so many alternative opportunities where the new approaches are encouraged. On the other hand, if we broaden what is recognized, we have a much better chance to attract and retain faculty that are taking advantage of the new opportunities available to (bio)statistics students.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:32:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-10-08-the-economic-consequences-of-moocs/",
    "title": "The economic consequences of MOOCs",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2018-10-08",
    "categories": [],
    "contents": "\ntl;dr check out our new paper on the relationship between MOOC completion and economic outcomes!\nLast Monday we launched our Chromebook Data Science Program so that anyone with an internet connection, a web browser, and the ability to read and follow instructions could become a data scientist.\nWhy did we launch another MOOC program? Aren’t MOOCs dead?\nWell we didn’t think so :). We have been pretty excited about MOOCs for a while now and now run five different MOOC programs through the Johns Hopkins Data Science Lab. Our enthusiasm is motivated by our desire to make education available to everyone and help the world.\nThere has been a significant amount of research on attendance in MOOCs, on learner attrition, and how to keep students motivated. But there has been almost no research on whether MOOCs can actually help you improve your economic situation.\nA postdoc in our group,Aboozar Hadavand, set out to study this in our main data science MOOC program, the JHU Data Science Specialization on Coursera. We wanted to know whether completing our program, with an average cost less than 1,000 dollars, would lead to any economic benefits in terms of increase in salary or job mobility.\nWe created a survey that we sent out to all students who had ever enrolled in the most popular course in the sequence - R programming. The survey asked questions about salary before and after the sequence, location, and job mobility (the number of times a person switched jobs). By emailing everyone who enrolled we surveyed both people who completed our sequence and people who did not complete.\nWe compared the respondents to our survey to the demographics we have from students who enrolled in our sequence and found them to be broadly similar.\nOur survey population and Coursera’s demographics match up broadlyUsing this survey we wanted to know whether there are any significant differences between the people who enrolled and completed the program versus those who enrolled but never completed in both income and job mobility. We used propensity score weighting to match up the people who completed 0, 1-5, and 5 or more courses (left panel). Based on these propensity score weighted data it looks like there is an association between completion of the courses and post-completion income. For users who completed at least 5 courses (compared to those who completed none) we observe an increase of $2,790 to $7,820 depending on the choices we make about which countries to include in the analysis (middle panel). We then used generalized propensity score weighting to calculate the percent increase in income for each fraction of the Data Science Specialization that students completed (right panel). The majority of the increase occured after completing 25% of the Specialization with a relatively steady increase beyond that. While there are still a lot of challenges with MOOCs, we are very excited about the potential return on investment for learners. When you compare our MOOC program to other educational programs the potential ROI for these program is what has us so excited about doing things like Chromebook Data Science\nResults of our surveyIf you want to read more about how we did the survey and the other interesting analyses we did you should go check out Aboozar’s paper on SSRN.\nCOI Disclosure: Dr. Leek receives financial compensation through the Johns Hopkins Tech Transfer Program from revenue generated by the Johns Hopkins Data Science Specialization.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:32:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-10-01-chromebook-data-science-an-online-data-science-program-for-anyone-with-a-web-browser/",
    "title": "Chromebook Data Science - a free online data science program for anyone with a web browser.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2018-10-01",
    "categories": [],
    "contents": "\nThe Johns Hopkins Data Science Lab has been teaching massive online open courses for more than 5 years now. During that time we’ve reached more than 5 million learners who want to break into the number one rated job in America.\nWhile we have been incredibly excited about the results of these training programs, we’ve also learned over the last 5+ years that there are still significant barriers to getting into data science.\nYou have to know data science is a thing before you can start learning it\nData science training programs often start by assuming advanced math or programming knowledge\nData science training programs often require expensive computers\nData science training itself is often expensive\nData science jobs are concentrated in tech centers\nGetting a job in data science requires connections\nAbout a year ago we started a project where our goal was to help overcome these barriers. Our goal was to create a system so that anyone with the ability to read, write, and do basic math could get into data science using nothing but a web browser and an internet connection.\nToday I’m excited to announce the first part of our new system, a new set of massive online open courses called Chromebook Data Science. These MOOCs are for anyone from high schoolers on up to get into data science. If you can read and follow instructions you can learn data science from these courses!\nThe reason they are called Chromebook Data Science is because philosophically our goal was that anyone with a Chromebook could do the courses. All you need is a web browser and an internet connection. The courses all take advantage of RStudio Cloud so that all course work can be completed entirely in a web browser. No need to install software or have the latest MacBook Computer.\nThe program has several relatively unique components. It starts at the beginning, introducing data science, what it is, how it works. The Course Set covers how to use Google, through basic use of R and data analysis, all the way up to soft skills and getting a job. I’ll be posting more about some of the unique aspects later, but we are actively working to make this a program that anyone can do.\nFinally, this set of MOOCs is our first set of MOOCs offered through Leanpub. Just like with books we have released in the past, each course has a free minimum price, so finances won’t be a barrier. At the same time, if you do decide to pay for the courses, all the money that makes its way back to the Data Science Lab will be used to support a new in-person tutoring and support system called CBDS+ that we are piloting in Baltimore.\nThis project is an exciting one for the JHU DaSL and I’m really grateful to all the people who have built the program. Shannon Ellis has done a huge amount of work in leading the project, designing the curriculum, creating the content, and fixing technology bugs. Aboozar Hadavand has been a huge help in creating content, designing the program, and helping set up our CBDS+ program that you’ll hear more about later. John Muschelli and Sean Kross have helped build new technologies that allowed us to produce the content for CBDS rapidly and efficiently using AI to synthesize videos and R to manage course publication. Leslie Myint, Sarah McClymont, Leah Jager, and Leonardo Collado Torres have all made important contributions to our final curriculum. The whole Leek Group as well as Ira Gooding and the rest of the Data Science Lab have helped contribute ideas and support. Finally, I’d like to thank Leanpub RStudio, and DataCamp for building the technology that makes our program possible. We are also grateful to a number of people who have graciously allowed their content to be used in our courses.\nYou can learn more about the program or sign up for courses at: http://jhudatascience.org/chromebookdatascience/.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:32:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-09-28-the-complex-process-of-obtaining-puerto-rico-mortality-data-a-timeline/",
    "title": "The complex process of obtaining Puerto Rico mortality data: a timeline",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2018-09-28",
    "categories": [],
    "contents": "\n\n\n\nRolando Acosta and I recently posted a manuscript on bioRxiv describing the effects of Hurricane María, based on an analysis of mortality data provided by the Demographic Registry. I was also an author on a paper published in May based on a survey of 3,000 households. These are very different datasets. Assuming it is complete, the Demographic Registry dataset provides much more precise quantitative information. However, this dataset was not made publicly available until June 1, 2018, three days after the paper based on the survey data was released. The story of how all this happened is somewhat complex. In this post I describe this process in some detail.\nThere is also a bit of confusion about why we performed a survey at all, as opposed to using the Demographic Registry as done by other groups. The main reason is that we were not provided the 2017 data and did not know if what was being published was official government data or not. We eventually obtained preliminary 2017 data, after our survey was finished, but, as I show below, these data appeared to be incomplete for November and December. The complete data for 2017 was not released until June 1.\nAs I was looking through my emails to remind myself of the multiple ways we requested the data, I found that the entire story to be interesting and informative. So I put together an annotated timeline. Note that I might update this list if colleagues or other involved parties send me corrections or more information.\nSep 20 – Hurricane María makes landfall in Puerto Rico.\nSep 20-30 – Reports from friends and family point to a dire situation. None of them have power, few have running water, and some have no way to communicate.\nOct 03 – The president of the USA visits Puerto Rico. In a press conference the governor states that the death count is 16.\n\n\n“Every death is a horror,” Trump said, “but if you look at a real catastrophe like Katrina and you look at the tremendous – hundreds and hundreds of people that died – and you look at what happened here with, really, a storm that was just totally overpowering … no one has ever seen anything like this.”\n“What is your death count?” he asked as he turned to Puerto Rico Gov. Ricardo Rosselló. “17?”\n“16,” Rosselló answered.\n“16 people certified,” Trump said. “Sixteen people versus in the thousands. You can be very proud of all of your people and all of our people working together.\n\n\nOct 3 - 23 - The number 16 did not make sense to me. And if the government is incorrectly assuming things are fine, the response will not be appropriate and people will be at risk. This is when I first decide I should look for daily mortality data as we can probably get a decent estimate from studying the jump right after September 20.\nOct 23 – Dr. Caroline Buckee, the lead author of the survey paper, contacts me for the first time asking if I am interested in helping assess the situation in Puerto Rico. Based on reports from a colleague doing field work in Puerto Rico, she is “concerned that the mortality estimates in Puerto Rico following the hurricane are wildly wrong”. She is already organizing a cluster survey to try to quantify the true mortality caused by the hurricane. I agree to help. We start studying the literature on this topic and thinking about who to collaborate with groups in Puerto Rico.\nOct 31 - Caroline starts contacting people in Puerto Rico asking for help with the survey.\nNov 01 - We start designing the study. For the power calculation it would be convenient to get an idea of a possible range of death rates. We start trying to find public mortality data that can help us do this.\nNov 08 - The New York times publishes an article based on funeral home data confirming our suspicion that the situation in Puerto Rico is much worse than was reported on October 3.\nNov 14 – Not having any luck finding official government data online I email the Puerto Rico Institute of Statistics (PRIS) asking for help.\nNov 16 – PRIS replies that the Demographic Registry has these data. They email the directory of the Demographic Registry on our behalf.\nNov 20 – CNN publishes an article describing a funeral home survey estimates. They estimate about 499 excess deaths.\nNov 21 – The Penn State University (PSU) study with the first estimate of excess deaths based on mortality data is posted on SocArXiv\nThey estimate about 1,000 excess deaths. This estimate is based on historical data from 2010-2016 and a count for September 2017 that the authors obtained from a public statement made by Puerto Rico’s Public Security Secretary.\nNov 27 - I email the corresponding author of the PSU study asking if they have the data.\nDec 03 - I scrape the data from PSU study, as described in an earlier post. This data helps guide our study design. Here is a plot of the data that clearly shows that there are more deaths than expected. The plot includes a count for October which is publicized later (see Dec 07 entry).\n\n\n\nDec 05 - We receive data from a Demographic Registry demographer, but it does not include the most important part: the 2017 data. They claim that they “still do not have these data available”.\nDec 05 - We start finalizing the study design for a survey. Based on the limited information we have, we perform a power calculations and decide to make the sample size as large as our funds permit.\nDec 06 - PSU study author replies. But email with data appears to be lost in the mail.\nDec 07 - Centro de Periodismo Investigative (CPI) publishes an estimate of excess deaths based on September and October data of about 1,000. It appears they have 2017 data!\nDec 07 – From this tweet, it appears PSU investigator also has the data. I ask on twitter if CPI or PSU investigator have official data.\nDec 08 – I email PSU investigator asking for data.\nDec 08 - New York Times published a comprehensive article with very nice data visualization and an estimate of 1,052. They have daily data!\nDec 08 – I email first author of New York Times article. She says it took 100 emails/calls to get the data and suggest we contact the Registry director. So now we know the Demographic Registry does in fact have the data.\nDec 08 – It appears that the 2017 data does exist and three different groups were given access. We were told, three days earlier, by the Demographic Registry that they “still do not have these data available”. So I email PRIS again to see if they can help.\nDec 09 – I ask Dr. Buckee to email Registry director to check that it is not just me that is being denied the data.\nDec 11 - PRIS replies. They give us the name of a Registry demographer that gave data to others. PRIS emails Registry directory, again, on our behalf.\nDec 13 – The official death count is about 55 now and Public Security Secretary dismisses current excess deaths estimates. He says this:\n\n\n\nIt should be noted that similar numbers are seen in December and January of previous years when no atmospheric phenomenom took place.\n\n\n\nSource: https://twitter.com/julito77/status/940945756399243264\nThis statement shows ignorance of the well-known fact that death rates in the winter are higher than in the fall. You can clearly see this from the Demographic Registry data:\n\n\n\nComment - At this point we become quite concerned. A high ranking government official, who has seen the reports of excess deaths around 1,000 by three different groups, ignores the warnings and instead makes a misguided statement claming what we are observing is normal. It is also concerning that the government is only selectively sharing the data.\nJan 07 - The New York Times shares the raw data they obtained with us. It’s in PDF! But scrapable.\nThis data shows very clearly that in September and October ther was a huge spike in deaths. However, it is immediately obvious from exploratory data analysis that data for November and December are incomplete:\n\n\n\nJan 16 - Field workers are deployed in Puerto Rico and our survey starts.\nJan 25 - Demographic Registry finally responds to Dr. Buckee saying “we are not authorized to provide new 2017 mortality data.” \nFeb 01 - PSU investigator emails Public Security Secretary\nasking them to make data public.\nFeb 22 – Puerto Rico government announces that they have contracted George Washington University (GWU) to perform an independent investigation into the death toll.\nFeb 24 - Our household survey is completed. We start analyzing data right away. Our goal is to get the paper published before June 1, the start of hurricane season.\nFeb 28 - Latino USA publishes an article showing data provided to them by PRIS. I email PRIS again and they send us data that same day. We immediately see that, like the data we received from New York Times, it appears incomplete:\n\n\n\nMar 16 - First draft of our survey paper is completed and submitted to the New England Journal of Medicine (NEJM). A particularly troubling finding is the large proportion of deaths attributed to lack of access to medical services. We also see evidence of indirect effects lasting until December 31, the end of the survey period.\nMay 04 - Survey paper is tentatively accepted by NEJM.\nMay 25 - Official death count is still at 64. We send a draft of our paper to PR governors office.\nMay 29 – Our NEJM paper comes out and gets extensive media coverage: 410 outlets including articles in NPR, Washignton Post, New York Times, and CNN. Despite our best efforts, including rewriting our university’s press release, most headlines focus on the point estimate and don’t report uncertainty. All the data and code is made available on GitHub.\nMay 31 - We post an FAQ (in English and Spanish) explaining the uncertainty in our estimate and making it clear that our study does not say that 4645 (the point estimate) people died.\nJun 01 – PR Governor is interviewed by CNN’s Anderson Cooper who grills him on why the government did not share the data. Governor says data was always available and that there will be “hell to pay if data not available”. That afternoon, PR government makes data public. Dr. Buckee requests the data again. This time we get it almost immediately.\nJun 06 - I post my first analysis with the official mortality data here. The newly released data confirms that the data we were provided earlier were in fact incomplete and that there was a sustained indirect effect lasting past October.\nAug 27 - GWU study comes out with a preliminary estimate of about 3,000 deaths.\nSep 05 - Rolando Acosta and I post a preprint describing an analysis of the newly released data including a comparison to the effects of other hurricanes. We also provide a preliminary estimate of about 3,000 deaths due to indirect effects lasting until April.\n\n\n\n",
    "preview": "posts/2018-09-28-the-complex-process-of-obtaining-puerto-rico-mortality-data-a-timeline/index_files/figure-html5/deaths-above-monthly-average-1.png",
    "last_modified": "2022-12-27T15:39:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-09-14-divergent-and-convergent-phases-of-data-analysis/",
    "title": "Divergent and Convergent Phases of Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-09-14",
    "categories": [],
    "contents": "\nThere are often discussions within the data science community about which tools are best for doing data science. The most recent iteration of this discussion is the so-called “First Notebook War”, which is well-summarized by Yihui Xie in his blog post (it is a great read).\nOne thing that I have found missing from many discussions about tooling in data analysis is an acknowledgment that data analysis tends to advance through different phases and that different tools can be more or less useful in each of those phases. In fact, a tool that is very useful in one phase can be less useful or even detrimental in other phases. Yihui touches on this in his blog post and my goal here is to expand on this idea here.\nA Double Diamond for Data Analysis\nOne image that is commonly found in the literature on design thinking is the “double diamond”, which is a model for the design process. I think this model can be usefully adapted to help us think about data analysis and my best representation is here (apologies for the scribble).\nDouble Diamond for Data AnalysisIn this figure I’ve identified four phases of data analysis that alternate between divergent and convergent forms of thinking. The x-axis is roughly “time” or the timeline of the analysis. The y-axis can be thought of as indicating the range of possibilities. Okay, it’s not exactly a numerical graph, but you get the idea. The wide parts of each diamond are meant to indicate that there are many possibilities under consideration while the narrow “choke points” indicate that there should be one or a few things under consideration.\nHow does all this relate to data analysis? Let’s first take a closer look at the four phases.\nPhase 1: Exploration\nThe goal of Phase 1 is to explore the possibilities inherent in the data. This part is familiar to all data analysts.\nThe dataset lands in your lap and there are a lot of things to do and a lot of questions to answer. Did you get the right dataset? Are all the data there? Are they in the right format for analysis? This is often where a lot of data wrangling occurs. We must consider what question is being asked and whether the data are appropriate for that question (at least without making unreasonable assumptions). We might also consider what question motivated the creation of the data.\nAt this point we may think we have a question to ask, but usually that question is only vaguely formed or is in need of further information. Here is where we need the data to help us out. For example, an important general question is “Can the data even be used to answer my question?” We need to look at the data to figure that out. How we look at the data will vary across people, context, and many other factors. Regardless of the specific situation, we likely will need to make lots and lots of plots, summaries, and tables. We’ll need to look at the data, perhaps even in a program like Excel, and just get a sense of the data.\nThis phase of analysis is highly divergent, with many possibilities being considered for how to ask the question and what approach to take. In my experience, I’m making tons of plots and looking at various transformations of individual variables and bivariate relationships. I’ve never bothered to count, but it wouldn’t surprise me if there were thousands of plots made in this phase. This is the “sketching” phase of analysis, figuratively speaking, but sometimes literally speaking. Sketches of plots or tables are often a useful planning device.\nThis phase of analysis is almost always fun, because we are opening up the possibilities. But all good things must eventually come to an end.\nPhase 2: Refining the Problem\nPhase 2 is challenging because it involves making decisions and choices. Nobody likes to do that. Inevitably, most of the work you did in Phase 1 will be left on the cutting room floor. You might love all your children equally but you still need to pick a favorite. The reason is nobody has the resources to pursue every avenue of investigation. Furthermore, pursuing every avenue would likely not be that productive. You are better off sharpening and refining your question. This simplifies the analysis in the future and makes it much more likely that people (including you!) will be able to act on the results that you present.\nThis phase of analysis is convergent and requires synthesizing many different ideas into a coherent plan or strategy. Taking the thousands of plots, tables, and summaries that you’ve made and deciding on a problem specification is not easy, and to my surprise, I have not seen a lot of tools dedicated to assisting in this task. Nevertheless, the goal here is to end up with a reasonably detailed specification of what we are trying to achieve and how the data will be used to achieve it. It might be something like “We’re going to fit a linear model with this outcome and these predictors in order to answer this question” or “We’re building a prediction model using this collection of features to optimize this metric”.\nIn some settings (such as in consulting) you might need to formally write this specification down and present it to others. At any rate, you will need to justify it based on your exploration of the data in Phase 1 and whatever outside factors may be relevant. Having a keen understanding of your audience becomes relevant at the conclusion of this phase.\nPhase 3: Model Development\nThis phase is the bread and butter of most statisticians and statistics education programs. Here, we have a reasonably well-specified problem, a clear question, an appropriate dataset, and we’re going to engineer the solution. But that doesn’t mean we just push a button and wait for the result to come out. For starters, what will the results look like? What summaries do we want to produce and how will they be presented? Having a detailed specification is good, but it’s not final. When I was a software engineer, we often got highly detailed specifications of the software we were supposed to build. But even then, there were many choices to make.\nThus begins another divergent phase of analysis, where we typically build models and gauge their performance and robustness. This is the data analyst’s version of prototyping. We may look at model fit and see how things work out relative to our expectations set out in the problem specification. We might consider sensitivity analyses or other checks on our assumptions about the world and the data. Again, there may be many tables and plots, but this time not of the data, but of the results. The important thing here is that we are dealing with concrete models, not the rough “sketches” done in Phase 1.\nBecause the work in this phase will likely end up in some form in the final product, we need to develop a more formal workflow and process to track what we are doing. Things like version control play a role, as well as scriptable data analysis packages that can describe our work in code. Even though many aspects of this phase still may not be used, it is important to have reproducibility in mind as work is developed so that it doesn’t have to be “tacked on” after the fact (an often painful process).\nPhase 4: Narration\nIn the final convergent phase of data analysis, we must again make choices amongst the many pieces of work that we did in Phase 3. We must choose amongst the many models and results and decide what will make it into the final product, whether it is a paper, a report, a web site, or a slide deck.\nIn order to make these choices, it is useful to develop a narrative of the analysis. Building the narrative is dimension reduction for results and it allows you to choose from the various results the ones that follow your narrative. Simply “presenting the data” is first, not really possible, and second, not desirable. It’s information overload and rarely allows the audience to make an intelligent conclusion. Ultimately, the analyst must decide on a narrative and select the various results that tell that story.\nSelecting a narrative doesn’t mean that everything else is thrown out. There are often many parts of an analysis that cannot make it into the main product but can be put in some form of “supplementary materials”. For example, FiveThirtyEight.com doesn’t put its election statistical model on its web site, it puts it in a PDF document that you can download separately. In many scientific papers, there can be dozens of tables in the supplementary materials that didn’t make it into the paper. Many presenters often have backup slides hidden at the end of the deck in case there are questions. Some people may disagree with the choice of narrative, but that doesn’t eliminate the need for choices to be made.\nImplications\nAt this point it’s worth recalling that all models are wrong, but some are useful. So why is this model for data analysis useful? I think there are a few areas where the model is useful as an explanatory tool and for highlighting where there might be avenues for future work.\nDecision Points\nOne thing I like about the visual shape of the model is that it highlights two key areas–the ends of both convergent phases–where critical decisions have to be made. At the end of Phase 2, we must decide on the problem specification, after sorting through all of the exploratory work that we’ve done. At the end of Phase 4, we must decide on the narrative that we will use to explain the results of an analysis.\nAt both decision points, we must balance a set of competing priorities and select from a large number of outputs to create a coherent product. When developing the problem specification, we must avoid the desire to say “We’re going to do all the things” and when producing the final product we must avoid the “data dump” where we shift the burden of interpretation and distillation on to the audience.\nIn my experience, data analysts really enjoy the divergent phases of analysis because no choices have to be made. In fact, in those phases we’re doing the opposite of making choices–we’re trying out everything. But the convergent phases cannot be avoided if we want to produce good data analysis.\nTooling\nWhen it comes to discussions about tooling in data science it’s useful to preface such discussion with an indication of which phase of data analysis we are talking about. I cannot think of a single tool that is simultaneously optimal for every phase of analysis. Even R is sub-optimal for Phases 2 and 4 in my opinion. For example, when developing data visualizations for presentation, many people resort to tools like Adobe Illustrator.\nI find most debates on tooling to be severely confounded by a lack of discussion about what phase of analysis we are focusing on. It can be difficult for people to accept that a given tool may be optimal for one phase but detrimental in another phase. I think the four-phase model for analysis can explain some of the recent “debates” (a.k.a flame wars) on tooling in the data science world.\nLet’s start with notebooks. I think the issue here is essentially that notebooks can be useful for Phase 1 but potentially dangerous for Phase 3. Reproducibility is a critical consideration for developing the final results, but is less important when exploring the data in Phase 1. The need for consistent state and in-order execution of commands is more important in Phases 3 than in Phase 1 when lots of ideas are being tried and will likely be thrown out. There’s no either-or decision to be made here with regard to notebooks. It just depends on what phase of analysis you’re in. I generally prefer R Markdown but can see the attractiveness of notebooks too.\nConsider the age-old base graphics vs. ggplot2 debate. This is another debate confounded by the phases of data analysis. In the divergent phases (like Phase 1), there is a need to do things very quickly so that good ideas can be discovered and bad ideas can be discarded. Having a system like ggplot2 that allows you to rapidly “sketch” out these ideas is important. While the base graphics system gives you fine control over every aspect of a plot, such a “feature” is really a detriment in Phase 1 when you’re exploring the data. Nathan Yau seems to edge towards base graphics but I think this is largely because he is considering developing “final” data presentations in Phase 4. If you look at the visualizations on his web page, they are akin to works of art. They are not sketches. When developing those kinds of visualizations, having fine control over every detail is exactly what you want.\nEducation\nIn my experience as both a student and a teacher, statistical education tends to focus on Phases 3 and 4 of the data analysis process. These phases are obviously important and can be technically demanding, making them good things to emphasize in a statistics program. In the classroom, we typically start with a reasonably well-specified problem and proceed with engineering the solution. We discuss the many options for developing the solution and the various strengths and weaknesses of those options. Because the work in Phase 3 ends up in the final results, it’s important to get this phase right.\nWhen getting a graduate degree, Phases 1 and 2 are usually taught as part of developing the thesis using an apprenticeship model with an advisor (“just watch what I do for a few years”). Some good programs have courses in exploratory data analysis that fall squarely into Phase 1, but I don’t think I’ve ever seen a course that covers Phase 2. Phase 4 is sometimes covered in visualization courses which discuss the design process for good data presentation. There are also a number of good books that cover this phase. But we rarely discuss the role of developing a coherent narrative of an analysis in the classroom. Usually, that is covered using an apprenticeship approach.\nData Analytic Errors\nOne general problem that occurs when I see others do data analysis is confusing the different phases. It’s common for people to think that they are in one phase when they’re really in another. This can lead to data analytic problems and even mistakes.\nConfusing Phase 3 for Phase 1 is arguably the most common and the most dangerous problem that one can encounter in science. In Phase 1 we are developing the specification of the problem, but in Phase 3 that specification should be more or less locked down. If we discover something else interesting in Phase 3, we cannot pretend like that’s what we meant to conclude in Phase 1. That’s a recipe for p-hacking. Phase 1 should be broad and exploratory with the data. But once you’ve decided on a problem in Phase 2, Phases 3 and 4 should be solely dedicated to solving that problem. Changing up the problem in mid-stream in Phases 3 and 4 is often tempting but almost always a mistake.\nA related problem is confusing Phase 2 with Phase 4. I have observed people who explore the data extensively, making quick plots, sketches, and summaries, and then immediately draw a conclusion like “X causes Y” or “A is associated with B”. Essentially, they jump from Phase 1 straight to Phase 4. This is problematic because often in the exploratory process we only look at rough sketches, often just bivariate plots or 2x2 tables which reveal strong relationships. Making inferences from this kind of exploratory work can be misleading without carefully considering second order factors like confounding or the effects of missing data. Often, those issues are more easily dealt with when using formal models or more detailed visualization.\nThinking through and formally specifying a problem in Phase 2 can often help to solve problems without having to fit a single model. For example, if exploratory analysis suggests strongly that missing data is missing completely at random, then there may be no need to model the missing data process in Phase 3. In the extreme case, you may conclude that the question of interest cannot be answer with the data available, in which case other data needs to be obtained or the analysis is over.\nI think awareness of the different phases of an analysis can help to address this problem because it provides the analyst with a way to ask the question of “Where am I?” When using a map, in order to figure out where you’re going, you first need to figure out where you are. The four-phase model serves as a roadmap to which analysts can point to identify their location.\nFuture Work\nI will end this with some thoughts on what the four-phase model implies for potential future work in the field of data analysis. Seeing the model written down, some immediate gaps jump out at me that I think the data science community should consider addressing.\nTooling\nMuch of the software that I use for data analysis, when I reflect on it, is primarily designed for the divergent phases of analysis. Software is inherently designed to help you do things faster so that you can “analyze at the speed of thought”. Good plotting and data wrangling software lets you do those activities faster. Good modeling software lets you execute and fit models faster. In both situations, fast iteration is important so that many options can be created for consideration.\nSoftware for convergent phases of analysis are lacking by comparison. While there are quite a few good tools for visualization and report writing in Phase 4, I can’t think of a single data analytic software tool designed for Phase 2 when specifying the problem. In particular, for both Phases 2 and 4, I don’t see many tools out there for helping you to choose between all the options that you create in the divergent phases. I think data scientists may need to look outside their regularly scheduled programming to find better tools for those phases. If no good tools exist, then that might make for a good candidate for development.\nEducation\nData analytic education tends to focus on the model development and presentation phases of data analysis. Exploratory data analysis is an exception that largely falls into Phase 1, but EDA is not the only element of Phase 1. We often do not teach aspects of challenging a question or expanding beyond what is given to us. Typical data analytic education takes the original problem as the final specification and goes from there. There is no re-litigating the problem itself. In other words, we start at the end of Phase 2.\nI think more could be done to teach students aspects of both convergent phases of analysis (Phases 2 and 4). For Phase 2, teaching students to hone and refine a question is critical for doing research or working elsewhere in data science. I think providing them with greater exposure to this process and the chance to practice repeatedly would be a shift in the right direction. I also think we could do more to teach more narrative-development skills for Phase 4 of analysis, whereby results are synthesized into a coherent presentation.\nRegardless of what we might do more or less of, I think the four phase model allows you to consider what aspect of data analysis you might want to teach at any given moment. It can be difficult to simulate a full-fledged data analysis from start to finish in a single classroom setting. However, splitting up the process and focusing on individual phases could be useful way to modularize the process into more manageable chunks. I am interested in exploring this approach myself in the future.\nSummary\nStealing the “double diamond” model from design thinking and adapting it for describing data analysis has some important benefits as a mental model for the process of analyzing data. It lays out four phases of analysis that all of unique activities (and potentially a unique toolbox) associated with them. I find it a useful model for explaining the various debates going on in the data science community, for exposing gaps in the tooling and in data science education, and for describing certain classes of mistakes that can be made during data analysis.\n\n\n\n",
    "preview": "posts/2018-09-14-divergent-and-convergent-phases-of-data-analysis/images/double_diamond_data_analysis.png",
    "last_modified": "2021-11-12T09:09:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-09-07-being-at-the-center/",
    "title": "Being at the Center",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-09-07",
    "categories": [],
    "contents": "\nHilary Parker and I just released part 2 of our book club discussion of Nigel Cross’s book Design Thinking and it centers around a profile of designer Gordan Murray, who spent his career designing Formula One race cars. One of the aspects of his job as a designer is taking a “systems approach” to solving problems. Coupled with that approach is his role in balancing the various priorities of members of his team. He describes himself as both dictator and diplomat in doing this aspect of the job.\nWhen designing a complex object like a race car, there will be many people contributing who have specific expertise. It is their job to focus on what they think is the highest priority, but it is the designer’s job to put the whole car together and, on the way, raise some priorities and lower other priorities. The designer is at the center of activity and must have good relationships with every member of the team in order for everything to come together on time and on budget.\nData Analyst at the Center\nA mentor once told me that in any large-ish coordinated scientific collaboration there will usually be regular meetings to discuss the data collection, data analysis, or both. Basically, a meeting to discuss data. And that these meetings, over time, tend to become the most important and influential meetings of the entire collaboration. It makes sense: Science is ultimately about the data and any productivity that results from the collaboration will be a function of the data collected. My mentor’s implication was that as a statistician directing the analyses of the group, these data meetings were an important place to be.\nI have been in a few collaborations of this nature (both small and large) and can echo the advice that I got. The data-related meetings tend to be the most interesting and often are where people get most animated and excited. For scientific collaborations, that is in fact where the “action” occurs. As a result, it’s important that the data analyst running the analyses know what their job is.\nIf these meetings are about data analysis, then it’s important to realize that the product that the group is developing is the data analysis. As such, the data analyst should play the role of designer. Too often, I see analysts playing a minor role in these kinds of meetings because it’s their job to “just run the models”. Usually, this is not their fault. Meetings like this tend to be populated with large egos, high-level professors, principal investigators, and the like. The data analyst is often a staff member for the team or a junior faculty, so comparatively “low ranked”. It can be difficult to even speak up in these meetings, much less direct them.\nHowever, I think it’s essential that the data analyst be at the center of a meeting about data analysis. The reason is simply that they are in the best position to balance the priorities of the collaboration. Because they are closest to the data, they have the best sense of what information and evidence exists in the data and, perhaps more importantly, what is not available in the data. Investigators will often have assumptions about what might be possible and perhaps what they would like to achieve, but these things may or may not be supported by the data.\nIt’s common that different investigators have very different priorities. One investigator wants to publish a paper as quickly as possible (perhaps they are a junior faculty that needs to publish papers or they know there is a competitor doing the same research). Another wants to run lots of models and explore the data more. Yet another thinks that there’s nothing worth publishing here and yet another wants to wait and collect more data. And there’s always one investigator who wants to “rethink the entire scientific question”. There’s no one thing to be done here, but the analyst is often the only one who can mediate all these conflicts.\nWhat happens in these situations is a kind of “statistical horse trading”. You want a paper published quickly? Then we’ll have to use this really fast method that requires stronger assumptions and therefore weakens the conclusions. If you want to collect more data, maybe we design the analytic pipeline in such manner that we can analyze what we have now and then easily incorporate the new data when it arrives. If there’s no time or money for getting more data, we can use this other model that attempts to use a proxy for that data (again, more assumptions, but maybe reasonable ones).\nManaging these types of negotiations can be difficult because people naturally want to have “all the things”. The data analyst has to figure out the relative ordering of priorities from the various parties involved. There’s no magical one-liner that you can say to convince people of what to do. It’s an iterative process with lots of discussion and trust-building. Frankly, it doesn’t always work.\nThe analyst, as the designer of the ultimate product, the data analysis, must think of solutions that can balance all the priorities in different ways. There isn’t always a solution that threads the needle and makes everyone happy or satisfied. But a well-functioning team can recognize that and move forward with an analysis and produce something useful.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:32:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-08-24-constructing-a-data-analysis/",
    "title": "Constructing a Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-08-24",
    "categories": [],
    "contents": "\nThis week Hilary Parker and I have started our “Book Club” on Not So Standard Deviations where we will be discussing Nigel Cross’s book Design Thinking: Understanding How Designers Think and Work. We will be talking about how the work of designers parallels the work of data scientists and how many of the principles developed in design port over so well to data analysis. While data visualization has always taken cues from design, I think much broader aspects of data analysis could benefit from the work studying design. At any rate, I think this is a topic that should be discussed more amongst statisticians and data analysts.\nOne of the first revelations I’ve had recently is realizing that data analyses are not naturally occurring phenomena. You will not run into a data analysis while walking out in the woods. Data analyses must be created and constructed by people.\nOne way to think about a data analysis is to think of it as a product to be designed. Data analysis is not a theoretical exercise. The goal is not to reveal something new about the world or to discover truth (although knowledge and truth may be important by-products). The goal of data analysis is to produce something useful. Useful to the scientist, useful to the product manager, useful to the business executive, or useful to the policy-maker. In that sense, data analysis is a fairly down-to-Earth activity.\nProducing a useful product requires careful consideration of who will be using it. Good data analysis can be useful to just about anyone. The fact that many different kinds of people make use of data analysis is not exactly news, but what is new is the tremendous availability of data in general.\nIf we consider a data analysis as something to be designed, this provides for us a rough road map for how to proceed.\nQuestioning the Question\nA favorite quotation from John Tukey, legendary statistician and data analyst at Princeton, is\n\nFar better an approximate answer to the right question, which is often vague, than the exact answer to the wrong question, which can always be made precise.\n\nWhat does these words mean in the context of data analysis? In data analysis, we often start with a dataset or a question. But good data analysts do not solve the problem that is handed to them. The reason is not necessarily arrogance. often the problem as initially stated is only a first attempt. And that’s okay.\nA good data analyst recognizes that the problem itself requires examination. For example, someone might ask “Is air pollution bad for your health?” That’s a great question, and a critical one for public policy, but it’s difficult to map to a specific data analysis. There are many different types of air pollution and there are many health outcomes we might be worried about. Prioritizing and refining the original problem is a key initial step for any data analysis. In my experience, this process usually leads to a question that is more meaningful and whose answer could lead to clear action.\nThe first job of the data analyst is to discover the real underlying problem. The fact that the problem we end up with may not be the problem we started with is not anyone’s fault in particular. It’s just the nature of things. Often, scientific collaborators will come to my office essentially just to talk. They come in with a clear question, but as I probe and ask questions–“What kinds of data are available?”; “What action might result in answering this question?”; “What resources are available to do this analysis?”; “Is it possible to collect new data?”–the question can shift and evolve. Good collaborators are not offended by this process, but rather appreciate it as it hones their thinking.\nThe bad collaborator comes in with the goal of handing off a question and waiting for the solution to appear. I have seen my fair share of these and it almost never works except perhaps with the most trivial problems. The process of designing a good data analysis cannot be modularized where we cleanly move from question, to data, to analysis, and to results, with each person involved doing their job and not talking to anyone else. One may wish it were so, because it would be much simpler this way, but wishing doesn’t make it so. This initial discussion of figuring out the right problem is an important part of designing a data analysis. But if one has a thorough discussion, we’re not done questioning the question yet.\nSometimes a problem does not become clear until we’ve attempted to solve it. A data analyst’s job is to propose solutions to a problem in order to explore the problem space. For example, in the above air pollution example, we might first express interest in looking at particulate matter air pollution. But when we look at the available data we see that there are too many missing values to do a useful analysis. So we might switch to looking at ozone pollution instead, which is equally important from a health perspective.\nAt this point it is important that you not get too far into the problem where a lot of time or resources have to be invested. Making that initial attempt to look at particulate matter probably didn’t involve more than downloading a file from a web site, but it allowed us to explore the boundary of what might be possible. Sometimes the proposed solution “just works”, but more often it raises new questions and forces the analyst to rethink the underlying problem. Initial solution attempts should be “sketches”, or rough models, to see if things will work. The preliminary data obtained by these sketches can be valuable for prioritizing the possible solutions and converging on the ultimate approach.\nThis initial process of questioning the question can feel frustrating to some, particularly for those who have come to the data analyst to get something done. Often to the collaborator, it feels like the analyst is questioning their own knowledge of the topic and is re-litigating old issues that have previously been resolved. The data analyst should be sensitive to these concerns and explain why such a discussion needs to be had. The analyst should also understand that the collaborator is an expert in their field and probably does know what they’re talking about. A better approach for the analyst might be to frame this process as a way for the analyst to learn more about the subject matter at hand, rather than simply challenging long-standing assumptions or beliefs. This has the co-benefit of actually being true, since the analyst is likely not an expert in the data before them. By asking simple questions in an attempt to learn the subject matter, often collaborators will be forced to re-visit some of their own ideas and to refine their thinking about the topic.\nEngineering the Solution\nOnce we have refined the question we are asking and have carefully defined the scope of the problem we are solving, then we can proceed to engineer the solution. This will similarly be an iterative process, but we will be iterating over different things. At this point we will need a reasonably precise specification of the problem so that tools and methodologies can be mapped to various aspects of the problem. In addition, a workflow will need to be developed that will allow all of the interested parties to participate in the analysis and to play their proper role.\nThe analyst will need to setup the workflow for the analysis and adapt it to the various needs and capabilities of the collaborators. Every project will likely have a different workflow, especially if every project has a different set of collaborators involved. This is not just a comment about the tools involved for managing the workflow, but more generally about how information is exchanged and relayed to different people. Sometimes the analyst is a “central hub” through which all information is passed and sometimes it’s more of a “free for all” where everyone talks to everyone else. There’s no one right approach but it’s important that everyone understands what approach is being used.\nThe analyst is also responsible for selecting the methodologies for obtaining, wrangling, and summarizing the data. One might need to setup databases to store the data or retrieve it. Statistical methodologies might be a t-test for a two group comparison or a regression model to look at multivariate relationships. The selection of these tools and methodologies will be guided in part by the specification of the problem as well as the resources available and the audience who will receive the analysis. Identifying the optimal methodological approach, given the constraints put on the problem, is the unique job of the analyst. The analyst may need to delegate certain tasks based on the expertise available on the team.\nWrangling the Team\nAny interesting or complex data analysis will likely involve people from different disciplines. In academia, you might be working with a biologist, an engineer, a computer scientist, and a physician at the same time. In business, you might need to interact with finance, marketing, manufacturing, and data engineering in a given analysis. A difficult part of an analyst’s job is managing all of these people’s interests simultaneously while integrating them into the final analysis.\nThe challenge facing the analyst is that each discipline is likely to think that their interests take priority over everyone else’s. Furthermore, collaborators are likely to think that the problems relevant to their discipline are the most important problems to address. However, the analyst cannot accept that every discipline is “tied for first place”; priorities must be set and a reasonable assessment must made regarding which problems should addressed first, second, and so on. This is a delicate operation on the part of the analyst and doing it successfully requires open communication and good relationships with the collaborators.\nThe fight for priority is also why it can be so difficult for a data analysis to be modularized. If an analysis is passed from person to person, then each person is inclined to take their specific disciplinary perspective on the problem and ignore the others. This is a naturally occurring phenomenon and it is the analyst’s job to prevent it from happening, lest the analysis be watered down to be incoherent or too tightly focused on one aspect. Ultimately, the analyst must take responsibility to see the “big picture” of the analysis, weigh each collaborator’s views, and choose a path that is acceptable to everyone.\nIt’s not an easy job.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:32:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-08-15-the-law-and-order-of-data-science/",
    "title": "The Law and Order of Data Science",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-08-15",
    "categories": [],
    "contents": "\nOne conversation I’ve had a few times revolves around the question, “What’s the difference between science and data science?” If I were to come up with a simple distinction, I might say that\n\nScience starts with a question; data science starts with the data.\n\nWhat makes data science so difficult is that it starts in the wrong place. As a result, a certain amount of extra work must be done to understand the context surrounding a dataset before we can do anything useful.\nProcedure for procedurals\nOne of my favorite shows growing up was Law & Order, one of the longest running “procedural” shows that also produced a number of spinoffs. I remember coming home from school in the afternoon, flipping on TNT and watching whatever Law & Order marathon was currently being aired (there was always something).\nOne distinct feature of the show is that pretty much every episode followed the exact same format:\nThe episode starts with the discovery that a crime has occurred, usually a murder.\nThe first half of the episode involves the police retracing history trying to figure out who committed the crime (the “Order” part).\nThe police catch a suspect at exactly 22 minutes into the episode.\nIn the second half of the episode, the district attorneys take over and prosecute the suspect in court (the “Law” part). Usually, they win.\nThe format of the show is in fact highly unusual. While it starts off with a mystery (who committed this murder?) as many shows do, the mystery is solved half way through the episode! I can imagine some TV executive 30 years ago wondering, “What the heck are you going to do for the rest of the episode if you solve the mystery half way through?”\nWhat made the show interesting to me was that in the second half of every episode, the lawyers for the government had to convince a jury that they had the right suspect. They had to present evidence and make an argument that this person was guilty. Of course, because this is TV, they usually won the argument, but I think many TV shows would have been satisfied with just catching the criminal. In most shows, presenting the data and the evidence to an audience is not interesting, but on Law & Order, it was.\nThe Law & Order metaphor\nMany data science projects essentially follow this format, because we start with the data. Data are available. We often don’t get to participate in its generation. Perhaps the data were collected as part of some administrative records system, or as part of some logging system, or as part of some other project addressing a different question. The initial challenge of any data science project is figuring out the context around the data and question that motivated its origination. A key milestone is then figuring out how exactly the data came to be (i.e. who committed this “crime”?).\nOnce we’ve figured out the context around the data, essentially retracing the history of the data, we can then ask “Are these data appropriate for the question that I want to ask?” Answering this question involves comparing the context surrounding the original data and then ensuring that it is compatible with the context for your question. If there is compatibility, or we can create compatibility via statistical modeling or assumptions, then we can intelligently move forward to analyze the data and generate evidence concerning a different question. We will then have to make a separate argument to some audience regarding the evidence in the data and any conclusions we may make. Even though the data may have been convincing for one question, it doesn’t mean that the data will be convincing for a different question.\nData science often starts with the data, but in an ideal world it wouldn’t. In an ideal world, we would ask questions and carefully design experiments to collect data specific to those questions. But this is simply not practical and data need to be shared across contexts. The difficulty of the data scientist’s job is understanding each dataset’s context, making sure it is compatible with the current question, developing the evidence from the data, and then getting an audience to accept the results.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:32:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-08-09-the-trillion-dollar-question/",
    "title": "The Trillion Dollar Question",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-08-09",
    "categories": [],
    "contents": "\nRecently, Apple’s stock price rose to the point where the company’s market valuation was above $1 trillion, the first U.S. company to reach that benchmark. Subsequently, numerous articles were published describing Apple’s journey to this point and why it got there. Most people describe Apple as a technology company. They make technology products: iPhones, iPads, Macs, etc. These are all computing devices. But there is another way to think of Apple and what kind of company they are as well as how they became so successful.\nNeil Cybart, an analyst over at Above Avalon, likes to describe Apple as a design company focused on building useful tools for people. Of the latest round of profiles on Apple reaching a $1 trillion market valuation, he writes:\n\nDespite supposedly being about chronicling how Apple went from near financial collapse in the late 1990s to a trillion-dollar market cap, a number of articles did not include any mention of Jony Ive [Apple’s Chief Design Officer], or even design for that matter. To not include Jony Ive in an article about Apple’s last 20 years is unfathomable, demonstrating a clear misunderstanding of Apple’s culture and the actual reasons that contributed to Apple’s success. Simply put, such profiles failed in their pursuit of describing Apple’s journey to a trillion dollars. Apple is where it is today because of design – placing an emphasis on how Apple products are used. Every other item or variable is secondary. [emphasis added]\n\nAs long as I have followed computers people have complained that Apple’s hardware is substandard. Other companies like Dell, Gateway, Acer, and Lenovo, had long been making computers that were “better” than Apple’s hardware. Apple’s value has always been selling good hardware coupled with premium software. But for a long time that was not appreciated by the market and Apple almost went bankrupt as a result.\nThe “Speeds and Feeds” Era for Data Analysis\nWhen I was growing up, computers were all about so-called “speeds and feeds”. The only things people talked about were the megahertz of their processor or how many megabytes of RAM a computer had. A computer with a higher megahertz CPU was by definition better than a computer with a lower megahertz CPU. More RAM was better than less RAM and more disk space was better than less disk space. It was easy to compare different computers because we had quantitative metrics to go by. The hardware itself was a commodity and discussion about software was nonexistent because every computer ran the same software: Windows.\nWe are very much in the “speeds and feeds” era for data analysis right now. There is tremendous focus on and fascination with the tools and machinery underlying data analysis. Deep learning is only one such example, along with an array of related machine learning tools. Web sites like Kaggle promote a culture of “performance” where the person who can cobble together the most accurate algorithm is a winner. It’s easy to compare different algorithms to each other because there is often a single metric of performance that we can easily agree to compare.\nSerious investment is being made in improving algorithms to make them more accurate, efficient, and powerful. We need these algorithms to be better so that we can have self-driving cars, intelligent assistants, fraud detection, and music discovery. Even the hardware itself is being optimized to improve the performance of these specific algorithms. This is the call of “more gigahertz, more RAM, more disk space” of our time. As easy hardware wins are fading into the past (as shown by Intel’s struggle), the focus is on improving the performance of machine learning software running on top of it.\nAll of this is necessary if we want to reap the benefits of machine learning algorithms in our daily lives. But if the computing industry has anything to teach the data science industry, it’s that perhaps the more interesting stuff is yet to come. Furthermore, it suggests that the companies (and perhaps individuals) with the best speeds and feeds will not necessarily be the winners.\nWhat Comes Next?\nToday, it could be argued that the most profitable “computer” in the world is the iPhone, which to be sure, has better “speeds and feeds” than any computer from my childhood. But it is by no means the fastest computer today. Nor does it have the most RAM, the most disk space, or the best graphics. How can that be?\nOf course, the focus of computing changed from desktop to laptop to mobile, in part due to the great advancement in chip technology and miniaturization. So the benefit was not in greater speeds and feeds, but rather in smaller sizes for the same speeds and feeds. With these smaller, more personal, devices, the software and the design of the system became of greater importance. People were not using these devices to “crunch numbers” or do complex, but highly specialized, tasks. Rather, they were using them to do everyday tasks, like checking email, surfing the web, and communicating with friends. These were not business machines; they were for the mass market.\nArguable, the emphasis that Apple places on design has made it the most successful computer company of today because design is what creates the best user experience today in the mass market. Data science remains a niche area of work today even though its popularity and application has exploded over just a few years. It’s difficult for me to see how it might move into a mass market position, but I can see more and more people doing and consuming data analysis in the future. As the population of data analysis consumers grows, I think people will become less focused on accuracy and prediction metrics and more focused on whether a given analysis achieves a specified goal. In other words, data analyses will have to be designed to accomplish a certain task. The better individuals are at designing good data analyses, the more successful they will be.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:32:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-27-why-i-indent-my-code-8-spaces/",
    "title": "Why I Indent My Code 8 Spaces",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-07-27",
    "categories": [],
    "contents": "\nJenny Bryan recently gave a wonderful talk at the Use R! 2018 meeting in Brisbane about “Code Smells and Feels” (I recommend you watch a video of that talk). Her talk covers various ways to detect when your code “smells” and how to fix those smells through refactoring. While there is quite a bit of literature on this with respect to other programming languages, it’s not well-covered with respect to R.\nIn the video version of the talk (not in the slides) Jenny calls out my particular indentation rule, which is to use 8 spaces. In my experience, people tend to find this a rather extreme indentation policy, with maybe 4 spaces being at the outer limit of what they could imagine. But I’ve been using 8 spaces for a long time now and I’ve found that it has a number of benefits.\nFirst off, I did not make up the 8 space indent. I got it from the Linux kernal coding style document. Chapter 1 says:\n\nTabs are 8 characters, and thus indentations are also 8 characters. There are heretic movements that try to make indentations 4 (or even 2!) characters deep, and that is akin to trying to define the value of PI to be 3.\n\nI’ve found the Linux kernal coding style to be pretty useful for my R programming, but a lot of it is C-specific and so not relevant. Nevertheless, it’s worth a quick peruse.\nPersonally, I’ve found 8 spaces is good for my aging eyes. I think my rule is that the appropriate number of spaces of indentation is proportional to the square of my age (I’m still working on that model though). At this point, code with a 2 space indent is indistinguishable from flush left.\nBefore going on, I have to emphasize that the 8 space indent cannot exist in isolation. It has to be coupled with a right-hand side limit of 80 columns. Otherwise, you could just indent yourself off to infinity and there would be no consequences. An 80 column limit forces you to keep your code within reasonable limits. Also, if someone ever needs to read your code on a PDP/11 you’ll be A-okay.\nMost importantly though, I’ve found that the 8 space indent serves as a kind of early warning system for “smelly” code. Jenny gave some smelly examples in her talk and I thought I’d reproduce them here. This first example, as Jenny describes, suffers from not using the available class predicate functions to test for “numeric” or “integer”. Here’s the example with a 2 space indent.\nbizarro <- function(x) {\n  if (class(x)[[1]] == \"numeric\" || class(x)[[1]] == \"integer\") {\n    -x\n  } else if (class(x)[[1]] == \"logical\") {\n    !x\n  } else { \n    stop(...) \n  }\n}\nThat first if state sticks out a little bit because it’s rather long. Better code might use the is.numeric() and is.integer() functions.\nHere’s the same example with an 8 space indent.\n\n\nbizarro <- function(x) {\n        if (class(x)[[1]] == \"numeric\" || class(x)[[1]] == \"integer\") {\n                -x\n        } else if (class(x)[[1]] == \"logical\") {\n                !x\n        } else { \n                stop(...) \n        }\n}\n\n\n\nAlthough, it’s not egregious, that first line is pushing up against the 80 column limit on the right-hand side. You might not do anything about it in this case, but the purpose of the indenting system is to at least trigger a reaction.\nThe next example from Jenny’s talk is a bit more obvious. Here she gives a lesson in excessive if-else statements. The original code with 2 space indent is here.\nget_some_data <- function(config, outfile) {\n  if (config_ok(config)) {\n    if (can_write(outfile)) {\n      if (can_open_network_connection(config)) {\n        data <- parse_something_from_network()\n        if(makes_sense(data)) {\n          data <- beautify(data)\n          write_it(data, outfile)\n          return(TRUE)\n        } else {\n          return(FALSE)\n        }\n      } else {\n        stop(\"Can't access network\")\n      }\n    } else {\n      ## uhm. What was this else for again?\n    }\n  } else {\n    ## maybe, some bad news about ... the config?\n  } \n}\nNow, it’s fair to say that this code already looks a bit smelly (fishy?), but it’s maybe passable from a visual standpoint. Let’s take a look at it with 8 space indents.\n\n\nget_some_data <- function(config, outfile) {\n        if (config_ok(config)) {\n                if (can_write(outfile)) {\n                        if (can_open_network_connection(config)) {\n                                data <- parse_something_from_network()\n                                if(makes_sense(data)) {\n                                        data <- beautify(data)\n                                        write_it(data, outfile)\n                                        return(TRUE)\n                                } else {\n                                        return(FALSE)\n                                }\n                        } else {\n                                stop(\"Can't access network\")\n                        }\n                } else {\n                        ## uhm. What was this else for again?\n                }\n        } else {\n                ## maybe, some bad news about ... the config?\n        } \n}\n\n\n\nNow the code looks downright absurd, practically crying out for refactoring, and rightly so! The five levels of nesting will be unreadable as soon as you blink your eyes.\nThat’s basically it. I’ve found zero downsides to using an 8 space indent and a number of upsides, including cleaner, more modular code. Because the visual indicator penalizes against lots of indenting, you are usually forced to write out separate functions to handle different tasks rather than go one more level in. This not only has the benefit of being modular, but it’s also useful for things like profiling (it can be very uninformative to profile a single monolithic function).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:32:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-23-partitioning-the-variation-in-data/",
    "title": "Partitioning the Variation in Data",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-07-23",
    "categories": [],
    "contents": "\nOne of the fundamental questions that we can ask in any data analysis is, “Why do things vary?” Although I think this is fundamental, I’ve found that it’s not explicitly asked as often as I might think. The problem with not asking this question is that it can often lead to a lot of pointless and time-consuming work. Taking a moment to ask yourself, “What do I know that can explain why this feature or variable varies?” can often make you realize that you actually know more than you think you do. Developing an understanding of the sources of variation in the data is a key goal of exploratory data analysis.\nWhen embarking on a data analysis, ideally before you look at the data, it’s useful to partition the variation in the data. This can be roughly broken down into to categories of variation: fixed and random. Within each of those categories, there can be a number of sub-categories of things to investigate.\nFixed variation\nFixed variation in the data is attributable to fixed characteristics in the world. If we were to sample the data again, the variation in the data attributable to fixed characteristics would be exactly the same. A classic example of a fixed characteristic is seasonality in time series data. If you were to look at a multi-year time series of mortality in the United States, you would see that mortality tends to be higher in the winter season and lower in the summer season. In a time series of daily ozone air pollution values, you would see that ozone is highest in the summer and lowest in the winter. For each of these examples, the seasonality is consistent pretty much every year. For ozone, the explanation has to do with the nature of atmospheric chemistry; for mortality the explanation is less clear and more complicated (and likely due to a combination of factors).\nData having fixed variation doesn’t imply that it always has the same values every time you sample the data, but rather the general patterns in the data remain the same. If the data are different in each sample, that is likely due to random variation, which we discuss in the next section.\nUnderstanding which aspects of the variation in your data are fixed is important because often you can collect data on those fixed characteristics and use them directly in any statistical modeling you might do. For example, season is an easy covariate to include because we already know when the seasons begin and end. Using a covariate representing month or quarter will usually do the trick.\nExplaining the variation in your data by introducing fixed characteristics in a model can reduce uncertainty and improve efficiency or precision. This may require more work though, in the form of going out and collecting more data or retrieving more variables. But doing this work will ultimately be worth it. Attempting to model variation in the data that is inherently fixed is a waste of time and will likely cost you degrees of freedom in the model.\nIn my experience looking at biomedical data, I have found that a lot of variation in the data can be explained by a few fixed characteristics: age, sex, location, season, temperature, etc. In fact, often so much can be explained that there is little need for explicit models of the random variation. One difficult aspect of this approach though is that it requires a keen understanding of the context surrounding the data as well as having a good relationship with a subject matter expert who can help inform you about the sources of variation. Investing in learning more about the data, before digging into the data itself, can save you a lot of time in the modeling phase of data analysis.\nRandom variation\nOnce we’ve partitioned out all of the variation in the data that can be attributed to fixed characteristics, what’s left is random variation. It is sometimes tempting to look at data and claim that all of the variation is random because then we can model it without collecting data on any more covariates! Developing new and fancy models can be fun and exciting, but let’s face it, we can usually eliminate the need for all that by just collecting a little better data. It’s useful to at least hypothesize about what might be driving that observed variation and collect the extra data that’s needed.\nRandom variation causes data to look different every time we sample it. While we might be quite sure that ozone is going to be high in the summer (versus the winter), that doesn’t mean that it will always be 90 parts per billion on June 30. It might be 85 ppb one year and 96 ppb another year. Those differences are not easily explainable by fixed phenomena and so it might be reasonable to characterize them as random differences. The key thing to remember about random variation in the data is\n\nRandom variation must be independent of the variation attributable to fixed characteristics\n\nIt’s sometimes said that random variation is just “whatever is leftover” that we could not capture with fixed features. However, this is an uncritical way of looking at the data because if there are fixed characteristics that get thrown in the random variation bin, then you could be subjecting your data analysis to hidden bias or confounding. There are some ways to check for this in the modeling stage of data analysis, but it’s better do what you can to figure things out beforehand in the discovery and exploration phases.\nOne application where random variation is commonly modeled is with financial market data, and for good reason. The efficient-market hypothesis states that, essentially, if there were any fixed (predictable) variation in the prices of financial assets, then market participants would immediately seize upon that information to profit through arbitrage opportunities. If you knew for example that Apple’s stock price was always low in the winter and high in the summer, you could just buy in the winter and sell in the summer and make money without risk. But if everyone did that, then eventually that arbitrage opportunity would go away (as well as the fixed seasonal effect). Any variation in the stock price that is leftover is simply random variation, which is why it can be so difficult to “beat the market”.\nIs it really random?\nWhen I see students present data analyses, and they use a standard linear model that has an outcome (usually labeled Y), a predictor (X), and random variation or error (e), my first question is always about the error component. Usually, there is a little confusion about why I would ask about that since that part is just “random” and uninteresting. But when I try to lead them into a discussion of why there is random variation in the data, often we discover some additional variables that we’d like to have but don’t have data on.\nUsually, there is a very good explanation of why we don’t have those data. My point is not to criticize the student for not having data that they couldn’t get, but to emphasize that those features are potential confounders and are not random. Just because you cannot obtain data about something doesn’t mean you can declare something random by fiat. If data cannot be collected on those features, it might be worth investigating whether a reasonable surrogate can be found. Finding a surrogate may not be ideal, but it can usually give you a sense of whether your model is completely off or not.\nOne example of using a surrogate involves estimating smoking prevalence in a population. Data about smoking behaviors is available in some surveys in the United States, but comprehensive data across the nation is not. In a recent study about mortality and air pollution by Zeger et al., they used lung cancer as a surrogate. The logic here is that lung cancer is generally caused by smoking, and so although it’s not a perfect indicator of smoking prevalence, it is a rough surrogate for that behavior.\nSummary\nPartitioning your data into fixed and random components of variation can be a useful exercise even before you look at the data. It may lead you to discover that there are important features for which you do not have data but that you can go out and collect. Making the effort to collect additional data when it is warranted can save a lot of time and effort trying to model variation as if it were random. More importantly, omitting important fixed effects in a statistical model can lead to hidden bias or confounding. When data on omitted variables cannot be collected, trying to find a surrogate for those variables can be a reasonable alternative.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:32:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-12-use-r-keynote-2018/",
    "title": "Teaching R to New Users - From tapply to the Tidyverse",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-07-12",
    "categories": [],
    "contents": "\nAbstract\nThe intentional ambiguity of the R language, inherited from the S language, is one of its defining features. Is it an interactive system for data analysis or is it a sophisticated programming language for software developers? The ability of R to cater to users who do not see themselves as programmers, but then allow them to slide gradually into programming, is an enduring quality of the language and is what has allowed it to gain significance over time. As the R community has grown in size and diversity, R’s ability to match the needs of the community has similarly grown. However, this growth has raised interesting questions about R’s value proposition today and how new users to R should be introduced to the system.\nNOTE: A video of this keynote address is now available on YouTube if you would prefer to watch it instead.\nIntroduction\nIf we go to the R web site in order to discover what R is all about, the first sentence we see is\n\nR is a free software environment for statistical computing and graphics.\n\nI haven’t been to the R web site in quite some time, but it struck me that the word “data” does not appear in that first sentence.\nIf we similarly travel to the tidyverse web site, we find that\n\nThe tidyverse is an opinionated collection of R packages designed for data science\n\nIt turns out that these two sentences, found on these two web sites. say a lot about the past, present, and future of R.\nIntentional ambiguity\nR inherits many features from the original S language developed at Bell Labs and subsequently sold as S-PLUS by Insightful (and now owned by Tibco). So it’s useful look back into some of the S history to gain some insight into R. John Chambers, one of the creators of the S language, said in the “Stages in the Evolution of S” (sadly no longer available online except via the Internet Archive):\n\nThe ambiguity [of the S language] is real and goes to a key objective: we wanted users to be able to begin in an interactive environment, where they did not consciously think of themselves as programming. Then as their needs became clearer and their sophistication increased, they should be able to slide gradually into programming, when the language and system aspects would become more important.\n\nChambers was referring to the difficulty in naming and characterizing the S system. Is it a programming language? An environment? A statistical package? Eventually, it seems they settled on “quantitative programming environment”, or in other words, “it’s all the things.” Ironically, for a statistical environment, the first two versions did not contain much in the way of specific statistical capabilities. In addition to a more full-featured statistical modeling system, versions 3 and 4 of the language added the class/methods system for programming (outlined in Chambers’ Programming with Data).\nIn discussing the rationale for developing the S system, Chambers writes\n\nWe were looking for a system to support the research and the substantial data analysis projects in the statistics research group at Bell Labs.\n\nHe further writes\n\n…little or none of our analysis was standard, so flexibility and the ability to program were essential from the start.\n\nFrom the beginning flexibility and the ability to program were two key needs that had to be satisfied by the S language.\nR Enters the Fray\nIt was into this world that R came to be. As a dialect of the S language, R proved easy to adopt for previous users of S-PLUS. The fact that R was also free software didn’t hurt adoption either. But it’s worth noting that for the most part, people already had tools for analyzing data. They came in the form of SAS, Stata, SPSS, Minitab, Microsoft Excel, and my personal favorite, XLisp-Stat (thanks Luke Tierney!). But the commonly used data analysis packages had some key downsides:\nThe graphics were too “quick and dirty” and did not allow much control over the details; they plotted the data, but that was about it;\nThere was relatively little ability to build custom tools on top of what was available (although some capability was added to most packages later).\nR solved these two key problems by providing an “environment for statistical computing and graphics”. Like with S, R provided flexibility and the ability to program whatever one needed. The graphical model was similar to the S-PLUS system, whereby total control was given to the user and data graphics could be built up piece by piece. In other words, few decisions were made by the system when creating data graphics. The user was free to make all the decisions.\nWith the non-graphics aspects of the R system, the user was similarly free to handle data however the language would allow. This feature was well-suited to the academic audience that initially adopted R for implementing new statistical methodology. Furthermore, those who already knew programming languages like C (or Lisp!) found R’s programming model very familiar and easily extended. R’s package system allowed developers to extend the existing functionality by adding new kinds of models, plots, and tools.\nR for Data Analysis?\nGoing back to Chambers’ philosophy of S and the “user-developer” spectrum, it’s interesting to think about how R fit into this spectrum, given the population of users that were drawn to it.\nUser-developer spectrumWhile Chambers thought of S as covering the entire spectrum at the time, it seems clear in retrospect that R actually fell quite squarely on the “developer” end of the spectrum. That is, the features that made R stand apart, given the context in which R was initially released, were really developer features. The programming language and the ability to take fine control over all the details are things that would appeal to people who are developing new things.\nWhat this meant is that initially, R was not a great system for doing data analysis. The truth is it was harder to do data analysis in R than it was in most other systems. Programs like SAS, Stata, and SPSS had been designed as interactive data analysis environments and had simple ways to do complex data wrangling.\nImagine a new user to R who’s interested in doing data analysis. Consider the basic task of splitting a data frame according to the levels of some grouping (factor) variable and then taking the mean of another variable within the groups. Today, you might do something like\nlibrary(dplyr)\ngroup_by(airquality, Month) %>% \n    summarize(o3 = mean(Ozone, na.rm = TRUE))\nThis code uses the dplyr package from the tidyverse set of packages to take the monthly mean of ozone in the airquality dataset in R. To understand it, you need to understand the concept of a data frame and of tidy data. But beyond that, the code is reasonably self-explanatory.\nUsing just the functionality that came with the base R system, you’d have to do something like\naggregate(airquality[, “Ozone”], \n          list(Month = airquality[, “Month”]), \n          mean, na.rm = TRUE)\nRight away, questions arise:\nWhat are the square brackets [ for? They are used for subsetting a data frame.\nWhat is list()? It’s a type of R object.\nWhy is the mean() function just sitting there all by itself? It is being passed to the subsets of the data frame via a kind of internal lapply(). (Follow up: What is lapply()?)\nIs na.rm = TRUE an argument to aggregate()? No, it is an argument to mean(), but it is being passed to mean() via the ... argument of aggregate().\nIn other words, in order to take the mean within groups of a variable in a data frame, a pretty basic data summary operation for almost all other packages, you had to explain\ndifferent kinds of R objects;\nsubset operators;\nfunctional mapping;\nand variable argument lists.\nAt this point, you were basically describing R as a programming language to people who likely had no interest in programming (at least not yet).\nAs confusing (and possibly ridiculous) as this list of requirements might sound, it often was not a barrier to people back in the year 2000. The reason is that people who were new to R had not chosen to learn R because they needed a system for doing data analysis. They already had a system for doing data analysis. Rather, they were looking for a system to do the things their current system didn’t allow them to do. In other words, they were looking for a system that had “flexibility and the ability to program.” R was a great system for that.\nMaking R a “Real Programming Language” Almost Killed It\nAs is so often the case with technological products, the aspects of those products that are considered its advantages can quickly become their key weaknesses. R’s flexibility and focus on programming language power served as a major barrier to those who simply wanted to analyze data. However, like with many classic case studies of disruption, this barrier was not immediately recognized as such. The result was continuing efforts to make R a better programming language.\nOne of the major developments of the 2000s was the development of the S4 class/methods system. In many ways, the S4 system was a major advance, because it provided a “real” system for doing object oriented programming in R. The system was formal, unlike the ad hoc S3 system which contained within it many ambiguities. Other features added to R during this time (to name a few) were the namespace system for packages and the localization/internationalization features. All of these features made up for key deficiencies in the system and provided powerful new tools for developers to create more sophisticated R packages. They brought R closer to a “real” programming languages that could be discussed in the same breath as python or perl. While these features benefited users (particularly the localization and internationalization efforts), none of them directly empowered non-programming users to do more with R. They were primarily developer-focused.\nAt this point in R’s development the user who simply wanted to analyze some data was being “over-served”. They did not need a formal system for object-oriented programming and did not need a mechanism to allow others to provide translations for their R packages. They just wanted to take the mean of a bunch of columns within groups in a data frame. We still had no simple solution for that problem. This state of affairs left a substantial gap to be filled for simple R-based data analytic tools.\nTidyverse Disruption with a Twist\nWhen tidyverse-related tools, including ggplot2, first started showing up (long before the name “tidyverse” existed), long-time users of R did not see why they were needed. It was difficult for many of us to see the perspective of the new user who had basic data analytic needs. These new users\nwere not familiar with a collection of existing data analysis packages;\nhad not used S-PLUS previously;\ndid not necessarily have experience with other programming languages like C or Perl;\nwere likely encountering data analysis and statistics for the first time\nThe people with this set of characteristics represented a new and fast-growing population of potential R users. Many were being turned away from R because of\nflexibility –> complexity; and\nability to program –> requirement for programming\nSuddenly, the flexibility and ability to program so valued by a previous population of R users were lead weights dragging the system down for new users. A different interface was needed.\nThe tidyverse set of tools, in addition to providing that different interface for new users, adopted a particular point of view on how the various tools would work together. The focus on “tidy data” as a unifying principle allowed a relatively small set of tools to provide a wide range of operations when it came to data wrangling. The opinionated nature of the tools naturally limited somewhat the flexibility of how things could be done. But this reduced complexity was what made the tools so appealing to new users. There were just fewer things to learn. The use of non-standard evaluation was (perhaps ironically) more intuitive for data wrangling tasks because there was no need to quote variable names and see everything from the perspective of a developer or programmer.\nFurthermore, the tidyverse did not abandon all that had come before it. Rather, it built on top of the infrastructure that had been built previously:\nThe tidyverse tools are built as R packages, eliminating the need to make direct changes to the core R system (and to get permission to do so);\nAlthough the packages eschew the formal S4 class/methods system, they make extensive use of the S3 class/method system, which when coupled with the namespace mechanism for R packages are a powerful alternative.\nIn addition to all this, the ggplot2 package exploded in popularity, largely because it provided an easy way to make good graphics quickly (one might dare say “quick and dirty”). It “shortened the distance between the mind and the page” and removed the need for R users to learn intricate plot arguments and incantations. Rather than carefully building plots piece by piece as with the base R system, most of the defaults for ggplot2 were good enough and the beginner did not have to make a lot of decisions. Between ggplot2 and the initial tidyverse tools, R users finally had a system for doing data analysis in the manner of packages that came before it. However, that functionality was not simply replicated; it greatly improved upon it, as is evidenced by the popularity of both ggplot2 and the tidyverse.\nWith traditional technological disruption, the members of the old class are left behind. But far from wiping the slate clean, as perhaps might have occurred with other software packages, the tidyverse provided functionality that all R users could choose to take advantage of or not. People with long-standing workflows could keep their workflows. The tidyverse essentially built a new “language” on top of the existing language without breaking anything fundamental for existing users. Ultimately, the tidyverse was able to disrupt the existing R system without needing to leave previous users behind. Everyone could come along for the ride and benefit.\nIntentional Ambiguity Redux\nWith the tidyverse “interface” to the R language, along with its existing programming paradigm dating back to the system’s origin, we could argue that R has achieved Chambers’ original vision of “intentional ambiguity”. For some users, R is a flexible system allowing the ability to program and develop new things. For other users, R is a powerful data analysis system that contains tools for data wrangling, visualization, and statistical modeling. The system spans the entire “user-developer” spectrum without anyone having to make a hard choice between either end. Rather, R users are free to wander back and forth between the two ends.\nBut nothing ever stands still, and users of the tidyverse system will need a system for programming too. However, they will not want to program in the style of the original R system. This would be like asking a user of laptop computers to go back to using a desktop computer. No, they want more power in their laptop.\nOne thing to be wary of going forward is that one of the tidyverse’s greatest assets–the generous use of non-standard evaluation–could end up being a critical liability. While non-standard evaluation greatly simplifies interactive work in R, programming in a world with non-standard evaluation can be confusing, especially when one must simultaneously deal with other tools that make use of standard evaluation. Tools and infrastructure are needed to allow users to become developers in this new paradigm without too much pain. Thankfully, many are working on this and I have little doubt that a new programming environment will exist in the near future that allows for the easy development of “tidyverse tools” by a large segment of the R community.\nSelling R\nAt this point, one might have some difficulty describing the value proposition of R to someone who had never seen it before. Is it an interactive system for data analysis or is it a sophisticated programming language for software developers? Or is system for developing reproducible workflows in data analysis? Or is it a platform for developing interactive graphics, dashboards, and web apps? Or is it a language for doing complex data wrangling and data management? Or…\nThe confusion over how to “sell R” is reflective of the increasingly diverse audience of people using R. The open source nature of R gives it quick entry into many different fields, as long as there are interested users intent on adapting R to their needs. Over time we have seen R enter many scientific communities, including ecology, astronomy, high throughput biology, neuroimaging, and many others. Outside academia, we have seen R adopted in journalism, tech companies, and various businesses for data analysis. The spread of R is in part because members of those communities saw something attractive about R that could be applied in their field. Yes, free and open source is a big win, but not every organization is short of cash and there are other systems (like Python, or even XLisp-Stat) that share those same properties. So there must be other reasons to adopt R.\nIn the distant past, my pitch for using R usually involved three things:\nFree. R was both free as in cost and free as in free software. The free cost part made it a highly accessible package and the free software part allowed for anyone to tinker with the package, inspect its code, and make improvements. This was particularly important in the old days when R was unknown and it’s “accuracy” was put in question.\nGraphics. R was able to produce high quality “publication ready” graphics and it gave you detailed control over all the graphical elements. S-PLUS could also do this but S-PLUS didn’t come with the free part mentioned above.\nProgramming language. Unlike packages like SAS, Stata, or SPSS, R came with a robust and sophisticated Lisp-like programming language that was well-suited for data analysis applications. In addition, you could use it to build packages that could extend the core R system. Furthermore, it was a language designed for doing data analysis. That made it appear weird to people familiar with traditional programming languages.\nMuch has changed since those early days and I’ve varied my pitch quite a bit to focus on a few different things that didn’t exist back then. In particular, the audience has changed. I talk to many more people who are just getting started in data analysis and therefore are a bit more open-minded about which software to use. They don’t have years of SAS baked into their brains. They may have heard of Python as an alternate system, but they are much more likely to be open-minded about what system to use.\nSome of the things I focus on now are\nReproducibility, Reporting, and Automation. With the development of knitr and its combination with R Markdown, the writing of reproducible reports was made infinitely easier. (Markdown itself, probably deserves its own discussion, but it’s not specifically R-related.)\nGraphics. R still has the ability to make great data graphics and with the introduction of ggplot2, it has become easier to make and extend good graphics.\nR Packages and Community. With over 10,000 packages on CRAN alone, there’s pretty much a package to do anything. More importantly, the people contributing those packages and the greater R community have expanded tremendously over time, bringing in new users and pushing R to be useful in more applications. Every year now there are probably hundreds if not thousands of meetups, conferences, seminars, tutorials, and workshops all around the world, all related to R.\nRStudio. The development of the RStudio IDE has made getting started with R much easier. Having a powerful IDE was important to me for learning other languages and I’m glad R finally has something solid for itself. RStudio has significantly simplified the development of R packages via devtools and roxygen2. While it’s not yet perfect, these tools have changed what used to be a labor-intensive and finicky process into a more manageable and easier to learn work flow. In addition, RStudio has funded the development of many critical R packages, including the members of the tidyverse.\nAt a recent unconference held in Melbourne, a number of people had different approaches to convincing others to use R. Here is just a summary:\nSomeone mentioned Bioconductor, which is a huge resource for those doing research in the world of high throughput biology. Not many other analysis packages have something similar so it makes an obvious selling point to people working in this area.\nThe idea of using R end-to-end came up, meaning using R to clean up messy data and taking it all the way to some interactive Shiny app on the other end. The idea that you can use the same tool to do all the things in between made for a compelling case for using R.\nFor the spreadsheet audience, the dplyr package specifically was sometimes a good selling point. The idea here was that you could show people how much time could be saved by automating analyses and using dplyr to clean up data.\nThe open source nature of R came up a few times, primarily as a means for developing transferable skills. If you work at a company/institution that specializes in some proprietary package, it’s often difficult to transfer those skills somewhere else if your new job doesn’t use that package. The fact that R is open source means that, in theory, you could use it anywhere and the skills that you build up in R are (again, in theory) applicable everywhere.\nSomeone mentioned that if you want to convince someone to learn/use R, just show them the multitude of jobs available to R programmers (and in particular, the salaries attached to them).\nThe fact that R is obtainable for free is still important, given that Matlab and SAS licenses have not gotten any cheaper over time. In my experience, this is particularly important in non-industrialized countries where for many people paying for expensive licenses is not an option.\nThe reasons for using R are as varied as the people using R, and that’s a great thing. Moving forward, we must resist the urge to view R through a narrow window and to sell it as a single thing with a few features. The freedom given to the R community to characterize it the way they see best fit is what will give R increased relevance over time.\nConclusion\nR has grown significantly over the past 20 years, but has only recently has developed all the elements of John Chambers’ original vision of a full-fledged data analysis system coupled with a sophisticated programming language. With the variety of tools that have been tacked on to the core system via packages, R’s chimera-like appearance can be a bit baffling to long-time users. But this appearance reflects R’s greatest feature, and it’s most valuable selling point, which is its ability to disrupt and reinvent itself to suit the needs of a new population of users.\nIt’s worth noting in closing that R is a language for data analysis. If R seems a bit confusing, disorganized, and perhaps incoherent at times, in some ways that’s because so is data analysis. The fact that we can have one language serve the infinite possible ways that people might approach any data analysis problem is pretty remarkable. But it does lead to a little bit of head scratching for people who are used to more “normal” programming languages.\nAs time goes on, R will be confronted with a new population of users who take different things for granted (e.g. the tidyverse) and will have very different requirements. When this happens, we must ensure that the things that make R great now do not become the very reasons for resisting change. That said, I believe we can be confident that the R language is flexible enough to meet the requirements for future new users.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:32:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-06-data-creators/",
    "title": "What Should be Done When Data Have Creators?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-07-06",
    "categories": [],
    "contents": "\nI was listening to the podcast The West Wing Weekly recently and Episode 4.17 (“Red Haven’s on Fire”) featured former staff writer Lauren Schmidt Hissrich. In introducing her, the podcast co-hosts mentioned that Hissrich was a writer for the Netflix series Daredevil, based on the Marvel Comics character. She is also the showrunner for a new Netflix series called The Witcher, which is based on a book by Andrzej Sapkowski.\nAll this got me thinking about how screenwriters are often limited in what they can write by the fact that the material they are writing was originated by someone else. The characters in their stories have “creators” and there is an extent to which writers have to remain true to their creators’ intent for the characters. From a business perspective, it can be seen as a safer bet if a writer creates some story based on existing intellectual property, since that IP may already be popular. But, writers also have to be careful that they don’t go too far in a different direction or those same people who loved the original will dislike the new material.\nLessons from Screenwriters\nScreenwriters often write their own original material, particularly when they are at the top of their profession and have the artistic freedom to do more of what they want. They have earned this freedom and the studios/producers trust them to produce original material that is good and financially successful. In these cases, screenwriters can create their characters and their stories and take them where they want to go, subject to the many constraints inherent in producing TV or movies.\nIn many other situations, screenwriters are writing other people’s material. A writer working on a TV show likely did not create the TV show and did not create any of its main characters or storylines. They have to enter the show midstream, adapt to its characters, and write material that would be appropriate for the show and the individual characters. While a writer still needs to be creative in this role, there are quite a few more constraints in that they have to conform to what has come before. In particular, the show’s creators may have very strong opinions about how characters and storylines should work and a staff writer will have to go along with it in order to work out.\nThe Data Creator\nIn data science, data have “creators” and a big part of being a good data analyst is respecting the role of the data creator and collaborating with them effectively. We might also call the data creator the “subject matter expert”, depending on the context. The data creator typically originated the reason for collecting the data, asked the originating question that led to data collection, and maybe oversaw the process of collecting the data itself. They know the ins and outs of how the data were collected, the strengths and weaknesses of the process, and all of the unwritten context about the data. This unwritten context, unfortunately, is by definition not documented, and over time is typically lost or forgotten.\nThe data creator can answer questions like\nWhy are there missing data? How might data come to be missing?\nWhy is this observation so low/high? Did anything funny happen there?\nWhat types of error might be introduced in collecting or measuring the data?\nWhy did you collect this variable or feature instead of that variable or feature?\nA data analyst must understand how to “fit themselves into the data” so that they know when appropriate questions are being asked of the data and when questions are being asked that cannot be answered by the data. The data were likely there before the analyst was, and so the analysts does not have absolute freedom to do whatever they want with the data. The data have a context and an original reason for being, and a data analyst should understand and respect that context.\nI recently worked on a study where people were being evaluated in a clinic every 3 months. On one of the visits, many of the outcome values were abnormally high. My first reaction was to go through the individual numbers and see if there was anything amiss. Not seeing anything, we checked the paper forms on which the data were originally collected to see if there was a transcription error–nothing unusual there either. Finally, we talked to the study staff who actually saw the patients in the clinic to see if they had seen anything unusual. Still nothing. Ultimately, the data are what they are, but it was reassuring to be able to goto the people on the ground to get their impressions. This kind of information is difficult to code and quantify, so it’s important to be able to talk with the “data creators” to get the full story.\nFound Data, Surrogate Creators, and off-label usage\nSome data do not have identifiable creators or their creators are long gone and cannot be contacted. Such “found data” are everywhere and the original reason that they were collected is largely unknown. It’s tempting to run off with such data if they appear to serve a given purpose, but some effort should be made to identify the data’s original purpose for being. In these cases the analyst must serve as a “surrogate creator” and put themselves in the position of understanding all of the aspects of the data that a creator would normally know.\nJust because data were collected for a specific purpose does not mean they cannot be used to ask a different question. Such “off-label” uses of data can be very fruitful and creative use of data off-label can lead to breakthroughs in unrelated areas. However, one must still have a good understanding of the data’s creation because of all the context that comes attached to the data. That said, it is often the case that aspects of the data that were highly relevant to the original question are less relevant for the off label usage.\nExample: Air pollution monitoring\nAmbient air pollution data in the United States is collected primarily for regulatory purposes. The question that the data are attempting to answer is essentially “Are air pollution levels in this county higher than the national ambient air quality standard?” In order to answer this question, it’s important to have a rigorous standard for collecting the data so that the measurements are highly accurate and precise. It is incredibly important to all parties that the numbers are reliable. The methodology for measuring air pollution at federal monitors is known as the “federal reference method” and is documented as a Federal Information Processing Standard (FIPS). People who work in monitoring and who develop new monitors are highly aware of the need to reduce error and to maximize reliability.\nHowever, when using monitoring data for epidemiological studies of air pollution and population health, the error in the data introduced at the monitoring level, while real, is often negligible in comparison to the the exposure measurement error that is introduced by assigning monitor values to members of the population. Epidemiological studies use the regulatory monitor data off-label because the data are numerous and freely available, and furthermore the errors in the monitor data are less relevant in the epidemiological setting than they are in the regulatory setting.\nThat said, I’ve been in numerous debates with engineers who are familiar with the way the monitors are built about how the errors in the monitoring might affect my epidemiological studies. They often insist (politely) that my results are not valid because the monitors are too variable. Now, I realize it doesn’t sound good when my argument is essentially, “I know my study is valid because the errors that I introduced by using your data and merging it with health data are way bigger than the measurement errors that your monitor produces!” But this is indeed the case. And the engineers were rightfully worried about their data being used in a different context.\nWhile the engineers might sweat over the deviations produced by a given monitor, even in the worst case scenario, it’s unlikely that the scale of variation that they are worried about would have any impact on the health risk models that we were fitting to the data. I’m better off spending more time looking at the sensitivity of the models to the exposure measurement error rather than worry about individual monitor variability. Nevertheless, it’s through conversations with these engineers that I can get a sense of all the sources of variation in the data. Being aware of these sources of variability and communicating any uncertainties that may or may not result is a big part of building trust in my data analyses.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:32:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-30-cultural-data-viz/",
    "title": "Cultural Differences in Map Data Visualization",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-06-30",
    "categories": [],
    "contents": "\nMatthew Panzarino had an interesting article in TechCrunch on Apple’s process for rebuilding their Maps app. While most of the article describes the laborious process of data collection, one part jumped out at me, which was the team that Panzarino describes as the “Department of Details.” They are responsible for a number of odds and ends regarding how maps are presented, but they are particularly concerned with presenting maps to people around the world.\n\nThe maps need to be usable, but they also need to fulfill cognitive goals on cultural levels that go beyond what any given user might know they need. For instance, in the U.S., it is very common to have maps that have a relatively low level of detail even at a medium zoom. In Japan, however, the maps are absolutely packed with details at the same zoom, because that increased information density is what is expected by users.\n\nWhat struck me is that I’ve never heard much discussion in the statistics data visualization literature about cultural differences in data perception. My assumption all along was that there would be perception theory that would try to determine the “optimal” data density for a given level of zoom (for example), based on metrics of, say, remembering certain features, or reasoning about what is seen. The idea that whatever the optimal density is might depend on the cultural background of the viewer is new to me.\nI’ll be the first to admit that I’m not terribly familiar with the literature on data visualization and perception. But I’d be happy to accept pointers to where this has been studied more thoroughly.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:32:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-26-creativity/",
    "title": "Creativity in Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-06-26",
    "categories": [],
    "contents": "\nI’ve often heard that there is a need for data analysts to be creative in their work. But why? Where and how exactly is that creativity exercised?\nOn one extreme, it could be thought that a data analyst should be easily replaced by a machine. For various types of data and for various types of questions, there should be a deterministic approach to analysis that does not change. Presumably, this could be coded up into a computer program and the data could be fed into the program every time, with a result presented at the end. For starters, this would eliminate the notorious researcher degrees of freedom problem. If there were substantial institutional knowledge of data analysis, this might indeed be possible. How is it that every data analysis is so different that a human being is needed to craft a solution?\nWell, it’s not true that every analysis is different. Many power calculations, for example, are identical or very similar, and can be automated to some degree. However, exactly how those power calculations are used or interpreted can vary quite a bit from project to project. Even the very same calculation for the same study design can be interpreted differently in different projects, depending on the context. The same is true for other kinds of analyses like regression modeling or machine learning.\nCreativity is needed because of the constraints placed on the analysis by context, resources, and the audience, all things that we might think of as being “outside” the data. The context around which the data are created, the resources (time, money, technology) available to do the analysis, and the audience to which the results will be presented, all play a key role in determining the strategy that an analyst develops to analyze the data. The analyst will often have to employ some amount of creativity in order to execute a strategy that produces useful output.\nThe Role of Context\nThe context of a problem has great influence over how we frame a question, how we translate it into a data problem, and how we go about collecting data. The context also allows us to answer questions regarding why the data appear to be the way they do. The same number for the same type of measurement can have different interpretations based on the context.\nMissing Data\nMissing data are present in almost every dataset and the most important question a data analyst can ask when confronted with missing data is “Why are the data missing?” It’s important to develop some understanding of the mechanism behind what makes the data missing in order to develop an appropriate strategy for dealing with missing data (i.e. doing nothing, imputation, etc.) But the data themselves often provide little information about this mechanism; often the mechanism is coded outside the data, possibly not even written down but stored in the minds of people who originally collected the data.\nTake a two-arm clinical trial with an experimental treatment and a placebo. Sometimes with experimental treatments, there are side effects and people will drop out of the trial (or even die) because they cannot handle the side effects. The result is more missing data in the experimental arm of the trial than in the placebo arm. Now the data themselves will reveal a differential in the rate of missing data between the arms as it will be clear that the treatment arm has a higher rate. But the data will not reveal the exact reason why they dropped out. Depending on the nature of the trial and the question being asked, there might be a few different ways to handle this problem. Imputation may be feasible or perhaps some sort of matching scheme. The exact choice of how to proceed will depend on what external data are available, how much data are missing, and how the results will be used, among many other factors.\nAnother example might be in the analysis of outdoor particulate matter air pollution data. Monitors run by the US EPA typically take measurements once every six days. The reason is that it is expensive to process the filters for PM data and so the 1-in-6 day schedule is a designed compromise to balance cost with quantity of data. Of course, this means that in the data records 5 out of every 6 days is “missing”, even though the missingness was introduced deliberately. Again, the data don’t say why they are missing. One could easily imagine a scenario where the monitor doesn’t record data when the values of PM are very high or very low, a kind of informative missingness. But in this case, the missing data can be ignored and typically doesn’t have a large impact on subsequent modeling. In fact, imputation can be detrimental here because it does not provide much benefit but can greatly increase uncertainty.\nIn both cases, the data analyst’s job is to assess the situation, look at the data, obtain information about the context and why the data are missing (from a subject matter expert), and then decide on an appropriate path forward. Even with these two scenarios, there is no generic path forward.\nThe Role of the Audience\nThe audience is another key factor that primarily influences how we analyze the data and present the results. One useful approach is to think about what final products need to be produced and then work backwards from there to produce the result. For example, if the “audience” is another algorithm or procedure, then the exact nature of the output may not be important as along as it can be appropriately fed into the next part of the pipeline. A priority will be placed on making sure the output is machine readable. In addition, interpretability may not weigh that heavily because no human being will be looking at the output of this part. However, if a person will be looking at the results, then you may want to focus on a modeling approach that lets that person reason about the data and understand how the data inform the results.\nIn one extreme case, if the audience is another data analyst, you may want to do a relatively “light” analysis (maybe just some preprocessing) but then prepare the data in such a way that it can be easily distributed to others to do their own analysis. This could be in the form of an R package or a CSV file or something else. Other analysts may not care about your fancy visualizations or models; they’d rather have the data for themselves and make their own results.\nA data analyst must make a reasonable assessment of the audience’s needs, background, and preferences for receiving data analytic results. This may require some creative guessing. If the audience is available to the analyst, the analyst should ask questions about how best to present results. Otherwise, reasonable assumptions must be made or contingencies (e.g. backup slides, appendices) can be prepared for the presentation itself.\nResources and Tools\nThe data analyst will likely have to work under a set of resource constraints, placing boundaries on what can be done with the data. The first and foremost constraint is likely to be time. One can only try so many things in the time allotted, or some analyses may take too long to complete. Therefore, compromises may need to be made unless more time and resources can be negotiated. Tooling will be limited in that certain combinations of models and software may not exist and there may not be time to develop new tools from scratch.\nA good data analyst must make an estimate of the time available and determine whether it is sufficient to complete the analysis. If resources are insufficient, then the analyst must either negotiate for more resources or adapt the analysis to fit the available resources. Creativity will almost certainly be required when there are severe resource constraints, in order to squeeze as much productivity out of what is available.\nSummary\nContext, audience, and resources can all place different kinds of constraints on a data analysis, forcing the analyst to employ different kinds of creativity to get the job done. Although I’ve presented each context, audience, and resources separately here, in most analyses all of these factors will play a role simultaneously. The complexity of the constraint environment (and their various interactions) can grow quickly, placing increasing pressure on the analyst to think creatively to produce useful results.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:32:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-18-the-role-of-resources-in-data-analysis/",
    "title": "The Role of Resources in Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-06-18",
    "categories": [],
    "contents": "\nWhen learning about data analysis in school, you don’t hear much about the role that resources—time, money, and technology—play in the development of analysis. This is a conversation that is often had “in the hallway” when talking to senior faculty or mentors. But the available resources do play a significant role in determining what can be done with a given question and dataset. It’s tempting to think that the situation is binary—either you have sufficient resources to do the “right” analysis, or you simply don’t do the analysis. But in the real world there are quite a few shades of gray in between those two endpoints. There are many situations in data analysis where the optimal approach is not feasible, but it is nevertheless important to do some sort of analysis. Thus, a critical skill for a data analyst to master is the ability to reconcile conflicting ideas while still producing something useful.\nAll analyses must deal with constraints on time and technology and that often shapes the plan for what can be done. For example, the complexity of the statistical model being used may be constrained by computing power available to the analyst, the ability to purchase more computing power, and the time available to run complex Markov chain Monte Carlo simulations. The analysis that is needed tomorrow will be different from the analysis that is needed next week. Yet the only thing different between the two is the time available to do the work.\nThe key resources of time, money, and technology have different effects on how a data analysis ultimately completed:\nTime. Time usually serves as the biggest constraint and is obviously related to money. However, even if money is in abundance, it cannot buy more time if none is available. Complex analyses often involve many separate pieces, and complex data must be validated, checked, and interrogated before one can be confident about the results. All of this takes time and having less time leads to doing less of all those things. Similarly some analyses may require multiple people’s time, if one person cannot fit it all into their schedule. If multiple people are not currently available, that will change the nature of the analysis done.\nTechnology. I use the word “technology” broadly to refer to both computing resources and statistical “resources”. Some models may be more optimal than others, but characteristics of the dataset (such as its size) may prevent them from being applied. Better analyses may be done with more computing power, but a constraint on available computing power will determine what models get fit and how much extra work is done. Technological constraints may also be related to the audience that will receive the analysis. Depending on how sophisticated the audience is, one may tune the technology applied to do the analysis.\nApproximations\nPerhaps the oldest tool that statisticians have in their toolbox for dealing with resource constraints is approximation. Often it is straightforward to write down what the exact or ideal solution to a problem is but the computational burden makes it difficult to compute that solution. For example, many Bayesian computations require calculating complex high-dimensional integrals that were impossible before the invention of the digital computer. For complex non-linear solutions, a classic trick is to use a linear approximation and perhaps combine it with an assumption about asymptotic normality.\nIn most cases where computation was intractable, statisticians either resorted to (asymptotic) approximations, substituting difficult calculations with (sometimes dubious) assumptions, or chose different methods. A key point is that the harsh reality of the the real world’s resource constraints forced a different approach to analyzing data. While it might be unsatisfying to use a sub-optimal approach, it might be equally unsatisfying to not analyze the data at all.\nAs computing power has grown in the last century, we have been slowly replacing those old assumptions with computation. There is no need for asymptotic Normality if we can compute a less restrictive solution with a powerful computer. A simple example of this is the two-sample permutation test which is as powerful as a standard t-test but without any distributional assumptions. The problem, of course, is that those old assumptions die hard, and even today it can be cumbersome to code up a solution when a formula is right at hand.\nCheaper Hierarchical Modeling\nOne example from my own work involves hierarchical modeling of air pollution and health time series data. In the early 2000s, we were looking at national data on mortality and air pollution in the U.S. We had daily data on mortality and pollution (and many other covariates) in 100 major U.S. cities covering a time span of about 14 years. In order to make efficient use of this huge dataset, the goal was to employ a hierarchical model to estimate both a “national” association between air pollution and mortality, as well as city-specific estimates that borrowed strength across cities. It was a familiar approach that worked perfectly well in smaller datasets. The “right” approach would have been to use a Poisson likelihood for each of the cities (to model the mortality count data) and then have Normal random effects for the intercept and air pollution slopes.\nBut at the time, we didn’t have a computer that could actually compute the estimate from the model (or in our case, the posterior distributions). So the “right” model was not an option. What we ended up doing was using a Normal approximation for the Poisson likelihood, justified by the fairly large samples that we had, which allowed for a Normal-Normal two-stage model that could be computed without having to load all the data into memory (in the simplest case it could be done in closed form). To this day, this is the standard approach to modeling multi-site time series data of air pollution and health because it is fast, cheap, and easy to understand.\nTrustworthiness\nUltimately, those resource constraints can affect how trustworthy the analysis is. In a trustworthy analysis, what is presented as the analysis is often backed up by many facts and details that are not presented. These other analyses have been done, but the analyst has decided (likely based on a certain narrative of the data) that they do not meet the threshold for presentation. That said, should anyone ask for those details, they are readily available. With greater resources, the sum total of all of the things that can be done is greater, thus giving us hope that the things left undone are orthogonal to what was done.\nHowever, with fewer resources, there are at least two consequences. First, it is likely that fewer things can be done with the data. Fewer checks on the data, checks on model assumptions, checks of convergence, model validations, etc. This increases the number of undone things and makes it more likely that they will have an impact on the final (presented) results. Secondly, certain kinds of analysis may require greater time or computing power than is available. In order to present any analysis at all, we may need to resort to approximations or “cheaper” methodology. These approaches are not necessarily incorrect, but they may produce results that are noisier or otherwise sub-optimal. That said, the various other parties involved in the analysis, such as the audience or the patron, may prefer having any analysis done, regardless of optimality, over having no analysis. Sometimes the question itself is still vague or a bit rough, and so it’s okay if the analysis that goes with it is equally “quick and dirty”. Nevertheless, analysts have to draw the line between what what is a reasonable analysis and what is not, given the available resources.\nAlthough resource constraints can impair the trustworthiness of an analysis, sometimes the use of approximations to deal with resource constraints can produce benefits. In the example above regarding the air pollution and mortality modeling, the approximation that we used made fitting the models to the data very fast. The benefit of the cheapness of the computation in this case allows the analyst to cycle through many different models to examine robustness of the findings to various confounding factors and to conduct important sensitivity analyses. If each model took days to compute, you might just settle for a single model fit. In other words, it’s possible that resource constraints could produce an analysis that, while approximate, is actually more trustworthy than the optimal analysis.\nThe Analyst’s Job\nThe data analyst’s job is to manage the resources available for analysis and produce the best analysis possible subject to the existing constraints. The availability of resources may not be solely up to the analyst, but the job is nevertheless to recognize what is available, determine whether the resources are sufficient for completing a reasonable analysis, and if not, then request more from those who can provide them. I’ve seen many data analyses go astray as a result of a mismatch in the understanding of the resources available versus the resources required.\nA good data analyst can minimize the chance of a gross mismatch and will continuously evaluate the resource needs of an analysis going forward. If there appears to be a large deviation between what was expected and the reality of the analysis, then the analyst must communicate with others involved (the patron or perhaps subject matter expert) to either obtain more resources or modify the data analytic plan. Negotiating additional resources or a modified analytic plan requires the analyst to have a good relationship with the various parties involved.\nYou can hear more from me and the JHU Data Science Lab by subscribing to our weekly newsletter Monday Morning Data Science.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:31:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-15-people-vs-institutions-in-data-analysis/",
    "title": "People vs. Institutions in Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-06-15",
    "categories": [],
    "contents": "\nIn my post about relationships in data analysis I got a little push back regarding whether human relationships would ever not be important in data analysis and whether that has anything to do with the “maturity” of the field. I believe human beings will always play a role in data analysis, but it’s possible that over time they will play different roles. I wanted to discuss in this post what I meant about “institutions” and “institutional knowledge” in the context of data analysis and when the specific person who does the analysis is critical to how the analysis is done.\nSome lessons from finance\nPeter Lynch, the legendary investor and manager of the Fidelity Magellan Fund, once said\n\nInvest in businesses any idiot could run because someday one will.\n\nA perhaps more detailed version of this sentiment comes from fellow legendary investor Warren Buffett, in his testimony before the U.S. Financial Crisis Inquiry Commission in 2010,\n\nI knew nothing about the management of Moody’s [a bond rating agency]. The—I’ve also said many times in reports and elsewhere that when a management with reputation for brilliance gets hooked up with a business with a reputation for bad economics, it’s the reputation of the business that remains intact.\n\n\nIf you’ve got a good enough business, if you have a monopoly newspaper, if you have a network television station—I’m talking of the past—you know, your idiot nephew could run it. And if you’ve got a really good business, it doesn’t make any difference.\n\nIn both cases, they are talking about businesses that are generally thought to be “boring”. This is because they are mature, have well-established (good) economics, and are not highly depended on the people running the company. In this sense, a business is mature of you don’t care who the CEO is (within reason, of course). Christopher Mims made the latest version of this argument ($) in the Wall Street Journal in reference to tech founder “superheroes” and how they are ultimately bad for business.\nI would argue that in data analysis, for better or for worse, you do care who is doing your data analysis.\nMaturity of Data Analysis\nMy thinking along these lines was that mature organizations or countries tend to replace people with institutions with the hope that the organization can continue forward regardless of who is running the institution, as long as the institution itself can keep functioning. Large corporations can replace their CEO and still continue to make money. Many large companies turnover much of their staff every year and continue to grow and profit. If you go to a startup company with three employees and replace the CEO, what you are left with is a completely different company. I believe the field of data analysis is closer to the startup company end of the spectrum than the mature company end. I don’t think we have much “institutional knowledge” about what to do in data analysis, largely because I believe we have a flawed model for what is a data analysis and because we do not have a transparent means of communicating to others what exactly we do in a data analysis.\nMany data analysts have likely had the experience of a collaborator approaching them with some data and telling them something along the lines of, “This is easy, just run the regression on the data.” Collaborators like these think of data analysis as having deep institutional knowledge, whereby everyone has the exact same understanding of what it means to “run the regression”. But the reality is the regression modeling is a complex task that typically requires iterating repeatedly with collaborators over what should be done. This is because “running the regression” is intricately tied up with the context in which the data were generated and the audience to which the results will be presented. All of this is to say that the person doing the analysis is critical to the analysis itself, in part because they will have to manage the relationships with other parties involved. If “running the regression” were in fact as simple as calling the lm() function in R and spitting out the summary, then yes, it would hardly matter which person called that function.\nBy contrast, let’s take a look at a different task: matrix inversion. Matrix inversion is a critical task for any statistical modeling. For one, it lies at the heart of regression modeling! However, most analysts probably don’t think twice about doing it; the lm() function just does it for you (buried somewhere down in all that Fortran code). In the past, one might have had to call up the numerical analyst to develop a deep understanding of our design matrices and implement the best possible approach to inverting its cross product. Now we can just call solve() (although you should probably not do that for other reasons!). Knowledge of matrix inversion has gotten to the point where for most cases, and in most reasonable situations, we can do it without much thinking. It is in that sense that we have “institutional knowledge”. We can leverage that institutional knowledge and implement it in R code that anyone can call. If someone came to me and say “Hey, invert this matrix,” I would happily push the button to do so. The result would be the same no matter who called the solve() function. There are obviously some corner cases in which we might want to talk with a numerical analyst, but the number of those cases is small (and probably shrinking).\nThe beauty of linear regression is that it is so general that it can be applied to a myriad of problems and questions. But its generality is what makes it difficult to formalize the “regression modeling” process because every single context in which linear regression is applied is different. Furthermore, regression modeling is typically not a single procedure, but rather a collection of procedures, including visualization, model diagnostics, and uncertainty quantification. It may be possible to automate the process within a very narrow context, it’s unlikely that knowledge gained in that narrow context could be brought over to a different context.\nThe Dream\nI’ve written previously about how the dream of many companies today is to completely “institutionalize” the process of data analysis (I think I used the word “productize” before). If the process of data analysis were so mature that its critical elements could be coded in software, then yes, I think that could be a money-making idea. But the problem is that we still have much to learn about data analysis. Sure, individual components are well understood, but we are far from being able to push a button and “run the regression”. We will still need people involved in the process for the near future.\nYou can hear more from me and the JHU Data Science Lab by subscribing to our weekly newsletter Monday Morning Data Science.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:31:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-10-an-ode-to-king-james/",
    "title": "An ode to King James",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2018-06-10",
    "categories": [],
    "contents": "\nThe NBA season is over and, once again, what I will most remember are King James’ heroics.\nAs a lifelong Boston Celtics fan, I am supposed to hate LeBron James. But I don’t. As a fan of the game of basketball, and a statistician, I just can’t help but be in awe of the best player ever to play the game. Also how can you hate this guy (don’t miss the wrist watch)?\n\n\n\nOf all the ridiculous stats one can rattle about LeBron, to me, the most impressive actually relate to a simple one: career totals. The graph below shows you his total points, assists, steals and rebounds as a function of time. The consistency, durability, and all-around production is simply unprecedented.\nAt 33, he is already top 25 all time in three categories: points, assists and steals. If he can keep up his current pace for five more years (I know it’s a big if), he will be top 3 in points, assists and steals, and top 25 in rebounds. No other player comes close.\n\n\n\nHere is the code that produces the graph above:\n\n\nlibrary(dplyr)\nlibrary(rvest)\nurl <- \"https://web.archive.org/web/20180605212404/https://www.espn.com/nba/player/stats/_/id/1966/lebron-james\"\nh <- read_html(url)\nlebron <-html_nodes(h, \"table\") %>% .[[3]] %>% rvest::html_table(fill=TRUE) %>% .[,1:17]\nnames(lebron) <- as.character(lebron[2,])\nlebron <-lebron[3:17,]\nind <- 11:17\nlebron[ ,ind] <- apply(lebron[,ind], 2, as.numeric)\n\nstats <- c(\"pts\",\"ast\",\"stl\",\"trb\")\nfull_stats <- c(\"Points\", \"Assists\", \"Steals\", \"Rebounds\")\nurls <- paste0(\"https://web.archive.org/web/20180617065113/https://www.basketball-reference.com/leaders/\", stats, \"_career.html\")\ntabs <- lapply(urls, function(url){\n  tab <- read_html(url) %>%\n    html_nodes(\"table\") %>%\n    .[[2]] %>%\n    html_table()\n   tab <- tab %>% mutate(Player = gsub(\"\\\\*\", \"\", Player))\n   names(tab)[3] <- \"Value\"\n   tab\n})\nnames(tabs) <- stats\nnames(lebron)[11] <- \"TRB\"\nind <- match(toupper(stats), names(lebron))\n\nrafalib::mypar(2,2)\nfor(i in seq_along(stats)){\n  y <- lebron[[ ind[i] ]]\n  r <- mean(y)\n  x <- 2004:2018\n\n  YLIM <- c(min(y), max(tabs[[i]]$Value))\n  YLIM[2] <- YLIM[2]*(1.05)\n  XLIM <- c(min(x), 2018 + 5)\n  plot(x, cumsum(y), xlim = XLIM, ylim = YLIM, xaxt=\"n\",\n       main = full_stats[i], xlab=\"\", ylab=\"Total\")\n  axis(side=1, 2004:2023, 2004:2023, las =2)\n  abline(-2003*r, r, lty = 2, col = \"red\")\n  j <- c(1:3,25)\n  the_names <- tabs[[i]]$Player[j]\n  the_names[4] <- paste0(\"Top 25\")\n  z <-  tabs[[i]]$Value[j]\n  abline(h = z, lty = 2)\n  text(2008.5, z, the_names, pos=3, offset = 0.1, cex = 0.7)\n}\n\n\n\n\n\n",
    "preview": "http://n.sinaimg.cn/sinacn/w600h686/20180104/c476-fyqinct9321280.jpg",
    "last_modified": "2022-12-27T15:36:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-08-a-first-look-at-recently-released-official-puerto-rico-death-count-data/",
    "title": "Estimating mortality rates in Puerto Rico after hurricane María using newly released official death counts",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2018-06-08",
    "categories": [],
    "contents": "\n\n\n\nLate last Friday, the Puerto Rico Department of Health finally released monthly death count data for the time period following Hurricane Maria:\n\n\nBREAKING: the Puerto Rico Health Department has buckled under pressure and released the number of deaths for each month, through May of 2018. In September 2017, when Hurricane Maria made landfall, there was a notable spike, followed by an even larger one in October. pic.twitter.com/3Irw1eUOTC\n\n— David Begnaud (@DavidBegnaud) June 1, 2018\n\nThe news came three days after the publication of our paper describing a survey conducted to better understand what happened after the hurricane.\nWe did not have access to these data and, after this announcement, we requested daily data. We received it on Tuesday. The data comes as a PDF file which we make public here. The code to extract the data is long and messy so we include it at the end of the post and only show the package loading part here.\n\n\nlibrary(pdftools)\nlibrary(tidyverse)\nlibrary(gridExtra)\ndslabs::ds_theme_set()\nlibrary(stringr)\nlibrary(lubridate)\n\n\nThe wrangling produces the object official:\n\n\nhead(official)\n\n# A tibble: 6 × 5\n  month   day  year deaths date      \n  <dbl> <dbl> <dbl>  <dbl> <date>    \n1     1     1  2015    107 2015-01-01\n2     1     2  2015    101 2015-01-02\n3     1     3  2015     78 2015-01-03\n4     1     4  2015    121 2015-01-04\n5     1     5  2015     99 2015-01-05\n6     1     6  2015    104 2015-01-06\n\nHere I include an analysis showing that the newly released data confirms that the official death count of 64 is a gross underestimate and that, according to these data, the current count is closer to 2,000 than 64.\nThe raw data\nHere is a plot of the daily data for 2017 and 2018:\n\n\nofficial %>% filter(year >= 2017 & deaths > 0) %>%\n  ggplot(aes(date, deaths)) + \n  geom_point(alpha = 0.5) + \n  geom_vline(xintercept = make_date(2017,09,20), lty=2, col=\"grey\") +\n  scale_x_date(date_labels = \"%b\", date_breaks = \"1 months\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  ggtitle(\"Raw data for 2017 and 2018\")\n\n\n\nWe clearly see the effects of the hurricane: there is a jump of between 25-50 deaths per day for about 2 weeks. It also seems that the 3-4 following months show a higher rate than expected. But this is hard to discern due to the seasonal trend: more deaths in the winter. The plot also shows that deaths have not yet been tallied for the last couple of weeks. So we restrict the analysis and exclude data from past April 15th.\nAdjusting for an aging population, seasonal effect, and depopulation\nWe start by estimating the population size. We know that Puerto Rico has been losing population for the last 10 years. This file includes population size estimates. We also know that many people left the island after the hurricane. Teralytics tried to estimate this using data collected from cell phones. We extracted their estimates (by hand) from this post. We then interpolated to generate an estimate of population for the days in our data:\n\n\ntmp <- bind_rows(\n  data.frame(date = make_date(year = 2010:2017, month = 7, day = 2), \n             pop = population_by_year$pop),\n  data.frame(date = make_date(year=c(2017,2017,2017,2017,2018,2018), \n                              month=c(9,10,11,12,1,2), \n                              day = c(19,15,15,15,15,15)),\n             pop = c(3337000, 3237000, 3202000, 3200000, 3223000, 3278000)))\ntmp <- approx(tmp$date, tmp$pop, xout=official$date, rule = 2)\npredicted_pop <- data.frame(date = tmp$x, pop = tmp$y)\np1 <- qplot(date, pop, data=predicted_pop, ylab = \"population\", geom = \"line\")\n\n\nTo account for differences in population sizes we compute rates.\n\n\nofficial <- official %>% left_join(predicted_pop, by = \"date\") %>%\n  mutate(rate = deaths/pop*365*1000)\n\n\nBecause the population is getting older, we also need to adjust for a year effect. To be able to compare 2017 to previous years, we use only the days before September 20 to compute a yearly median rate. Because we have no post hurricane data for 2018, we also assume that 2018 has the same rate as 2017.\n\nyear\nyear_rate\n2015\n8.176414\n2016\n8.387966\n2017\n8.749981\n2018\n8.749981\n\nWith these estimates in place, we can now estimate a seasonal trend using 2015 and 2016 data. We do this by fitting a periodic smooth trend using loess:\n\n\navg <- official %>% filter(year(date) < 2017 & !(month==2 & day == 29)) %>%\n  left_join(population_by_year, by = \"year\") %>%\n  group_by(month,day) %>%\n  summarize(avg_rate = mean(rate - year_rate), .groups = \"drop\") %>%\n  ungroup()\nday <- as.numeric(make_date(1970, avg$month, avg$day)) + 1\nxx <- c(day - 365, day, day + 365)\nyy<- rep(avg$avg_rate, 3)\nfit <- loess(yy ~ xx, degree = 1, span = 0.15, family = \"symmetric\")\ntrend <- fit$fitted[366:(365*2)]\ntrend <- data.frame(month = avg$month, day = avg$day, trend = trend)\nofficial <- official %>% left_join(trend, by = c(\"month\", \"day\"))\n\n\nHere is what the estimate population size and seasonal trend look like:\n\n\np2 <- official %>%  \n  filter(date >=ymd(\"2017-01-01\") & date < ymd(\"2018-01-01\")) %>%\n  ggplot(aes(date, trend)) +\n  geom_line() + \n  scale_x_date(date_labels = \"%b\", date_breaks = \"1 months\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + \n  ggtitle(\"Death rate trend in PR\")\ngrid.arrange(p1, p2, ncol=2)\n\n\n\nAdjusted Death Rates\nWith all this in place, we can now adjust the raw count data and compute an observed minus expected death rate. We use loess, with a break-point at September 20, 2017, to estimate a smooth version of the excess death rate as a function of time.\n\n\nafter_smooth <- official %>% \n  filter(date >=ymd(\"2017-01-01\") & date < ymd(\"2017-09-20\")) %>%\n  mutate(y = rate - year_rate -  trend, x = as.numeric(date)) %>%\n  loess(y ~ x, degree = 2, span = 2/3, data = .)\nbefore_smooth <- official %>% \n  filter(date >=ymd(\"2017-09-20\") & date < ymd(\"2018-04-15\")) %>%\n  mutate(y = rate - year_rate -  trend, x = as.numeric(date)) %>%\n  loess(y ~ x, degree = 2, span = 2/3, data = .)\ntmp <- official %>% \n  filter(date>=ymd(\"2017-01-01\") & date < ymd(\"2018-04-15\")) %>%\n  mutate(smooth = c(after_smooth$fitted, before_smooth$fitted),\n         se_smooth = c(predict(before_smooth, se=TRUE)$se, \n                       predict(after_smooth, se=TRUE)$se))\n\n\nHere is what the excess death rate looks like:\n\n\ntmp %>% mutate(diff = rate - year_rate - trend) %>%\n  ggplot(aes(date, diff)) +\n  geom_point(alpha=0.5) +\n  geom_ribbon(aes(date, \n                  ymin = smooth - 1.96 * se_smooth, \n                  ymax = smooth + 1.96 * se_smooth), \n              fill=\"blue\", alpha = 0.25) +\n  geom_line(aes(date, smooth), col = \"blue\") +\n  scale_x_date(date_labels = \"%b\", date_breaks = \"1 months\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + \n  ggtitle(\"Increase in death rate\") + \n  ylab(\"Observed rate minus expected rate\") + \n  geom_hline(yintercept = 0, lty = 2)\n\n\n\nWe can now clearly see that following the big spike right after the hurricane, the death rate continued to be higher than expected, by about 10% until past December.\nThis gives us the excess death rate. To compute an excess count we need to plug in a population size and a baseline rate. If we assume all of 2017 had the same population, the pre-hurricane yearly effect and seasonal trend as other years we get the following excess deaths by date:\n\n\nthe_pop <- filter(population_by_year, year == 2016) %>% .$pop\ntmp %>%\n  filter(date >=ymd(\"2017-09-20\")) %>%\n  mutate(diff = rate - year_rate - trend,\n         raw_cdf = cumsum(diff*the_pop/1000/365), \n         smooth_cdf =cumsum(smooth*the_pop/1000/365)) %>%\n  ggplot() +\n  geom_step(aes(date, raw_cdf)) +\n  scale_x_date(date_labels = \"%b\", date_breaks = \"1 months\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + \n  ggtitle(\"Excess deaths by date\") + \n  ylab(\"Excess death estimate\") +\n  geom_hline(yintercept = 64, lty=2, col=\"grey\")\n\n\n\nWe can see that 64 deaths were reached during the first few days and 2,000 deaths are reached around February.\nOne could argue that these assumptions are not optimal because, for example, young people may have left at a higher rate past the hurricane making the expected rate a bit higher. I encourage others to change the assumptions and re-run the code to see how the final results change.\nCode for wrangling PDF\nHere is the code to extract the data from the PDF file:\n\n\nlibrary(pdftools)\nfilename <- \"https://github.com/c2-d2/pr_mort_official/raw/master/data/RD-Mortality-Report_2015-18-180531.pdf\"\ntxt <- pdf_text(filename)\nlibrary(tidyverse)\nlibrary(gridExtra)\ndslabs::ds_theme_set()\nlibrary(stringr)\nlibrary(lubridate)\ndat <- lapply(seq_along(txt), function(i){\n  s <- str_replace_all(txt[i], \"\\\\*.*\", \"\") %>%\n    str_remove_all(\"Fuente: Registro Demográfico - División de Calidad y Estadísticas Vitales\") %>%\n    str_replace_all(\"Y(201\\\\d)\\\\*?\", \"\\\\1\") %>%\n    str_replace(\"SEP\", \"9\") %>%\n    str_replace(\"OCT\", \"10\") %>%\n    str_replace(\"NOV\", \"11\") %>%\n    str_replace(\"DEC\", \"12\") %>%\n    str_replace(\"JAN\", \"1\") %>%\n    str_replace(\"FEB\", \"2\") %>%\n    str_replace(\"MAR\", \"3\") %>%\n    str_replace(\"APR\", \"4\") %>%\n    str_replace(\"MAY\", \"5\") %>%\n    str_replace(\"JUN\", \"6\") %>%\n    str_replace(\"JUL\", \"7\") %>%\n    str_replace(\"AGO\", \"8\") %>%\n    str_replace(\"Total\", \"@\") \n  \n  tmp <- str_split(s, \"\\n\") %>% \n    .[[1]] %>% \n    str_trim %>% \n    str_split_fixed(\"\\\\s+\", 50) %>%\n    .[,1:5] %>%\n    as_tibble()\n  colnames(tmp) <- tmp[3,]\n  tmp <- tmp[-(1:3),]\n  j <- which(tmp[,1]==\"@\")\n  if(colnames(tmp)[1]==\"2\") { ## deal with february 29\n    k <- which(tmp[[1]]==29)\n    the_number <- unlist(tmp[k,-1])\n    the_number <- the_number[the_number!=\"\"]\n    tmp[k, colnames(tmp)!=\"2016\" & colnames(tmp)!=\"2\"] <- \"0\"\n    tmp[k, \"2016\"] <- the_number\n  }\n  tmp <- tmp %>% slice(1:(j-1)) %>% mutate_all(funs(as.numeric)) %>%\n    filter(!is.na(`2015`) & !is.na(`2016`) & !is.na(`2017`)  & !is.na(`2017`))\n  tmp <- mutate(tmp, month = as.numeric(names(tmp)[1]))\n  names(tmp)[1] <- \"day\"\n  tmp <- tmp[,c(6,1,2,3,4,5)]\n  ones <- which(tmp$day==1) ##1 2 3 4 appears due to graph... let's take it out\n  if(length(ones)>1) tmp <- tmp[-ones[-1],]\n  if(any(diff(tmp$day)!=1)) stop(i) ## check days are ordered\n  ##check if negative. this means a black was left and the diff between 2016 and 0 was used!\n  tmp[tmp<0] <- NA\n  pivot_longer(tmp, c(\"2015\", \"2016\", \"2017\", \"2018\"), \n               names_to = \"year\", values_to = \"deaths\") %>%\n    mutate(year = as.numeric(year))\n})\nofficial <- do.call(rbind, dat) %>% \n  arrange(year, month, day) %>% \n  filter(!(month==2 & day==29 & year != 2016))\n## add date\nofficial <-official %>% mutate(date = ymd(paste(year, month, day,\"-\")))\n\n\n\n\n\n",
    "preview": "posts/2018-06-08-a-first-look-at-recently-released-official-puerto-rico-death-count-data/index_files/figure-html5/raw-data-1.png",
    "last_modified": "2022-12-27T15:39:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-04-trustworthy-data-analysis/",
    "title": "Trustworthy Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-06-04",
    "categories": [],
    "contents": "\nThe success of a data analysis depends critically on the audience. But why? A lot has to do with whether the audience trusts the analysis as well as the person presenting the analysis. Almost al presentations are incomplete because for any analysis of reasonable size, some details must be omitted for the sake of clarity. A good presentation will have a structured narrative that will guide the presenter in choosing what should be included and what should be omitted. However, audiences will vary in their acceptance of that narrative and will often want to know if other details exist.\nThe Presentation\nConsider the following scenario:\n\nA person is analyzing some data and is trying to determine if two features, call them X and Y, are related to each other. After looking at the data for some time, they come to you and declare that the Pearson correlation between X and Y is 0.85 and therefore conclude that X and Y are related.\n\nThe question then is, do you trust this analysis?\nGiven the painfully brief presentation of the scenario, I would imagine that most people experienced in data analysis would say something along the lines of “No”, or at least “Not yet”. So, why would we not trust this analysis?\nThere are many questions that one might ask before one were to place any trust in the results of this analysis. Here are just a few:\nWhat are X and Y? Is there a reason why X and Y might be causally related to each other? If mere correlation is of interest, that’s fine but some more detail could be illuminating.\nWhat is the sampling frame here? Where did the data come from? We need to be able to understand the nature of uncertainty.\nWhat is the uncertainty around the correlation estimate? Are we are looking at noise here or a real signal. If there is no uncertainty (because there is no sampling) then that should be made clear.\nHow were the data processed to get to the point where we can compute Pearson’s correlation? Did missing data have to be removed? Were there transformations done to the data?\nThe Pearson correlation is really only interpretable if the data X and Y are Normal-ish distributed. How do the data look? Is the interpretation of correlation here reasonable?\nPearson correlation can be driven by outliers in X or Y. Even if the data are mostly Normal-ish distributed, individual outliers could make the appearance of a strong relationship even if the bulk of the data are not correlated. Were there any outliers in the data (either X or Y)?\nThe above questions about the presentation and statistical methodology are all reasonable and would likely come up in this scenario. In fact, there would likely be even more questions asked before a one could be assured that the analysis was trustworthy, but this is just a smattering.\nDone but Not Presented\nI think it’s reasonable to think that a good analyst would have concrete answers to all of these questions even though they were omitted from the presentation.\nThey would know what X and Y were and whether it made sense to determine if they were correlated, based on the context of the data and questions being asked.\nThey would know how the data came to them, whether they represented a sample, and what the population was.\nThey would have calculated the uncertainty around the correlation estimate (e.g. a 95% confidence interval) and would have it on hand to present to you.\nIdeally, they would have processed the data themselves and would be able to explain what had to be done in order to get the data to the point where correlations could be computed. If they didn’t process the data, they would at least be able to describe what was done and whether those actions have a major impact on the results.\nTo justify the use of Pearson correlation, they would have made a histogram (or a Q-Q plot) to look at the marginal distributions of X and Y. If the data weren’t Normal looking they would have considered some possible transformations if possible.\nTo check for outliers, a scatterplot of X and Y would have been made to examine if any small number of points was driving the observed correlation between X and Y. Even though they didn’t show you the scatterplot, they might have it on hand for you to examine.\nOne might think of other things to do, but the items listed above are in direct response to the questions asked before.\nDone and Undone\nMy “analysis of variance” representation of a data analysis is roughly\nData Analysis DecompositionHere we have\nA: The presentation, which in the above example, is the simple correlation coefficient between X and Y\nB: The answers to all of the questions that likely would come up after seeing the presentation\nC: Anything that was not done by the analyst\nWe can only observe A and B and need to speculate about C. The times when I most trust an analysis is when I believe that the C component is relatively small, and is essentially orthogonal to the other components of the equation (A and B). In other words, were one to actually do the things in the “Not Done” bucket, they would have no influence on the overall results of the analysis. There should be nothing surprising or unexpected in the C component.\nNo matter what data is being analyzed, and no matter who is doing the analysis, the presentation of an analysis must be limited, usually because of time. Choices must be made to present a selection of what was actually done, therefore leaving a large number of items in the “Done but not Presented” component. An analogy might be when one writes slides for a presentation, often there are a few slides that are left in the back of the slide deck that are not presented but are easily retrieved should a question come up. The material in those slides was important enough to warrant making a slide, but not important enough to make it into the presentation. In any substantial data analysis, the number of “slides” presented as the results is relatively small while the number of “slides” held in reserve is potentially huge.\nAnother large part of a data analysis concerns who is presenting. This person may or may not have a track record of producing good analyses and the background of the presenter may or may not be known to the audience. My response to the presentation of an analysis tends to differ based on who is presenting and my confidence in their ability to execute a good analysis. Ultimately, I think my approach to reviewing an analysis comes down to this:\nIf it’s a first presentation, then regardless of the presenter, I’ll likely want to see A and B, and discuss C. For a first presentation, there will likely be a number of things “Not Done” and so the presenter will need to go back and do more things.\nIf we’re on the second or third iteration and the presenter is someone I trust and have confidence in, then seeing A and part of B may be sufficient and we will likely focus just on the contents in A. In part, this requires my trust in their judgment in deciding what are the relevant aspects to present.\nFor presenters that I trust, my assumption is that there are many things in B that are potentially relevant, but I assume that they have done them and have incorporated their effects into A. For example, if there are outliers in the data, but they do not seem to introduce any sensitivity into the results, then my assumption is that they looked at it, saw that it didn’t really make a difference, and moved on. Given this, my confidence that the elements of C are orthogonal to the results presented is high.\nFor presenters that I’m not familiar with or with whom I have not yet built up any trust, my assumptions about what lies in B and C are not clear. I’ll want to see more of what is in B and my skepticism about C being orthogonal to the results will be high.\nOne of the implications of this process is that two different presenters could make the exact same presentation and my response to them will be different. This is perhaps an unfortunate reality and opens the door to introducing all kinds of inappropriate biases. However, my understanding of the presenters’ abilities will affect how much I ask about B and C.\nAt the end of the day, I think an analysis is trustworthy when my understanding of A and B is such that I have reasonable confidence that C is orthogonal. In other words, there’s little else that can be done with the data that will have a meaningful impact on the results.\nImplications for Analysts\nAs an analyst it might be useful to think of what are the things that will fall into components A, B, and C. In particular, how one thinks about the three components will likely depend on the audience to which the presentation is being made. In fact, the “presentation” may range from sending a simple email, to delivering a class lecture, or a keynote talk. The manner in which you present the results of an analysis is part of the analysis and will play a large role in determining the success of the analysis. If you are unfamiliar with the audience, or believe they are unfamiliar with you, you may need to place more elements in components A (the presentation), and perhaps talk a little faster. But if you already have a long-term relationship with the audience, a quick summary (with lots of things placed into component B) may be enough.\nOne of the ways in which you can divide up the things that go into A, B, and C is to develop a good understanding of the audience. If the audience enjoys looking at scatterplots and making inquiries about individual data points, then you’re going to make sure you have that kind of detailed understanding in the data, and you may want to just put that kind of information up front in part A. If the audience likes to have a higher level perspective of things, you can reserve the information for part B.\nConsidering the audience is useful because it can often drive you to do analyses that perhaps you hadn’t thought to do at first. For example, if your boss always wants to see a sensitivity analysis, then it might be wise to do that and put the results in part B, even if you don’t think it’s critically necessary or if it’s tedious to present. On occasion, you might find that the sensitivity analyses in fact sheds light on an unforeseen aspect of the data. It would be nice if there were a “global list of things to do in every analysis”, but there isn’t and even if there were it would likely be too long to complete for any specific analysis. So one way to optimize your approach is to consider the audience and what they might want to see, and to merge that with what you think is needed for the analysis.\nIf you are the audience, then considering the audience’s needs is a relatively simple task. But often the audience will be separate (thesis committee, journal reviewers/editors, conference attendees) and you will have to make your best effort at guessing. If you have direct access to the audience, then a simpler approach would be to just ask them. But this is a potentially time-consuming task (depending on how long it takes for them to respond) and may not be feasible in the time frame allowed for the analysis.\nTrusting vs. Believing\nIt’s entirely possible to trust an analysis but not believe the final conclusions. In particular, if this is the first analysis of it’s kind that you are seeing, there’s almost no reason to believe that the conclusions are true until you’ve seen other independent analysis. An initial analysis may only have limited preliminary data and you may need to make a decision to invest in collecting more data. Until then, there may be no way to know if the analysis is true or not. But the analysis may still be trustworthy in the sense that everything that should have been done was done.\nLooking back at the original “presentation” given at the top, one might ask “So, is X correlated with Y?”. Maybe, and there seems to be evidence that it is. However, whether I ultimately believe the result will depend on factors outside the analysis.\nYou can hear more from me and the JHU Data Science Lab by subscribing to our weekly newsletter Monday Morning Data Science.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:31:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-05-24-context-compatibility-in-data-analysis/",
    "title": "Context Compatibility in Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-05-24",
    "categories": [],
    "contents": "\nAll data arise within a particular context and often as a result of a specific question being asked. That is all well and good until we attempt to use that same data to answer a different question within a different context. When you match an existing dataset with a new question, you have to ask if the original context in which the data were collected is compatible with the new question and the new context. If there is context compatibility, then it is often reasonable to move forward. If not, then you either have to stop or come up with some statistical principle or assumption that makes the two contexts compatible. Good data analysts will often do this in their heads quickly and may not even realize they are doing it. However, I think explicitly recognizing this task is important for making sure that data analyses can provide clear answers and useful conclusions.\nUnderstanding context compatibility is increasingly important as data science and the analysis of existing datasets continues to take off. Existing datasets all come from somewhere and it’s important to the analyst to know where that is and whether it’s compatible with where they’re going. If there is an imcompatibility between the two contexts, which in my experience is almost always the case, then any assumption or statistical principle invoked will likely introduce uncertainty into the final results. That uncertainty should at least be communicated to the audience, if not formally considered in the analysis. In some cases, the original context of the data and the context of the new analysis will be so incompatible that it’s not worth using the data to answer the new question. Explicit recognition of this problem can save a lot of wasted time analyzing a dataset that is ultimately a poor fit for answering a given question.\nI wanted to provide two examples from my own work where context compatibility played an important role.\nExample: Data Linkage and Spatial Misalignment\nAir pollution data tends to be collected at monitors, which can reasonably be thought of as point locations. Air pollution data collected by the US EPA is primarily collected to monitor regulatory compliance. The idea here (roughly) is that we do not want any part of a county or state to be above a certain threshold of air pollution, and so we strategically place the monitors in certain locations and monitor their values in relation to air quality standards. The monitors may provide representative measurements of air pollution levels in the general area surrounding the monitor, but how representative they are depends on the specific pollutant being measured and the nature of pollution sources in the area. Ultimately, for compliance monitoring, it doesn’t really matter how representative the monitors are because an exceedance at even one location is still a problem (the regulations have ways of smoothing out transient large values).\nHealth data tends to be measured at an aggregate level, particularly when it is coming from administrative sources. We might know daily counts of deaths or hospitalizations in a county or province or post code. Linking health data with air pollution data is not possible because of a context mismatch: Health data are measured areally (counts of people living within some political boundary) and pollution data are measured at point locations, so there is an incompatibility in the spatial measurement scale. We can only link these to data sources together if we do one of the following:\nAssume that the monitor values are representative of population exposure in the entire county\nDevelop a model that can make predictions of pollution levels at all points in the county and then take the average of those values as a representative of the average county levels\nThis problem is well-known in spatial statistics and is referred to as spatial misalignment or as change of support. The misalignment of the pollution and health data is the context mismatch here and arises because of the different measurement schemes that we use for each type of data. As a result, we must invoke either an assumption or a statistical model to link the two together.\nThe assumption of representativeness is easier to make because it requires no additional work, but it can introduce unknown uncertainties into the problem if the pollution values are not representative of population exposure. If a pollutant is regional in nature and is spatially homogeneous, then the assumption may be reasonable. But if there a lot of hyper-local sources of pollution that introduce spatial heterogeneity, the assumption will not hold. The statistical modeling approach is more work, but is straightforward (in principle) and may offer the ability to explicitly characterize the uncertainty introduced by the modeling. In both cases, there is a statistical price to pay for linking the datasets together.\nData linkage is a common place to encounter context mismatches because rarely are different datasets collected with the other datasets in mind. Therefore, careful attention must be paid to the contexts within which each dataset was collected and what assumptions or modeling most be done in order to achieve context compatibility.\nExample: The GAM Crisis\nA common way to investigate the acute or short-term associations between air pollution levels and health outcomes is via time series analysis. The general idea is that you take a time series of air pollution levels, typically from an EPA monitor, and then correlate it with a time series of some health outcome (often death) in a population of interest. The tricky part, of course, is adjusting for the variety of factors that may confound the relationship between air pollution and health outcomes. While some factors can be measured and adjusted for directly (e.g. temperature, humidity), other factors are unmeasured and we must find some reasonable proxy to adjust for them.\nIn the late 1990s investigators started using generalized additive models to account for unmeasured temporal confounders in air pollution time series models. With GAMs, you could include smooth functions of time itself in order to adjust for any (smoothly) time-varying factors that may confound the relationship between air pollution and health. It wasn’t a perfect solution, but it was a reasonable and highly flexible one. It didn’t hurt that there was already a nice S-PLUS software implementation that could be easily run on existing data. By 2000 most investigators had standardized on using the GAM approach in air pollution time series studies.\nIn 2002, investigators at Johns Hopkins discovered a problem with the GAM software with respect to the default convergence criterion. The problem was that the default convergence criterion used to determine whether the backfitting algorithm used to fit the model had converged was set to 0.0001, which for most applications of GAM was more than sufficient. The typical application of GAM was for scatterplot smoothing to look at potential nonlinearities in the relationship between the outcome and a predictor. However, in models where the nonparametric terms were highly correlated (a situation referred to as “concurvity”), then the default criterion was not stringent enough. It turned out that concurvity was quite common in the time series models and the end result was most models published in the air pollution literature had not actually converged in the fitting process.\nAround this time, the US EPA was reviewing the evidence from time series studies and asked the investigators who published most of the studies to redo their analyses using alternate approaches (including using a more stringent convergence criterion). To make a long story short, many of the published risk estimates dropped by around 40%, but the overall story remained the same. There was still strong evidence of an association between air pollution and health, it just wasn’t as large of an effect as we once thought. Francesca Dominici wrote a comprehensive post-mortem of the saga that contains many more details.\nThe underlying problem here was an undetected shift in context with respect to the GAM software. In previous usage of GAMs, the default convergence criterion was likely fine because there were not strong dependencies between the various smoother components in the model and the relationships being modeled did not have time series characteristics. However, when the same GAM software was used in a totally different context, one which the original authors likely did not foresee, suddenly the same convergence criterion was inadequate. The low-concurvity environment of previous GAM analyses was incompatible with the high-concurvity environment of air pollution time series analysis. The lesson here is that software used in a different context from which it was developed is essentially new software. And like any new software, it requires testing and validation.\nSummary\nContext shifts are critically important to recognize because they can often determine if the analyses you are conducting are valid or not. They are particularly important to discuss in data science applications here often the data are pre-existing but are being applied to a new problem or question. Methodologies and analytic approaches that are totally reasonable under one context can be inappropriate or even wrong under a different context. Finally, any assumptions made or models applied to achieve context compatibility can have an effect on the final results, typically in the form of increased uncertainty. These additional uncertainties should not be forgotten in the end, but rather should be communicated to the audience or formally incorporated in the analysis.\nYou can hear more from me and the JHU Data Science Lab by subscribing to our weekly newsletter Monday Morning Data Science.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:31:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-05-17-awesome-postdoc-opportunities-in-computational-genomics-at-jhu/",
    "title": "Awesome postdoc opportunities in computational genomics at JHU",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2018-05-17",
    "categories": [],
    "contents": "\nJohns Hopkins is a pretty amazing place to do computational genomics right now. My colleagues are really impressive, for example five of our faculty are part of the Chan Zuckerberg Initiative and we have faculty across a range of departments including Biostatistics, Computer Science, Biology, Biomedical Engineering, Human Genetics. A number of my colleagues are activitely looking for postdocs and in an effort to make the postdoc job market a little less opaque I’m listing this non-comprehensive list of opportunities I know about here.\nJob: Postdoc(s) in methods development for precision medicine in oncology and immunotherapy (single cell RNA-seq, TCR sequencing).\nEmployer: Elana Fertig and Elizabeth Jaffee\nTo apply: email Elana\nDeadline: Review ongoing\n===============\nJob: Timp lab is hiring postdocs! We work on sequencing technology and analysis development for direct RNA seq, epigenetics, cancer genomics, infectious disease diagnosis, non-model organism genomics, protein sequencing and all of the above!\nEmployer: Winston Timp\nTo apply: email Winston\nDeadline: Until position filled\n===============\nJob: The Goff lab (www.gofflab.org) has 2 open postdoc positions for a variety of projects at the intersection of computational biology and neurodevelopment, degeneration, and disease. We develop and utilize single cell analysis techniques, including single cell RNA-Seq, to explore cell state transitions across continuous biological processes. Current funded projects include cross-model analysis of fALS, Parkinson Disease, and Kabuki Syndrome, as well as examination of cell-type-specific neuronal plasticity response, and developmental fate specification in the CNS.\nEmployer: Loyal Goff and Solange Brown\nTo apply: email Loyal\nDeadline: 12/31/2018 or until position filled\n===============\nJob: The Hansen lab (www.hansenlab.org) is hiring postdocs in computational biology. We are funded to continue development of the recount2 (preprint) project, which involves joint processing, normalization, and analysis of all publicly available human RNA-seq samples. This project combines genomics with analysis of extremely large-scale RNA-seq data. The work is in collaboration with Jeff Leek, Ben Langmead and Alexis Battle at JHU.\nEmplyer: Kasper D. Hansen\nTo apply: Email Kasper.\nDeadline: Review ongoing\n===============\nJob: The Jaffe lab at the Lieber Institute for Brain Develpoment (affiliated with JHMI) is hiring at several different levels (research assistant, research associate, postdoc fellow)! We work at the intersection of genomics, biostatistics, and computational biology, leveraging large human brain datasets to better understand how genomic signatures associate with brain development and subsequent dysregulation in mental illness.\nEmployer: Andrew Jaffe\nTo apply: LIBD Careers\nDeadline: Until position filled\n===============\nJob: The Battle lab (battlelab.jhu.edu) is hiring postdocs in genomics and machine learning/probabilistic modeling. Projects include rare genetic variation, single cell genomics, time series genomics, complex human disease, large scale integrative transcriptomic/eQTL studies, and more.\nEmployer: Alexis Battle\nTo apply: email Alexis\nDeadline: Until positions filled\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:31:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-05-15-rethinking-academic-data-sharing/",
    "title": "Rethinking Academic Data Sharing",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-05-15",
    "categories": [],
    "contents": "\nThe sharing of data is one of the key principles of reproducible research (the other one being code sharing). Using the data and code a researcher has used to generate a finding, other researchers can reproduce those findings and examine the process that lead to them. Reproducibility is critical for transparency, so that others can verify the process, and for speeding up knowledge transfer. But recent events have gotten me thinking more about the data sharing aspect of reproducibility and whether it is tenable in the long run.\nMuch, if not most, data sharing can occur without any sort of controversy. For example, in our air pollution studies, we make available air pollution monitoring levels that are inputs to our models (these data are already publicly available from the US EPA although we process them a little). But the majority of my research involves health data which contains sensitive information about individuals. These data range from national studies using Medicare and Medicaid insurance claims to more local Baltimore studies involving patients at Johns Hopkins Hospital. In most cases, I hesitate to publish these data unless there is an obvious way to do so that doesn’t violate privacy. For example, we sometimes publish high-level summary statistics of the data or will publish small clinical datasets with identifiers removed. My discussions with others about sharing health data for research purposes have always been vague and hand-wavy. In general, there’s a sense that “reproducibility is good” and that “data sharing is good” but as someone who has implement these vague ideas, there is frustratingly little guidance on how.\nTimes have changed quite a bit since I first started thinking seriously about reproducible research back in 2006. Today we have companies like Facebook, Google, and other social networks that are routinely collecting troves of information about our behaviors. Credit scoring companies like Equifax collect a tremendous amount of financial data on us and are vulnerable to hacking. People for the most part do not have a problem with the fact that the data are collected because there is an exchange there (data for services). But when personal data is shared with others without their knowledge or consent, they are rightfully outraged. An interesting aspect of the most recent Facebook/Cambridge Analytica controversy is that it was an academic researcher who shared the data with consulting firm Cambridge Analytica. If he had written an academic paper and shared the data with people who wanted to reproduce the findings, would that have been the right thing to do? For the sake of reproducibility?\nI think research on humans is moving in the direction of making it harder to share rather than easier. It is getting easier and cheaper to collect highly detailed information about individuals and their behavior and it is tempting to incorporate that into research questions. From my own work, in a paper published in 2008 we linked health data aggregated to the county level to air pollution concentrations in that county. In a more recent study on coarse particulate matter exposure, we linked health data at the ZIP code level (a smaller unit of aggregation) because we were able to build a machine learning model to predict air pollution exposure at smaller scales. When you further stratify a ZIP code by race, gender, and age, the counts start to get very small. While using data at this level of detail is useful for answering important new research questions, it is impossible to share this kind of data for reproducibility purposes. That said, this particular dataset (Medicaid billing claims) is not proprietary; any researcher can obtain it from the Center for Medicare and Medicaid Studies for a fee and a research protocol.\nThere are a few points that need some serious consideration before we can have any real sharing of health data and at this point I don’t think there are good answers to the questions raised.\nData masking. People often ask why you can’t just “mask” the data in some way in order to protect people’s privacy. Masking can come in the form of aggregation, or jittering, or otherwise adding noise. First, I know of no generic way to do this for any dataset that guarantees that you can’t identify someone. Second, there is no way to guarantee that the masked dataset cannot be linked with an outside dataset in order to then identify someone in your dataset. Given the increasing number of “outside datasets” that are available (Facebook, Google, LinkedIn, etc.) I find it difficult to imagine that this task would get easier in the future. Finally, masking data to the point where they are reasonably unidentifiable often makes them useless for reproducibility purposes. For example, in the Medicaid example above, we could aggregate the data up (a lot), but that would defeat the entire purpose of using that dataset in the first place.\nLiability. I don’t have any clue who would be held liable if a person’s privacy were violated due to a dataset that I published. At this point, I’m just going to assume it’s me. I have some difficulty seeing my university or anyone else running to my defense if something like this were to occur. While Mark Zuckerberg may have a team of lawyers and public relations specialists to help him testify before Congress when Facebook has a data breach, I have no such resources. You can see how this uncertainty might give any researcher pause before publishing a health-related dataset. For example, Netflix was sued over its famous “Netflix Prize” where millions of movie ratings were made publicly available. If you’re thinking “That won’t happen to me, I’m just an academic researcher,” all I can say is good luck with that defense.\nSecurity. In the past, there was a sense that if you shared a dataset with “a few other researchers” it wasn’t such a big deal. With this ancient logic, it would have been difficult for a dataset to “escape” because it’s not like someone was really going to go through the effort of copying a bunch of disks or tapes. Alas, the Internet is a thing now and all it takes is one hack for data to be exposed to the world. If I share my data with someone with a password of 123456, is that really good for anyone?\nCost. Data sharing in general is a time consuming business but it’s much easier if you can format a dataset in a reasonably cross-platform way and just push it to a well-supported public repository. This is how most discussions of data sharing are framed, with the implication being that once a dataset is shared, there are no ongoing costs. However, with health data, that’s not possible, so the current approach is to essentially deal with requests as they come in and handle them on a case-by-case basis. This takes time and the process is not particularly well organized. Furthermore, there is no funding for this process because usually data is shared after the grant funding has gone away. I’ve seen very little discussion of sustainable business models for sharing of protected academic health data. As researchers publish more studies, they essentially incur more unfunded mandates to deal with the requests for the underlying data. Larger institutions may have the ability to aggregate resources to pay for this, but smaller institutions likely do not.\nRecent controversies at Facebook and various other companies have highlighted the vast quantities of data these companies collect and how vulnerable the data are to exposure. There is a reasonable debate going on regarding whether companies should be able to share this data and for what purposes. Academics have to realize that they are also part of this debate and that any decisions made in that domain will likely affect them. For example, the Health Insurance Portability and Accountability Act (HIPAA) was not a law that particularly had academics in mind, but it nevertheless had a profound effect on the way that academics handled health insurance data for research purposes. Should governments around the world decide to restrict the amount of data that social networks like Facebook can share with third parties, those decisions will likely affect academics too. The recently implemented EU General Data Protection Regulation is a step in this direction and there will likely be more to follow.\nWhile there are many complexities involved in sharing health data, I do think data sharing serves an important practical purpose, beyond the transparency and the “it’s the right thing to do” reasons. Lack of data sharing serves to entrench incumbents in the field, and so large and rich institutions can afford to hoard their data and collect new data whenever they feel like it. This hoarding of data feeds an upward cycle where these institutions can therefore get more funding to collect even more data that they don’t share. Newer entrants to a given field are left with fewer resources and an inability to collect the same kinds of data. A rich infrastructure of data sharing allows these new entrants to get “up to speed” quicker and to ask more interesting questions from the get go. The same is true in the commercial world, as huge companies like Facebook and Google can afford to hold on to their data and comply with various complex regulations. They may be especially motivated to do so if it prevents upstarts and competitors from appearing. The recent release of Facebook’s dating feature is a near perfect example of this. Facebook might be very motivated to not share its data if it can build its own separate business while simultaneously jeopardizing the business models of its competitors Match.com, Tinder, and OkCupid, which depend critically on Facebook’s social graph.\nIn the end, doing things for research does not give someone a blank check to do anything. Over time we have restricted the activities that researchers can engage in, because society has decided they are unethical or otherwise inappropriate. I think it’s inevitable that the sharing of data will require a society-wide discussion about whether sharing for research purposes provides a benefit that outweighs the costs. Right now, we have researchers essentially making unilateral decisions on an ad hoc basis and I don’t think that is sustainable.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:31:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-05-03-software-as-an-academic-publication/",
    "title": "Software as an academic publication",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-05-03",
    "categories": [],
    "contents": "\nSoftware has for while now played a weird an uncomfortable role in the academic statistics world. When I first started out (circa 2000), I think developing software was considered “nice”, but for the most part, was not considered valuable as an academic contribution in the statistical universe. People were generally happy to use the software and extol its virtues, but when it came to evaluating a person’s scholarship, software usually ranked somewhere near the bottom of the list, after papers, grants, and maybe even JSM contributed talks.\nJournals like the Journal of Statistical Software tried to remedy the situation by translating the contribution of software into a commodity that people already understood: papers. (Okay, thinking of papers as “commodities” is probaby worth another blog post.) The idea with JSS, as I understood, was that you could publish a paper about your software, or maybe even a manual, and then that would be peer-reviewed in the usual way. If it passed peer-review, it would be published (electronically) and then you could list that paper on your CV. Magic! The approach was inefficient from the start, because it placed an additional burden on software developers. Not only did they have to write (and support) the software, but they had to write a separate paper to go along with it. Writers of theorems were not saddled with such requirements. But at the time of its founding around 2004, the JSS approach was perhaps a reasonable compromise.\nThese days, I think many people would be comfortable seeing a piece of software as a per se academic contribution. A software package can, for the most part, stand alone and people will recognize it. I personally do not think authors should be forced to write a separate paper, essentially translating the software from code into English or whatever natural language. The Journal of Open Source Software is a move in this “opposite” direction, where no paper is required, beyond the software itself and a brief summary of the software’s purpose and how to use it. This process is similar to ideas that people have had regarding the publishing of data: why not just let people who collect data publish the dataset alone and get credit for doing so? I think the JOSS approach is basically the way to go for academic software contributions, with one small caveat.\nEvaluation of most software places a heavy emphasis on usefulness. The instructions for authors on the JSS web site indicate how each paper and software would be evaluated for publication:\n\nThe review has two parts: both the software and the manuscript are reviewed. The software should work as indicated, be clearly documented, and serve a useful purpose. Reviewers are instructed to evaluate both correctness and usefulness. Special emphasis is given on the reproducibility of the results presented in the submission.\n\nJOSS takes a slightly different approach but ultimately has a similar standard. The software\n\nShould be a significant contribution to the available open source software that either enables some new research challenges to be addressed or makes addressing research challenges significantly better (e.g., faster, easier, simpler)\n\nIn both cases, there is an emphasis on usefulness which, as a goal, is difficult to argue against. However, if you look at the goals of most academic journals, usefulness is not amongst the criteria for evaluating contributions. If it were, you’d have to delete most of the journals in existence today. This is a frequent argument that basic scientists and theorists have with policymakers—whether their work is “useful” is not the point. Advancing the state of knowledge about the world is important in and of itself and the usefulness of that knowledge may be difficult to ascertain in the short run. Journals typically strive to publish papers that represent an advance in knowledge in a particular field. Often there is a vague mention of “impact” of the work, but how wide that impact is likely to be will depend on the nature of the field.\nWhat exactly is the advance in knowledge that is obtained when software is developed and distributed? I don’t ask this question because I don’t think there is an advance. I ask it because I think there is definitely knowledge that is gained in the process of developing software, but it’s usually not communicated to anyone. This knowledge is typically not obtained via the scientific process. Rather it is often gained through personal experience. We have well-established methods for distributing software once it is developed, but we do not have well-established venues of distributing any knowledge that is obtained, particularly any informal knowledge or personal stories.\nIf you’ve ever seen someone give a presentation or talk about some software they’ve written, you know it’s not the same as reading the manual for the software. One reason why is that the presenter will describe the process through which they went about writing the software and, usually as asides, mention some lessons learned along the way. I think these “lessons learned” are critically important, and make up a relevant contribution to the scientific community. If I mention that I started out writing this software using R’s S4 class/method system but realized it was too complicated and annoying and so went back to S3, that’s a useful lesson learned. As a fellow developer, I might reconsider starting my next project with S4. However, unless we take a critical eye to the git logs for a given software package, we would never know this by simply using the software. It would seem as if the developer went with S3 from the get go for unknown reasons.\nI think it would be nice if software publications included a brief summary of any lessons learned in the process of developing the software. Many developers already do this via blog posts or similar media. But many do not and we are left to wonder. Maybe they did some informal user testing and found that people preferred one interface over another interface. Anything that other people (and developers) might find useful, beyond the software itself. It might be a paragraph or even just a set of bullet points. It might be more but I’m not inclined to require any specific length or format. To be clear, this is not equivalent to a scientific study, but it’s potentially useful information nonetheless.\nOne good example of something like this is the original 1996 R publication in the Journal of Computational and Graphical Statistics by Robert Gentleman and Ross Ihaka. In fact, the abstract for the article says it all:\n\nIn this article we discuss our experience designing and implementing a statistical computing language. In developing this new language, we sought to combine what we felt were useful features from two existing computer languages. We feel that the new language provides advantages in the areas of portability, computational efficiency, memory management, and scoping.\n\nWith many popular software packages there are often blog posts that people write to describe how they use the software for their particular purpose. These posts will sometimes praise or criticize the software and in aggregate provide a good sense of the user experience. The kind of information I’m talking about gives us insight into the developer experience. For any reasonably well-developed piece of software, there are always some lessons learned that the author ultimately keeps to themselves. I think that’s a shame and it would be nice if we could all learn something from their experience.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:31:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-04-30-relationships-in-data-analysis/",
    "title": "Relationships in Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-04-30",
    "categories": [],
    "contents": "\nI recently finished reading Steve Coll’s book Directorate S, which is a chronicle of the U.S. war in Afghanistan post 9-11. It’s a good book, and one line stuck out for me as I thought it had relevance for data analysis. In one chapter, Coll writes about Lieutenant Colonel John Loftis, who helped run a training program for U.S. military officials who were preparing to go serve in Afghanistan. In reference to Afghan society, he says, “Everything over there is about relationships.” At the time, Afghanistan had few independent institutions and accomplishing certain tasks depended on knowing certain people and having a good relationship with them.\nI find data analysis to be immature as an independent field. It uses many tools–mathematics, statistics, computer science–that are mature and well-studied. But the act of analyzing data is not particularly well-studied. And like any immature organization (or nation), much of data analysis still has to do with human relationships. I think this is an often ignored aspect of data analysis because people hold out hope that we can build the tools and technology to the point where we do not need to rely on relationships. Eventually, we will find the approaches that are universally correct and so there will be little need for discussion.\nHuman relationships are unstable, unpredictable, and inconsistent. Algorithms and statistical tools are predictable and in some cases, optimal. But for whatever reason, we have not yet been able to completely characterize all of the elements that make a successful data analysis in a “machine readable” format. We haven’t developed the “institutions” of data analysis that can operate without needing the involvement of specific individuals. Therefore, because we have not yet figured out a perfect model for human behavior, data analysis will have to be done by humans for just a bit longer.\nIn my experience, there are a few key relationships that need to be managed in any data analysis and I discuss them below.\nData Analyst and Patron\nAt the end of the day, someone has to pay for data analysis, and this person is the patron. This person might have gotten a grant, or signed a customer, or simply identified a need and the resources for doing the analysis. The key thing here is that the patron provides the resources and determines the tools available for analysis. Typically, the resources we are concerned with are time available to the analyst. The Patron, through the allocation of resources, controls the scope of the analysis. If the patron needs the analysis tomorrow, the analysis is going to be different than if they need it in a month.\nA bad relationship here can lead to mismatched expectations between the patron and the analyst. Often the patron thinks the analysis should take less time than it really does. Conversely, the analyst may be led to believe that the patron is deliberately allocating fewer resources to the data analysis because of other priorities. None of this is good, and the relationship between the two must be robust enough in order to straighten out any disagreements or confusion.\nData Analyst and Subject Matter Expert\nThis relationship is critical because the data analyst must learn the context around the data they are analyzing. The subject matter expert can provide that context and can ask the questions that are relevant to the area that the data inform. The expert is also needed to help interpret the results and to potentially place them in a broader context, allowing the analyst to assess the practical significance (as opposed to statistical significance) of the results. Finally, the expert will have a sense of the potential impact of any results from the analysis.\nA bad relationship between the analyst and the expert can often lead to\nIrrelevant analysis. Lack of communication between the expert and the analyst may lead the analyst to go down a road that is not of interest to the audience, no matter how correct the analysis is. In my experience, this outcome is most common when the analyst does not have any relationship with a subject matter expert.\nMistakes. An analyst’s misunderstanding of some of the data or the data’s context may lead to analysis that is relevant but incorrect. Analysts must be comfortable clarifying details of the data with the expert in order to avoid mistakes.\nBiased interpretation. The point here is not that a bad relationship leads to bias, but rather a bad relationship can lead the expert to not trust the analyst and their analysis, leading the expert to rely more strongly on their preconceived notions. A strong relationship between the expert and the analyst could lead to the expert being more open to evidence that contradicts their hypotheses, which can be critical to reducing hidden biases.\nData Analyst and Audience\nThe data analyst needs to find some way to assess the needs and capabilities of the audience, because there is always an audience. There will likely be many different ways to present the results of an analysis and it is the analyst’s job to figure what might be the best way to make the results acceptable to the audience. Important factors may include how much time the audience has to see the presentation, how mathematically inclined/trained they are, whether they have any background in the subject matter, what “language” they speak, or whether the audience will need to make a clear decision based on your analysis. Similar to the subject matter expert, if the analyst has a bad relationship with the audience, the audience is less likely to trust the analysis and to accept its results. In the worst case, the audience will reject the analysis without seriously considering its merits.\nAnalysts often have to present the same analysis to multiple audiences, and they should be prepared to shift and rearrange the analysis to suit those multiple audiences. Perhaps a trivial, but real, example of this is when I go to give talks at places where I know a person there has developed a method that is related to my talk, I’ll make sure to apply their method to my data and compare it to other approaches. Sometimes their method is genuinely better than other approaches, but most of the time it performs comparably to other approaches (alas, that is the nature of most statistical research!). Nevetheless, it’s a simple thing to do and usually doesn’t require a lot of extra time, but it can go a long way to establishing trust with the audience. This is just one example of how consideration of the audience can change the underlying analysis itself. It’s not just a matter of how the results are presented.\nImplications\nIt’s tempting to think that the quality of a data analysis only depends on the data and the modeling applied to it. We’re trained to think that if the data are consistent with a set of assumptions, then there is an optimal approach that can be taken to estimate a given parameter (or posterior distribution). But in my experience, that is far from the reality. Often, the quality of an analysis can be driven by the relationships between the analyst and the various people that have a stake in the results. In the worst case scenario, a breakdown in relationships can lead to serious failure.\nI think most people who analyze data would say that data analysis is easiest when the patron, subject matter expert, the analyst, and the audience are all the same person. The reason is because the relationships are all completely understood and easy to manage. Communication is simple when it only has to jump from one neuron to another. Doing data analysis “for yourself” is often quick, highly iterative, and easy to make progress. Collaborating with others on data analysis can be slow, rife with miscommunication, and frustrating. One common scenario is where the patron, expert, and the audience are the same person. If there is a good relationship between this person and the analyst, then the data analysis process here can work very smoothly and productively.\nCombining different roles into the same person can sometimes lead to conflicts:\nPatron is combined with the audience. If the audience is paying for the work, then they may demand results that confirm their pre-existing biases, regardless of what evidence the data may bring.\nSubject matter expert and the analyst are the same person. If this person has strong hypotheses about the science, they may be tempted to drive the data analysis in a particular direction that does not contradict those hypotheses. The ultimate audience may object to these analyses if they see contradictory evidence being ignored.\nAnalyst and audience are the same. This could lead to a situation where the audience is “too knowledgeable” about the analysis to see the forest for the trees. Important aspects of the data may go unnoticed because the analyst is too deep in the weeds. Furthermore, there is potential value and forcing an analyst to translate their findings for a fresh audience in order to ensure that the narrative is clear and that the evidence as strong as they believe.\nSeparating out roles in to different people can also lead to problems. In particular, if the patron, expert, analyst, and audience are all separate, then the relationships between all four of those people must in some way be managed. In the worst case, there are 6 pairs of relationships that must be on good terms. It may however be possible for the analyst to manage the “information flow” between the various roles, so that the relationships between the various other roles are kept separate for most of the time. However, this is not always possible or good for the analysis, and managing the various people in these roles is often the most difficult aspect of being a data analyst.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:31:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-04-23-what-can-we-learn-from-data-analysis-failures/",
    "title": "What can we learn from data analysis failures?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-04-23",
    "categories": [],
    "contents": "\nBack in February, I gave a talk at the Walter and Eliza Hall Research Institute in Melbourne titled “Lessons in Disaster: What Can We Learn from Data Analysis Failures?” This talk was quite different from talks that I usually give on computing or environmental health and I’m guessing it probably showed. It was nevertheless a fun talk and I got to talk about space-related stuff. If you want to hear some discussion of the development of this talk, you can listen to Episode 53 of Not So Standard Deviations.\nIt’s difficult to have a discussion about data analysis without some mention of John Tukey. In particular, his paper “The Future of Data Analysis”, published in the Annals of Statistics in 1962, weighs heavily. In fact, it weighs so heavily that the paper required its own table of contents! One paragraph from the end of Tukey’s massive paper has always struck me, in that his description of how we should teach data analysis is relatively simple, but we seem unable to implement it.\n\nWe would teach [data analysis] like biochemistry, with emphasis on what we have learned…with relegation of all question of detailed methods to the “laboratory work”. All study of detailed proofs…or comparisons of ways of presentation would belong in “the laboratory” rather than “in class”.\n\nMy interest is in taking this statement rather broadly and asking how often do we actually do this when it comes to data analysis?\nAnother statement that has fascinated me comes from Daryl Pregibon, who wrote in a 1991 National Research Council report titled The Future of Statistical Software,\n\nThroughout American or even global industry, there is much advocacy of statistical process control and of understanding processes. Statisticians have a process they espouse but do not know anything about. It is the process of putting together many tiny pieces, the process called data analysis, and is not really understood.\n\nThe “putting together many tiny pieces” aspect of data analysis is really key. My guess is that Pregibon was referring to putting together many statistical tools and making all the little decisions about data that one always makes. However, often those little “pieces” are in fact people, and getting all of those people to fit together can be an equally challenging and equally critical aspect of success.\nLearning about how data analyses succeeds or fails (but more importantly, fails) is extremely challenging without actually going through the process yourself. I don’t think I ever learned about it except through first hand experience, which took place over the course of years. There are a few reasons for this that I have observed over time:\nSuccess in scientific data analysis is usually concerned with whether the claims made based on the results are true or not. If the results feel true, and the analysis appears rigorous, then that’s usually the end of the discussion. Focus is put on the result and what should come next. The underlying idea here is not necessarily misguided: Progress in science depends on independent replication, and any given analysis cannot be assigned too much weight.\nWhen analyses fail, the results are usually vague and confusing. Furthermore, the public rarely finds out about them because they are not published. This is mostly due to human nature: it’s difficult to motivate oneself to write about an experience that was inconclusive and perhaps incoherent. It can also be embarrassing if honest mistakes were made. Publication of negative studies is a separate matter, because I would regard a truly negative study to be, in fact, conclusive. But often, we don’t even have that much clarity.\nIn the rare cases where we do find out about data analysis failures, the focus is often on who or what is to blame. In cases where criminal activity has taken place, this is an important aspect. However, identifying who or what is to blame usually doesn’t provide us with generalizable knowledge that we can apply to our own data analyses. The underlying assumption of this approach is that this failure was a unique situation that could never have happened if the individual to blame had not been involved. Occasionally, I see cases where there is a clear bug in some software that leads to erroneous results. Fixing the bug in the code will “fix” the results, but even in that situation it’s not clear to me that the bug is the ultimate cause of failure (although in this case it is the proximate cause).\nI want to use one case study to think about what kinds of generalizable knowledge we can obtain from data analysis failures. The one I describe below is special because it had serious implications and large parts of it played out in public. While we likely will never know all of the details, we know enough to have a meaningful discussion.\nThe Duke Saga\nThe “Duke Saga” has been a tough nut to crack for me for many years now. While it’s fascinating because of the sheer number of problems that occurred, I’ve always struggled to identify exactly what went wrong. In other words, given what I know now, what intervention would I have taken to prevent a similar episode from happening in the future. I’ve long felt that the lessons people take away from this saga are not the correct ones in that applying the lessons to future work would not prevent failure.\nFirst some background. Note that this is a highly abbreviated timeline:\nIn 2006, Nature Medicine published a paper by Potti et al. titled “Genomic signature to guide the use of chemotherapeutics”. The paper claimed to have developed a classifier based on applying microarray technology to cell lines maintained by the National Cancer Institute (NCI). They claimed the classifier could determine which patients would respond to chemotherapy treatment.\nKeith Baggerly and Kevin Coombes at MD Anderson Cancer Center were inundated by requests from (justifiably) excited colleagues who wanted to use this technology. Baggerly and Coombes attempted to reproduce the results using the published description but were unable to do so. They were able to reproduce certain results in the paper after deliberately introducing a series of errors into the data analysis.\nSince this initial incident, a number of other papers from the same lab were scrutinized and numerous errors in analyses were found, many that one might consider basic data handling and wrangling mistakes. In addition, Baggerly and Coombes found circumstantial evidence of deliberate fraud, such as claiming that certain genes were critical to a classifier even though those genes are not included in the microarray claimed to have been used.\nClinical trials were started at Duke where patients were randomized into different arms of the trial based on the flawed techniques developed by Potti. After numerous scientists wrote a letter to NCI director Harold Varmus, Duke suspended the trials to investigate the situation. An internal Duke panel eventually cleared Potti and colleagues of any wrongdoing and restarted the trials. Time passes and eventually it is discovered by the The Cancer Letter that Potti lied on an application for federal funding about once being a Rhodes Scholar. Eventually, the trials were stopped, but only after much public scrutiny and a series of lawsuits (some still ongoing).\nI’ve obviously left out a lot of detail and if you want to hear more about this you can hear about it from Keith Baggerly himself in this nice lecture. However, I just wanted to give a sketch of what happened over a now more than 10 year period.\nIn my opinion, the details of the Duke Saga were salacious, but it was difficult to draw any conclusion about what actually went wrong and what approach should be taken to prevent something like this from happening again. Most people were just speculating about what could have happened and the people who really would know the details weren’t talking very much. Here’s how I would summarize the basic points that most people seemed to take away from the publicly available information about the saga:\nReproducibility. There was definitely a reproducible research angle to this saga, in that the analyses that were conducted lacked transparency. There was only sketchy code that was published along with paper and data were not immediately available. However, in a twisted sense, I think much of what came to light did so because the work was ultimately partially reproducible. That is in fact how Baggerly and Coombes discovered all the problems. They were able to reproduce the findings after deliberately introducing mistakes in the data. If one went back in time and magically forced everyone in the lab to use R Markdown or Juypter Notebooks, it’s not clear to me how that would have prevented anything. For starters, everyone within the team had access to the analyses and the data. It’s possible that people outside the team might have discovered problems sooner if the work had been completely reproducible, but Baggerly and Coombes figured things out relatively quickly. Also, that is besides the point: We should not depend on people outside the research team as a primary defense against data analytic failure. I don’t think reproducibility is one of the lessons learned from this saga because I don’t think it would have made a difference in this case.\nExpertise. The basic narrative explaining this saga was that the data analyses were poorly done. Statisticians in particular have focused on the use of proprietary software, non-reproducible workflows (like pointing and clicking in Excel), and incorrect application of otherwise sound statistical methodology (e.g. cross validation). I’ve been involved in some discussions that suggested that if better-trained people had been doing the analyses, none of this would have happened. Perhaps genomic analyses are too complicated for the traditionally trained laboratory scientist. The idea is that this kind of work is “hard to do” and that you need better people (or improve existing people). I think that is the gist of the summary in this segment from 60 Minutes on the entire saga. I will discuss this more below.\nIndividual behavior. Anil Potti was eventually fired from Duke over this scandal and I don’t think anyone would disagree with that decision. If Duke had fired him 10 years ago, then yes, this research would not have happened at Duke, but it might have happened somewhere else, or it might have happened at Duke but with a different principal investigator. So while Potti was ultimately responsible for the analyses, his firing does not provide a useful “lesson learned”.\nIn January 2015, The Cancer Letter published a memo written by Bradford Perez, who in 2008 was a medical student trainee in the Potti lab. He saw what was going on in the lab and recognized its shoddiness. Problems that Baggerly and Coombes had to essentially reverse engineer, Perez saw first hand and immediately recognized them as serious. In fact, in 2008 he wrote a memo to the leadership of his institute describing some of those problems:\n\n“Fifty-nine cell line samples with mRNA expression data…were split in half to designate sensitive and resistant phenotypes. Then in developing the model, only those samples which fit the model best in cross validation were included. Over half of the original samples were removed…. This was an incredibly biased approach which does little more than give the appearance of a successful cross validation.” [emphasis added]\n\nHe further wrote,\n\nAt this point, I believe that the situation is serious enough that all further analysis should be stopped to evaluate what is known about each predictor and it should be reconsidered which are appropriate to continue using and under what circumstances…. I would argue that at this point nothing…should be taken for granted. All claims of predictor validations should be independently and blindly performed.”\n\nThe memo was ignored by the leadership. Nothing was stopped and nothing was changed at the time. Perez eventually took his name off a series of papers and left the lab.\nLessons Learned\nThis memo is critical in my opinion because it fundamentally changes the narrative about what went wrong in this entire saga. Yes, genomic analyses are “hard to do” but clearly there was expertise in the lab to recognize that difficulty and to recognize when statistical methods were being incorrectly applied. The problem was not a lack of training, nor was it simply the result of a few honest data management mistakes here and there. The problem was a breakdown in communication and a total lack of trust between investigators and members of the data analytic team. Perez clearly felt uncomfortable raising these issues in the lab and wrote the memo knowing that he had “much to lose”. He thought the problem in the lab was that statistical methods were being misapplied, but the real problem in the lab was that he did not feel comfortable discussing it. A breakdown in the relationship between an analyst and an investigator is a serious data analytic problem.\nIt’s possible for me to imagine an alternate scenario where a data analyst like Perez sees a problem with the way models are being developed or applied, mentions this to the principal investigator and has a detailed discussion, perhaps seeks outside expertise (e.g. from a statistician), and then modifies the procedure to fix the problem. It’s easy for me to imagine this because it happens pretty much every day. No data analysis is perfect from start to finish. Changes and course corrections are constantly made along the way. When I analyze data and run into problems that can be traced to data collection, I will raise this with the PI. When I give results to other investigators, sometimes the results don’t seem right to them and they come to me and seek clarification. If it’s a mistake on my part, I’ll fix it and send them updated results.\nWhen the relationships between an analyst and various members of the investigator team are strong and there is substantial trust between them, honest mistakes are just minor bumps in the road that can be uncovered, discussed, and fixed. When there is a breakdown in those relationships, the exact same mistakes are covered up, denied, and buried. A breakdown in the relationships between analysts and other investigators on the team generally cannot be fixed with a better statistical method, or a reproducible workflow, or open source software. Recognizing that this is the problem is difficult because often there is no easy solution.\nI think the data analytic lesson learned from the Duke Saga is that data analysts need to be allowed to say “stop”. But also, the ability to do so depends critically on the relationships between the analyst and members of the investigator team. If an analyst feels uncomfortable raising analytic issues with other members, then arguably all analyses done by the team are at risk. No amount of statistical expertise or tooling can fix this fundamental human problem.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:31:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-04-19-process-versus-outcome/",
    "title": "Process versus outcome productivity",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2018-04-19",
    "categories": [],
    "contents": "\nSeveral times over the last few weeks my hatred of Doodle polls has come up in meetings. I think the polling technology is great, but I’m still frustrated by the polls. Someone asked what I’d rather have happen and I said:\n\n“set the meeting, then let me know when it is, if I can come I will. if i’m not there then i’m happy for you to decide without me”\n\nI tried to think about why I feel this way about meetings and I realized that there is a tradeoff between process focused and outcome focused productivity. I’m sure this is a really well known phenomenon but I’m just figuring it out now.\nThe way I think about it is process focused productivity happens by performing things according a standard model. The pros and cons of this model are:\nPros:\nThe model can be easily communicated to new people\nIt is easy to measure whether people are doing “the right” thing\nIt makes it easier for people to know what to be working on\n\nCons:\nAnything outside of the process is difficult\nProcesses may not be optimized for a specific goal\nSuboptimal choices can be justified by process\n\nOutcome focused productivity on the other hand focuses on accomplishing a goal but does not impose a standard process. The pros and cons of outome focused productivity are:\nPros:\nCan be very efficient since the outcome is the focus\nRemoves barriers imposed by process bureucracy\nCan reduce or eliminate unnecessary activities\n\nCons:\nRequires everyone to know the goal\nCan be much more free flowing and so harder to measure progress\nCan be frustrating for people who have to be flexible about process\n\nElon Musk’s productivity suggestions are a limiting case of outcome focused productivity.\nI realized that I much prefer outcome focused productivity to process focused productivity personally. But working for a large organization like JHU that uses process focused productivity can be frustrating. Doodle polls, standardized email servers, specific exam requirements, processes for meeting scheduling, etc. are all components of a process focused place like JHU. But they drive a person like me who likes outcome focused productivity totally bonkers. On the other hand, I’m sure I drive people around here nuts when I make decisions on the fly/at the last minute to adapt to circumstances to achieve the outcome I’m after.\nI’m not sure there is any real solution for this conflict, I just realized this is the reason for a lot of separate different frustrations that have been bubbling up over the last few weeks/months. I wonder if anyone has a good idea on how to be an outcome focused productivity person inside a process focused productivity organization?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:31:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-04-17-what-is-a-successful-data-analysis/",
    "title": "What is a Successful Data Analysis?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2018-04-17",
    "categories": [],
    "contents": "\nDefining success in data analysis has eluded me for quite some time now. About two years ago I tried to explore this question in my Dean’s Lecture, but ultimately I think I missed the mark. In that talk I tried to identify standards (I called them “aesthetics”) by which we could universally evaluate the quality of a data analysis and tried to make an analogy with music theory. It was a fun talk, in part because I got to play the end of Charles Ives’ Second Symphony.\nStatisticians, in my experience, do not discuss this topic very much. It’s either because it’s so stupid that everyone has an (unspoken) understanding of it, or that everyone kind of has a slightly different understanding of it, or that no one understands it. Either way, in my close to twenty years as a statistician, I don’t think I’ve had many in-depth conversations with anyone about what makes a data analysis successful. The most that I’ve ever discussed this topic is on Not So Standard Deviations with Hilary Parker, where this is a frequent topic of conversation. Recently, Hilary gave a talk related to this topic (slides here), and so I was inspired to write something.\nI think I’ve come around to the following definition of data analysis success, which is,\n\nA data analysis is successful if the audience to which it is presented accepts the results.\n\nThere are a number of things to unpack here, so I will walk through them. Two key notions that I think are important are the notions of acceptance and the audience.\nAcceptance\nThe first idea is the notion of acceptance. It’s tempting to confuse this with belief, but they are two different concepts that need to be kept separate (although that can be difficult at times). Acceptance of an analysis involves the analysis itself—the data and the methods applied to it, along with the narrative told to explain the results. Belief in the results depends on the analysis itself as well as many other things outside the analysis, including previous analyses, existing literature, and the state of the science (in purely Bayesian terms, your prior). A responsible audience can accept an analysis without necessarily believing its principal claims, but these two concepts are likely to be correlated.\nFor example, suppose a team at your company designs an experiment to collect data to determine if lowering the price of a widget will have an effect on profits for your widget-making company. During the data collection process, there was a problem which resulted in some of the data being missing in a potentially informative way. The data are then handed to you. You do your best to account for the missingness and the resulting uncertainty, perhaps through multiple imputation or other adjustment methods. At the end of the day, you show me the analysis and conclude that lowering the price of a widget will increase profits 3-fold. I may accept that you did the analysis correctly and trust that you did your best to account for the problems encountered during collection using state-of-the-art methods. But I may disagree with the conclusion, in part because of the problems introduced with the missing data (not your fault), but also in part because we had previously lowered prices on another product that we sell and there was no corresponding increase in profits. Given the immense cost of doing the experiment, I might ultimately decide that we should abandon trying to modify the price of widgets and leave things where they are (at least for now). The analysis was a success.\nThis simple example illustrates two things. First, acceptance of the analysis depends primarily on the details of the analysis and my willingness to trust what the analyst has done. Was the missing data accounted for? Was the uncertainty properly presented? Can I reason about the data and understand how the data influence the results? Second, my belief in the results depends in part on things outside the analysis, things that are primarily outside the analyst’s control. In this case, these are the presence of missing data during collection and a totally separate experience lowering prices for a different product. How I weigh these external things, in the presence of your analysis, is a personal preference.\nAcceptance vs. Validity\nIn scientific contexts it is tempting to think about validity. Here, a data analysis is successful if the claims made are true. If I analyze data on smoking habits and mortality rates and conclude that smoking causes lung cancer, then my analysis is successful if that claim is true. This definition has the advantage that it removes the subjective element of acceptance, which depends on the audience to which an analysis is presented. But validity is an awfully high bar to meet for any given analysis. In this smoking example, initial analyses of smoking and mortality data could not be deemed successful or not until decades after they were done. Most scientific conclusions require multiple replications occurring over many years by independent investigators and analysts before the community believes or concludes that they are true. Leaving data analysts in limbo for such a long time seems impractical and, frankly, unfair. And ultimately, I don’t think we want to penalize data analysts for making conclusions that turn out to be false, as long as we believe they are doing good work. Whether those claims turn out to be true or not may depend on things outside their control.\nA related standard for analyses is essentially a notion of intrinsic validity. Rather than wait until we can validate a claim made by an analysis (perhaps decades down the road), we can evaluate an analysis by whether the correct or best approach was done and the correct methods were applied. But there are at least two problems with this approach. In many scenarios it is not possible to know what is the best method, or what is the best combination of methods to apply, which would suggest that in many analyses, we are uncertain of success. This seems rather unsatisfying and ultimately impractical. Imagine hiring a data analyst and saying to them “In the vast majority of analyses that you do, we will not know if you are successful or not.” Second, even in the ideal scenarios, where we know what is correct or best, intrinsic validity is necessary but far from sufficient. This is because the context in which an analysis performed is critical in understanding what is appropriate. If the analyst is unaware of that context, they may make critical mistakes, both from an analytical and interpretative perspective. However, those same mistakes might be innocuous in a different context. It all depends, but the analyst needs to know the difference.\nOne story that comes to mind comes from the election victory of George W. Bush over Al Gore in the 2000 United States presidential election. That election hinged on votes counted in the state of Florida, where Bush and Gore were very close. Ultimately, lawsuits were filed and a trial was set to determine exactly how the vote counting should proceed. Statisticians were called to testify for both Bush and Gore. The statistician called to testify for the Gore team was Nicolas Hengartner, formerly of Yale University (he was my undergraduate advisor when I was there). Hengartner presented a thorough analysis of the data that was given to him by the Gore team and concluded there were differences in how the votes were being counted across Florida and that some ballots were undercounted. However, on cross-examination, the lawyer for Bush was able to catch Hengartner in “gotcha” moment which ultimately had to do with the manner in which the data were collected, about which Hengartner had been unaware. Was the analysis a success? It’s difficult to say without the having been directly involved. Nobody challenged the methodology that Hengartner used in the analysis, which was by all accounts a very simple analysis. Therefore, one could argue that it had intrinsic validity. However, one could also argue that he should have known about the issue with how the data were collected (and perhaps the broader context) and incorporated that into his analysis and presentation to the court. Hengartner’s analysis was only one piece in a collection of evidence presented and so it’s difficult to say what role it played in the ultimate outcome.\nAudience\nAll data analyses have an audience, even if that audience is you. Ultimately, the audience may accept the results of an analysis or they may fail to accept it, in which case more analyses may need to be done. The fact that an analyst’s success may depend on a person different from the analyst may strike some as an uncomfortable feature. However, I think this is the reality of all data analyses. Success depends on human beings, unfortunately, and this is something analysts must be prepared to deal with. Recognizing that human nature plays a key role in determining the success of data analysis explains a number of key aspects of what we might consider to be good or bad analyses.\nThe Role of Narrative\nData analysis is supposed to be about the data, right? Just the facts? And for the most part it is, up until the point you need to communicate your findings to an audience. The problem is that in any data analysis that would be meaningful to others, there are simply too many results to present, and so choices must be made. Depending on who the audience is, or who the audience is composed of, you will need to tune your presentation in order to get the audience to accept the analysis. How is this done? Here are two extremes.\nIn the worst case scenario, it is done through trickery. Graphs with messed up axes, or tables that obscure key data; we all know the horror stories. A sophisticated audience might detect this kind of trickery and reject the analysis, but maybe not. That said, let’s assume we are pure of heart. How does one organize a presentation to be successful? We all know the other horror story, which is the data dump. Here, the analyst presents everything they have done and essentially shifts the burden of interpretation on to the audience. Rarely is this desired. In some cases the audience will just want the data to do their own analyses, but then there’s no need for the analyst to waste their time doing any analysis.\nUltimately, the analyst must choose what to present, and this can cause problems. The choices must be made to fit the analyst’s narrative of “what is going on with the data”. They will choose to include some plots and not others and some tables and not others. These choices are directed by a narrative and an interpretation of the data. When an audience is upset by a data analysis, and they are being honest, they are usually upset with the chosen narrative, not with the facts per se. They will be upset with the combination of data that the analyst chose to include and the data that the analyst chose to exclude. Why didn’t you include that data? Why is this narrative so focused on this or that aspect?\nThe Role of Creativity\nOn one extreme, it could be thought that a data analyst should be easily replaced by a machine: For various types of data and for various types of questions, there should be a deterministic approach to analysis that does not change. Presumably, this could be coded up into a computer program and the data could be fed into the program every time, with a result presented at the end. How is it that every data analysis is so different that a human being is needed to craft a solution? How can the words “creativity” and “data analysis” even appear in the same sentence?\nWell, it’s not true that every analysis is literally different. Many power calculations, for example, are identical. However, exactly how those power calculations are used can vary quite a bit from project to project. Even the very same calculation for the same study design can be interpreted differently in different projects. The same is true for other kinds of analyses like regression modeling or other more fancy modeling. The reason creativity is needed in data analysis has to do fundamentally with things that we might traditionally think are “outside” the data.\nThe audience is a key factor that is “outside the data” and influences how we analyze the data and present the results. One useful approach is to think about what final products need to be produced and then work backwards from there to produce the result. For example, if the “audience” is another algorithm or procedure, then the exact nature of the output may not be important as along as it can be appropriately fed into the next part of the pipeline. In particular, interpretability may not weigh that heavily because no person will be looking at the output of this part. However, if a person will be looking at the results, then you may want to focus on a modeling approach that lets that person reason about the data and understand how the data inform the results. For example, you might want to make more plots of the data, or show detailed tables if the dataset is not that large.\nIn one extreme case, if the audience is another data analyst, you may want to do a relatively “light” analysis, but then prepare the data in such a way that it can be easily distributed to others to do their own analysis. This could be in the form of an R package or a CSV file or something else. Other analysts may not care about your fancy visualizations or models; they’d rather have the data for themselves and make their own results.\nCreativity is needed in part because a data analyst must make a reasonable assessment of the audience’s needs, background, and preferences for receiving data analytic results. If the analyst has access to the audience, the analyst should ask questions about how best to present results. Otherwise, reasonable assumptions must be made or contingencies (e.g. backup slides, appendices) can be prepared for the presentation itself.\n“Inconsistent” Results\nMany times I’ve had the experience of giving the same presentation to two different audiences. One audience loves it while the other hates it. How can that be if the analyses and presentation were exactly the same in both cases? The truth is that an analysis can be accepted or rejected by different audiences depending on who they are and what their expectations are. A common scenario involves giving a presentation to “insiders” who are keenly familiar with the context and the standard practices in the field. Taking that presentation verbatim to an “outside” audience that is less familiar will often result in failure because they will not understand what is going on. If that outside audience expects a certain set of procedures be applied to the data, then they may demand that you do the same, and refuse to accept the analysis until you do so.\nI vividly remember one experience that I had presenting the analysis of some air pollution and health data that I had done. In practice talks with my own group everything had gone well and I thought things were reasonably complete. When giving the same talk to an outside group, they refused to accept what I’d done (or even interpret the results) until I had also run a separate analysis using a different kind of spline model. It wasn’t an unreasonable idea, so I did the separate analysis and in a future event with the same group I presented both analyses side by side. They were not wild about the conclusions, but the debate no longer centered on the analyses themselves and instead focused on other scientific aspects. In retrospect, I give them credit for accepting the analyses even if they did not necessarily believe the conclusion.\nSummary\nI think my proposed definition of a successful data analysis is challenging (and perhaps unsettling) because it suggests that data analysts are responsible for things outside the data. In particular, they need to understand the context around which the data are collected and the audience to which results will be presented. I also think that’s why it took so long for me to come around to it. But I think this definition explains much more clearly why it is so difficult to be a good data analyst. When we consider data analysis using traditional criteria developed by statisticians, we struggle to explain why some people are better data analysts than others and why some analyses are better than others. However, when we consider that data analysts have to juggle a variety of factors both internal and external to the data in order to achieve success, we see more clearly why this is such a difficult job and why good people are hard to come by.\nAnother implication of this definition of data analysis success is that it suggests that human nature plays a big role and that much of successful data analysis is essentially a successful negotiation of human relations. Good communication with an audience can often play a much bigger role in success than whether you used a linear model or quadratic model. Trust between an analyst and audience is critical when an analyst must make choices about what to present and what to omit. Admitting that human nature plays a role in data analysis success is difficult because humans are highly subjective, inconsistent, and difficult to quantify. However, I think doing so gives us a better understanding about how to judge the quality of data analyses and how to improve them in the future.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:31:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-04-02-input-on-the-draft-nih-strategic-plan-for-data-science/",
    "title": "Input on the Draft NIH Strategic Plan for Data Science",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2018-04-02",
    "categories": [],
    "contents": "\nThe NIH is soliciting input for their Strategic Plan for Data Science. If you are interested, today, April 2, is the deadline. You can provide input here. Below is what I plan to submit.\nSummary\nMy main critique is that the report is somewhat vague. More specifics and concrete examples should be included.\nMy main concern is that the draft describes initiatives with the goal of improving the back end of data science (data storage, data management, and computing infrastructure) without realizing that to do this one needs to understand the needs of those working on the front end of data science (data exploration, quality assessment, interactive data analysis, and method development).\nMy main recommendation is that the NIH Data Science leadership complement their current expertise with individuals experienced with the front end of data science. The ideal candidate has\nexperienced the pain of explaining to a collaborator how to prepare a spreadsheet that is appropriate for data analysis,\nperformed interactive data analysis in R, Python or Matlab,\ndeveloped a software package used by researchers that are not lab members or collaborators,\nhas downloaded, imported and visually explored a publicly available dataset, and (not or)\nhas fitted a model or trained a machine learning algorithm to data.\nThese individuals will have a very good sense of the specifics of what is required from a common architecture and infrastructure for it to facilitate successful data analysis and method development.\nBelow I include more detailed comments.\nThe perspective of the interactive data analysts\nThe draft defines Data Science as “the interdisciplinary field of inquiry in which quantitative and analytical approaches, processes, and systems are developed and used to extract knowledge and insights from increasingly large and/or complex sets of data”. Here I provide the perspective of a researcher that develops analytical approaches and performs, or supervises, the primary data analysis of a collaborative project interested in “[extracting] knowledge and insights from increasingly large and/or complex sets of data”. Note that I do not consider myself to be unique in this regard. Rather, I am part of a large community of statisticians and computational biologists with many commonalities in the way we work, the way we share the products of our work, and the way we depend and interact with the NIH.\nIn genomics, NIH already does a great job of supporting data science\nBy funding numerous investigator initiated grants, the NIH has fostered a competitive environment that has served as a driving force for innovation and resulted in an impressive arsenal of data analysis and data processing methods. The NIH has also provided funding for the maintenance and improvement of software implementation of these ideas. My work, and the work of many others in my community, depend on these NIH funded tools and infrastructure. Examples include Bioconductor and its many packages, Bedtools, SAMtools, Bowtie, Recount, and IGV, just to name a few. We also depend on NIH funded and organized resources such as the Short Read Archive (SRA), the Gene Expression Omnibus (GEO), and RefSeq as well as NIH datasets such as those produced by GTEX, TCGA, and ENCODE, again, just to name a few.\nWhat can improve?\nThe main pain points in my career have been\nThe sub-optimal sample annotations provided by GEO and SRA.\nThe bureaucratic burden imposed by dbGap.\nThat lack of access to paired GWAS genotype and phenotype data, electronic medical record data, cohort studies, and clinical trials data.\nThe fact that often (not always!) data generators lack computational sophistication and basic statistical knowledge.\nMy favorite parts of the draft\nI was happy to see that the draft includes a proposal to separate funding mechanisms for data generation from the development of data analysis tools. This will indeed be a great improvement as it will give more independent groups with expertise in this arena an opportunity to compete. Currently too much of the data analysis funding goes to data generators which tend to use these to fund inexperienced trainees. This is particularly true of the large consortia.\nI was also happy to see that the draft includes a proposal to better support current successful developers to “transform, or harden, innovative algorithms and tools created by academic scientists into enterprise-ready resources that meet industry standards of ease of use and efficiency of operation.”\nI was most happy about the proposal to increase support for training and education.\nAvoid top-down approaches when deciding on a “common architecture, infrastructure, and set of tools”\nIn my view, the two main reasons the current ecosystem has been successful at producing data analysis tools are:\nMany groups are funded and permitted to compete for users of the data analysis workflows they produce. Which workflows are best is decided by an unplanned process similar to a free market that includes independent assessment, productive debates, and sharing of experiences. Satisfied users translate into increased citations and gains in reputation, which in turn improve ones chances of promotion and funding.\nData is made publicly available as they come off the measurement instruments, what we refer to as raw data. In my experience, when a bureaucratic process is used to decide data formats, important information is often left out.\nMy nightmare scenario is that the NIH stops making the raw data available and computationally accessible (say via secure copy or wget) and that instead we are forced to log on to a server on “the cloud” and access only processed data.\nWe already follow FAIR principles\nBy using existing open source tools many of us are already following the Findable, Accessible, Interoperable, and Reusable (FAIR) principles. Achieving this took us years and we built on the shoulder of open source giants. We build these as a solution to make the way we analyze data for a variety of diverse projects more efficient. It was not developed before the front end data analytical challenges were understood since it would have been impossible to predict what would be appropriate.\nMy institute provides enough computing and data storage resource to download raw data when needed. We also store data from our collaborators. Using a Linux file system and best practices for file management we organize the raw data in a way that is shareable and findable among those with access to our system. File system permissions are used to assure data is only accessed with those with permission to access it. We use UNIX shell commands or Python to organize workflows based on open source command-line tools, Bowtie for example, to convert raw data into formats we have determined to be sufficient for front end analysis. These workflows are constantly changing, but the shell or Python scripts permit reproducibility and back compatibility. Interactive data analysis is done with open source data analysis software such as R and Python. Tools such as Biocondoctor implement object oriented programming to ensure interoperability and reusability across data from different technologies. R packages from the tidyverse also make use of interoperability and reusability concepts for downstream analysis and the generation of figures. We use git for version control and GitHub to share code publicly.\nThe NIH should not assume that they can do better by designing a new approach from first principles and making decisions by committee. One really needs to walk in the shoes of a data analysts to do this right. I understand that the above approach will not work for everybody since it requires much institutional support as well as expertise. But we do recommend against top-down approaches to solving this problem. We recommend that that independent groups compete for funding and once funded compete for users.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:31:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-02-08-what-do-fahrenheit-comma-separated-files-and-markdown-have-in-common/",
    "title": "What do Fahrenheit, comma separated files, and markdown have in common? ",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2018-02-08",
    "categories": [],
    "contents": "\nAnil Dash asked people what their favorite file format was. David Robinson replied:\n{{% tweet \"961610642083012608\" %}}\nHis tweet reminded me a lot of this tweet from Stephen Turner\n{{% tweet \"568746345919459328\" %}}\nThere is a spectrum for tools from the theortically optimal to the most human usable. It is almost always a tradeoff between purity/completeness and ease of human use.\nFor data science tools/statistical methods development there is a ton of work that needs to be done to govern edge cases. That work can either be done by the human users of the software/tool/data or it can be done on the back end by the developers of tools. Once you start looking for this tradeoff you see it everywhere - one of my favorite recent examples is flexdashboard versus Shiny. Shiny is llows for a lot of flexibility, deals directly with edge cases, and asks the user to pay attention to those cases. Flexdashboard “just works” in the sense that for most of the stuff I’m doing it seems to almost intuit what a person with minimal training would do.\nI think both kinds of approaches have their place. Most of the time I just want to use the thing that reads my mind and takes minimal intellectual overhead. Same thing for when training people new to data science (or anything else). But eventually I start running into too many edge cases and want to move to something more sophisticated.\nOne thing I have a really hard time with is the transition from the “just works for humans” to the “theoretically optimal”. Almost always that transition is hard/rough. I wish there was a way (both selfishly and as an instructor) to smooth the transition from the intuitive, easy tool to the more flexible but necessarily more complicated tool. That always feels like a cliff to me.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:31:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-01-22-the-dslabs-package-provides-datasets-for-teaching-data-science/",
    "title": "Some datasets for teaching data science",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2018-01-22",
    "categories": [],
    "contents": "\nIn this post I describe the dslabs package, which contains some datasets that I use in my data science courses.\nA much discussed topic in stats education is that computing should play a more prominent role in the curriculum. I strongly agree, but I think the main improvement will come from bringing applications to the forefront and mimicking, as best as possible, the challenges applied statisticians face in real life. I therefore try to avoid using widely used toy examples, such as the mtcars dataset, when I teach data science. However, my experience has been that finding examples that are both realistic, interesting, and appropriate for beginners is not easy. After a few years of teaching I have collected a few datasets that I think fit this criteria. To facilitate their use in introductory classes, I include them in the dslabs package:\n\n\ninstall.packages(\"dslabs\")\n\n\nBelow I show some example of how you can use these datasets. You can see the datasets that are included here:\n\n\nlibrary(\"dslabs\")\ndata(package=\"dslabs\")\n\n\nNote that the package also includes some of the scripts used to wrangle the data from their original source:\n\n\nlist.files(system.file(\"script\", package = \"dslabs\"))\n\n [1] \"make-admissions.R\"                   \n [2] \"make-brca.R\"                         \n [3] \"make-brexit_polls.R\"                 \n [4] \"make-death_prob.R\"                   \n [5] \"make-divorce_margarine.R\"            \n [6] \"make-gapminder-rdas.R\"               \n [7] \"make-greenhouse_gases.R\"             \n [8] \"make-historic_co2.R\"                 \n [9] \"make-mnist_27.R\"                     \n[10] \"make-movielens.R\"                    \n[11] \"make-murders-rda.R\"                  \n[12] \"make-na_example-rda.R\"               \n[13] \"make-nyc_regents_scores.R\"           \n[14] \"make-olive.R\"                        \n[15] \"make-outlier_example.R\"              \n[16] \"make-polls_2008.R\"                   \n[17] \"make-polls_us_election_2016.R\"       \n[18] \"make-reported_heights-rda.R\"         \n[19] \"make-research_funding_rates.R\"       \n[20] \"make-stars.R\"                        \n[21] \"make-temp_carbon.R\"                  \n[22] \"make-tissue-gene-expression.R\"       \n[23] \"make-trump_tweets.R\"                 \n[24] \"make-weekly_us_contagious_diseases.R\"\n[25] \"save-gapminder-example-csv.R\"        \n\nIf you want to learn more about how we use these datasets in class, you can read this paper or this online book.\nUS murders\nThis dataset includes gun murder data for US states in 2012. I use this dataset to introduce the basics of R program.\n\n\ndata(\"murders\")\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(ggrepel)\n\nr <- murders %>%\n  summarize(pop=sum(population), tot=sum(total)) %>%\n  mutate(rate = tot/pop*10^6) %>% .$rate\n\nds_theme_set()\nmurders %>% ggplot(aes(x = population/10^6, y = total, label = abb)) +\n  geom_abline(intercept = log10(r), lty=2, col=\"darkgrey\") +\n  geom_point(aes(color=region), size = 3) +\n  geom_text_repel() +\n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Populations in millions (log scale)\") +\n  ylab(\"Total number of murders (log scale)\") +\n  ggtitle(\"US Gun Murders in 2010\") +\n  scale_color_discrete(name=\"Region\") \n\n\n\nGapminder\nThis dataset includes health and income outcomes for 184 countries from 1960 to 2016. It also includes two character vectors, OECD and OPEC, with the names of OECD and OPEC countries from 2016. I use this dataset to teach data visualization and ggplot2.\n\n\ndata(\"gapminder\")\n\nwest <- c(\"Western Europe\",\"Northern Europe\",\"Southern Europe\",\n          \"Northern America\",\"Australia and New Zealand\")\n\ngapminder <- gapminder %>%\n  mutate(group = case_when(\n    region %in% west ~ \"The West\",\n    region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n    region %in% c(\"Caribbean\", \"Central America\", \"South America\") ~ \"Latin America\",\n    continent == \"Africa\" & region != \"Northern Africa\" ~ \"Sub-Saharan Africa\",\n    TRUE ~ \"Others\"))\ngapminder <- gapminder %>%\n  mutate(group = factor(group, levels = rev(c(\"Others\", \"Latin America\", \"East Asia\",\"Sub-Saharan Africa\", \"The West\"))))\n\nfilter(gapminder, year%in%c(1962, 2013) & !is.na(group) &\n         !is.na(fertility) & !is.na(life_expectancy)) %>%\n  mutate(population_in_millions = population/10^6) %>%\n  ggplot( aes(fertility, y=life_expectancy, col = group, size = population_in_millions)) +\n  geom_point(alpha = 0.8) +\n  guides(size=FALSE) +\n  theme(plot.title = element_blank(), legend.title = element_blank()) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"Fertility rate (births per woman)\") +\n  ylab(\"Life Expectancy\") +\n  geom_text(aes(x=7, y=82, label=year), cex=12, color=\"grey\") +\n  facet_grid(. ~ year) +\n  theme(strip.background = element_blank(),\n        strip.text.x = element_blank(),\n        strip.text.y = element_blank(),\n   legend.position = \"top\")\n\n\n\nContagious disease data for US states\nThis dataset contains yearly counts for Hepatitis A, measles, mumps, pertussis, polio, rubella, and smallpox for US states. Original data courtesy of Tycho Project. I use it to show ways one can plot more than 2 dimensions.\n\n\nlibrary(RColorBrewer)\ndata(\"us_contagious_diseases\")\nthe_disease <- \"Measles\"\nus_contagious_diseases %>%\n  filter(!state%in%c(\"Hawaii\",\"Alaska\") & disease ==  the_disease) %>%\n  mutate(rate = count / population * 10000 * 52 / weeks_reporting) %>%\n  mutate(state = reorder(state, rate)) %>%\n  ggplot(aes(year, state,  fill = rate)) +\n  geom_tile(color = \"grey50\") +\n  scale_x_continuous(expand=c(0,0)) +\n  scale_fill_gradientn(colors = brewer.pal(9, \"Reds\"), trans = \"sqrt\") +\n  geom_vline(xintercept=1963, col = \"blue\") +\n  theme_minimal() +  theme(panel.grid = element_blank()) +\n  ggtitle(the_disease) +\n  ylab(\"\") +\n  xlab(\"\")\n\n\n\nFivethirtyeight 2016 Poll Data\nThis data includes poll results from the US 2016 presidential elections aggregated from HuffPost Pollster, RealClearPolitics, polling firms and news reports. The dataset also includes election results (popular vote) and electoral college votes in results_us_election_2016. I use this dataset to teach inference.\n\n\ndata(polls_us_election_2016)\npolls_us_election_2016 %>%\n  filter(state == \"U.S.\" & enddate>=\"2016-07-01\") %>%\n  select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %>%\n  rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %>%\n  gather(candidate, percentage, -enddate, -pollster) %>% \n  mutate(candidate = factor(candidate, levels = c(\"Trump\",\"Clinton\")))%>%\n  group_by(pollster) %>%\n  filter(n()>=10) %>%\n  ungroup() %>%\n  ggplot(aes(enddate, percentage, color = candidate)) +  \n  geom_point(show.legend = FALSE, alpha=0.4)  + \n  geom_smooth(method = \"loess\", span = 0.15) +\n  scale_y_continuous(limits = c(30,50))\n\n\n\nStudent reported heights\nThese are self-reported heights in inches for males and females from data science course across several years. I use this to teach distributions and summary statistics.\n\n\ndata(\"heights\")\nheights %>% \n  ggplot(aes(height, fill=sex)) + \n  geom_density(alpha = 0.2)\n\n\n\nThese data have been highly wrangled as students would often reported heights in values other than inches. The original entries are here:\n\n\ndata(\"reported_heights\")\nreported_heights %>% filter(is.na(as.numeric(height))) %>% select(height) %>% .$height \n\n [1] \"5' 4\\\"\"                 \"165cm\"                 \n [3] \"5'7\"                    \">9000\"                 \n [5] \"5'7\\\"\"                  \"5'3\\\"\"                 \n [7] \"5 feet and 8.11 inches\" \"5'11\"                  \n [9] \"5'9''\"                  \"5'10''\"                \n[11] \"5,3\"                    \"6'\"                    \n[13] \"6,8\"                    \"5' 10\"                 \n[15] \"Five foot eight inches\" \"5'5\\\"\"                 \n[17] \"5'2\\\"\"                  \"5,4\"                   \n[19] \"5'3\"                    \"5'10''\"                \n[21] \"5'3''\"                  \"5'7''\"                 \n[23] \"5'12\"                   \"2'33\"                  \n[25] \"5'11\"                   \"5'3\\\"\"                 \n[27] \"5,8\"                    \"5'6''\"                 \n[29] \"5'4\"                    \"1,70\"                  \n[31] \"5'7.5''\"                \"5'7.5''\"               \n[33] \"5'2\\\"\"                  \"5' 7.78\\\"\"             \n[35] \"yyy\"                    \"5'5\"                   \n[37] \"5'8\"                    \"5'6\"                   \n[39] \"5 feet 7inches\"         \"6*12\"                  \n[41] \"5 .11\"                  \"5 11\"                  \n[43] \"5'4\"                    \"5'8\\\"\"                 \n[45] \"5'5\"                    \"5'7\"                   \n[47] \"5'6\"                    \"5'11\\\"\"                \n[49] \"5'7\\\"\"                  \"5'7\"                   \n[51] \"5'8\"                    \"5' 11\\\"\"               \n[53] \"6'1\\\"\"                  \"69\\\"\"                  \n[55] \"5' 7\\\"\"                 \"5'10''\"                \n[57] \"5'10\"                   \"5'10\"                  \n[59] \"5ft 9 inches\"           \"5 ft 9 inches\"         \n[61] \"5'2\"                    \"5'11\"                  \n[63] \"5'11''\"                 \"5'8\\\"\"                 \n[65] \"708,661\"                \"5 feet 6 inches\"       \n[67] \"5'10''\"                 \"5'8\"                   \n[69] \"6'3\\\"\"                  \"649,606\"               \n[71] \"728,346\"                \"6 04\"                  \n[73] \"5'9\"                    \"5'5''\"                 \n[75] \"5'7\\\"\"                  \"6'4\\\"\"                 \n[77] \"5'4\"                    \"170 cm\"                \n[79] \"7,283,465\"              \"5'6\"                   \n[81] \"5'6\"                   \n\nWe use this as an example to teach string processing and regex.\nMargarine and divorce rate\nFinally, here is a silly example from the website Spurious Correlations that I use when teaching correlation does not imply causation.\n\n\nthe_title <- paste(\"Correlation =\",\n                round(with(divorce_margarine,\n                           cor(margarine_consumption_per_capita, divorce_rate_maine)),2))\ndata(divorce_margarine)\ndivorce_margarine %>%\n  ggplot(aes(margarine_consumption_per_capita, divorce_rate_maine)) +\n  geom_point(cex=3) +\n  geom_smooth(method = \"lm\") +\n  ggtitle(the_title) +\n  xlab(\"Margarine Consumption per Capita (lbs)\") +\n  ylab(\"Divorce rate in Maine (per 1000)\")\n\n\n\n\n\n\n",
    "preview": "posts/2018-01-22-the-dslabs-package-provides-datasets-for-teaching-data-science/index_files/figure-html5/us-murders-1.png",
    "last_modified": "2022-12-27T15:39:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-12-20-a-non-comprehensive-list-of-awesome-things-other-people-did-in-2017/",
    "title": "A non-comprehensive list of awesome things other people did in 2017",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2017-12-20",
    "categories": [],
    "contents": "\nEditor’s note: For the last few years I have made a list of awesome things that other people did (2016,2015, 2014, 2013). Like in previous years I’m making a list, again right off the top of my head. If you know of some, you should make your own list or add it to the comments! I have also avoided talking about stuff I worked on or that people here at Hopkins are doing because this post is supposed to be about other people’s awesome stuff. I write this post because a blog often feels like a place to complain, but we started Simply Stats as a place to be pumped up about the stuff people were doing with data.\nAmelia McNamara and Aran Lunzer made this crazy nice illustration of how histograms work.\nI am just continuously impressed by the work coming out of Rahul Satija’s group - this year it was this paper on aligning cell populations in single cell data.\nKarl Broman and Kara Woo wrote the definitive guide to data in spreadsheets with the winner for most dense delivery of valuable information in a sentence in the last sentence of the abstract.\nThe rest of the papers in the Peerj data science collection are also awesome.\nThomas Lin Pederson came out with the patchwork package for composing plots. This is the plot organization software I’ve always wanted :).\nGetting Avericked became a word as Mara Averick somehow makes everything R related look so cool.\nThe folks at Fast.AI do so many impressive things (they are MOOC geniuses) its hard to list just one to list, but this post on data ethics might be my favorite.\nKristian Lum wrote the most important blog post in statistics and jump-started the discussion throughout our field on the standard of behavior we want to hold ourselves to as a field.\nHadley Wickham is the LeBron James of data science - so effortlessly great it is almost expected. usethis is yet another example of how he’s revolutionizing package development (again).\nI looooovveeddd this post from Julia Silge on how to do word2vec like things with tidy data. Such a beautiful distillation of something complicated into something we all can do.\nI only bought one stats book all year. It was David Robinson’s Empirical Bayes Book. An insta-classic.\nI’ve been following Nick Strayer’s dive into deep learning all year on Twitter, this post on LSTM with a baseball analogy is a good example of the awesomeness.\nJeff Kao had a tour de force post on identifying fake comments on the net neutrality repeal. Such an impressively timed and high impact piece of forensic data science.\nMike Love released his amazing lecture notes for his intro to computational Biology class. I want to take a class from Mike.\nThe keras R package is sooooo nice. I love everything JJ Allaire does - whenever he writes software it always seems like he’s read my mind and the way I’d want the software to work it just does.\nJesse Meagan’s post on educational data wrangling was realy close to my heart. Plus it is so awesome to read about her (impressive) transition from Excel to R. Something I’m thinking about a ton these days.\nI’ve been gobbling up Cristoph Molnar’s book on interpretable machine learning.\nThe book on tidytextmining came out this year. What a tour-de-force from Julia Silge and David Robinson.\nI read a lot of FastForward blog posts this year - one of my favorites was this one on auditing machine learning models - such an important topic.\nI love this post on the ten fallacies of data science - my favorite one? “The data exist”.\nI basically read every one of Maëlle Salmon’s blog post but I found her analysis of seeds that people use in R to be fascinating. I’m looking forward to more on the epidemiology of code :).\nI dug Emily Robinson’s post on making R code faster this is something I’m adding to every class I teach in the future - so nice to see all the steps laid out so clearly!\nNatalie Telis did one of the most impressive crowd sourced studies of who is asking questions at the major genetics conference. I think solutions like this are so clever when they are low cost/low overhead but produce super interesting results.\nI liked Stephen Turner’s updated post on how to stay current in Bioinformatics. Keeping up is super hard to do well - so this is a big service for the community.\nIts hard to point to just one thing Karthik Ram does for the R community in any given year, but I was pumped to see this call for rOpenSci fellows. I know I’m going to hit peak FOMO whenever Karthik is organizing.\nMy main goal as an advisor is to be like Marta Kutas- “I don’t care what they become,” she says of her students, “so long as it’s decent, thinking human beings!”\nI’m such a fan of Rob Hyndman’s forcasting work, and was pumped to see him on Datacamp. The data camp folks have been just vacuming up awesome people to teach courses.\nAndrew Bream did an amazing, polite, and thorough job of setting me straight on deep learning.\nThe UpSetR package was a heartwarming contribution for those of us who are terrified of out of control Venn diagrams. * I think Florian Markowetz consistently is one of the most thoughtful computational scientists. I read his stuff relentelessly but loved his idea for a pre-registration of the Human Cell Atlas.\nIt was the year of datasaurus! I am in the process of replacing all my Anscomb quartet lecture notes. What a cool idea by Justin Matejka and George Fitzmaurice.\nThere are times when I wonder whether Emma Pierson has created multiple AI copies of herself. How can one person write so, many, awesome, things in one year?\nBen Haibe-Kains and colleagues put together a tour-de-force in reproducibility with PharmacoDB.\nThere is often talk of how academic statistics isn’t supportive enough of non-standard (read: non-math) contributions. There is definitely truth there, but I love Lance Waller on navigating these non-standard paths in academics.\nI am so amped about the skimr package, especially the histograms as part of the summary statistics.\nChristie Aschwanden added another awesome piece to her collection of the most level-headed, responsible, and thoughtful discusison of the statistical issues in science.\nThat’s all I have for the moment. I’m sure I missed a ton and I’m sorry if I missed you - please add more in the comments I always love to see cool stuff!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:30:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-12-20-thoughts-on-david-donoho-s-fifty-years-of-data-science/",
    "title": "Thoughts on David Donoho’s \"Fifty Years of Data Science\"",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-12-20",
    "categories": [],
    "contents": "\nNote: This post was originally published as part of a collection of discussion pieces on David Donoho’s paper. The original paper and collection of discussions can be found at the JCGS web site.\nProfessor Donoho’s commentary comes at a perfect time, given that, according to his own chronology, we are just about due for another push to “widen the tent” of statistics to include a broader array of activities. Looking back at the efforts of people like Tukey, Cleveland, and Chambers to broaden the meaning of statistics, I would argue that to some extent their efforts have failed. If you look at a textbook for a course in a typical PhD program in statistics today, I believe it would look much like the textbooks used by Cleveland, Chambers, and Tukey in their own studies. In fact, it might even be the same textbook! Progress has been slow, in my opinion. But why is that? Why can’t statistics grow to more fully embrace the many activities that Professor Donoho describes in his six divisions of “Greater Data Science”?\nOne frustration that I think many statisticians have in discussions of data science is that if you look at each of the six divisions that Donoho lays out—data exploration, data transformation, computing, modeling, visualization, and science of data science—statisticians do all those things. But the truth is, we do not teach most of them. Over the years, the teaching of statistics has expanded to include topics like computing and visualization, but typically as optional or ancillary courses. Many of the activities on Donoho’s list are things that students are assumed to “figure out on their own” without any formal instruction. Asymptotic theory, on the other hand, requires formal instruction.\nIn my own experience, the biggest challenge to teaching the areas of Greater Data Science is that it is difficult and can be very inefficient. Ultimately, I believe these are the reasons that we as a field choose not to teach this material. Many of the areas in the six divisions, data exploration and transformation, can be frustratingly difficult to generalize. If I clean up administrative claims data from Medicare and link them to air pollution data from the Environmental Protection Agency, does any of the knowledge I gain from those activities apply to the processing of RNA-seq data and linking it with clinical phenotypes? It’s difficult to see any connection. On the other hand, both datasets will likely serve as inputs to a generalized linear model. Rather than teach one course on cleaning administrative claims data and another course on processing RNA-seq data, consider how many birds can be hit with the three stones of an exponential family, a link function, and a linear predictor? Furthermore, the behavior of generalized linear models can be analyzed mathematically to make incredibly useful predictions about the variability of their estimates.\nThe lack of a formal framework for “data cleaning” reduces the teaching of the subject to a parade of special cases. While each case might be of interest to some, it’s unlikely that any case would be applicable to all. In any institution of higher learning with finite resources, it’s impossible to provide formal instruction on all the special cases to everybody who needs them. It’s much more efficient to teach the generalized linear model and the central limit theorem.\nIs the lack of a formal framework for some areas of data science attributable to some fundamental aspect of those topics, or does it arise simply from a lack of trying? In my opinion, the evidence to date lays the blame on our field’s traditional bias towards to use of mathematics as the principal tool for analysis. Indeed, much of the interesting formal work being done in data cleaning and transformation makes use of a completely different toolbox, one largely drawing from computer science and software engineering. Because of this different toolbox, our field has been blinded to recent developments and has missed an important opportunity to cultivate more academic leaders in this area.\nProfessor Donoho rightly highlights the work of Hadley Wickham and Yihui Xie, both statisticians, who have made seminal contributions to the field of statistics in their development of the ggplot2, knitr, dplyr, and many other packages for R. It is notable that Wickham’s paper outlining the concept of “tidy data”, a concept which has sparked a minor revolution in the field of data analysis, was originally published in the Journal of Statistical Software, a nominally “applied” journal. I would argue that such a paper more properly belongs in the Annals of Statistics than in a software journal. The formal framework offered in that paper has inspired the creation of numerous “tidy data” approaches to analyzing data that have proven remarkably simple to use and understand. The “grammar” outlined in Wickham’s paper and implemented in the dplyr package serve as an abstraction whose usefulness has been demonstrated in a variety of situations. The lack of mathematical notation in the presentation of dplyr or any of its “tidyverse” relatives does not make it less useful nor does it make it less broadly applicable.\nFinally, it is worth a comment that the people that Professor Donoho cites as driving previous pushes to widen the tent of statistics either did not initially come from academia or at least straddled the boundary. Cleveland, Chambers, and Tukey all spent significant time in industry and government settings and that experience no doubt colored their perspectives. Moving to today, it might just be a coincidence that both Wickham and Xie are employed outside of academia by RStudio, but I doubt it. Perhaps it is always the case that the experts come from somewhere else. However, I fear that academic statistics will miss out on the opportunity to recruit bright people who are making contributions in a manner not familiar to many of us.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:31:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-12-19-how-do-machines-learn/",
    "title": "How Do Machines Learn?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-12-19",
    "categories": [],
    "contents": "\nI like all of CGP Grey’s videos but most of them have to do with voting systems and so aren’t really relevant to this blog. But his latest video titled “How Do Machines Learn?” is highly relevant and I thought very well done. That said, although the animations of the robots were very cute and helped to tell the story, I found them a bit disconcerting in a way that I can’t quite explain.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:30:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-12-19-puerto-rico-s-governor-wants-recount-of-hurricane-death-toll/",
    "title": "Puerto Rico's governor wants recount of hurricane death toll",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-12-19",
    "categories": [],
    "contents": "\nA quick followup to Rafa’s analysis of the death toll from Hurricane Maria, from Axios:\n\nPuerto Rican Governor Ricardo Rosselló ordered a recount Monday of every death on the island since Hurricane Maria made landfall on September 20, as evidence continues to show that the official death toll grossly undercuts the true number, reports the New York Times.\n\nThere are at least two ways to do this. One way is inferential in nature, taking a look at what we might expect the mortality to be and looking at what was observed. This is tricky due to a variety of potential confounding factors and also comes with it statistical uncertainty. It seems the Governer is going to take a more “census-like” approach:\n\nThe recount will require interviewing doctors and family members of the dead to learn whether their cause of death could have been linked to the fallout from the storm. For example, a heart attack may have been brought on by the stress of the hurricane, or roads leading to the hospital may have been blocked by debris.\n\nThis approach does not have statistical uncertainty but has uncertainty of a different kind—separating proximate cause of death from ultimate cause of death. It’s going to be difficult either way.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:30:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-12-18-data-analysis-and-engagement-does-caring-matter/",
    "title": "Data Analysis and Engagement - Does Caring About the Analysis Matter?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-12-18",
    "categories": [],
    "contents": "\nSometimes, when you’re recording a podcast, it’s actually difficult to listen. That’s because while you’re recording you’re monitoring the network lag, the sound levels, the show notes, and the outline. On some episodes I’m just barely hanging on by a thread.\nWhile Hilary Parker and I were recording Episode 50 of Not So Standard Deviations we had a discussion about her experience doing A/B testing at Etsy and how one experiment, which involved showing customers their passwords as they typed them, resulted in an increase in the number of failed login attempts, which was not what they were expecting. In the episode, we discuss how the problem was discovered and resolved and concluded that part of solving the problem involved a familiarity with how web forms can work.\n\nRoger: It feels like your intuition and experience using the web, and apps, and—it’s all coming together, right?\n\n\nHilary: Exactly…which is part of why…it’s so important for data scientists to have genuine interest in the products they’re working on.\n\nThe idea here is that in this particular problem, an analyst’s experience with web forms played a significant role in interpreting and analyzing the data from the A/B test. They were then able to make modifications to the test and run it again. One thing that is interesting about this story is that understanding how web forms work has nothing to do with the data. It’s what you might think of as “prior knowledge”. In an alternative universe, one could imagine looking at the data, seeing that login failures were increasing, and then just nixing the feature, concluding that the “data had spoken”.\nHilary’s comment passed me by at the time, but now that I’ve had some time to consider it, I think if it’s true, it has profound implications for the field of data science. Perhaps another way to frame it is to ask the question: Does the quality of your data analysis depend on how much you care about the problem? It’s maybe not the best question because I think the answer can be both yes and no, depending on the problem.\nHowever, I think there are multi-billion dollar unicorn startups whose future depends critically on the answer being “No”. I wrote a little about this in regards to Palantir. At the time, I wrote that Palantir was bouncing along a spectrum with boring old consulting company on one end and cool highly scalable software company on the other end. In particular, it appeared that their expertise in some areas wasn’t translating well to other areas:\n\nit’s clear that Coke didn’t feel comfortable collaborating with Palantir’s personnel. Like any data science collaboration, it’s key that the data scientist have some familiarity with the domain. In many cases, having “deep expertise” in an area can give a collaborator confidence that you will focus on the things that matter to them. But developing that expertise costs money and time and it may prevent you from working with other types of clients where you will necessarily have less expertise. For example, Palantir’s long experience working with the US military and intelligence agencies gave them deep expertise in those areas, but how does that help them with a consumer products company?\n\nIf Palantir couldn’t translate their success into multiple areas, how could it justify it’s multi-billion dollar valuation?\nNot having to care about the ins and outs of a given area of data analysis means that there are dramatically fewer costs involved with getting into the area. We don’t need to invest time understanding whatever products may be involved and understanding why other people like them and how other poeple use them. Ultimately, I think the need to care about an area or product when doing data analysis negatively affects the profit potential of that area.\nDoes Caring Matter?Whether caring matters for data analysis also has implications for how to build a data analytic team. If you need your data analyst to be 100% committed to a product and to be fully invested, it’s difficult to achieve that with contractors or consultants, who are typically working on multiple projects with multiple companies. Some data analysis work is arguable generic, and thus can be done without emotional investment. But separating out the parts that “require caring” and those that don’t is arguably an important part of a data science manager’s job.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:30:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-12-15-this-is-a-brave-post-and-everyone-in-statistics-should-read-it/",
    "title": "This is a brave post and everyone in statistics should read it",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2017-12-15",
    "categories": [],
    "contents": "\nThis post by Kristian Lum is incredibly brave. It points out some awful behavior by people in our field and should be required reading for everyone.\nIt took a lot of courage for Kristian to post this but we believe her, think this is a serious and critical issue for our field, and will not tolerate this kind of behavior among our colleagues.\nHer post has aleady inspired important discussions among the faculty at Johns Hopkins Biostatistics and is an important contribution to making sure our field is welcoming for everyone.\nLike many others we’ll be waiting to see the response of the ISBA. The ASA is also creating a task force on sexual harrassment. We’ll be looking for more details on how to get involved in these efforts as we all think about what we can do about this critical issue.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:30:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-12-03-data-do-not-agree-with-hurricane-official-death-count/",
    "title": "Hurricane María official death count in conflict with mortality data",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2017-12-03",
    "categories": [],
    "contents": "\nA recent preprint by Alexis R. Santos-Lozada and Jeffrey T. Howard concludes that\n\n\nThe mortality burden may [be] higher than official counts, and may exceed the current official death toll by a factor of 10.\n\n\nThe authors used monthly death records from the Puerto Rico Vital Statistics system from 2010 to 2016. Although data for 2017 was apparently not available, they extracted data from a statement made by Héctor Pesquera, the Secretary of Public Safety:\n\n\nThe number of deaths for September 2017 is 2,838, with 95% of the deaths processed by the Puerto Rico Department of Health.”\n\n\nTheir final conclusions rely on assumptions and methodology needed to predict October figures. But just by looking at the data, we can see that the official figure of 55 deaths appears to be way off.\n\n\n\n\n\n\n\n\n\nTo create this plot, I downloaded the Microsoft Word version of the preprint, converted it to PDF, then scraped the data from Table 1. Because there is month-to-month variability in total deaths in Puerto Rico, I computed the difference between each data point and the average for their respective month.\nThe September 2017 data point is a clear outlier, 455 deaths above average, and is well beyond 55 deaths above the largest deviation from the monthly average. Keep in mind that Hurricane María hit Puerto Rico on September 20th, so only 10 days account for the observed difference. The official figure includes September and October so it covers at least 40 days.\nBelow is a plot of the total deaths, which the preprint shows in Figure 2.\n\n\n\nNote that 55 was the official figure at the time the preprint was written.\n\n\n\n",
    "preview": "posts/2017-12-03-data-do-not-agree-with-hurricane-official-death-count/index_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-12-27T15:39:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-11-27-some-roadblocks-to-the-broad-adoption-of-machine-learning-and-ai/",
    "title": "Some roadblocks to the broad adoption of machine learning and AI ",
    "description": {},
    "author": [
      {
        "name": "",
        "url": {}
      }
    ],
    "date": "2017-11-27",
    "categories": [],
    "contents": "\nI read two blog posts on AI over the Thanksgiving break. One was a nice post discussing the challenges for AI in medicine by Luke Oakden-Rayder and the other was about the need for increased focus on basic research in AI motivated by AlphaGo by Tim Harford.\nI’ve had a lot of interactions with people lately who want to take advantage of machine learning/AI in their research or business. Despite the excitement around AI and the exciting results we see from sophisticated research teams almost daily - the actual extent and application of AI is much smaller. In fact, most AI usually ends up being humans in the end.\nWhile the promise of AI/ML has never been clearer, there are still only a handful of organizations that are using the technology in a major way. Sometimes even apparent success stories turn out to be problematic.\nI was thinking about the lifecycle of developing an AI application. I have defined this type of application previously as having three parts: (i) an interface to humans, (ii) a data set, and (iii) an algorithm for turning the data into interactions. I started thinking about the extension of this idea to the development of an AI application and all the steps involved. Then I started thinking about potential barriers.\n\nTo develop an AI application you need a few things:\nA group of people who are willing to let you have their data.\nA technology for data capture from people (this could be as simple as a website, or an Echo, or as complex as a robot).\nA data storage mechanism for collecting the raw data from this input (this could just be a database)\nA set of algorithms and scripts for organizing the data for analysis.\nA definition of the problem you’d like to solve in quantitative terms - usually generated through exploratory analysis.\nAn algorithm trained on a massive data set or at minimum trained with a good prior or expert knowledge.\nA way to structure the responses and provide feedback either to the original users of your application or to other users (researchers or executives at a company for example).\nThe pipeline to take those formatted responses and return them to the user in a way that they can take advantage of.\nI think that a lot of attention is focused on step 6 and how costly talent is for designing AI algorithms. I think for the big players where a lot of the other steps have been solved this is for sure the limiting factor and it is no wonder that the talent war is fierce.\nBut I think that for 95% of organizations - whether they be researchers, businesses, or individuals the problem isn’t in developing the algorithm. A random forest can be fit with one line of R code and while it won’t be as accurate as an expertly trained neural network on a gigantic training data set, it will be really useful.\nSo I think that most of the roadblocks to the democratization of AI are actually in the other steps and in particular the “glue” between the steps. For example:\nGetting access to people’s data (Step 0) - It is very hard, even for researchers, to get access to health information if you aren’t DeepMind and Google. It can take years to deal with the bureaucracy of getting access to even simple data sets.\nHaving the infrastructure to capture data (Step 1) if you aren’t a major player you might not even be capturing complete data from people visiting your website, let alone sensors, images, text, and everything else you would want to do to perform AI.\nStoring data centrally (Step 2) In almost all of the organizations I’ve talked to data are scattered and managed across multiple systems and with different protocols. Just knowing where and what the data are can be a multi-month process.\nTidying the data (Step 3) There is an entire industry of data scientists built to tackle this problem, but if you can’t find the data or if the data isn’t stored centrally (Step 2), then this can be delayed. Even if it isn’t, there is rarely a standardized data tidying pipeline even in places that only have one data type - so that makes it hard to do the next step.\nDefining a question AI can answer (Step 4) I would venture to say this is maybe one of the biggest bottlenecks. To create an AI system as they currently exist a human needs to (i) define a concrete scientific/business problem, (ii) create a quantitative definition of that model, and (iii) define an objective function to optimize. This process can take a huge amount of expert/knowledgeable work.\nFitting the algorithm (Step 5) While I think there are some commodity technologies that work well here - streamlining the process from modeling to implementation is probably where a lot of AI applications could use work. This can take a while to just get the model set up even after you know exactly what you want to fit.\nMaking the model output mean something to humans (Step 6) Even in the cases where we ideally want the computer to do everything (self driving cars) we’d still like to summarize the choices the AI might make so humans can decide if they are ethical and how to regulate those decisions. But there is still a whole field of AI that is interpretable that needs to be developed and disseminated. So even places that have models built often struggle to communicate the results in a way that they can be used.\nAutomating the use of AI models (Step 7) Even if you get past all those other hurdles and have a working, interpretable AI model, you need humans to use it. Whether that is a doctor using the output of a radiology scan to make diagnostic/treatment decisions, or a car that can actually drive, the last step of actually making the model useful is still a major barrier to many projects.\nI think a lot of these barriers come down to the fact that for the most part we don’t have strict standards for data capture/tidying/organization/use that are used across organizations. We also don’t have the “glue” steps between each of these components automated. So while I think that the algorithms for AI will continue to develop rapidly in accuracy and range, for organizations to keep up they will need a lot more than just a way to fit the latest model. The reason that I think some organizations are leaping so far ahead is that they already have spent a huge amount of time thinking about all the Steps but the model fitting, so now they can focus their time/energy/resources on making algorithms do things we didn’t imagine were possible.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:30:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-11-21-a-few-things-that-would-improve-reproducibility-replicability-in-science/",
    "title": "A few things that would reduce stress around reproducibility/replicability in science",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2017-11-21",
    "categories": [],
    "contents": "\nI was listening to the Effort Report Episode on The Messy Execution of Reproducible Research where they were discussing the piece about Amy Cuddy in the New York Times. I think both the article and the podcast did a good job of discussing the nuances of the importance of reproducibility and the challenges of the social interactions around this topic. After listening to the podcast I realized that I see a lot of posts about reproducibility/replicability, but many of them are focused on the technical side. So I started to think about compiling a list of more cultural things we can do to reduce the stress/pressure around the reproducibility crisis.\nI’m sure others have pointed these out in other places but I am procrastinating writing something else so I’m writing these down while I’m thinking about them :).\nWe can define what we mean by “reproduce” and “replicate” Different fields have different definitions of the words reproduce and replicate. If you are publishing a new study we now have an R package that you can use to create figures that show what changed and what was the same betweeen the original study and your new work. Defining concretely what was the same and different will reduce some of the miscommunication about what a reproducibility/replicability study means.\nWe can remember that replication is statistical, not deterministic If you are doing a new study where you re-collect data according to a protocol from another group - you should not expect to get exactly the same answer. So if a result is statistically significant in one study and not significant in another, that may be within the bounds of what we’d expect to see.\nWe can remember that there is a difference between exploratory and confirmatory research There is a reason that randomized trials are the basis for regulatory decisions by the FDA and others. But if we require every single study to meet the requirements of a pre-registered, randomized, double blind controlled trial with a huge sample size we might miss some important discoveries.\nWe can remember that a failed replication isn’t always a scientific failure One thing Roger and Elizabeth point out is that many scientific studies won’t replicate - nor should we expect all studies to replicate. Sometimes a study is preliminary, exploratory, or the first observation of an unusual event. It may be a perfectly well executed study, and not replicate because the sample size was too small or there was an unmodeled confounder. This doesn’t mean that the scientific study was a failure, it just was an observation that didn’t pan out.\nWe can stop publicizing scientific results as solutions so quickly University press offices, startup companies, and researchers stressed for funding are under pressure to label every discovery as a “cure”, a “diagnosis”, a “solution to the crisis”. A lot of the frustration in the scientific community arises from this overstatement of results. It is hard to escape this (for me too!) - but we can excercise skepticism about claims of solutions on the basis of single scientific papers.\nWe can be persistent and private as long as possible Like many people I’ve run into frustrating cases where data isn’t available from a paper that has been published. I have contacted the authors only to be rebuffed. I have found that it takes work to convince them to provide the data, but I can often do it without having to resort to publicizing the problems and trying to make it adversarial.\nWe can make the realization that data is valuable but in science you don’t own it There is still discussion of data parasites and data symbionts. I have been both a data collector and a data analyst. I realize there is frustration to releasing your data and seeing others quickly publish ideas you may have had. At the same time I’ve seen how frustrating it can be to see people keep their data private and inaccessable indefinitely after publication. The reality is that people do deserve credit for collecting data, but that they don’t own the data they collect.\nWe should cut each other some slack I think that a lot of the frustration around reproducibility and replicability can come from the way the problem is approached. On the one hand, if you publish a scientific paper and someone tries to reproduce or replicate your work you can realize that they are doing that because they are interested and try to help them. Even if they find some flaws (as they inevitably will) or the study doesn’t replicate (as it might not) that is not a failure by you. On the other hand if you are reproducibing or replicating, remember that the scientist on the other end is a person. That person is subject to all the same funding, publication, and promotion stresses as everyone else. So we should try not to make the discovery of reproducibility/replicability problems high profile “gotchas” but focus on pointing out the real scientific issues that arise and trying to move forward in a positive way.\nLike many others, I have noticed that the stakes around reproducibility/replicability have been ratcheted up by social media and the rise in the prominence of the field of reproducibility. As someone who has experienced the real stress all of this can create in the life of real scientists, I’d love to see if we could move forward in a way that was more positive and collaborative while we address the explosion of data in the scientific enterprise.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:30:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-11-20-follow-up-on-reasoning-about-data/",
    "title": "Follow Up on Reasoning About Data",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-11-20",
    "categories": [],
    "contents": "\nSometimes, when I write a really long blog post, I forget what the point was at the end. I suppose I could just update the previous post…but that feels wrong for some reason.\nI meant to make one final point in my last post about how better data analyses help you reason about the data. In particular, I meant to tie together the discussion about garbage collection to the section on data analysis. With respect to garbage collection in programming, I wrote\n\nThe promise of garbage collection is clear: the programmer doesn’t have to think about memory. Lattner’s criticism is that when building large-scale systems the programmer always has to think about memory, and as such, garbage collection makes it harder to do so. The goal of [Automatic Reference Counting] is to make it easier for other people to understand your code and to allow programmers to reason clearly about memory.\n\nTo pick this apart just a little bit, it’s tempting to think that “not having to think about memory management” is a worthy goal, but Lattner’s point is that it’s not. At least not right now. Better to make tools that will help people to think more effectively about this important topic (a “bicycle for the mind”).\nI think the analogy for data analysis is that I think it’s tempting to think of the goal of methods development as removing the need to think about data and assumptions. The “ultimate” method is one where you don’t have to worry about distributions or nonlinearities or interactions or anything like that. But I don’t see that as the goal. Good methods, and good analyses, help us think about all those things much more efficiently. So what I might say is that\n\nWhen doing large-scale data analyses, the data analyst always has to think about the data and assumptions, and as such, some approaches can actually make that harder to do than others. The goal of the good data analysis is to make it easier to reason about how the data are related to the result, relative to the assumptions you make about the data and the models.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:30:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-11-16-reasoning-about-data/",
    "title": "Reasoning About Data",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-11-16",
    "categories": [],
    "contents": "\nIn my ongoing discussion in my mind about what makes for a good data analysis, one of the ideas that keeps coming back to me is this notion of being able to “reason about the data”. The idea here is that it’s important that a data analysis allow you to understand how the data, as opposed to other aspects of an analysis like assumptions or models, played a role in producing the outputs. I think for a given problems, some kinds of analysis do a better job of that than others.\nTo Collect Garbage or Not?\nProgrammers talk a lot about the importance of being able to “reason about code”, in part so that you can understand what’s going on and perhaps make modifications in the future. If there are two software solutions that produce equivalent results, one might prefer the one that allows you to reason about the code and about what the software is doing, even if it is less efficient. Aside from having a certain common sense logic to it, there are also important implications for maintaining the code.\nAlong these lines, a while back I was listening to one of my favorite podcasts, the Accidental Tech Podcast, when Chris Lattner made a surprise appearance. Chris previously created LLVM and worked at Apple where he developed the Swift programming language. On this episode, around the 1 hour and 56 minute mark, Lattner and John Siracusa get into a discussion about memory management models for programming languages. In particular, Siracusa asks Lattner to discuss garbage collection versus automatic reference counting (ARC), a system that Lattner developed while working at Apple.\n\nJohn Siracusa: Objective-C had garbage collection, as you mentioned. Eventually, Objective-C dropped the garbage collection and got ARC [Automatic Reference Counting], and of course, Swift doesn’t have garbage collection at all. Can you talk about the trade-offs there and why Swift is the way it is?\n\nJust as a quick summary, garbage collection is a system by which programmers can allocate memory for objects and not have to worry about de-allocating the memory when their done with it. The idea is that a separate process called the garbage collector periodically goes through all the memory that a program is using and checks to see if it still being used by some aspect of the program. If the memory is not being used, the garbage collector marks the memory as available so that it can be allocated for something else. If the garbage collector never ran, then eventually all the memory would get allocated and nothing would free up.\nHere’s how Siracusa sells it:\n\nSiracusa: Well, the idea [is] that memory management is completely out of the hands of the programmer, and that some magical fairy behind the scenes will make it all good for you. What you’re giving up, as you mentioned before…you lack some amount of control…\n\nIf you use R, you’re using an application that uses a garbage collector (originally written by Luke Tierney). If you’ve ever noticed that sometimes you run an operation in R that normally takes a short amount of time and it takes an unusually long time to run, that’s often because R had to stop for a second or two to let the garbage collector run in the background. This “stutter” is the bane of all applications that use garbage collection. You can actually force the garbage collector to run in R by calling the gc() function (although you should never have to do that in practice). I don’t think the “stutter” is a big issue in R, but it can be a big problem for applications that demand a fluid user experience.\nOne downside of garbage collection is that it addes a random aspect to program operation. Since you never know when the garbage collector is going to get called (that is based on runtime conditions), you never know when your program is going to be halted to allow the garbage collector to do its thing. The upside though is that programmers don’t have to worry about memory management, which is often the source of nasty bugs.\nAutomatic reference counting is essentially a fancier version of manual memory management, where programmers have to manual allocate and deallocate. But instead, with ARC the compiler does most of the work for you. In order to allow for this, certain restrictions must be placed on the language to allow the compiler to analyze programs and determine whether certain blocks of memory will be in use at a given time. The advantage here is that the analysis occurs at compile time, thus allowing for the running of a program to be deterministic. The downside, of course, is that the programmer has to do more deliberate thinking about memory.\nA key question then, as posed by Lattner is this:\n\nChris Lattner: You said GC [garbage collection] means that you don’t have to think about memory. Is that true?\n\nFrankly, before hearing the rest, I would have said “yes, it’s true”. But Lattner continues (emphasis added):\n\nLattner: The stutter problem, to me, isn’t really the issue, even though that’s what GC-haters will bring up all the time. It’s more about being able to reason about when the memory goes away. The most important aspect of that is that ARC gets rid of finalizers.\n\n\nIf you use a garbage-collected language, you use finalizers. Finalizers are the thing that gets run when your object gets destroyed. Finalizers have so many problems that there are entire bodies of work talking about how to work around problems with finalizers.\n\n\nFor example, the finalizer gets run on the wrong thread, it has to get run multiple times, the object can get resurrected while the finalizer’s running. It happens non-deterministically later. You can’t count on it, and so you can’t use it for resource management for database handles and things like that, for example. There are so many problems with finalizers that ARC just defines [it] away by having deterministic destruction.\n\nEven without knowing what a finalizer is, it’s clear that there’s a lot of complexity involved with using a garbage collector. Lattner’s final point is critical:\n\nLattner: There’s this question of if you’re building a large scale system, do you want people to “never think about memory?” Do you want them to think about memory all the time, like they did in Objective-C’s classic manual retain-and-release? Or do you want something in the middle?\n\nHis argument is that ARC is that “something in the middle”.\n\nLattner: I think that ARC strikes a really interesting balance, whether it’s in Objective-C or Swift. I look at manual retain-and-release as being a very imperative style of memory management…where you’re telling the code, line by line…this is what you should do at this point in time.\n\n\nARC then takes that model and bubbles it up a big step, and it makes it be a very declarative model…. The cool thing about that to me is that not only does it get rid of the mechanics of maintaining reference counting and define away tons of bugs by doing that, it also means that it is now explicit in your code what your intention was. That’s something that people who maintain your code benefit from.\n\nThe promise of garbage collection is clear: the programmer doesn’t have to think about memory. Lattner’s criticism is that when building large-scale systems the programmer always has to think about memory, and as such, garbage collection makes it harder to do so. The goal of ARC is to make it easier for other people to understand your code and to allow programmers to reason clearly about memory.\nWhat About Data?\nWhat does any of this have to do with data analysis? Well, nothing, directly. But I think there is an analogous spectrum of techniques in data analysis. In programming, one can think of garbage collection (“no thinking”) on one end of the spectrum and manual memory management (“constantly thinking”) on the other end. With data analysis, there are certain black box-type procedures proposed on one end and, well, just an uncoordinated jumble of procedures on the other end of the spectrum.\nWhen I mention “black box-type” procedures, I’m not referring to machine learning prediction models, which are often derided as “black boxes” because of their complexity. Rather, I’m thinking of a way of doing data analysis that focuses primarily on outcomes and results. With these kinds of approaches, the discussion is almost entirely focused on whether the outcomes are “correct” or not, which I think is misguided in many scientific contexts. With prediction models, at least initially, it makes sense to focus on outcomes. But with data analysis, we often want to know more about how the data are connected to those outcomes.\nOne famous example of how seemingly equivalent methods can diverge is Anscombe’s Quartet. Anscombe presented four datasets that have exactly the same summary statistics but appear very different when plotted. In particular, the correlations for the four datasets are the same. My take from his paper is that computing a correlation coefficient provides a result and the correlation will provide you some information about the data. We could have endless discussions about whether this result is correct, whether it truly exists in the population, whether it might replicate in the next study, or whether we should have used some other correlation metric. But none of that matters once you make a scatterplot of the data. The scatterplots show you how the data inform the result. In this case, there are four different ways in which the data can inform a given correlation coefficient, some of which are perhaps more concerning than others.\nThe scatterplots in Anscombe’s Quartet allow us to reason about the data in a way that the correlation coefficient alone does not. They tell us more about what is going on with the data and aid us in interpreting the correlation coefficient. In this case, I think an analysis that includes the scatterplots is a better data analysis than one that does not. Of course, the scatterplots alone may not be sufficient because they don’t actually give you a number, which may be needed in subsequent analyses. But there’s nothing that says you can’t do both.\nThis is more than simply saying that you should make plots (although you should). When you run a simple linear regression, there are two contributing parts: (a) the data and (b) the assumption of linearity. Reporting the regression coefficient gives you a sense of the strength of the relationship between the predictor and the response, but does that relationship arise from the data or from the model’s assumptions? Any analysis that can help the reader elicit the relative contributions of those two components is better than an analysis that doesn’t.\nEvaluating Data Analysis\nInterestingly, Anscombe laid out four “myths” that he hoped to debunk:\nNumerical calculations are exact, but graphs are rough;\nFor any particular kind of statistical data there is just one set of calculations constituting a correct statistical analysis;\nPerforming intricate calculations is virtuous, whereas actually looking at the data is cheating.\nUnfortunately, I think these three ideas are alive and well in much of the world of data analysis, especially points 2 and 3. In discussions with various people about what makes for a good data analysis, I’ve found there’s a common focus on “correctness”. For example, if a data analysis leads to a result that later replicates, then the original analysis is a good one. It’s hard to be against correctness, but I think it’s misguided. In particular, the correctness of a scientific claim cannot usually be evaluated with a single study, so this would imply that you cannot evaluate the data analysis that went into making that claim. If a replication study occurs 10 years later, that would imply that we will be waiting 10 years to determine if a data analysis is good.\nI think it should be possible to evaluate the quality of a data analysis once it’s done, not 10 years later when a replication study verifies a scientific claim. One way to do so is to ask whether the analysis allows us to reason about the data relative to another possible analysis. If we can answer the question, “How do the data in this study inform the result?” then I think we are in good shape.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:30:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-10-30-how-do-you-convince-others-to-use-R/",
    "title": "How do you convince other people to use R?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-10-30",
    "categories": [],
    "contents": "\nI just got back from the rOpenSci OzUnconf that was run in Melbourne last week. I’d like to give a big thanks to the organizers (Nick Tierney, Di Cook, Rob Hyndman and others) for putting on a great unconference. These events are always a great opportunity to meet people just getting started in the R community and to get them involved.\nAs is typical for these unconferences, topic ideas were pitched via issues on the OzUnconf GitHub repo. One issue that I filed was titled “How do you pitch R to new users?”. While this issue was not taken up during the unconference (for good reason, I think), Nick was kind enough to initiate a lunch time discussion of the topic (with Daniel Falster kindly taking notes).\nThe topic came up in my mind because I’ve found that I’ve had to change the way that I “sell” R to new users. Through various discussions at the unconference and in many other venues, I’ve found that many R users, even today, are the “lone R user” in their group/institution/organization. While they may enjoy using R, it’s often made difficult by the fact that others in their group do not use R and therefore there is some negotiation over “how things get done”. Convincing others in the group to use R is one way to go (I suppose abandoning R is the other) and the question of how best to do this for different audiences is the question that arose in my mind. As a teacher, you can usually design the curriculum so that the students are forced to use R. But in most other environments, a different approach is needed.\nAs part of the introduction to the topic, I talked about how I used to convince others to use R. Bear in mind, this was almost 20 years ago and the majority of people I was talking to were using SAS, Stata, SPSS, Excel, or some other commercial package. These were largely interactive packages, perhaps with graphical user interfaces, designed to do more or less traditional statistical analyses. My pitch usually involved three things:\nFree. R was both free as in cost and free as in free software. The free cost part made it a highly accessible package and the free software part allowed for anyone to tinker with the package, inspect its code, and make improvements.\nGraphics. R was able to produce high quality “publication ready” graphics and it gave you detailed control over all the graphical elements. S-PLUS could also do this but S-PLUS didn’t come with the free part mentioned above.\nProgramming language. Unlike packages like SAS, Stata, or SPSS, R came with a robust and sophisticated Lisp-like programming language that was well-suited for data analysis applications. In addition, you could use it to build packages that could extend the core R system.\nMuch has changed since those early days and I’ve varied my pitch quite a bit to focus on a few different things (that didn’t exist back then). In particular, the audience has changed—I talk to many more people who are just getting started in data analysis and therefore are a bit more open-minded about which software to use. Also, python has come on the scene as a viable alternative to R for data science and so there are even more arguments to consider. Some of the things I focus on now are\nReproducibility and Reporting. With the development of knitr and its combination with R Markdown, the writing of reproducible reports was made infinitely easier. (Markdown itself, probably deserves its own discussion, but it’s not specifically R-related.)\nRStudio. The development of the RStudio IDE has made getting started with R much easier. Having a powerful IDE was important to me for learning other languages and I’m glad R finally has something solid for itself. RStudio has significantly simplified the development of R packages via devtools and roxygen2. While it’s not yet perfect, these tools have changed what used to be a labor-intensive and finicky process into a more manageable and easier to learn work flow.\nGraphics. R still has the ability to make great data graphics and with the introduction of ggplot2, it has become easier to make good graphics.\nR Packages and Community. With over 10,000 packages on CRAN alone, there’s pretty much a package to do anything. More importantly, the people contributing those packages and the greater R community have expanded tremendously over time, bringing in new users and pushing R to be useful in more applications. Eveyr year now there are probably hundreds if not thousands of meetups, conferences, seminars, and workshops all around the world, all related to R.\nDiscussion\nAt the unconference, a number of people had different approaches to how to convince others to use R. Here are just a summary:\nSomeone mentioned the Bioconductor, which is a huge resource for those doing research in the world of high throughput biology. Not many other packages have something similar so it makes an obvious selling point to people working in this area\nThe idea of using R end-to-end came up, meaning using R to clean up messy data and taking it all the way to some interactive Shiny app on the other end. The idea that you can use the same tool to do all the things in between made for a compelling case for R.\nFor the spreadsheet audience, the dplyr package was sometimes a good selling point. The idea here was that you could show people how much time could be saved by automating analyses and using dplyr to clean up data.\nThe open source nature of R came up a few times, primarily as a means for developing transferable skills. If you work at a company/institution that specializes in some proprietary package, it’s often difficult to transfer those skills somewhere else if your new job doesn’t use that package. The fact that R is open source means that, in theory, you could use it anywhere and the skills that you build up in R are (again, in theory) applicable everywhere.\nSomeone mentioned that if you want to convince someone to learn/use R, just show them the multitude of jobs available to R programmers (and in particular, the salaries attached to them).\nThe fact that R is obtainable for free is still important, given that Matlab and SAS licenses have not gotten any cheaper over time. In my experience, this is particularly important in non-industrialized countries where for many people paying for expensive licenses is not an option.\nThis just a brief summary of our discussion at the unconference and I was heartened to see all of the enthusiasm for R there. Even with R’s incredible growth over the last 20 years, there will still come a time when a case needs to be made to use R over something else. I’m just glad that we have so many more reasons today than we used to.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:30:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-10-09-it-costs-money-to-get-it-right/",
    "title": "It Costs Money to Get It Right",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-10-09",
    "categories": [],
    "contents": "\nOn the latest episode of Not So Standard Deviations I talked with Hilary about Apple’s efforts to train machine learning algorithms in their Face ID technology in the iPhone X. The gist of Face ID is that it recognizes your face using a mathematical representation and then unlocks the phone when it can confirm that it is you. In its keynote presentation, Apple mentioned that it’s using machine learning to do this and even had developed its own custom chips to do the computations.\nThere were many questions regarding Face ID, including how exactly Apple had trained this machine learning model without it ever being released to the public. There were natural concerns that the system would only recognize certain kinds of faces, perhaps of certain ethnic backgrounds or of certain shapes or sizes. Two bits of evidence indicate that Apple likely spent a lot of money in this training process. The first bit comes from Craig Federighi, Senior Vice President for Software Engineering, in an interview with Tech Crunch,\n\n“Phil [Schiller] mentioned that we’d gathered a billion images and that we’d done data gathering around the globe to make sure that we had broad geographic and ethnic data sets. Both for testing and validation for great recognition rates,” says Federighi. “That wasn’t just something you could go pull off the internet.”\n\n\nEspecially given that the data needed to include a high-fidelity depth map of facial data. So, says Federighi, Apple went out and got consent from subjects to provide scans that were “quite exhaustive.” Those scans were taken from many angles and contain a lot of detail that was then used to train the Face ID system.\n\nThe second bit of information comes from the Accidental Tech Podcast where a listener wrote in to describe a study that he had recently enrolled in that took very detailed images of his face. Although there is no explicit connection, it would sound by the description that this study could have been related to the Face ID efforts (you can hear Casey Liss discuss the study here). Apparently, the study was being conducted by Exponent which from its web site appears to be a scientific and technical consulting firm.\nGiven the two bits of information, I don’t think it’s a stretch to say that\nApple decided it needed a huge dataset to train the Face ID algorithm and that no such dataset existed that they could access\nHaving no real desire to collect such data themselves, Apple outsourced the study to a third party company (in much the same way a biotech company might outsource a clinical trial to a clinical research organization) to collect the data to their specifications. The outsourcing also allowed Apple to shield its identity in the process so that they could keep the whole Face ID system secret.\nThe collected dataset (and the subsequently trained model) effectively serve as the “prior” distribution in a complex classification scheme that is adapted to a specific user once a user allows the iPhone X to collect their face data.\nApple claims that user data is not sent back to Apple for privacy reasons, so that the “big” model is not updated according to user data. Perhaps Apple will continue to collect data separately from user data, but there was no mention of that.\nIf any of this is true, there are a few things worth noting:\nThis is not the way things are done Silicon Valley these days. A perhaps less costly way would be to give some app away for free (like a fun game or something similar), let the app take a bunch of pictures of you from different angles, and then gather all the data to train a model. Of course, such an approach would be subject to the selection bias that all approaches like this suffer from, because the data only represents the people whom the company could get to use the product.\nIt must have been very expensive to conduct this study, although that is less of a big deal when you have over $150 billion in net cash. That said, only a handful of companies could afford to do this and certainly not your average startup.\nIf the Face ID system works well and is capable of recognizing the diversity of faces that represent the iPhone customer base, then it would suggest that getting this kind of machine learning right costs a lot of money. In particular, it highlights the limitations of crowdsourcing-as-data-collection and it would suggest that old-fashioned ideas of sampling and experimental design are still needed once in a while.\nIs It Research?\nOn Twitter, someone mused that wouldn’t it be nice if they released all this data to the public? Certainly, it would be a valuable and unique resource if they did and would likely spur a wealth of new innovations. Of course, this will almost certainly not happen, if only because Apple considers this dataset a key to their competitive advantage in the area of face detection, which they believe is the future of authentication. Why give this up to their competitors? Indeed, I don’t think anyone would expect Apple to give up this dataset any more than we might expect Google to give up data it collects from its search engine.\nOn the other hand, if I had conducted a study collecting similar face information (albeit on a smaller scale!) and published a paper about my findings about face morphology, there would likely be an expectation of me to release my data to the public along with the findings. And rightly so! Reproducibility is critical to moving science forward and releasing the data allows others to reproduce the findings. Furthermore, such a dataset would likely be useful to others investigations and would benefit all of science.\nBut why is there is no expectation for Apple to release data from its study but there is an expectation on me to release data from my study?\nIn my opinion, Apple is not conducting research, even though it kind of looks like it is. Rather, Apple is doing product development, which requires some gathering of information as part of the process. The fact that the information gathering was done on such a large scale isn’t relevant. Just as any consumer packaged goods company might gather a focus group before developing their next toothpaste, Apple gathered face data in order to develop Face ID.\nI often argue with people over whether companies like Google, Facebook, and Apple do research. My argument is that for the most part, they do not, because they are not interested in creating new knowledge. They do not make any specific public claims or inferences about the data they’ve gathered and so there isn’t really anything for them to defend. They are interested in taking whatever information they collected and channeling it into products. Yes, all of these companies occasionally publish a paper (I think Apple has a grand total of five), and I would say that those papers represent real research. But I would wager that those papers represent a small fraction of the work going on in those companies.\nAs a side note, Mark Neuman on Twitter suggested that this kind of work does qualify as research for the purposes of evaluating the ethical treatment of research sbjects. I would have to agree here, and I would hope that all of these companies obtain proper informed consent from subjects before collecting their data (at least their lawyers spend a lot of time thinking about it). The fact that these companies may be collecting these data for product development and not for research doesn’t absolve them of the need to treat subjects properly.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:30:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-10-02-creating-an-expository-graph-for-a-talk/",
    "title": "Creating an expository graph for a talk",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2017-10-02",
    "categories": [],
    "contents": "\nI’m co-teaching a data science class at Johns Hopkins with John Muschelli. I gave the lectures on EDA and he just gave a lecture on how to create an “expository graph”. When we teach the class an exploratory graph is the kind of graph you make for yourself just to try to understand a data set. An expository graph is one where you are trying to communicate information to someone else.\nWhen you are making an exploratory graph it is usually simple, with no axes, legends, fancy colors, or other effort to make it pretty, understandable and clear. John has a great blog post on how to build up a figure that is expository.\nRecently I gave a talk at McGill University and needed to create a plot for the talk. I figured one more example is always better for everything, so I thought I’d go through my process here.\nI wanted to show the p-value distribution from the tidy-pvals package. So first I loaded the data:\n\n\nlibrary(tidypvals)\nlibrary(ggridges)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(forcats)\ndata(allp)\n\n\nI knew I wanted to use the ggridges package so I read the docs and started with the easiest version:\n\n\nallp %>% \n  ggplot(aes(x = pvalue, y = field)) +\n    geom_density_ridges()\n\n\n\nRight away I saw there were some problems here. First of all, clearly a p-value greater than one shouldn’t be in there, so that was a mistake. I also don’t like that you can’t really see the values because most of the action is near zero.\nSo let’s fix the x-axis a bit. I spent a few minutes fiddling and decided I just wanted to see the values between 0 and 0.25.\n\n\nallp %>% \n  ggplot(aes(x = pvalue, y = field)) +\n    geom_density_ridges() + \n  xlim(c(0,0.25))\n\n\n\nOk that’s better, but I don’t really like the grey background so let’s pick a different background color\n\n\nallp %>% \n  ggplot(aes(x = pvalue, y = field)) +\n    geom_density_ridges() + \n  xlim(c(0,0.25)) + \n  theme_ridges(grid = FALSE)\n\n\n\nThat’s a bit prettier, but we see that field is sometimes NA so we need to remove those values.\n\n\nallp %>% \n  filter(!is.na(field)) %>%\n  ggplot(aes(x = pvalue, y = field)) +\n    geom_density_ridges() + \n  xlim(c(0,0.25)) + \n  theme_ridges(grid = FALSE)\n\n\n\nAnd actually the density plots are a little weird for p-values, lets see if we can turn them into something a little more like a histogram, which I think fits this data type better. To do that we have to change the parameters in geom_density_ridges.\n\n\nallp %>% \n  filter(!is.na(field)) %>%\n  ggplot(aes(x = pvalue, y = field)) +\n    geom_density_ridges(stat = \"binline\") + \n  xlim(c(0,0.25)) + \n  theme_ridges(grid = FALSE)\n\n\n\nOk but I think it would look better if it was a little bit higher resolution, let’s up the number of bins\n\n\nallp %>% \n  filter(!is.na(field)) %>%\n  ggplot(aes(x = pvalue, y = field)) +\n    geom_density_ridges(stat = \"binline\",bins=50) + \n  xlim(c(0,0.25)) + \n  theme_ridges(grid = FALSE)\n\n\n\nOk but as people have pointed out the spike at 0.05 is due to censoring (p-values reported like \\(P < 0.05\\)). So let’s break it down by operator.\n\n\nallp %>% \n  filter(!is.na(field)) %>%\n  ggplot(aes(x = pvalue, y = field,fill=operator)) +\n    geom_density_ridges(stat = \"binline\",bins=50) + \n  xlim(c(0,0.25)) + \n  theme_ridges(grid = FALSE)\n\n\n\nOk there aren’t that many greater than p-values and it makes the plot messy so let’s drop those\n\n\nallp %>% \n  filter(!is.na(field)) %>%\n  filter(operator != \"greaterthan\") %>%\n  ggplot(aes(x = pvalue, y = field,fill=operator)) +\n    geom_density_ridges(stat = \"binline\",bins=50) + \n  xlim(c(0,0.25)) + \n  theme_ridges(grid = FALSE)\n\n\n\nThe histograms overlap a bit so let’s alpha blend the colors.\n\n\nallp %>% \n  filter(!is.na(field)) %>%\n  filter(operator != \"greaterthan\") %>%\n  ggplot(aes(x = pvalue, y = field,fill=operator)) +\n    geom_density_ridges(stat = \"binline\",\n                        bins=50,alpha=0.25) + \n  xlim(c(0,0.25)) + \n  theme_ridges(grid = FALSE)\n\n\n\nThere is some funkiness in how the histogram bins are computed so I went to the internet and figured out I needed to set the boundary at 0 and make the bins be closed on the right.\n\n\nallp %>% \n  filter(!is.na(field)) %>%\n  filter(operator != \"greaterthan\") %>%\n  ggplot(aes(x = pvalue, y = field,fill=operator)) +\n    geom_density_ridges(stat = \"binline\",\n                        bins=50,alpha=0.25,\n                        boundary=0,closed=\"right\") + \n  xlim(c(0,0.25)) + \n  theme_ridges(grid = FALSE)\n\n\n\nNow we make sure that there isn’t wasted space on the y-axis by using the expand argument.\n\n\nallp %>% \n  filter(!is.na(field)) %>%\n  filter(operator != \"greaterthan\") %>%\n  ggplot(aes(x = pvalue, y = field,fill=operator)) +\n    geom_density_ridges(stat = \"binline\",\n                        bins=50,alpha=0.25,\n                        boundary=0,closed=\"right\") + \n  xlim(c(0,0.25)) + \n  theme_ridges(grid = FALSE) + \n  scale_y_discrete(expand=c(0,0))\n\n\n\nRemove the baseline from the plot for true ggridges coolness\n\n\nallp %>% \n  filter(!is.na(field)) %>%\n  filter(operator != \"greaterthan\") %>%\n  ggplot(aes(x = pvalue, y = field,fill=operator)) +\n    geom_density_ridges(stat = \"binline\",\n                        bins=50,alpha=0.25,\n                        boundary=0,closed=\"right\",\n                        draw_baseline=FALSE) + \n  xlim(c(0,0.25)) + \n  theme_ridges(grid = FALSE) + \n  scale_y_discrete(expand=c(0,0))\n\n\n\nThat’s definitely not a perfect plot, but it worked for my talk and was at least able to communicate a couple of the key points (about variation by field, variation by operator, and spikes at critical values).\nIf I was going beyond the talk I’d probably reduce the number of fields displayed or really increase the size of the plot. I’d probably make the bin width even smaller and I’d add a title. I’d also probably clean up the “greaterthan” and “lessthan” to be “Greater than” and “Less than”.\nRegardless, I’m always surprised how much work it takes to go from an exploratory plot I’m just looking at myself to one I’d show to other people.\n\n\n\n",
    "preview": "posts/2017-10-02-creating-an-expository-graph-for-a-talk/index_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-12-27T15:39:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-09-19-recording-podcasts-with-a-remote-cohost/",
    "title": "Recording Podcasts with a Remote Co-Host",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-09-19",
    "categories": [],
    "contents": "\nI previously wrote about my editing workflow for podcasts and I thought I’d follow up with some details on how I record both Not So Standard Deviations and The Effort Report. This post is again going to be a bit Mac-specific because, well, that’s what I do.\nCommunication\nBoth of my podcasts have a co-host who is not in the same physical location as me. Therefore, we need to use some sort of Internet-based communication software (Skype, Google Hangouts, FaceTime, etc.) to talk with each other. Another simpler option is to use web-based communications platforms that are built for podcasting, including Zencastr or Cast. These platforms take advantage of WebRTC to record audio on each co-hosts machine (more on this later).\nWeb Platforms\nFor those who are just starting up, I would recommend something like Zencastr or Cast because they are easy to setup and basically “just work”. (One note: if you are using Safari 10.x on the Mac (which doesn’t support WebRTC), these sites won’t work. Use Google Chrome instead.) For these web sites, you need nothing more than a pair of headphones with a built in microphone (like the ones that likely came with your cell phone) and a computer. Communication occurs over the Internet, as you might expect, but the audio that is recorded is the audio that comes from your microphone. The audio that goes over the Internet is not recorded.\nThe advantage of this system is that\nYou don’t have to worry about synchronization—the audio is recorded simultaneously.\nThe audio quality is good because it is recorded locally.\nFiles can be uploaded immediately to the cloud so that the editor can quickly assemble them (i.e. Zencastr uploads files to Dropbox).\nThe disadvantages of these services are\nThey charge a monthly fee with tiered plans (Zencastr has a free “hobbyist” plan)\nThey only can handle certain host setups\nIn my experience, communications noise can still affect the audio (not sure why).\nBoth Zencastr and Cast are fairly new offerings and therefore occasionally have to work through some bugs and wonkiness. Hilary and I started out using Zencastr for Not So Standard Deviations but eventually moved off of it. This was in part because one time we thought we recorded an entire hour-long episode and it turned out the file was corrupted (we had to re-record it the next day). To this day I’m not sure what happened, but after that I resolved to have more control over the process.\n“Manual” Platforms\nThe alternative to the web-based platforms is to use a more manual approach. I have found this system to be more reliable, if not a bit more complicated. Here, you can use Google Hangouts, Skype, FaceTime, or another Internet-based communications program. I’ve used most of them and I’ve found that Google Hangouts is probably the best (but it is pretty close all around). Obviously, the quality of your Internet connection will be more important than which software you use, and that may depend on exactly where in the world you are.\nRecording\nThe basic idea is that there are two types of audio streams: the audio generated by each co-host/speaker and the audio that is passing across the Internet. The basic process here is\nEach speaker/co-host records their own audio file on their computer using a program like Quicktime or Audacity or something similar. On the Mac, this is super simple—just open up Quicktime Player and goto File > New Audio Recording…. After that, just click the red circle button and you’re recording!\nYou can also record the Internet audio using a program like Ecamm or Audio Hijack. This is strictly speaking not necessary but it is useful as a backup audio recording and also for synchronizing the various speaker audio files in the editing phase.\nIf you’re going down the road of manual recording (and you use a Mac) I strongly recommend that you invest in getting Audio Hijack. It is a phenomenal program that lets you do basically anything on the Mac involving audio. If your Mac makes a sound in any way, you can easily record it.\nWith Audio hijack, you can set things up to record audio, save it to a file in various formats, and send the audio to different outputs. Here is my setup:\nAudio Hijack setupHere’s how it breaks down:\nI am recording my audio using an ATR USB Microphone (the exclamation point is there because my mic is not connect right now)\nI record my co-host’s audio through Google Chrome (via Google Hangouts).\nBoth my and my co-host’s audio is saved to separate uncompressed 16-bit mono AIFF files that will later be imported into Logic Pro X for editing.\nI then route each audio stream through Peak/RMS meters to make sure our respective microphones are tuned properly and that the audio isn’t peaking.\nThen my co-host’s audio is routed to my headphones (Internal Speakers) so that I can hear her.\nThe combined audio with both my and my co-host’s voice is then saved to a single 256 kbps MP3 file. This final file is the backup audio file that I can use to help with synchronization.\nThe co-host then sends me her locally recorded audio file through Google Drive or Dropbox.\nThe advantage of this approach is that it can be adapted and modified in a variety of ways including a combination of in-person and remote guests (the web platforms can only really do remote guests). For example, if you have an in-person guest you can record both in-person audio streams and send it through a loopback device so that the remote person can hear it. “Multi-ender” podcasts are starightforward because everyone just records their audio separately and sends me the resulting audio file. I still have the combined recording as a backup just in case.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:30:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-09-18-editing-podcasts-with-logic-pro-x/",
    "title": "Editing Podcasts with Logic Pro X",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-09-18",
    "categories": [],
    "contents": "\nI thought I’d write a brief description of how I edit podcasts using Logic Pro X because when I was first getting into podcasts, I didn’t find a lot of useful stuff out there. A lot of it was YouTube videos of advanced editing or very basic stuff. I don’t consider myself a sound expert in any way, but I wanted a good workflow that would produce decent quality stuff. When I first started Not So Standard Deviations with Hilary Parker I actually edited the first few episodes using Final Cut Pro X, a video editor, because that’s just what I was familiar with. Eventually, I learned Logic Pro X, which is Apple’s digital audio workstation, largely because of this post by Jason Snell.\nOn with the show. Unfortunately, this is going to be an Apple-heavy post given that Logic Pro X is only available on Macs. Sorry, rest of world!\nRecording the Episode\nI assume you’ve recorded your podcast and that you have one file for each speaker. In both The Effort Report and Not So Standard Deviations, I record with a co-host. As of this writing, both podcasts are recorded remotely because of geographic constraints. The way we do this is that each co-host records her microphone input locally and then sends me the audio file (I also record my own microphone input). This way, we don’t have to worry about any noise going over the Internet connection (and there is a lot sometimes). Unfortunately, there is no way to deal with the connection delays that inevitably come up (more on that later). As a backup, I also record the combined conversation going over the Internet using Audio Hijack. Besides being a backup, this recording is also useful for synchronizing the separate tracks, but at the end of the day, I don’t actually use it. I’ll probable write a separate post on the details of how I record each podcast.\nStarting the Project\nI start each project in Logic Pro X by selecting the “Multi-Track” option after clicking File > New from Template…. This just opens you up in an empty project with a whole bunch of empty tracks setup.\nEmpty ProjectNext I import three audio files: One containing just my voice, another containing just my co-host’s voice, and another containing our combined conversation. These tracks are then labeled according to the speaker’s name. Below is an example from Episode 54 of The Effort Report after importing the audio.\nInitial audio file importAt this point I don’t worry about where the audio tracks are placed, I just get them into the project. This is a good time to save your project (and also give it a name).\nSynchronization\nThe first problem is that the audio tracks are not synchronized. For the most part this is not such a big deal as long as you have a recollection of how the conversation went. But it’s easier if you have a recording of the combined (synchronized conversation). Here I can line up my voice in the separate audio file with the combined audio file, and then line up Elizabeth Matsui’s voice in her audio file to the same combined audio. It takes a little trial and error, but doesn’t take too long.\nHere is a screenshot after I’ve synchronized the audio files. You can see that the waveforms nicely line up with each other in the combined audio file.\nSynchronized audio tracksAt this point I usually delete the combined audio track because it’s no longer needed (and is of lower quality because it is recorded over the Internet).\nStrip Silence\nPerhaps the key reason I use Logic Pro X for podcast editing (a task for which is not particularly well-suited) is the strip silence feature. What this does is it takes an audio track and just deletes anything that it considers to be silence. What exactly is “silence” is configurable and you may need to play with it depending on your recording levels. You can run strip silence by first selecting the track in the window so that it is highlighted and then pressing Ctrl-X. You will be presented with this window:\nStrip silence default configuration windowI find the defaults don’t work for me, so I modify them slightly. My parameters are:\nThreshold: 4%\nMinimum Time to accept as Silence: 1.0 sec\nPre Attack-Time: 0.1 sec\nPost Release-Time: 0\nAnd I leave the “Search Zero Crossing” box checked. Once you hit “OK”, the track will look something like this.\nAfter strip silence appliedAfter doing it again to my track, your track editor window will look something like this.\nAfter strip silence appliedNow you can see that the only thing that remains of each track are the parts where one of us is talking. We are getting there!\nDealing with Delay, Cross talk, and Noise\nAt this point you might think we were done, given that the audio has been synchronized. However, there are a few issues left that need to be dealt with:\nCross talk. Especially on remote connections with a delay (or on recordings with multiple speakers), there will likely be moments when people talk over each other. This is unpleasant to listen to and it’s nice to clean up those moments if possible.\nConnectivity Delay. Often with remote recording, there is a delay as a person’s voice is relayed by the Internet gnomes over thousands of miles. The fact that there is only a 1–3 second delay in a connection between Melbourne and Baltimore is, frankly, a miracle, but it’s still annoying to listen to.\nConversational Noise. Often in remote conversations, the person not doing the talking is prone to say “Uh uh”, or “yeah”, just to indicate that they’re still listening. This stuff is not necessary in the final recording and it’s good to clean that up.\nThe nice thing about all of the problems above is that they are easily dealt with after running strip silence. In partcular, in sections where a single person is just talking, there is nothing to do. So you can just skip over that. For example, in the section below, Elizabeth is speaking for just over a minute uninterrupted.\nMonologueThere’s nothing to edit here so you can skip to the next section.\nBy contrast in the section below, I am speaking and Elizabeth is agreeing with me by saying “right” and “yeah”. With strip silence it’s easy to recognize this pattern and I can just delete those little blurbs.\nConversational noiseFinally, with connectivity delays, there will often be a gap between when one speaker finishes and another responds. In the example below, when I’m done speaking, there’s about a 2 second delay between when I stop and when Elizabeth responds. Sometimes that’s on purpose, but usually it’s because of the connectivity delay.\nConnectivity delayThis can be easily fixed by clicking on Elizabeth’s track at that point, hitting Shift-F, which selects everything after that point, and then dragging the whole thing to the left a little bit. The result is below. It’s important to use Shift-F in order to preserve synchronization in the rest of the recording!\nShifting a track to close a gapThe nice thing about the strip slience feature is that you only need to navigate to all the points where one speaker stops and another starts. These boundaries are easy to find in the track editor. That doesn’t mean that this process isn’t tedious, especially in an hour-long episode. But it’s faster than it would be otherwise and it makes for a much cleaner recording and a much more listenable episode.\nLastly, cross talk is usually easy to spot because it looks something like this.\nCross talkHere, both Elizabeth and I are talking at the same time and it looks messy. In these cases I’ll listen to what was happening before and after the cross talk and see if I can clean it up. Usually, I just end up deleting the section with cross talk and try to paste together what came before to what comes after. Or if it makes sense, sometimes I can just shift one person’s voice over just a little bit so that both speakers’ points are made, but just not at the same time.\nExporting\nAfter all the editing is done, I export the final episode by bouncing to a file. You can hit Cmd-B to get the bounce menu. Typically I bounce to MP3 at 160 kpbs and I turn Normalize to “Off”.\nSummary\nThat’s pretty much it for editing. There are a few extra things that I do (like sound compression), but they’re not quite as important as getting the editing right. A lot of stuff out there encourages you to use free editors like Audacity or something similar—and that’s a good place to start—but I think a professional tool like Logic Pro X is essential for dealing with problems like noise, cross talk, and delay, which I think are a feature of every remotely recorded podcast.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:30:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-09-11-specialization-and-communication-in-data-science/",
    "title": "Specialization and Communication in Data Science",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-09-11",
    "categories": [],
    "contents": "\nI have been interested for a while now in how data scientists can better communicate data analysis activities to each other and to people outside the field. I believe that our current methods are inadequate because they have mostly been borrowed from other areas (notably, computer science). Many of those tools are useful, but they were not developed to communicate data analysis concepts specifically and often fall short. I talked about this problem in my Dean’s Lecture earlier this year and how the field of data science could benefit from developing its own theories, to simplify communication as other fields have done.\nOne thing that I have noticed is that in other fields, the development of those fields can be viewed in part as a trend towards increasing specialization. With people in a field who increasingly specialize in a certain sub-specialty, there is a parallel need for the specialists to communicate and coordinate with each other in order to produce a whole product. Over time, the separation of a field into an collection of specialists drives the development of communication tools that can serve as clearinghouses of mutually agreed upon information. Without adequate tools, the communications overhead involved with adding more people to a project will become too great and the entire enterprise can collapse. This phenomenon is famously described in Fred Brooks’s The Mythical Man-Month as it relates to software engineering projects.\nI thought it might be useful to talk about some of these other areas and how they have overcome increased specialization and the separattion of duties with communications tools. Tracing the history of other fields is instructive as it potentially provides a basis on which we can discuss data analysis. Listeners of my podcast with Hilary Parker know that we regularly have a segment that we refer to as “analogy corner” and this is the Simply Statistics version of that.\nSpecialization in Other Areas\nThe first example comes from filmmaking and the development of the screenplay. The Script Lab describes The History of the Screenplay and how filmmaking worked before the development of scripts:\n\nWhen contemplating the history of screenwriting, one cannot divorce the theories of screenwriting from the evolution of film production. The earliest films were often solo projects, from conception to completion. Referred to as the “cameraman system” this was the most primitive of filmmaking. Soon, directors became central to the process, but most movies were filmed with only a vague idea of what the director wanted to shoot. Often crews were kept waiting while the director planned what to film next.\n\nFilms were one-person projects and were developed in a more or less linear fashion. It was an inefficient system—most films today are produced in a highly nonlinear fashion to accommodate actors’ schedules and various production processes.\nToday, the screenplay serves as a critical communications hub around which many filmmaking departments (costume, makeup, hair, props, sets) can organize their activity. Imagine if representatives of each of these departments had to individually consult with the screenwriter or director about every detail of their work. It would be a nightmare of exponentially growing complexity. With a written document, such as as screenplay, that everyone can agree on as the authority of “what is happening in the film”, people can get their jobs done without the need of constant back and forth communication.\nThe second analogy comes from finance. In finance there was a similar development of specialization as it relates to limited liability. Here, the “specialization” refers to the separation of the owners of a company from its managers. As a result, there must be a way for managers of the company to communicate to the investors what exactly is going on with the operations of the company. Thus, the development of financial statements, accounting rules, and various publicly available documents that let investors analyze the health of a company. Graham and Dodd’s seminal Security Analysis is essentially a plea to investors to evaluate companies based on the publicly available data, rather than on common myths and legends about what makes for a good or safe investment. Today, with the separation of owners from managers and the creation of standardized communication formats between the two (e.g. S-1s, 10-Ks, 10-Qs, etc.), we have the basis of the global capital markets system.\nThe last analogy comes from western classical music, where there is often a division between the composer of the music and the performer. In more complex symphonic music, you might say there are three roles: the composer, the performer, and the interpreter/conductor. However, in early classical music, such a division didn’t exist and composers typically performed their own music, often by themselves. In this setup, there’s not much need to write things down, as the music can be stored in and performed from the composer’s head. This concept was well-captured in the movie Amadeus where Mozart describes his opera The Magic Flute as being “up here in my noodle” (the rest is just scribbling and bibbling).\n\n\nOf course, opera might be the ultimate example from classical music where some sort of communication tool is needed to coordinate between the musicians, singers, and set designers. Thus, for most classical music, we have the score, which specifies what every instrument and signer is doing at any given time. There is a standardized notation that allows others unfamiliar with the composer to quickly grasp what is going on and to assemble the time and resources needed to perform the work.\nWhat About Data Analysis?\nIn data science today, or really in science, much of what goes on follows the “vertically integrated” model, where the same person, asks the question, collects the data, and analyzes the data. The need for communication methods doesn’t really arise until that work needs to be disseminated to others (including yourself in the future). In large collaborations where communication over analyses needs to be done from the start, my experience has been that even in the best case scenarios, the methodology is ad hoc and difficult to re-create in another project involving different people.\nMost would agree that the software code that actually does the analysis is an important component of communicate what is being done. However, not everyone needs or wants all of the details provided by the code. Perhaps one concept we could steal from music is the distinction between the score and the parts. In a symphony, the conductor needs the full score because they need to know what everyone is doing at all times. But the first violinist only reads the first violin part—they don’t need to read the entire score in order to play a vital part in creating the finished product.\nDeveloping appropriate communications tools for data science is critical to scaling data analysis so that more people can be involved and to reproducibility/replicability so that more people can understand what is going on in an analysis. Until then, I think we will continue to jam tools from other fields into the data science process, and that is fine. These tools are useful, but I think ultimately are not a perfect fit.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:30:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-09-07-moon-shots-cost-more-than-you-think/",
    "title": "Moon Shots Cost More Than You Think",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-09-07",
    "categories": [],
    "contents": "\nIn a deeply reported article, Casey Ross and Ike Swetlitz report that IBM’s Watson isn’t living up to its hype when it comes to cancer care:\n\nThe interviews suggest that IBM, in its rush to bolster flagging revenue, unleashed a product without fully assessing the challenges of deploying it in hospitals globally. While it has emphatically marketed Watson for cancer care, IBM hasn’t published any scientific papers demonstrating how the technology affects physicians and patients. As a result, its flaws are getting exposed on the front lines of care by doctors and researchers who say that the system, while promising in some respects, remains undeveloped.\n\nI thought the article was very well-written, covering many angles and getting into the details. I would encourage reading the entire thing.\nWhile many issues are covered, I bumped on this one quote about the amount of investment (or lack thereof) that IBM is putting into Watson.\n\n[Peter] Greulich said IBM needs to invest more money in Watson and hire more people to make it successful. In the 1960s, he said, IBM spent about 11.5 times its annual earnings to develop its mainframe computer, a line of business that still accounts for much of its profitability today. If it were to make an equivalent investment in Watson, it would need to spend $137 billion. “The only thing it’s spent that much money on is stock buybacks,” Greulich said.\n\nThat Watson hasn’t cured cancer overnight—or lived up to its own marketing material—should come as no surprise. But I wonder if people are nevertheless up to investing in what it would take to find a cure.\nI’ve often heard of curing cancer as a “moon shot”, in reference to the Apollo program to land a man on the moon and return him safely to Earth. But that project cost a lot of money, about $112 billion in today’s dollars, or about $11 billion per year over the roughly 10 year span of the project. The National Cancer Institute’s annual budget is about $5 billion—so, it’s less. Also, I would argue we know substantially less about human cancer now than we did about physics and space in 1959 (although there was plenty to learn back then too). How much additional money would we need to make up for that lack of base knowledge?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:29:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-09-04-deep-dive-ogata/",
    "title": "Deep Dive - Y. Ogata's Residual Analysis for Point Processes",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-09-04",
    "categories": [],
    "contents": "\nFor a long time now—actually ever since we started the blog—I’ve wanted to do a series of deep dives into specific papers that I thought were great. Clearly, it’s taken a bit longer than I expected, but I figure better late than never. Actually, that’s become a bit of a theme for my work these days!\nOne problem I have with much academic writing on the Internet is that I feel like most of it is devoted to (1) promoting one’s own work; or (2) identifying weaknesses in others’ work. Now, there’s absolutely nothing wrong with either activity—both are essential to the functioning of science. But I think there’s room for more activity. In particular, I think putting up examples of good work, and explaining why they are good, is an important part of producing more good work in the community. I hope to make this entry the first in a series, but we’ll see if I can sustain it. Seeing as this is a statistics/data science blog most of the papers will be statistically oriented. But I may try to throw in a few biomedical/environmental papers if I can.\nThe first paper I want to write about it is one that had a huge influence on me when I was a grad student, many year ago, doing research on point process models. This is Yosihiko Ogata’s paper Statistical models for earthquake occurrences and residual analysis for point processes, originally published in the Journal of the American Statistical Association in 1988 (there is a pdf here). I haven’t picked up this paper in quite some time as I no longer do research in point process models, but it remains remarkable in my mind. So let’s get into it.\nA Road Map for Data Analysis\nOne immediately important aspect of this paper is that it presents a road map for how to apply point process models to real data, using a formal likelihood-based inference framework. This may sound unremarkable, but in the point process area, there was a lot of focus on (1) theoretical properties of various estimators; and (2) ad hoc fitting of models to data via 2nd-order methods. A lot of the methodologies in point process modeling were translated from time series, which involved a lot of 2nd order correlation methods. Likelihood methods were far less common, in part because they involved substantial computation. For people who are interested in applying point process models to data to make scientific inferences, I think this is still a great paper to read. In fact, I think it’s a great paper to read if you’re interested in data analysis at all, for any kind of model.\nScientific Theory\nOgata’s paper is focused on a theory of earthquakes that posits that there are main shocks followed by aftershocks. After a long decay in aftershock occurrences, there is a period of “quiescence”, after which there are foreshocks, followed by another main shock. The problem is, of course, that nature is not kind enough to label all these shocks as “main shock”, “aftershock” and “foreshock”. We need a model to help us infer which is which. The primary goal lying in the background is predicting the next main shock.\nOgata reviews some of the fundmental theory for earthquakes. There’s the famous Gutenberg-Richter law (equation 2) which explains the distribution of earthquake magnitudes, and the modified Omori law (equation 1) which explains the frequency of aftershocks. Given these two laws, Ogata puts them together in an “epidemic-type” conditional intensity model (equation 10). This model can be thought of as describing the instantaneous rate at which earthquakes occur at any given time. If the model is correct, we can predict the rate at which an earthquake will occur at any time (along with it magnitude), given the entire history of earthquakes to date. These kinds of models would later be described as epidemic-type aftershock sequence, or ETAS, models.\nThe bottom line here is that the ultimate model presented in equation 17 of the paper is rooted in formally hypothesized theories of how earthquakes work along with interpretable parameters. These theories may or may not be correct, but they are presented and modeled in a manner where we can determine whether evidence in the data favor them or not. In other words, we have a good chance of knowing whether the theory/model fits the data or not. This aspect is key—what’s important is not whether your model is correct or not, but whether you know if your model is correct or not. Not every field is so lucky to have parsimoniously specified physical models to go on. But the truth is, if the models are incorrect, or do not fit the data, then the fact that they are parsimonious is not that helpful. Nevertheless, it’s worth trying to develop a model that we can understand, and which will teach us something about the underlying phenomena, in this case how earthquakes occur.\nThe Data\nBefore getting into how to fit the model to data, Ogata presents the data—literally. This may seem quaint in today’s big data era, but Figure 1 and Table 1 show all the data. Table 1 takes 2 pages just to print out (there was no supplementary material back then). But there’s something very visceral and satisfying about being able to flip through the data like that. I feel like I notice details that I never would have noticed in a single plot or a table of summary statistics. The dataset is 483 earthquakes with magnitude 6 or more from 1885–1980.\nFigures 2 and 3 show more summaries of the data. Figure 3 in particular suggests a reasonable adherence to the Gutenberg-Richter law of magnitudes (i.e. straight line on a log-plot indicating an exponential distribution). Figures 4, 5, and 6 are designed to show whether the earthquake process is purely random (Poisson) or exhibits some clustering behavior. Suffice it to say that all plots show that there is a lot of clustering (non-Poisson). Under the Poisson assumption, Figure 4 should be a straight line, Figure 5 would follow the solid line, and Figure 6 would just be a horizontal scatter (without the little spike up near zero).\nThis section is short, but Ogata lets the data do some talking. He couples the literal presentation of the data with some summary statistics, some of which are highly specific to point process data. But ultimately, he takes a light touch to the summarization and tries to present as many angles as feasible/useful to the reader. This is one of my favorite aspects of this paper—the careful curation of how the data are presented so that you might best assess how the data eventually do or do not fit the hypothesized models.\nThe Modeling\nSection 3.2 shows the log-likelihood of the model and how we can compare models using AIC. There’s nothing super novel here, except that maximizing the likelihood must have been a pain in the butt at the time.\nResidual Analysis\nSection 3.3 is the heart and soul of this paper, in my view. Ogata explains that while AIC can be useful for comparing a set of models, it’s not useful for determining whether there is a better model outside that set. Here, we need the point process equivalent of residual analysis for regression models. The problem is that at the time there really wasn’t a settled understanding of what residual analysis was for point process models (or even what constituted a “residual”).\nOgata uses a theoretical result of Papangelou that says that you can shift the time points of the earthquake occurrences in a 1-to-1 manner dictated by the model so that the rescaled time points follow a Poisson process with rate 1. A Poisson process with constant rate 1 is the “white noise” of point process modeling. So, in other words, if your model captures the main features of the data, then the rescaled time points, which we will call the residuals, should essentially be noise. The nice thing about this approach is that there are already a bunch of tests designed to determine if a point process is Poisson or not. So we can apply that battery of tests to the residuals to see if we are missing anything. Furthermore, if we detect anything unusual in the residuals, we can easily map them back to the original data to see what was going on.\nFigures 9-15 look at the residuals carefully and most indicate that the residuals look Poisson. However, Figure 13(a) suggests a violation of the variance assumption, so Figure 15(3a) is used to diagnose that problem. It seems there was an unusual “swarm” of earthquakes in 1938—when these are removed, the residuals appear Poisson. Of course, the model will need to be re-examined in light of its inability to model this “swarm”, but the point is that these residual methods allowed us to easily identify this “outlying” group of earthquakes.\nInterestingly, the way that Ogata structures the article sets us up for this moment. In Section 3.1 he shows all these plots of the raw data and concludes that the data are highly clustered and definitely not Poisson. Later, in the residual analysis section, he can show us the exact same series of plots and say, “Look, they’re Poisson now!” Or at least, almost Poisson.\nIn this section, Ogata develops an entirely new tool for assessing point process models and shows how it can be used to identify interesting aspects of the data that are not captured by existing theories or models, even after evaluating a set of models with AIC. The thoughtful treatment of residuals and the visualization of how they do or do not adhere to the Poisson assumption is important for allowing the reader to evaluate for themselves the adequacy of the model.\nSummary\nThe remainder of the paper is an assessment of the theory of “quiescence”, which I will skip as this entry is long enough. In summary, I think this is a great applied statistics and data analysis paper. Some of the things that stand out to me are\nThere is a strong reliance on the theory of earthquakes, and Ogata constantly references those theories in each section as he evaluates the data. In other words, the point of the analysis is to develop evidence for or against the theory. Note that there’s very little emphasis on formal hypothesis testing. Nevertheless, Ogata puts the reader in a position where they can judge for themselves how well the models fit the data.\nOgata goes through great lengths to present the data, plainly yet informatively. This is not a data dump. I think the reason he can present the data in this informative manner is again because he is guided by the theory and therefore has a prioritization of what is or is not important to show.\nIn a way, Ogata buries the lede in this paper. He waits until Section 3.3 to introduce the idea of residual analysis for point process models, which in reality was a totally new applied statistics method. He could have written an entire paper about that approach. But ultimately, I think in his mind this paper was about earthquakes and earthquake models. Statisticians routinely have to decide whether a paper is about the method or the application—there is no right or wrong answer. Regardless, Ogata didn’t let the novelty of the method get in the way of answering the primary question.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:29:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-08-29-data-science-on-a-chromebook/",
    "title": "Data Science on a Chromebook",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2017-08-29",
    "categories": [],
    "contents": "\nAbout nine months ago I announced that I was attempting a Chromebook experiment for the 2nd time. At first I thought it was going to be a short term experiment just to see if it was possible to function with only a Chromebook. But in an interesting twist I got used to it and have been working exclusively on a Chromebook for the last few months since the experiment started.\nI set myself the following requirements:\nI could only use Chrome OS - no installing/booting to Linux\nI couldn’t use another computer for any task\nI had to be “fully cloudy” in the sense that I didn’t run any additional hardware\nOne of the reasons I did this was I wanted to see if it was possible to be a functioning/day to day data scientist without using an expensive laptop. This is part of a broader experiment I’m just beginning on how to democratize data science education.\nI’m not going to go into extreme detail on how I set everything up here (more on that in a second) but I thought I’d describe my Chromebook set up that I’ve been using for the last couple of months.\nI have been using two Samsung Chromebook Plus computers, one of which I keep at home and one which I keep at work. One of the best parts about the fully cloudy/Chrome OS requirement is this means that from the user perspective everything is always in sync. I log off the computer at home, come to work, log on and its like I’m on the same computer.\nI thought I’d just go through at a high level the software I’m using to keep everything running.\nGoogle Slides for presentations - (Cost:free) For the most part this has been really easy and is a smooth transition from Powerpoint. One thing I’ve found really useful is the laser pointer mode of the Chromebook plus for highlighting things on screen when presenting. I have also found that since they are using USB-C adapters I can participate in dongle communism with Apple users. I had to figure out the display mirroring menus in Chrome OS but after that this was easy.\nGoogle Docs/Paperpile for writing - (Cost:free) This works great and has been my work flow as I describe in my book since before the Chromebook experiment started.\nDocHub for signing things - (Cost:$4.99/month/billed yearly) Often I have to “sign” a document by adding my electronic signature. I used the note feature to create a jpeg of my signature. I can then upload the file to Docub\nOverleaf for writing latex - (Cost:free or $10/month/billed yearly) This is not necessary for all data scientists, but it has some nice features, including when I could live write a grant and people could watch.\nGmail for email - (Cost:free) this one is pretty obvious.\nGoogle Sheets for data - (Cost:free) this is again a choice I had been making frequently before I moved to Chromebooks. The googlesheets R package lets you do all sorts of cool things with google sheets.\nDigital Ocean for Rstudio - _(Cost: $20/mo)__ I set up an Rstudio server and run it remotely on Digital Ocean. I currently use the $20/month option but sometimes scale it up or down as needed. One great thing about the dockerized version of the software is that I can pause the instance, scale up the compute infrastructre, restart and everything is just as I left it but with more computational horsepower. I can then use that for a few hours as needed and scale back down. I use the terminal in Rstudio for most of my management of code/etc. on Github.\nGoogle Hangouts for video conferencing - (Cost:free) this is the default but honestly I wish I had a better option. I often find it complicated and laggy to work with, but still mostly better than Skype. Would be open to suggestions on that front.\nSlack for communication (Cost: $6.67/month) a variety of different teams here at JHU and around the country use Slack for group communication. I use it through the web browser, although the Chromebook Plus allows you to install Android apps.\nGoogle Music for listening to music/podcasts (Cost:$10/month) This is an unnecesary expense but I like having something to listen to while I work.\nTweetdeck for twitter - (Cost:free) I have a couple of accounts I manage and I do this through the web browser. For the most part this works great.\nSo my total monthly cost comes to something like $35 a month for various cloud services. At first doing this was sort of like writing a Haiku. I could still write, but the constraints made me think hard about how I did things. But after a while I have gotten so used to the form that it feels natural and I don’t miss my (really expensive) Apple products anymore.\nThe biggest headaches have been:\nWifi connectivity issues - this isn’t as big as I thought it would be, most places have wifi where I work and it is mostly ok. When I have trouble I stream from my phone.\nWifi blocking my DO server - this one has been a headache. I think if I just got a custom domain for the webserver and didn’t just use the IP address I could avoid it. When I have trouble I stream from my phone.\nhttr and Rstudio on a server - when I need to authenticate for websites I have run into trouble, but if I set httr_oob_default==TRUE (documentation here) then the Oauth process generates a code I can paste into my server.\nBeyond that it has actually been pretty straightforward to do almost anything I need. Stay tuned because this experiment has inspired a broader effort we are doing with Chromebooks here at the JHU Data Science Lab. If you want to hear about this effort as it gets underway, sign up for our weekly newsletter and you’ll be the first to hear new announcements.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:29:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-08-28-simple-queue-package-for-r/",
    "title": "Simple Queue Package for R",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-08-28",
    "categories": [],
    "contents": "\nI recently coded up a simple package that implements a file-based queue abstract data type. This package was needed for a different package that I’m working on involving parallel processing (more on that in the near future). Actually, this othe package is a project that I started almost nine years ago but was never able to get off the ground. I tried to implement a queue interface in the filehash package but it never served the purpose that I needed.\nRecently, I came across Rich FitzJohn’s thor package, which provides an interface to the LMDB database, which is a light-weight key-value style database. When I saw this, I realized that it was exactly what I needed because it supports transactions with blocking. So if multiple processes attempt to write to the database at once, it will block the other processes untill the currently writing one is finished, and then move on to the next one.\nThe code for the queue package is available on GitHub and can be installed in the usual way via devtools:\n\n\ndevtools::install_github(\"rdpeng/queue\")\n\n\nThe package just involves four functions:\nenqueue(): add an element to the queue\ndequeue(): remove an element from the queue (and return it)\npeek(): return the next element of the queue (but don’t remove it)\nis_empty(): is the queue empty:\nThere are also functions\ncreate_queue: create the queue and associated files on the disk\ninit_queue: initialize a queue that has been previously created\nEverything is implemented as S3 classes and methods. Queues are just sub-directories on the filesystem that contain some metadata for the underlying LMDB database.\nYou can create a queue with the create_Q() function.\n\n\nlibrary(queue)\nx <- create_queue(\"testq\")\nis_empty(x)\n\n[1] TRUE\n\nYou can then add arbitrary objects to the queue with enqueue(). Behind the scenes, objects are serialize()-ed. \n\n\nenqueue(x, \"hello\")\nenqueue(x, 1)\npeek(x)\n\n[1] \"hello\"\n\nFinally, you can remove an object from the front of the queue with dequeue().\n\n\nobj <- dequeue(x)\nobj\n\n[1] \"hello\"\n\npeek(x)\n\n[1] 1\n\nThe implementation is pretty basic right now but the guts of it are there. I don’t plan to add any more features, but hopefully will round out the documentation and add some tests.\n~/myDocuments/git/simplystats_distill/_posts/2017-08-28-simple-queue-package-for-r/index.Rmd\n\n\n\n",
    "preview": {},
    "last_modified": "2022-12-27T15:39:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-08-08-code-for-my-educational-gifs/",
    "title": "Code for my educational gifs",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2017-08-08",
    "categories": [],
    "contents": "\nDuring preparation for class I sometimes think up of animations that will explain the concept I am teaching. I sometimes share the resulting animations on social media via @rafalab.\nJohn Storey recently asked if the source code is publicly available. Because I am not that organized, and these ideas come about during last minute preparations, the code was spread across several unrelated files. John’s request motivated me to include the code in one post.\nAll these gifs are paginated R plots. You will see in the code that I used different approaches to converting individual plots to animated gifs. The first (not recommended) was to save files then do a system call to the ImageMagick convert tool. Through a simplystats comment, from Yihui Xie, I learned about the saveGIF function from the animation package, which is what I now use when the plots are made in R base. When using ggplot I use David Robinson’s gganimate package. Finally, if I want to add special effects, like phasing, I use the online Animated GIF maker.\nBelow is the code for each of the gifs I have shared roughly ordered by popularity. Remember this code was written last minute so please don’t judge me. Actually, you can critique all you want, that’s how we learn.\nSimpson’s Paradox\nThis gif illustrates Simpson’s paradox. We see that \\(X\\) and \\(Y\\) have strong negative correlation. However, once we stratify by a confounder \\(Z\\), encoded with color, the correlations flip to positive in each strata. The data is simulated, but we could see data like this if, for example, we looked at tutoring \\(X\\) and 9th grade test score \\(Y\\) data and then stratified students by their 8th grade test scores \\(Z\\).\n\n\n\nThis gif is made up of just three plots. I saved them using RStudio’s Export tool then used Animated GIF maker to create the gif. Here is the code for the three plots:\n\n\nlibrary(tidyverse)\nlibrary(dslabs)\nds_theme_set()\n\n## simulate data \nN <- 100\nSigma <- matrix(c(1,0.75,0.75, 1), 2, 2)*1.5\nmeans <- list(c(11,3), c(9,5), c(7,7), c(5,9), c(3,11))\ndat <- lapply(means, function(mu) \n  MASS::mvrnorm(N, mu, Sigma))\ndat <- tbl_df(Reduce(rbind, dat)) %>% \n  mutate(Z = as.character(rep(seq_along(means), each = N)))\nnames(dat) <- c(\"X\", \"Y\", \"Z\")\n\n## First plot\ndat %>% ggplot(aes(X,Y)) + geom_point(alpha = .5) +\n  ggtitle(paste(\"correlation = \", round(cor(dat$X, dat$Y), 2))) \n\n## second plot\nmeans <- tbl_df(Reduce(rbind, means)) %>% setNames(c(\"x\",\"y\")) %>%\n  mutate(z = as.character(seq_along(means)))\n  \ncorrs <- dat %>% group_by(Z) %>% summarize(cor = cor(X,Y)) %>% .$cor \n\np <- dat %>% ggplot(aes(X, Y, color = Z)) + \n  geom_point(show.legend = FALSE, alpha = 0.5) +\n  ggtitle(paste(\"correlations =\",  paste(signif(corrs,2), collapse=\" \")))\np\n\n## third plot\np + annotate(\"text\", x = means$x, y = means$y, \n             label = paste(\"Z=\", means$z), cex = 5)  \n\n\n\nLoess\nThe first educational animation I shared explains how local regression (loess) works. Basically, for each predictor value, say \\(x_0\\), assign positive weights to points close to that value, fit a line with weighted regression, keep the fitted value for \\(x_0\\), move to the next point.\n\n\n\nThe data here comes from a microarray experiment. The figure shows an MA-plot (log ratio versus average of logs). I use the animation package to save the gif.\n\n\nrafalib::mypar()\n\n## load data\nlibrary(SpikeIn) ##to install: rafalib::install_bioc(\"SpikeIn\")\ndata(SpikeIn95) \n\n## get M and A for two arrays\nspms <- pm(SpikeIn95)\ni <- 10; j  <- 9 ## Pick two samples that show a non-linear relationship\nM <- log2(spms[,i]) - log2(spms[,j])\nA <- (log2(spms[,i]) + log2(spms[,j]))/2\no <- order(A)\na <- A[o]\nm <- M[o]\nind <- round(seq(1,length(a),len=5000)) ##don't show all the points\na <- a[ind]\nm <- m[ind]\n\n## make gif\ncenters <- seq(min(a),max(a),0.1)\nwindowSize <- 1.5\nsmooth <- rep(NA,length(centers))\nlibrary(animation)\nsaveGIF({\n  for(i in seq(along=centers)){\n    center <- centers[i]\n    ind=which(a > center-windowSize & a<center+windowSize)\n    fit<-lm(m~a, subset=ind)\n    smooth[i]<-predict(fit, newdata=data.frame(a=center))\n    if(center<12){\n      plot(a, m, ylim=c(-1.5,1.5), col=\"grey\")\n      points(a[ind], m[ind])\n      abline(fit, col=3, lty=2, lwd=2)\n      lines(centers[1:i], smooth[1:i], col=2, lwd=2)\n      points(centers[i], smooth[i], col=2, pch=16)\n    }\n  }\n},'loess.gif', interval = .15)\n\n\n\nLife Expectancy versus Fertility Rates\nThis gif is recreating an animation shown by Hans Rosling’s in his talk New Insights on Poverty. The point of the animation is to show the power of data visualization for combating misconceptions. In this particular instance Hans Rosling shows that the world was more dichotomous 40 years ago than it is today. Dividing the world into western rich countries with small families/long life spans and a developing world with large families/short life spans is no longer accurate.\n\n\n\nThe code for this plot is quite simple, thanks to the gganimate package.\n\n\nlibrary(tidyverse)\nlibrary(dslabs)\ndata(gapminder)\n\nwest <- c(\"Western Europe\",\"Northern Europe\",\"Southern Europe\",\n          \"Northern America\",\"Australia and New Zealand\")\n\ngapminder <- gapminder %>% \n  mutate(group = case_when(\n    .$region %in% west ~ \"The West\",\n    .$region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n    .$region %in% c(\"Caribbean\", \"Central America\", \"South America\") ~ \"Latin America\",\n    .$continent == \"Africa\" & .$region != \"Northern Africa\" ~ \"Sub-Saharan Africa\",\n    TRUE ~ \"Others\"))\ngapminder <- gapminder %>% \n  mutate(group = factor(group, levels = rev(c(\"Others\", \"Latin America\", \"East Asia\",\"Sub-Saharan Africa\", \"The West\"))))\n\nlibrary(gganimate)\ntheme_set(theme_bw(base_size = 16))\nyears <- seq(1962, 2013)\np <- filter(gapminder, year%in%years & !is.na(group) & \n         !is.na(fertility) & !is.na(life_expectancy)) %>%\n  mutate(population_in_millions = population/10^6) %>%\n  ggplot(aes(fertility, y=life_expectancy, \n             col = group, frame = year, size = population_in_millions)) +\n  geom_point(alpha = 0.8) +\n  guides(size=FALSE) +\n  theme(plot.title = element_blank(), legend.title = element_blank()) + \n  coord_cartesian(ylim = c(30, 85)) + \n  xlab(\"Fertility rate (births per woman)\") +\n  ylab(\"Life Expectancy\") + \n  geom_text(aes(x=7, y=82, label=year), cex=20, color=\"grey\") \n\ngganimate(p, filename = \"life-expectancy-vs-fertility-rate\", title_frame = FALSE)\n\n\n\nUnited Nations Voting Patterns\nHere we used UN voting data provided by Erik Voeten and Anton Strezhnev to illustrate the concept of distance.\n\n\n\nBelow is the code. The wrangling code was provided by David Robinson. You will see that we smooth the distances across time to avoid having the points jump around too much.\n\n\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(countrycode)\nlibrary(lubridate)\nlibrary(cluster)\nlibrary(broom)\nlibrary(ggrepel)\ntheme_set(theme_bw(base_size=16))\n\n## Read in data and add country names. Change long names.\n\nvotes <- read_delim(\"https://github.com/datasciencelabs/data/raw/master/rawvotingdata13.tab\", delim=\"\\t\")\n\nvotes <- votes %>% filter(vote <= 3) %>%\n  mutate(country = countrycode(ccode, \"cown\", \"country.name\"),\n         continent = countrycode(ccode, \"cown\", \"continent\"),\n         region = countrycode(ccode, \"cown\", \"region\")) \n  \n## some wrangling\nmapping <- c(\"United States of America\"=\"USA\",\n          \"United Kingdom of Great Britain and Northern Ireland\"=\"UK\",\n          \"Republic of Korea\"=\"South Korea\",\n          \"Democratic People's Republic of Korea\"=\"North Korea\",\n          \"Lao People's Democratic Republic\"=\"Laos\",\n          \"Yemen People's Republic\"=\"South Yemen\",\n          \"Saint Vincent and the Grenadines\"=\"Saint Vincent\",\n          \"Congo\"=\"Congo Republic\",\n          \"Federal Republic of Germany\"=\"Germany\",\n          \"Russian Federation\"=\"Russia\")\n\nvotes <- votes %>% \n  mutate(country = plyr::revalue(country, mapping)) %>%\n  separate(country, into = c(\"country\", \"extra\"), sep=\",\", \n           fill=\"right\")\n\n## Read vote descriptions. Mainly to get \"important\" ones. Join with original data.\n\nurl <- \"https://raw.githubusercontent.com/datasciencelabs/data/master/un-resolutions-descriptions.csv\"\ndescriptions <- read_csv(url,\n                         col_types = list(date = \n                                            col_date(\"%m/%d/%y\")))\n##from warning and looking at csv we see line 1843 has an extra \"\ndescriptions[1483,\"ec\"] <-0\n\ny <- year(descriptions$date)\nyear(descriptions$date) <- ifelse(y > 2030, y - 100, y)\n\ndescriptions <- descriptions %>%\n  filter(year(date) != 2015)\n\nvotes <- votes %>%\n  inner_join(descriptions) %>%\n  select(-yes, -no, -abstain)\n\n## Computing Distance by year\n\n##Start by keeping only countries that have voted enough\nvotes <- mutate(votes, year = year(date))\ncountries_to_keep <- votes %>% filter(!is.na(continent)) %>%\n  group_by(country) %>% \n  summarize(n=n()) %>% \n  mutate(percent = n/max(n)) %>% filter(percent>0.1 | country==\"North Korea\") %>%\n  .$country\n\nget_dist <- function(dat, min_vote=0.70){\n  year <- dat$year  \n  X <- select(dat, -year, -rcid) %>% as.matrix() %>% t()\n  voted <- rowMeans(!is.na(X))\n  index <- which(voted>min_vote)\n  d <- daisy(X[index,], metric=\"gower\")\n  mds <- cmdscale(d)\n  ## fix arbitray sign\n  if(mds[\"USA\",1]<0) mds[,1]<-  -mds[,1]\n  if(mds[\"USA\",2]<0) mds[,2]<-  -mds[,2]\n    data.frame(country=rownames(mds), PC1=mds[,1], PC2=mds[,2],\n             row.names = NULL, stringsAsFactors = FALSE)\n}\n\n## us loess to smooth distances\ndat <- votes %>% filter(country%in%countries_to_keep) %>% \n  select(rcid, year, vote, country) %>%\n  spread(country, vote) %>%\n  group_by(year) %>%\n  do(get_dist(.)) %>% ungroup() %>% \n  group_by(country) %>%\n  do(augment(loess(PC1~year, span=1/3, degree=1,data=.), data=.)) %>%\n  mutate(PC1=.fitted) %>% select(-.fitted,-.se.fit) %>%\n  do(augment(loess(PC2~year, span=1/3, degree=1,data=.), data=.)) %>%\n  mutate(PC2=.fitted) %>% select(-.fitted,-.se.fit)\n  \n##add back contininent\ndat <- votes %>% group_by(country) %>% \n  summarize(continent = continent[1], region=region[1]) %>%\n  right_join(dat, by=\"country\")\n\n##fix germany\ndat <- dat %>% \n  mutate(continent=ifelse(country==\"Germany\", \"Europe\", continent))\n\n## create regions\ndat <- dat %>% mutate(group = continent) %>%\n  mutate(group = ifelse(region%in% c(\"Western Europe Eastern Europe\",\"Caribbean\",\"Southern Asia\"), region, group))\n\n## labels for select countries\nlabels <- c(\"Poland\",\"France\",\"USA\",\"Russia\",\"China\",\"Germany\",\"Cuba\",\"Israel\",\"Canada\",\"Libya\",\"North Korea\",\"UK\",\"Iran\", \"Australia\")\n\n## Now we make animation\n\np <- dat %>% ggplot(aes(frame = year)) +\n  geom_point(aes(PC1, PC2, fill=group),\n             pch=21, cex=4, alpha=.75) +\n  geom_text_repel(data = filter(dat,country%in%labels),\n                  aes(PC1,PC2,label=country),\n                  cex=5) +\n  theme(axis.ticks = element_blank(), axis.text = element_blank()) +\n  coord_fixed(ratio = 1) +\n  geom_text(aes(x=-0.38, y=0.35, label=year), \n            cex=30, color=\"grey\") +\n  guides(fill=guide_legend(title=NULL))+\n  theme(plot.title = element_blank())\n  \n\ngganimate(p, filename = \"un-dist.gif\", interval=0.35, ani.width = 850)\n\n\n\nRandom Forests\nI used to find it hard to understand how Random Forests can produce smooth estimates given that they are based on trees. The gif helps illustrate how this can happen. I use 2008 presidential election data because I assume it is mostly driven by a smooth trend but with a couple of sharp edges that loess, for example, won’t catch. Note that, because we only have one predictor, the gif does not illustrate another important feature of Random Forests: how the random feature selection reduces correlation between trees.\n\n\n\nIn the code you will see that I am using the old, not recommended way, of saving files and using a system call to convert.\n\n\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(tidyr)\nlibrary(XML)\nlibrary(rpart)\nlibrary(randomForest)\n\n## read in 2008 election data\ntheurl <- paste0(\"http://www.pollster.com/08USPresGEMvO-2.html\")\npolls_2008 <- readHTMLTable(theurl,stringsAsFactors=FALSE)[[1]] %>%\n  tbl_df() %>%\n  separate(col=Dates, into=c(\"start_date\",\"end_date\"), sep=\"-\",fill=\"right\") %>%\n  mutate(end_date = ifelse(is.na(end_date), start_date, end_date)) %>%\n  separate(start_date, c(\"smonth\", \"sday\", \"syear\"), sep = \"/\",  convert = TRUE, fill = \"right\")%>%\n  mutate(end_date = ifelse(str_count(end_date, \"/\") == 1, paste(smonth, end_date, sep = \"/\"), end_date)) %>%\n  mutate(end_date = mdy(end_date))  %>% mutate(syear = ifelse(is.na(syear), year(end_date), syear + 2000)) %>%\n  unite(start_date, smonth, sday, syear)  %>%\n  mutate(start_date = mdy(start_date)) %>%\n  separate(`N/Pop`, into=c(\"N\",\"population_type\"), sep=\"\\ \", convert=TRUE, fill=\"left\") %>%\n  mutate(Obama = as.numeric(Obama)/100,\n         McCain=as.numeric(McCain)/100,\n         diff = Obama - McCain,\n         day=as.numeric(start_date - mdy(\"11/04/2008\")))\npolls_2008 <-  filter(polls_2008, start_date>=\"2008-06-01\") %>%\n  group_by(X=day)  %>%\nsummarize(Y=mean(diff))\n\nset.seed(1)\npath <- tempdir()\nntrees <- 50\nsum <- rep(0,nrow(polls_2008))\nres <- vector(\"list\", ntrees)\nXLIM <- range(polls_2008$X)\nYLIM <- range(polls_2008$Y)\npath <- tempdir()\nfor(i in 0:ntrees){\n  png(file.path(path,sprintf(\"plot%02d.png\",i)), width = 480, height = 350)\n  rafalib::mypar(1,1)\n  if(i==0){\n    with(polls_2008, plot(X, Y, pch = 1, main=\"Data\", xlim=XLIM,\n                          ylim=YLIM,\n                          xlab = \"Days\", ylab=\"Obama - McCain\"))\n  } else{\n    ind <- sort(sample(1:nrow(polls_2008), replace = TRUE))\n    tmp <- polls_2008[ind,]\n    fit <- rpart(Y~X, data = tmp)\n    pred <- predict(fit, newdata = tmp)\n    res[[i]] <- data_frame(X = tmp$X, Y=pred)\n    pred <- predict(fit, newdata = polls_2008)\n    sum <- sum+pred\n    avg <- sum/i\n    with(tmp, plot(X,Y, pch=1, xlim=XLIM, ylim=YLIM, type=\"n\",\n                   xlab = \"Days\", ylab=\"Obama - McCain\",\n                   main=ifelse(i==1, paste(i, \"tree\"),paste(i, \"trees\"))))\n    for(j in 1:i){\n      with(res[[j]], lines(X, Y, type=\"s\", col=\"grey\", lty=2))\n    }\n    with(tmp, points(X,Y, pch=1))\n    with(res[[i]], lines(X, Y, type=\"s\",col=\"azure4\",lwd=2))\n    lines(polls_2008$X, avg, lwd=3, col=\"blue\")\n  }\n  dev.off()\n}\nfor(i in 1:5){\n  png(file.path(path,sprintf(\"plot%2d.png\",ntrees+i)), width = 480, height = 350)\n  rafalib::mypar(1,1)\n  with(polls_2008, plot(X, Y, pch = 1, main=\"Final\", xlim=XLIM, ylim=YLIM,\n                          xlab = \"Days\", ylab=\"Obama - McCain\"))\n  lines(polls_2008$X, avg, lwd=3, col=\"blue\")\n  dev.off()\n}\nsystem(paste0(\"cd \",path,\";convert -loop 0 -delay 50 *.png rf.gif; mv rf.gif \",getwd(),\"/\"))\n\n\n\nEcological Fallacy\nAfter sharing the Simpson’s Paradox gif, a couple of people asked me if this was the same as the ecological fallacy. These two are different. The ecological fallacy is when we extrapolate high correlation seen for the average of strata to individuals. To illustrate this I used data from gapminder included in the dslabs package. It shows logistic transformed infant survival rates versus log daily income. I start by showing a very high correlation at the region level and a lower correlation at the individual country level. This is because there is country to country variability within region.\n\n\n\nThe gif is just three plots. I saved them using RStudio’s Export tool used Animated GIF maker to create the gif.\nThe first shows the averages, the second shows the individual values for Sub-Saharan Africa so you can see how one average breaks into more variable data, and the third shows all the individual data. I highlighted a few countries that show the variability. Note that I used a colorblind friendly palette. The code is a bit complex because I have to wrangle the Gapminder data.\n\n\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(dslabs)\nds_theme_set()\n\n## load data\ndata(gapminder)\n\nwest <- c(\"Western Europe\",\"Northern Europe\",\"Southern Europe\",\n          \"Northern America\",\"Australia and New Zealand\")\npresent_year <- 2010\n\n\n## wrangle data and summarize \ngapminder <- gapminder %>%\n  mutate(dollars_per_day = gdp/population/365)\n\ngapminder <- gapminder %>%\n  mutate(group = case_when(\n    .$region %in% west ~ \"The West\",\n    .$region %in% \"Northern Africa\" ~ \"Northern Africa\",\n    .$region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n    .$region == \"Southern Asia\"~ \"Southern Asia\",\n    .$region %in% c(\"Central America\", \"South America\", \"Caribbean\") ~ \"Latin America\",\n    .$continent == \"Africa\" & .$region != \"Northern Africa\" ~ \"Sub-Saharan Africa\",\n    .$region %in% c(\"Melanesia\", \"Micronesia\", \"Polynesia\") ~ \"Pacific Islands\"))\n\nlogit <- function(p) log(p/(1-p))\n\ncolor_blind_friendly_cols <- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nsurv_income <- gapminder %>%\n  filter(year %in% present_year & !is.na(gdp) & !is.na(infant_mortality) & !is.na(group)) %>%\n  group_by(group) %>%\n  summarize(income = sum(gdp)/sum(population)/365,\n            infant_survival_rate = 1-sum(infant_mortality/1000*population)/sum(population))\n\n### First plot\nsurv_income %>% ggplot(aes(income, infant_survival_rate, label = group, color = group)) +\n  scale_x_continuous(trans = \"log2\", limits = c(0.25, 150)) +\n  scale_y_continuous(trans = \"logit\", limit = c(0.875, .9981),\n                     breaks = c(.85,.90,.95,.99,.995,.998)) +\n  geom_text_repel(size = 5, show.legend = FALSE) +\n  geom_point(size=4, show.legend = FALSE) +\n  ggtitle(paste(\"Correlation ≈\", signif(with(surv_income,cor(log(income), logit(infant_survival_rate))),2))) +\n  xlab(\"Average dollars per day\") +\n  ylab(\"Infant survival rate\") +\n  scale_color_manual(values=color_blind_friendly_cols)\n\n\nhighlight <- c(\"Sierra Leone\", \"Mauritius\",  \"Sudan\", \"Botswana\", \"Tunisia\",\n\"Cambodia\",\"Singapore\",\"Chile\", \"Haiti\", \"Bolivia\",\n\"United States\",\"Sweden\", \"Angola\", \"Serbia\")\n\n## Third plot. I make before second because easier \ntmp <- gapminder %>%\n  filter( year %in% present_year & !is.na(gdp) & !is.na(infant_mortality) & !is.na(group) ) %>%\n  mutate(infant_survival_rate = 1 - infant_mortality/1000)\ntmp %>%\n  ggplot(aes(dollars_per_day, infant_survival_rate, col = group, label = country)) +\n  scale_x_continuous(trans = \"log2\", limits=c(0.25, 150)) +\n  scale_y_continuous(trans = \"logit\",limit=c(0.875, .9981),\n                     breaks=c(.85,.90,.95,.99,.995,.998)) +\n  geom_point(alpha = 0.5, size = 3, show.legend = FALSE) +\n  geom_text_repel(size = 4, show.legend = FALSE,\n                  data = filter(tmp, year %in% present_year & country %in% highlight)) +\n  ggtitle(\"Correlation < 1\") +\n  xlab(\"Average dollars per day\") +\n  ylab(\"Infant survival rate\") +\n  scale_alpha_manual(values=c(1,1)) +\n  scale_color_manual(values=color_blind_friendly_cols) +\n  guides(colour = guide_legend(override.aes = list(alpha = 1)))\n\n## second plot\ntmp <- gapminder %>%  filter( year %in% present_year & !is.na(gdp) & !is.na(infant_mortality) & !is.na(group) ) %>%\n    filter(group %in% \"Sub-Saharan Africa\") %>%\n    mutate(infant_survival_rate = 1 - infant_mortality/1000)\n  \nsurv_income %>%\n  filter(!group %in% \"Sub-Saharan Africa\") %>%\n    ggplot(aes(income, infant_survival_rate, label = group, color = group)) +\n  scale_x_continuous(trans = \"log2\", limits = c(0.25, 150)) +\n  scale_y_continuous(trans = \"logit\", limit = c(0.875, .9981),\n                     breaks = c(.85,.90,.95,.99,.995,.998)) +\n  geom_text_repel(size = 5, show.legend = FALSE) +\n  geom_point(size=4, show.legend = FALSE) +\n  xlab(\"Average dollars per day\") +\n  ylab(\"Infant survival rate\") +\n  scale_color_manual(values=color_blind_friendly_cols) +\n  geom_point(mapping=aes(dollars_per_day, infant_survival_rate, col = group), data =tmp, alpha = 0.5, size = 3, show.legend = FALSE) +\n  geom_text_repel(mapping=aes(dollars_per_day, infant_survival_rate, col = group, label = country),\n                  size = 4, show.legend = FALSE,\n                  data = filter(tmp, year %in% present_year & country %in% highlight))+\n    ggtitle(\" \")\n\n\n\nBayes Rule\nThis simple animation shows, case by case, the results of applying a highly accurate diagnostic test to a population with low prevalence of disease. It helps illustrate how the posterior probability of having the disease given a positive test is lower than the accuracy of the test. You can use Bayes rule to determine the actual conditional probabilities. More details are here.\n\n\n\nBecause we are not plotting data but drawing a cartoon, the code is a bit complex and hard to read.\n\n\nlibrary(animation)\nset.seed(2)\nprev <- 1/20\nacc <- 0.90\nN <- 20; M <- 50\nN_sick <- round(N*M*prev)\nx<-sample(c(rep(1, N_sick), rep(0, N*M - N_sick)))\ncols <- c(\"grey\",\"red\")\npeople <- expand.grid(1:M,N:1)\npeople2 <- expand.grid(1:round(M/2),N:(round(N/2)+1))\ncols1 <- cols[x+1]\ncols2 <- rep(NA, nrow(people2))\ncount2 <- 1\ncols3 <- rep(NA, nrow(people2))\ncount3 <- 1\n\ni <- 1\nmaxCount <- nrow(people2) + 1\nsaveGIF({\n  while(count2<=maxCount & count3<=maxCount){\n    test <- sample(100,1)\n    min <- round(100*acc)\n    rafalib::mypar()\n    layout(matrix(c(1,2,1,3),2,2), heights = c(1.75,1))\n    plot(people, col=cols1, pch=16, xaxt=\"n\", yaxt=\"n\", xlab=\"\", ylab=\"\", \n         main=paste0(\"Population: \",round(mean(x)*100),\"% are red. \",\n                     \"Test accuracy is \", round(acc*100),\"%\"))\n    if(test>min){\n      axis(side=1, M/2, paste(\"Test\",i,\"Wrong\"), col=\"red\", tick=FALSE, \n           cex.axis=3,line=1) \n      } else{ \n        axis(side=1, M/2, paste(\"Test\",i,\"Right\"), col=\"black\", tick=FALSE,\n             cex.axis=2,line=1)\n      }\n    points(people[i,], pch=1, cex=1.5)\n    if(all(is.na(cols2))){\n      plot(people2, type=\"n\", pch=16, xaxt=\"n\", yaxt=\"n\", xlab=\"\", ylab=\"\",\n           main=\"Tested Positive\")\n      } else{\n        plot(people2, col=cols2, pch=16, xaxt=\"n\", yaxt=\"n\", xlab=\"\", ylab=\"\", \n             main=paste0(\"Tested Positive: \", \n                         round(mean(cols2==\"red\",na.rm=TRUE)*100),\"% are red\"))\n      }\n    if(all(is.na(cols3))){\n      plot(people2, type=\"n\", pch=16, xaxt=\"n\", yaxt=\"n\", xlab=\"\", ylab=\"\", \n           main=\"Tested Negative\")\n      } else{ \n        plot(people2, col=cols3, pch=16, xaxt=\"n\", yaxt=\"n\", xlab=\"\", ylab=\"\",\n             main=paste0(\"Tested Negative: \", \n                         round(mean(cols3==\"red\",na.rm=TRUE)*100,1),\n                         \"% are red\"))\n      }\n    outcome <- ifelse(x[i]==1, as.numeric(test<=min), as.numeric(test>min))\n    if(outcome==0) {\n      cols3[count3] <- cols1[i]\n      count3<-count3+1\n      } else {\n        cols2[count2] <- cols1[i]\n        count2<-count2+1\n      }\n    i<-i+1\n    }\n  },'bayes.gif', interval = .1)\n\n\n\nPacman\nFinally, I made this plot to show the only instance in which pie charts are useful.\n\n\n\nThe code fits in a tweet.\n\n\nsaveGIF({\n  N=10\n  for(i in 0:(N-1)){\n    x <- cos(2*pi/N*i)\n    y <- x+1\n    z <- (y-2)*22.5\n    pie(c(y,8-y), col=c(\"white\",\"yellow\"), init.angle=135-z, \n        border=FALSE, labels=NA)\n    }\n  }, \"pacman.gif\", interval = 0.1)\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:29:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-07-26-announcing-the-tidypvals-package/",
    "title": "Announcing the tidypvals package",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2017-07-26",
    "categories": [],
    "contents": "\nA few years ago I helped write a paper where we proposed scraping p-values from the medical literature to try to estimate the science-wise false discovery rate. The paper generated a ton of interesting discussion and inspired other groups to start collecting p-values from the literature.\nAs I’ve mentioned before the p-value is the most popular statistic ever invented so there are a lot of published p-values out there.\nThe tidypvals package is an effort to find previous collections of published p-values, synthesize them, and tidy them into one analyzable data set. The currently available p-value data sets in this package are:\njager2014 - This data set comes from the paper: An estimate of the science-wise false discovery rate and application to the top medical literature that first proposed p-value scraping from the medical literature for re-analysis.\nbrodeur2016 - This data set comes from the paper Star Wars: The empirics strike back which collected p-values from the economics literature.\nhead2015 - This data set comes from the paper The Extent and Consequences of P-Hacking in Science and is an extension of the jager2014 idea to a much larger collection of biological papers.\nchavalarias2016 - This data set comes from the paper Evolution of Reporting P Values in the Biomedical Literature, 1990-2015 and is an extension of the jager2014 idea to a much larger collection of medical papers.\nallp - merges the head2015, chavalarias2016, and brodeur2016 while removing duplicates. To see how it is created view the merging vignette.\nEach data set is “tidy” data frame and has the following columns:\npvalue - The reported p-value\nyear - The year of the publication where the p-value appeared\njournal - The journal where the publication appeared\nfield - The field of the paper, using the categorization in Head et al. 2015.\nabstract - Whether the p-value was in the abstract of the paper\noperator - Whether the p-value was reported as “lessthan”, “greaterthan”, or “equals”.\ndoi - When available the digital object identifier.\npmid - The pubmed ID for the paper when available\nCurrently the package is only available from Github, but when I figure out the ExperimentHub package from Bioconductor I hope to move the package there. For now you can install it with\ninstall.packages('devtools)\nlibrary(devtools)\ndevtools::install_github('jtleek/tidypvals')\nThen you can load the library and then access each data set by name.\nlibrary(tidypvals)\njager2014\nData sets can be easily merged, but be careful to avoid duplicated p-values across different data sets. You can see how each data set was obtained and tidied by viewing the corresponding vignette.\nvignette(\"jager-2014\",package=\"tidypvals\")\nOne purpose of tidying these data is to be able to do cross-study analysis of p-values in the literature. As a teaser for things coming soon, this plot represents more than 2.5 million p-values across 25 different fields. Notice anything funny?\nAll p-values\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:29:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-07-19-my-unfunded-hhmi-teaching-professors-proposal/",
    "title": "My unfunded HHMI teaching professors proposal",
    "description": {},
    "author": [
      {
        "name": "Jeff",
        "url": {}
      }
    ],
    "date": "2017-07-19",
    "categories": [],
    "contents": "\nA little over a year ago I saw a request from the Howard Hughes Medical Institute for proposals focused on undergraduate teaching. I decided to apply for this grant since it combines all of the things I’m interested in: teaching, education research, biology, and data science. So I put together a proposal, got a couple of colleagues to write me letters of support, and sent it off.\nI was optimistic about the proposal since we have a cool opportunity through our work in scalable education to hit a large student population and we have been spending a lot of time thinking about using this platform to create a “science of data science” platform (more about that soon!).\nIn my proposal I said that I wanted to do four things:\nPerform research into the concepts and behaviors that lead to better data analysis.\nUse data science as a science research to improve our data science MOOCs\nCreate a sequence of MOOCs in artificial intelligence and deep learning targeted at the advanced undergraduate level\nHost a yearly undergraduate biological data science app hackathon\nUnfortunately I just heard that my proposal wasn’t selected as a finalist :(. As is the usual case with these things I got a form letter so not too much feedback on what were the good/bad parts of the proposal or why it didn’t fit in.\nUsually I’d re-organize the grant and re-submit it somewhere, but this was for a very specific request and the format/content doesn’t really fit at other granting organizations (that I know of,but if you do, please let me know!). So instead I thought I’d post it so at least the work I put in doesn’t just disappear entirely. I’ve made it possible for people to leave comments so if you have any please feel free to leave them in the document and maybe something good will come from the experience!\nhttps://docs.google.com/document/d/17xFzGkJ2XJMeNA0LLt9UNjRgAq8lZIiwn4Kr0sJ6Ndk/edit?usp=sharing\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:29:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-07-14-my-podcast-podroll/",
    "title": "My Podcast Podroll",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-07-14",
    "categories": [],
    "contents": "\nAs a self-interested podcaster, I’m obviously interested in getting more people to listen to more podcasts. For those who may be interested in getting into podcasts but wondering where to begin I thought I’d thought I’d make a list of what I’m currently listening to (perhaps to be updated as I acquire new ones). In no particular order:\nHello Internet - YouTubers Brady Haran and CGP Grey basically just talk, often for an hour to an hour and a half. The conversation is always entertaining. The topics are wide ranging and a bit difficult to classify. Brady Haran pretty much makes this podcast.\nThe Talk Show With John Gruber - A tech podcast by John Gruber of Daring Fireball, Gruber talks to a rotating series of regular guests on each episode. Topics center around Apple-related stuff and other current tech topics. If you’re the kind of person who is fussy about how they make their coffee in the morning, this is the podcast for you.\nScriptnotes - One of my favorite podcasts and a podcast that I’ve used as a model for my own podcasts. John August and Craig Mazin are Hollywood screenwriters and each episode talks about the craft and business of writing movies. I don’t have anything to do with movies, but I’ve learned a lot from this podcast.\nUpgrade - Hosts Jason Snell (of Macworld fame) and Myke Hurley talk about the latest Apple news (notice a pattern). They occasionally discuss other tech topics. This podcast is part of the massive Relay FM podcasting network, which puts out a lot of good podcasts.\nOut of the Blocks - This Baltimore-based podcast by Aaron Henkin covers a single city block in Baltimore each episode. Even though I’ve lived in Baltimore for 14 years, I learned a lot from this podcast. If you want to learn more about the immediate neighborhood where I work, you can listen to the episode on 2100 East Monument Street.\nAccidental Tech Podcast - Also known as “ATP”, Marco Arment, Casey Liss, and John Siracusa talk about tech. Usually Apple-related stuff, but often touching on other tech and occasionally cars.\nRocket - Self-styled “ATP with chicks”, Christina Warren, Brianna Wu, and Simone de Rochefort talk about, wait for it, technology. Common topics include virtual reality, gaming, Apple-related stuff, much more Microsoft stuff now that Warren works at Microsoft, and the occasional celebrity discussion.\nExponent - A podcast about technology, business, and society by Ben Thompson and James Allworth. Ben Thompson runs the incredible Stratechery newsletter and many of the topics discussed on the podcast are deeper discussions of topics covered in the newsletter. Thompson and Allworth always have good analysis of the tech business and tech business strategy.\nLaidback Luke Mixmash Radio - This is basically a radio show by DJ Laidback Luke but featuring music from his label Mixmash Records. I’m not even the biggest Laidback Luke fan, but I appreciate what he’s doing with vlogging and podcasting so I always listen (and I usually pick up a song or two from each episode).\nLarry Wilmore: Black on the Air - Larry Wilmore, comedian, writer for The Daily Show, creater of The Nightly Show with Larry Wilmore, and producer of Black-ish, just started this podcast but I enjoy it a lot. The format so far is basically a short monologue where Wilmore comments on current events and then an extended interview with someone famous.\nBombshell - Wonky national security podcast with Erin Simpson, Loren DeJonge Schulman, and Radha Iyengar Plumb. Much of the discussion goes behind the scenes to talk about what’s actually happening in the White House, National Security Council, etc.\nSticky Notes: The Classical Music Podcast - Conductor Joshua Weilerstein gives a brief tour of a major classical (usually orchestral) work, in a kind of Music 101 style. Although I’m pretty familiar with the music he covers, I’ve learned something in every episode. His episode on living composers was a really good.\nCorner of the Sky - A podcast buy Quinn Rose about musicals. Because musicals. This podcast comes from Jason Snell’s Incomparable podcast network.\nCortex - Myke Hurley and CGP Grey talk about being independent content creators. Imagine two guys talking over lunch.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:29:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-07-14-optimizing-for-user-experience/",
    "title": "Optimizing for User Experience",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-07-14",
    "categories": [],
    "contents": "\nI’ve previously written about how business (that are built by humans) can initially become successful by optimizing the user experience. That great user experience is what defines the value that their product provides. However, over time, companies have to find a way to provide value that is defined outside of the business-user relationship. The difficulty here is that this new definition of value is not within the control of the company–it is defined by the community via laws.\nThe same is true for artificial intelligence. Ina Fried, writing for Axios Tech about artificial intelligence:\n\nIt also matters what the algorithms are optimizing for. Airbnb, in general, is looking to train its algorithms to learn what factors are most likely to lead to a positive experience for guests when they make their reservation. However, a customer with a racial bias, for example, may be more satisfied when they see only white hosts. But to further Airbnb’s goal of an open, non-discriminatory platform, the company has to both recognize this issue, choose to prioritize non-discrimination, and then program accordingly.\n\nCompanies like Airbnb have to choose to prioritize non-discrimination. As of yet, the machines will not do it for them.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:29:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-07-13-the-joy-of-no-more-violin-plots/",
    "title": "The joy of no more violin plots",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2017-07-13",
    "categories": [],
    "contents": "\nUpdate: The ggjoy package has been deprecated. The ggridges CRAN package provides the same functionality.\nI dislike violin plots because they look like Christmas ornaments.\n\n\n\n\n\n\nIt’s a pet peeve but there is somewhat of a practical reason as well. To demonstrate I created a dataset called dat that contains an outcome value from 25 different groups.\nOne of the first steps I take when analyzing data is to look at the distribution of my data. If there are groups, I like to stratify and look at the distributions. Histograms and smooth density estimates are my favorite tools for visualizing distributions. I have 25 groups in this data so I would start by quickly looking at a random subset:\n\n\ndat %>% filter(group %in% c(\"A\",\"H\",\"I\",\"P\")) %>% \n  ggplot(aes(value)) + \n  geom_histogram(binwidth = 0.5, color=\"black\") +\n  facet_grid(.~group)\n\n\n\nThis plot shows me that the data looks normally distributed. To see how the groups compare to each other, a first step is to look at boxplots with groups reordered by their median value.\n\n\ndat %>% mutate(group = reorder(group, value, median)) %>% \n  ggplot(aes(group, value)) + geom_boxplot()\n\n\n\nIf you look close enough you note that group G looks a bit different: larger variance. But by summarizing into only five numbers, the boxplot misses an important characteristic of the data. Violin plots let you see the distributions rather than these five points:\n\n\ndat %>% mutate(group = reorder(group, value, median)) %>% \n  ggplot(aes(group, value)) + geom_violin(fill = \"blue\")\n\n\n\nDo you see it? If you look closely, group G appears to be bimodal. But why is it hard for me to see?\nAdding a mirror image of the density to make them resemble boxplots in some manner makes them look like Christmas ornaments and distracts me.\nI am used to looking at histograms and densities with the variable in the x-axis not the y-axis.\nI therefore prefer to stack histograms or density estimators vertically. So I was happy to learn about the ggjoy package that let’s you make such a plot in one line of code and produces a much more aesthetically pleasing plot than stacking histograms using, for example, facets.\n\n\nlibrary(ggjoy)\ndat %>% mutate(group = reorder(group, value, median)) %>%\n  ggplot(aes(x=value, y=group, height=..density..)) +\n  geom_joy(scale=0.85)\n\n\n\nNote how quickly we notice the bimodal group. I am hoping people start using joy plots instead of violin plots. To install and try out the package use this:\n\n\nlibrary(devtools)\ninstall_github(\"clauswilke/ggjoy\")\n\n\n\n\n\n",
    "preview": "posts/2017-07-13-the-joy-of-no-more-violin-plots/index_files/figure-html5/histograms-1.png",
    "last_modified": "2022-12-27T15:39:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-07-10-the-machines-learn-but-we-don-t/",
    "title": "The Machines Learn But We Don't",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-07-10",
    "categories": [],
    "contents": "\nI was genuinely amazed at this article by George Nott published in Computerworld quoting Peter Norvig on explainable artificial intelligence. From the article:\n\nSpeaking at an event at UNSW in Sydney on Thursday, Norvig – who at NASA developed software that flew on Deep Space 1 – said: “You can ask a human, but, you know, what cognitive psychologists have discovered is that when you ask a human you’re not really getting at the decision process. They make a decision first, and then you ask, and then they generate an explanation and that may not be the true explanation.”\n\nThis first part seems reasonable to me. There are entire fields of study (e.g. behavioral economics) that are dedicated to understanding how humans make decisions and how they differ from the explanations that humans give for those decisions.\nBut then the article continues:\n\nJust as humans worked to make sense and explain their actions after the fact, a similar method could be adopted in AI, Norvig explained.\n“So we might end up being in the same place with machine learning where we train one system to get an answer and then we train another system to say – given the input of this first system, now it’s your job to generate an explanation.”\n\nAs I discussed in Episode 42 of Not So Standard Deviations, there are at least two issues with this reasoning.\nFirst, Norvig suggests that we can take the outputs of a machine learning model and “explain” them using a completely different model from what was used to generate the outputs. We do this for humans because we do not have a good model for understanding how human brain processes work. However, this suggests that a second, perhaps more explainable model exists that can predict the outcome. Why not just use that model instead? What are the tradeoffs here? If the explainable model is interpretable in a useful way and perhaps only takes a small hit in accuracy, why not use it?\nSecond, even if we accept that we should just treat the machines the same way that we treat the humans, there is a critical difference here. As of today, we have ways to hold humans accountable for their decisions; we don’t have general agreement over how to hold the machines accountable. Is it the fault of the companies that sell the machines? The developers that develop the code? The data scientists that built the models? Artificial intelligence reminds me of the tax-dodging multi-national corporation: simultaneously having many homes but also having no home at all.\nBut there is a last point that is implied by Norvig’s comments. It is an admission that the machine learning models teach us nothing at all. From developing these models, we learn nothing about the data or about the processes that generated the data. So while the machines do all the learning, at the end of the data we learn nothing. All we can do is take the output of the machines and start from scratch.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:29:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-06-20-lowering-the-gwas-threshold-would-save-millions-of-dollars/",
    "title": "Lowering the GWAS threshold would save millions of dollars",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2017-06-20",
    "categories": [],
    "contents": "\nA recent publication (pay-walled) by Boyle et al. introducing the concept of an omnigenic model has generated much discussion. It reminded me of a question I’ve had for a while about the way genetics data is analyzed. Before getting into this, I’ll briefly summarize the general issue.\nWith the completion of the human genome project, human geneticists saw much promise in the possibility of scanning the entire genome for the genes associated with a trait. Inherited diseases were of particular interest. The general idea is to genotype individuals with the disease of interest and a group to serve as controls, then test each genotype for association with disease outcome using, for example, a chi-squared test. These are referred to as genome-wide association studies (GWAS).\nSince, thousands of GWAS have been performed and have mostly failed to identify variants that explain observed variation: odds ratios for binary disease outcomes tend to be lower than 1.5. In contrast, odds ratios for eye color are above 20. This has generated much debate and speculation about why. Boyle et al. show data that point to the possibility that most inherited traits are so complex that they may involve cumulative effects of a very large proportion of, or even most, variants, but each with very small effects. If this is the case, it has several implications on how we analyze and interpret GWAS data.\nBecause current technologies permit millions of variants to be tested, current GWAS apply multiple comparison corrections after obtaining a p-value for each variant. The Bonferroni correction for an error rate of 0.05 is the standard, which implies that a p-value in the order of \\(10^{-8}\\) is needed to reach genome-wide significance.\nBecause the effect sizes are so small, to obtain genome-wide significance, sample sizes in the thousands are needed. As a result, GWAS tend to be expensive relative to other research projects. Furthermore, the better the high-throughput technologies get, the more tests are run, and as a result smaller p-value are required and larger sample sizes are needed. In some cases, after a first GWAS does not yield any variant reaching significance, more samples are collected to increase power.\nThis brings me to my question. If we know a trait is inherited, then the current hypothesis testing approach may not be appropriate. In particular, controlling the family-wide error rate at 0.05 is too conservative. Why not require a false discovery rate of 0.05, 0.10 or even 0.25? For a typical GWAS how much would conclusions change if we halved the sample sizes and relaxed the significance threshold?\nLacking access to GWAS data (genotypes are hard to get), to illustrate my point I ran a simulation with several genomic regions having a small effect. I simulated data for 5,000 cases and 5,000 controls. A total of 468 genotypes were tested.\n\n\nspike <- function(x, A=0.02, mu=0, theta=.015, phi=2000)\n  A*log((1 - theta)^(abs(x-mu)/theta) * phi + 1)/log(phi)\nchr <- rep( 1:9, round(seq(87, 17, len = 9)))\nchr_start <- c(which(!duplicated(chr)), length(chr))\nchr_start <- (chr_start[-length(chr_start)] + chr_start[-1])/2\nP <- length(chr) ## nunmber of features\nloc <- 1:P\ncenters <- c(0.05,0.25,0.55,.8,0.94)*P\nAs <- c(0.1,0.1,0.125,0.1,0.05)\ntmp <- sapply(seq_along(centers), function(i) spike(loc, mu = centers[i], A=As[i]))\neffect <- 1 + rowSums(tmp)\nset.seed(1)\nbaseline_p <- rmutil::rbetabinom(P, 100, 0.25, 100) / 100\nN <- 5000\ndisease <- sapply(effect*baseline_p, function(p) rbinom(N, 1, p = p))\ncontrol <- sapply(baseline_p, function(p) rbinom(N, 1, p = p))\nX <- rbind(control, disease)\nif(any(colMeans(X) ==0 | colMeans(X)==1)) stop(\"One column with MAF=0\")\ny <- rep(c(0,1), each=nrow(X)/2)\n\n\nWe can obtain odds ratios and p-values and create a Manhattan plot by simply using this code:\n\n\nres <- apply(X, 2, function(x){\n  tab<-table(x,y)\n  c(tab[1,1]*tab[2,2]/(tab[1,2]*tab[2,1]),\n    chisq.test(tab)$p.value)\n})\nrafalib::mypar()\nplot(loc, -log10(res[2,]) , pch=16, xaxt=\"n\", xlab=\"Chromosome\", ylab=\"-log (base 10) p-value\", ylim=c(0, -log10(0.05/P)))\naxis(1, chr_start, seq_along(chr_start), tick=FALSE)\nabline(h=-log10(0.05/P), lty=2)\n\n\n\nNote that no variant achieves genome-wide significant (dashed line). However, we do see peaks in the plot. If we smooth the odds ratio data, these peaks become even clearer:\n\n\nlogodds <- log(res[1,])\nfit <- predict(loess(logodds~loc, span = 0.1), se=TRUE)\nmat <- fit$fit + cbind(-2*fit$se.fit,0,2*fit$se.fit)\nmatplot(loc, mat, type=\"l\", col=c(\"grey\",\"black\",\"grey\"), lty=1, ylim=max(mat)*c(-1,1), ylab=\"Log odds\", xlab=\"Chromosome\", xaxt=\"n\")\naxis(1, chr_start, seq_along(chr_start), tick=FALSE)\nabline(h=0, lty=2)\n\n\n\nHowever, in the GWAS world, genome-wide significance at a 0.05 is required. So after obtaining the results above, the next step would be to obtain more funding and double the sample size.\n\n\ndisease2 <- sapply(effect*baseline_p, function(p) rbinom(N, 1, p = p))\ncontrol2 <- sapply(baseline_p, function(p) rbinom(N, 1, p = p))\nX <- rbind(control, control2, disease, disease2)\nif(any(colMeans(X) ==0 | colMeans(X)==1)) stop(\"One column with MAF=0\")\ny <- rep(c(0,1), each=nrow(X)/2)\n\n\nWith this larger sample size we now find four regions achieving genome wide significance:\n\n\nres2 <- apply(X, 2, function(x){\n  tab<-table(x,y)\n  c(tab[1,1]*tab[2,2] / (tab[1,2]*tab[2,1]),chisq.test(tab)$p.value)\n})\nplot(loc, -log10(res2[2,]), pch=16, xaxt=\"n\", xlab=\"Chromosome\", ylab=\"-log (base 10) p-value\")\naxis(1, chr_start, seq_along(chr_start), tick=FALSE)\nabline(h=-log10(0.05/P), lty=2)\n\n\n\nWe find four out of the five regions simulated to have effects with no false positives. Here is a plot of the simulated effects:\n\n\nplot(loc, effect, col=chr, pch=16, xaxt=\"n\", xlab=\"Chromosome\", ylab=\"Effect\")\naxis(1, chr_start, seq_along(chr_start), tick=FALSE)\n\n\n\nHowever, note that the shape of the Manhattan plot did not change much with the new data (see figure below). We mostly moved points up by increasing the sample size. But we could have identified those same regions with a relatively low false positive rate, by, for example looking at the top 10 variants, or simply lowering the threshold. In fact, statisticians have beed advocating for other formal statistical approaches.\nIn the following plot we compare the results using two different sample sizes. We denote the regions simulated to have the strongest effects with black points while the rest are grey.\n\n\nrafalib::mypar(2,1)\nplot(loc, -log10(res2[2,]), col=ifelse(effect>1.01, \"black\", \"grey\"), pch=16, xaxt=\"n\",\n     xlab=\"Chromosome\", ylab=\"-log (base 10) p-value\", main=\"Double the sample size\")\naxis(1, chr_start, seq_along(chr_start), tick=FALSE)\nabline(h=-log10(0.05/P), lty=2)\no <- order(res[2,])[1:25] ##top 25 variants\nplot(loc, -log10(res[2,]), col=ifelse(effect>1.01, \"black\", \"grey\"), pch=16, ylim=c(0, -log10(0.05/P)), xaxt=\"n\", xlab=\"Chromosome\", ylab=\"-log (base 10) p-value\", main=\"Lower the threshold\")\naxis(1, chr_start, seq_along(chr_start), tick=FALSE)\nabline(h = -log10(max(res[2,o])), lty=2)\n\n\n\nIf Boyle et al. are correct, then hypothesis testing is definitely not an appropriate statistical approach for GWAS: we know the null hypothesis is false for most variants. Going forward, to make sense of GWAS data, we should reinvest the millions of dollars currently used to satisfy the Bonferroni correction to measure other endpoints and develop new statistical approaches that can help us understand the molecular cause of disease and improve treatment outcomes.\n\n\n\n",
    "preview": "posts/2017-06-20-lowering-the-gwas-threshold-would-save-millions-of-dollars/index_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-12-27T15:39:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-06-13-the-future-of-scalable-education-is-plain-text/",
    "title": "The future of education is plain text",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2017-06-13",
    "categories": [],
    "contents": "\nI was recently at a National Academy meeting on Envisioning the Data Science Curriculum. It was a fun meeting and one of the questions that came up was what kind of infrastructure do we need to enable shared curricula, compatibility across schools, and not reinventing the wheel. My answer to this question was that we need lecture notes stored in plain text files (like rmarkdown files) and data stored in csv files with direct links.\nI am not nearly the first person to make this argument. Lorena Barba explained why her MOOC doesn’t use video almost two years ago now. More recently a blog post pointed out some advantages of plain text code files for data analysis. I’m sure there are more that I don’t know about.\nI think that the future of education is in plain text documents (not just for data science) and that the future of data storage is in simple csvs for all but the most complicated data sets. Why?\nPlain text is always compatible Every single operating system has a plain text editor and they are all compatible up to the encoding of the text. This means that if you develop your lecture notes on a Mac and I develop them on a PC then we can still easily share - no worrying if you have the right software.\nPlain text is easy to mix and match If your lecture materials are in a simple plain text format like markdown you can copy and paste the materials from one lecture into another and when the document is compiled make all the formatting/colors/etc. match. No more looking at hodgepodges of borrowed slides some with one ppt format and some with another.\nPlain text is easy to maintain We mostly work on scalable education here at the JHU Data Science Lab. We often think of scalable education in terms of the number of students, but here we have also run into the problem of scaling the number of courses/instructor. I am currently the lead instructor on more than half a dozen classes running all the time. Every time I have to re-record a video it takes set up time, recording time, editing time. If I have an error in a markdown file it is a quick edit to a text file.\nPlain text is lightweight Images can be stored online and the lecture notes themselves are small. This might not matter where internet access is good, but in places with limited resources or wifi, this can be the difference from easily accessible lecture notes and bad ones.\nPlain text is always forward compatible Regardless of the next platform, if we have all of our knowledge/lecture notes stored in plain text it will be easy to extact them. When you switch platform, or compiling software, or style, there isn’t a worry about the files not working appropriately.\nWe believe in this so much that we are working on ways to make course videos generatable from plain text files and more recently trying to figure out ways to make the puppets move from plain text files alone.\n\nSo if people are looking for infrastructure to fund - places to make it easier to edit/host/share plain text and plain csv files!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:29:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-06-06-papr-rate-papers-on-biorxiv-in-a-single-swipe-and-help-science/",
    "title": "papr - rate papers on biorxiv in a single swipe and help science!",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2017-06-06",
    "categories": [],
    "contents": "\nLike a lot of modern scientists I now find papers to read to a large extent based on what I see on social media. It is a great way to find out what my colleagues are reading and keep up with the newest cool research.\nA few weeks ago I released a really simple prototype for an app that would improve on this experience called papr, its like Tinder but for papers :). The basic idea is for you to log in to the system, and then swipe to rate preprints on biorxiv on one of two scales:\nBoring versus exciting\nProbably accurate versus questionable.\nSince then Lucy and Nick went crazy and turned it into something way more interesting. You can now rate papers just like before by swiping:\n\nBut now Lucy and Nick have added a recommendation engine that will learn from what you like and show you more papers that meet your personal tastes.\nBut there’s more! You can download your ratings to analyze yourself or you can take a sneak peak into our recommendation engine to find other papers you might like with this interactive PC plot:\n\nBut my favorite feature is that you can see other people who have liked similar papers and follow them on Twitter - so you can discover new scientists that work in your field!\n\nWe hope to aggregate the data and get a community level view of what is going on biorxiv. Lucy and Nick have done an amazing job and I really hope that you will check it out and rate some papers, who knows you might find a new paper or Twitter friend!\nhttps://jhubiostatistics.shinyapps.io/papr\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:28:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-05-24-toward-tidy-analysis/",
    "title": "Toward tidy analysis",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2017-05-24",
    "categories": [],
    "contents": "\nTidy data at its heart is a set of three rules for organizing a data set:\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\nThis is an incredibly useful abstraction for thinking about organizing data sets for analysis. In particular any data set that can be conveniently rectangled to look like this fun diagram from Jenny Bryan\nData rectanglecan then be assumed to have a common form and structure. This makes it much easier to write tools that operate on - because the format is standardized. Since then a whole suite of R packages have sprung up called the tidyverse (along with a host of other packages not in the official tidyverse) that operate on data of this variety.\nTidy data is great for a huge fraction of data analyses you might be interested in. It makes organizing, developing, and sharing data a lot easier. Its how I recommend most people share data and I’ve recently even written an entire NIH grant around the idea of tidy health data libraries.\nBut it starts to feel restrictive when you have to deal with edge cases like nested data sets, or data that are very large and lead to repetitive tidy data, or data that are best operated on as matrices. These non-tidy data represent a relatively small fraction of most data analysis being performed. For those analyses there are smart, organized things you can do, but the usefulness of the absraction often breaks down.\nLately I’ve been thinking a lot about what an equivalent set of rules for data analysis would be. I’ve written a book on data analysis which is essentially an extended checklist of rules of thumb. But I started to wonder, could we come up with a useful and simpler abstraction for data analysis? In particular I was interested in the question of whether we could come up with a set of three rules that would cover the broad majority of data analyses being done in practice and would simplify our evaluation and understanding of those analyses. It would also simplify the tool building for data analysis by restricting the space of choices. By necessity three rules will lead to some discomfort for edge cases just like the tidy data abstraction runs into discomfort around the edges. But for this exercise I’m willing to accept that level of discomfort.\nAfter thinking about it for a little while I settled on the following three rules and I’d love to get peoples feedback. A “tidy” analysis is one where:\nIt answers a single quantitative question that can be defined in terms of a parameter.\nIt uses a training set to do all model building and a test set to evaluate all model building.\nIt uses only linear models.\nI’ve told a few people about this idea and I’ve gotten reactions from laughter to lukewarm agreement to pushback. So I thought I’d give some justification for these rules.\nA single quantitative question defined by a parameter\nAs any applied/consulting statistician will tell you, a huge part of the job of working with collaborators is defining the question. In my experience, one of the key predictors of the success of a project is whether we can identify a single concrete and quantifiable question. In most data science/statistical consulting classes, a large amount of effort is dedicated to figuring out what question is being asked and how we can turn that into a quantitative statement. So any tidy analysis will lead with a single question defined in terms of a concrete parameter - this avoids the type of analysis where you see 1000 different plots, sub-analyses, and diversions.\nTraining and test sets\nThere are various models for the data analytic process, but most of them are defined in terms of a cycle. For example here is the process as defined by Hadley Wickham’s book R for Data Science\nData scienceAnother example is in the book The Art of Data Science:\nAlso data scienceThe trouble with this cyclic view of data science is that it opens the door to p-hacking and the garden of forking paths because you never know if the plot, processing step, or analysis you did is following signal or noise. As we have known for a long time the best way to avoid these overfitting problems is to evaluate the results of your analysis in a held out set. This would apply regardless of if you are doing inference, exploratory analysis, or prediction.\nLinear models\nThis one has been the most controversial when I’ve proposed this approach. But I think one of the best solutions to reducing researcher degrees of freedom is to reduce researcher degrees of freedom. Linear models are an incredibly useful tool and can be used for ANOVA, for modeling, for visualization, for prediction and a number of other tasks. Moreover, they have the benefit of being one of the more interpretable models. Perhaps most importantly in a linear model you know the exact relationship between any of the covariates and the outcome you are trying to evaluate.\nEarly in my career I often made the mistake of trying a linear model first, seeing no or little signal, and then wasting a lot of time trying fancier models. Almost inevitably, if you can’t see the signal with a suitable linear model, you won’t be able to find it with fancier methods. The fancier methods may just serve to make the signal stronger. So as a first pass I’d always like to see the linear model result to establish a baseline.\nThis does leave out some types of analysis such as unsupervised analysis. But I think that is the point, the goal is to find a set of rules that covers the majority of analyses, allows us to templatize/organize ourselves, and removes the most common barriers to working together.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:28:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-05-17-deans-lecture/",
    "title": "The Past and Future of Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-05-17",
    "categories": [],
    "contents": "\nOn May 3rd I gave my Dean’s lecture titled “The Past and Future of Data Analysis”, which was a lot of fun and gave me the opportunity to play lots of different kinds of music on stage! I talked a little bit about it on the latest episode of Not So Standard Deviations. Now the School has posted the full video of the lecture and you can watch it here:\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:28:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-05-09-data-on-the-comey-effect/",
    "title": "Data on the Comey Effect",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2017-05-09",
    "categories": [],
    "contents": "\nThere is currently a debate about whether or not the Comey letter flipped the election. Nate Cohn makes a convincing argument that the letter had little to no effect. Some time ago I looked at this myself and came to a similar conclusion. If anything, it was the ACA price hike announcement that had the bigger effect.\nTo test out blogdown (thanks Yihui Xie!) I decided to write this post showing the code I used for the simple analysis I performed, hoping to get others to look at the data, point out mistakes, or show me a better way to do what I did. There are many more things I can think of doing, such as account for pollster effect or try to better estimate the day the poll was actually taken.\nHere I read-in and wrangle the raw data prepared by 538:\n\n\nlibrary(tidyverse)\n##download data from 538 \nurl <- \"http://projects.fivethirtyeight.com/general-model/president_general_polls_2016.csv\"\nelection_2016 <- read_csv(url)\nelection_2016 <- filter(election_2016, type==\"polls-only\")\nelection_2016$diff <- with(election_2016, rawpoll_clinton-rawpoll_trump) ##define the difference\nelection_2016$startdate <- as.Date(election_2016$startdate,\"%m/%d/%Y\") ##turn enddate into date\nelection_2016$enddate <- as.Date(election_2016$enddate,\"%m/%d/%Y\") \nstart_day <- as.Date(\"2016-09-01\")\nelection_day <- as.Date(\"2016-11-08\")\ndat <- filter(election_2016, startdate > start_day & state==\"U.S.\") ##after start date and national polls\n\n\nThen I create a new data frame in which each day for each poll generates a row. I keep track of the reported difference, the number of days in the polling period, and the sample size.\n\n\npolls <- lapply(1:nrow(dat), function(i){\n  days <- (election_day - dat$startdate[i]):(election_day - dat$enddate[i])\n  return(cbind( dat[i,c(\"pollster\", \"diff\")], days, w=1/length(days), \n                n=dat$samplesize[i]))\n})\npolls <- Reduce(rbind,polls)\n\n\nThen I compute a weighted average of the difference for each day. Because we are interested in sharp declines, I perform no smoothing other than that imposed by the fact that poll dates span multiple days.\nIf a day was not included in the poll period that poll had 0 weight for that day. If it was include, then the weight was \\(1/\\mbox{number of days in poll period}\\). For example, if poll 1 ran from October 2-4 and poll 2 ran from Oct 1-7, the October 3 estimate would use weights proportional to 1/3 and 1/7 for polls 1 and 2 respectively. They are proportional because weights are scaled to add to 1 for each day. I filtered out days with wieghts less than 10 days. The sample size was not used to compute the weight.\n\n\nres <- polls %>% \n  group_by(days) %>% \n  filter(sum(w)>3) %>% \n  summarize(avg = sum(diff*w)/sum(w)) %>%\n  mutate(date = election_day - days) %>% \n  arrange(date) \n\n\nHere is the plot of these weighted averages:\n\n\nrafalib::mypar()\nwith(res, plot(date, avg, type=\"b\", pch = 21, bg = 1, xlab = \"Date\", ylab = \"Weighted Average of Clinton - Trump\"))\nabline(v =  as.Date(\"2016-10-28\"), col = 2, lty = 2)\ntext(as.Date(\"2016-10-28\"), 3,  \"Comey Letter\", srt= 90)\nabline(v =  as.Date(\"2016-10-24\"), col = 2, lty = 2)\ntext(as.Date(\"2016-10-24\"), 2, \"ACA price hike\", srt = 90)\nabline(v =  as.Date(\"2016-10-07\"), col = 2, lty = 2)\ntext(as.Date(\"2016-10-07\"), 3, \"Access Hollywood\", srt = 90)\nabline(v =  as.Date(\"2016-09-26\"), col = 2, lty = 2)\ntext(as.Date(\"2016-09-26\"), 3, \"First Debate\", srt = 90)\n\n\n\n\n\n\n",
    "preview": "posts/2017-05-09-data-on-the-comey-effect/index_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-12-27T15:39:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-05-07-ml-last-mile/",
    "title": "Will Machine Learning and AI Ever Solve the Last Mile?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-05-07",
    "categories": [],
    "contents": "\nFacebook just recently announced that they were hiring 3,000 people (on top of an existing 4,500) people to review images, videos, and posts for inappropriate content. From Popular Science:\n\nThe scale of this labor is vast: Facebook is hiring more people than work in the combined newsrooms of the New York Times, the Wall Street Journal, and the Washington Post. Facebook isn’t saying at this time if the jobs will be employees or contractors, and if they’ll be based in the United States or abroad.\n\nSimilar stories have recently popped up. YouTube had a problem when advertisers pulled their ads when it was discovered that some ads were being placed in highly objectionable videos. From Recode:\n\nGoogle has been scrambling to react over the past few weeks, as newspapers like the Times of London, the Guardian and the Wall Street Journal pointed out ads running next to videos from hate groups and other extremists. Those reports prompted big brands like AT&T and Verizon to pull their ads from YouTube.\n\nInterestingly, in both cases, the companies (Facebook and Google) claim, I believe accurately, that the problem is very small. Again, from Recode:\n\nA top Google executive says the company’s YouTube ad controversy — where big brands have discovered that some of their ads have run next to videos promoting hate and terror — is overblown.\n\n\nBut he says Google is making progress at fixing it anyway.\n\n\n“It has always been a small problem,” with “very very very small numbers” of ads running against videos that aren’t “brand safe,” said Google’s chief business officer, Philipp Schindler. “And over the last few weeks, someone has decided to put a bit more of a spotlight on the problem.”\n\nIt’s interesting that Google’s attitude was that this was a problem that they didn’t really have to fix, but they’re going to fix it anyway, seemingly out of the goodness of their own hearts.\nI think the reality is quite different. Machine learning and AI are increasingly being placed in situations where the times where the models don’t work, however infrequent, nevertheless have a huge impact. Think self-driving cars, for example. The common denominator is that even though each of the problems happened at a comically small scale, somehow the entire world ended up knowing about it. So, if the entire world knows about it, how can this be a small problem for the company? No matter, how good Google’s or Facebook’s algorithms get, if even one person sees it, they can post a screenshot for the entire world to see.\nLet’s be clear, this problem for Google, Facebook, and others is ultimately a problem of their own making.\nThe value proposition for these kinds of companies is that they have huge scale and reach millions, if not billions of people around the world\nHowever, because of their huge scale, even the smallest mistake in their algorithm will be detected by someone. If even one of their billions of users sees the problem, they can broadcast it to the entire world.\nIn order to improve their algorithms, companies like Google/Facebook need to collect more data, in part by recruiting new people into their user base.\nHowever, as their user base grows, the accuracy requirements for their algorithms will also grow because of the likelihood of one user spotting an error or mistake.\nRight now, it’s humans who take over when the machines fail. I suppose the goal will be to optimize the algorithms to the point where the need for humans to take over is relatively contained and the frequency with which these failures occur is acceptably low. But I don’t know if this is possible.\nIt will be interesting to see how this plays out over the next few years as machine learning and AI get put into more and more situations. I’m not confident that machine learning, or more importantly the data that is fed into the algorithms, will ever get to the point where they can detect inappropriate videos or other objectionable content at an acceptable rate. My guess is the solution will be changes to policies to ultimately disincentivize the posting of such content (as was YouTube’s solution).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:28:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-05-04-debt-haircuts/",
    "title": "Some default and debt restructuring data",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2017-05-04",
    "categories": [],
    "contents": "\nYesterday the government of Puerto Rico asked for bankruptcy relief in federal court. Puerto Rico owes about $70 billion to bondholders and about $50 billion in pension obligations. Before asking for protection the government offered to pay back some of the debt (50% according to some news reports) but bondholders refused. Bondholders will now fight in court to recover as much of what is owed as possible while the government and a federal oversight board will try to lower this amount. What can we expect to happen?\nA case like this is unprecedented, but there are plenty of data on restructurings. An op-ed by Juan Lara pointed me to this blog post describing data on 180 debt restructurings. I am not sure how informative these data are with regards to Puerto Rico, but the plot below sheds some light into the variability of previous restructurings. Colors represent regions of the world and the lines join points from the same country. I added data from US cases shown in this paper.\n\nThe cluster of points you see below the 30% mark appear to be cases involving particularly poor countries: Albania, Argentina, Bolivia, Ethiopia, Bosnia and Herzegovina, Guinea, Guyana, Honduras, Cameroon, Iraq, Congo, Rep., Costa Rica, Mauritania, Sao Tome and Principe, Mozambique, Senegal, Nicaragua, Niger, Serbia and Montenegro, Sierra Leone, Tanzania, Togo, Uganda, Yemen, and Republic of Zambia. Note also these restructurings happened after 1990.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:28:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-04-24-march-for-science/",
    "title": "Science really is non-partisan: facts and skepticism annoy everybody",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2017-04-24",
    "categories": [],
    "contents": "\nThis is a short open letter to those that believe scientists have a “liberal bias” and question their objectivity. I suspect that for many conservatives, this Saturday’s March for Science served as confirmation of this fact. In this post I will try to convince you that this is not the case specifically by pointing out how scientists often annoy the left as much as the right.\nFirst, let me emphasize that scientists are highly appreciative of members of Congress and past administrations that have supported Science funding though the DoD, NIH and NSF. Although the current administration did propose a 20% cut to NIH, we are aware that, generally speaking, support for scientific research has traditionally been bipartisan.\nIt is true that the typical data-driven scientists will disagree, sometimes strongly, with many stances that are considered conservative. For example, most scientists will argue that:\nClimate change is real and is driven largely by increased carbon dioxide and other human-made emissions into the atmosphere.\nEvolution needs to be part of children’s education and creationism has no place in Science class.\nHomosexuality is not a choice.\nScience must be publically funded because the free market is not enough to make science thrive.\nBut scientists will also hold positions that are often criticized heavily by some of those who identify as politically left wing:\nCurrent vaccination programs are safe and need to be enforced: without heard immunity thousands of children would die.\nGenetically modified organisms (GMOs) are safe and are indispensable to fight world hunger. There is no need for warning labels.\nUsing nuclear energy to power our electrical grid is much less harmful than using natural gas, oil and coal and, currently, more viable than renewable energy.\nAlternative medicine, such as homeopathy, naturopathy, faith healing, reiki, and acupuncture, is pseudo-scientific quackery.\nThe timing of the announcement of the March for Science, along with the organizers’ focus on environmental issues and diversity, may have made it seem like a partisan or left-leaning event, but please also note that many scientists criticized the organizers for this very reason and there was much debate in general. Most scientists I know that went to the march did so not necessarily because they are against Republican administrations, but because they are legitimately concerned about some of the choices of this particular administration and the future of our country if we stop funding and trusting science.\nIf you haven’t already seen this Neil Degrasse Tyson video on the importance of Science to everyone, I highly recommend it.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:28:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-04-06-huelga/",
    "title": "La matrícula, el costo del crédito y las huelgas en la UPR",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2017-04-06",
    "categories": [],
    "contents": "\nLa Universidad de Puerto Rico (UPR) recibe aproximádamente 800 millones de dólares del estado cada año. Esta inversión le permite ofrecer salarios más altos, lo cual atrae a los mejores profesores, tener las mejores instalaciones para la investigación y enseñanza, y mantener el precio por crédito más bajo que las universidades privadas. Gracias a estas grandes ventajas, la UPR suele ser la primera opción del estudiantado puertorriqueño, en particular los dos recintos más grandes, Río Piedras (UPRRP) y Mayagüez. Un estudiante que aprovecha su tiempo en la UPR, además de formarse como ciudadano, puede entrar exitosamente en la fuerza laboral o continuar sus estudios en las mejores escuelas graduadas. El precio módico del crédito, en combinación con las becas federales Pell, han ayudado a miles de estudiantes económicamente desaventajados a completar sus estudios sin tener que endeudarse.\nEn la pasada década una realidad preocupante ha surgido: mientras la demanda por la educación universitaria ha crecido, demostrado por el crecimiento de la matrícula en las universidades privadas, el número de estudiantes matriculados en la UPR ha bajado.\n\n¿Por qué ha bajado la matrícula en la UPR? Una explicación popular es que “la baja en matrícula es provocada por el aumento en el costo de la matrícula”. La teoría de que un alza en costos disminuye la matrícula es comúnmente aceptada pues tiene sentido económico: cuando el precio sube, las ventas bajan. Pero entonces ¿por qué ha crecido la matrícula en las universidades privadas? Tampoco lo explica un crecimiento en el número de estudiantes ricos ya que, en el 2012, la mediana de ingreso familiar de aquellos jóvenes matriculados en algún recinto de la UPR era de $32,379; en contraste, la mediana de ingreso de aquellos que están matriculados en una universidad privada era de $25,979. Otro problema con esta teoría es que, una vez ajustamos por inflación, el costo del crédito se ha mantenido más o menos estable tanto en la UPR como en las unversidades privadas.\n\nAhora, si miramos detenidamente los datos de la matrícula notamos que los bajones más grandes fueron precisamente en los años de huelga (2005, 2010, 2011). En el 2005 comienza una tendencia positiva en la matrícula del Sagrado, con el crecimiento más alto en el 2010 y el 2011.\n\nActualmente, varios recintos, incluyendo Río Piedras, están cerrados indefinidamente. En una asamblea nacional asistida por 10% de los más de 50,000 estudiantes del sistema, una huelga indefinida fue aprobada en una votación de 4,522 a 1,154. Para reiniciar labores los estudiantes exigen que “no se impongan sanciones a los estudiantes que participen en la huelga, que se presente un plan de reforma universitaria elaborado por la comunidad universitaria, que se audite la deuda pública y se restituya a los miembros de la comisión evaluadora de la auditoría pública y su prepuesto”. Esto ocurre como respuesta a la propuesta por la Junta de Supervición Fiscal (JSF) y el gobernador de reducir el presupuesto de la UPR como parte de sus intentos de resolver una grave crisis fiscal.\nDurante el cierre, los estudiantes en huelga le impiden la entrada al recinto al resto de la comunidad universitaria, incluyendo aquellos que no consideran la huelga una manera efectiva de protesta. Aquellos que se oponen y quieren continuar estudiando, se les acusa de ser egoistas o de ser aliados de quienes quieren destruir la UPR. Hasta ahora estos estudiantes tampoco han recibido el apoyo explícito de los profesores y administradores. No debe sorprendernos si los que quieren continuar estudiando recurren a pagar más en una universidad privada.\n\nAunque existe la posibilidad de que la huelga ejerza suficiente presión política para que se responda a las exigencias determinadas en la asamblea, hay otras posibilidades menos favorables para los estudiantes:\nLa falta de actividad académica resulta en el exilio de miles de estudiantes a las universidades privadas.\nLa JSF usa el cierre para justificar aun más recortes: una institución no requiere millones de dolares al día si está cerrada.\nLos recintos cerrados pierden su acreditación ya que una universidad en la cual no se da clases no puede cumplir con las normas necesarias.\nSe revocan las becas Pell a los estudiantes en receso.\nHay mucha evidencia empírica que demuestra la importancia de la educación universitaria accesible. Lo mismo no es cierto sobre las huelgas como estrategia para defender dicha educación. Y cabe la posibildad que la huelga indefinida tenga el efecto opuesto y perjudique enormemente a los estudiantes, en particular a los que se ven forzados a matricularse en una universidad privada.\nNotas:\nData proporcionada por el Consejo de Educación de Puerto Rico (CEPR).\nEl costo del crédito del 2011 no incluye la cuota.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:28:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-04-03-interactive-data-analysis/",
    "title": "The Importance of Interactive Data Analysis for Data-Driven Discovery",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2017-04-03",
    "categories": [],
    "contents": "\nData analysis workflows and recipes are commonly used in science. They are actually indispensable since reinventing the wheel for each project would result in a colossal waste of time. On the other hand, mindlessly applying a workflow can result in totally wrong conclusions if the required assumptions don’t hold. This is why successful data analysts rely heavily on interactive data analysis (IDA). I write today because I am somewhat concerned that the importance of IDA is not fully appreciated by many of the policy makers and thought leaders that will influence how we access and work with data in the future.\nI start by constructing a very simple example to illustrate the importance of IDA. Suppose that as part of a demographic study you are asked to summarize male heights across several counties. Since sample sizes are large and heights are known to be well approximated by a normal distribution you feel comfortable using a true and tested recipe: report the average and standard deviation as a summary. You are surprised to find a county with average heights of 6.1 feet with a standard deviation (SD) of 7.8 feet. Do you start writing a paper and a press release to describe this very interesting finding? Here, interactive data analysis saves us from naively reporting this. First, we note that the standard deviation is impossibly big if data is in fact normally distributed: more than 15% of heights would be negative. Given this nonsensical result, the next obvious step for an experienced data analyst is to explore the data, say with a boxplot (see below). This immediately reveals a problem, it appears one value was reported in centimeters: 180 centimeters not feet. After fixing this, the summary changes to an average height of 5.75 and with a 3 inch SD.\nEuropean OutlierYears of data analysis experience will show you that examples like this are common. Unfortunately, as data and analyses get more complex, workflow failures are harder to detect and often go unnoticed. An important principle many of us teach our trainees is to carefully check for hidden problems when data analysis leads you to unexpected results, especialy when the unexpected results holding up benefits us professionally, for example by leading to a publication.\nInteractive data analysis is also indispensable for the development of new methodology. For example, in my field of research, exploring the data has led to the discovery of the need for new methods and motivated new approaches that handle specific cases that existing workflows can’t handle.\nSo why I am concerned? As public datasets become larger and more numerous, many funding agencies, policy makers and industry leaders are advocating for using cloud computing to bring computing to the data. If done correctly, this would provide a great improvement over the current redundant and unsystematic approach of everybody downloading data and working with it locally. However, after looking into the details of some of these plans, I have become a bit concerned that perhaps the importance of IDA is not fully appreciated by decision makers.\nAs an example consider the NIH efforts to promote data-driven discovery that center around plans for the Data Commons. The linked page describes an ecosystem with four components one of which is “Software”. According to the description, the software component of The Commons should provide “[a]ccess to and deployment of scientific analysis tools and pipeline workflows”. There is no mention of a strategy that will grant access to the raw data. Without this, carefully checking the workflow output and developing the analysis tools and pipeline workflows of the future will be difficult.\nI note that data analysis workflows are very popular in fields in which data analysis is indispensible, as is the case in biomedical research, my focus area. In this field, data generators, which typically lead the scientific enterprise, are not always trained data analysts. But the literature is overflowing with proposed workflows. You can gauge the popularity of these by the vast number published in the nature journals as demonstrated by this google search:\nNature workflowsIn a field in which data generators are not data analysis experts, the workflow has the added allure that it removes the need to think deeply about data analysis and instead shifts the responsibility to pre-approved software. Note that these workflows are not always described with the mathematical language or computer coded needed to truly understand it but rather with a series of PowerPoint shapes. The gist of the typical data analysis workflow can be simplified into the following:\nworkflowsThis simplification of the data analysis process makes it particularly worrisome that the intricacies of IDA are not fully appreciated.\nAs mentioned above, data analysis workflows are a necessary component of the scientific enterprise. Without them the process would slow down to a halt. However, workflows should only be implemented once consensus is reached regarding its optimality. And even then, IDA is needed to assure that the process is performing as expected. The career of many of my colleagues has been dedicated mostly to the development of such analysis tools. We have learned that rushing to implement workflows before they are mature enough can have widespread negative consequences. And, at least in my experience, developing rigorous tools is impossible without interactive data analysis. So I hope that this post helps make a case for the importance of interactive data analysis and that it continues to be a part of the scientific enterprise.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:28:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-03-16-evo-ds-class/",
    "title": "The levels of data science class",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2017-03-16",
    "categories": [],
    "contents": "\nIn a recent post, Nathan Yau points to a comment by Jake Porway about data science hackathons. They both say that for data science/visualization projects to be successful you have to start with an important question, not with a pile of data. This is the problem forward not solution backward approach to data science and big data. This is the approach also advocated in the really nice piece on teaching data science by Stephanie and Rafa\nI have adopted a similar approach in the data science class here at Hopkins, largely inspired by Dan Meyer’s patient problem solving for middle school math class. So instead of giving students a full problem description I give them project suggestions like:\nOption 1: Develop a prediction algorithm for identifying and classifying users that are trolling or being mean on Twitter. If you want an idea of what I’m talking about just look at the responses to any famous person’s tweets.\nOption 2: Analyze the traffic fatality data to identify any geographic, time varying, or other characteristics that are associated with traffic fatalities: https://www.transportation.gov/fastlane/2015-traffic-fatalities-data-has-just-been-released-call-action-download-and-analyze.\nOption 3: Develop a model for predicting life expectancy in Baltimore down to single block resolution with estimates of uncertainty. You may need to develop an approach for “downsampling” since the outcome data you’ll be able to find is likely aggregated at the neighborhood level (http://health.baltimorecity.gov/node/231).\nOption 4: Develop a statistical model for inferring the variables you need to calculate the Gail score (http://www.cancer.gov/bcrisktool/) for a woman based on her Facebook profile. Develop a model for the Gail score prediction from Facebook and its uncertainty. You should include estimates of uncertainty in the predicted score due to your inferred variables.\nOption 5: Potentially fun but super hard project. develop an algorithm for self-driving car using the training data: http://research.comma.ai/. Build a model for predicting at every moment what direction the car should be going, whether it should be signalling, and what speed it should be going. You might consider starting with a small subsample of the (big) training set.\nEach of these projects shares the characteristic that there is an interesting question - but the data may or may not be available. If it is available it may or may not have to be processed/cleaned/organized. Moreover, with the data in hand you may need to think about how it was collected or go out and collect some more data. This kind of problem is inspired by this quote from Dan’s talk - he was talking about math but it could easily have been data science:\n\nAsk yourselves, what problem have you solved, ever, that was worth solving, where you knew knew all of the given information in advance? Where you didn’t have a surplus of information and have to filter it out, or you didn’t have insufficient information and have to go find some?\n\nI realize though that this is advanced data science. So I was thinking about the levels of data science course and how you would build up a curriculum. I came up with the following courses/levels and would be interested in what others thought.\nLevel 0: Background: Basic computing, some calculus with a focus on optimization, basic linear algebra.\nLevel 1: Data science thinking: How to define a question, how to turn a question into a statement about data, how to identify data sets that may be applicable, experimental design, critical thinking about data sets.\nLevel 2: Data science communication: Teaching students how to write about data science, how to express models qualitatively and in mathematical notation, explaining how to interpret results of algorithms/models. Explaining how to make figures.\nLevel 3: Data science tools: Learning the basic tools of R, loading data of various types, reading data, plotting data.\nLevel 4: Real data: Manipulating different file formats, working with “messy” data, trying to organize multiple data sets into one data set.\nLevel 5: Worked examples: Use real data examples, but work them through from start to finish as case studies, don’t make them easy clean data sets, but have a clear path from the beginning of the problem to the end.\nLevel 6: Just the question: Give students a question where you have done a little research to know that it is posisble to get at least some data, but aren’t 100% sure it is the right data or that the problem can be perfectly solved. Part of the learning process here is knowing how to define success or failure and when to keep going or when to quit.\nLevel 7: The student is the scientist: Have the students come up with their own questions and answer them using data.\nI think that a lot of the thought right now in biostatistics has been on level 3 and 4 courses. These are courses where we have students work with real data sets and learn about tools. To be self-sufficient as a data scientist it is clear you need to be able to work with real world data. But what Jake/Nathan are referring to is level 5 or level 6 - cases where you have a question but the data needs a ton of work and may not even be good enough without collecting new information. Jake and Nathan have perfectly identified the ability to translate murkey questions into data answers as the most valuable data skill. If I had to predict the future of data courses I would see them trending in that direction.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:28:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-03-08-when-do-we-need-interpretability/",
    "title": "When do we need interpretability?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-03-08",
    "categories": [],
    "contents": "\nI just saw a link to an interesting article by Finale Doshi-Velez and Been Kim titled “Towards A Rigorous Science of Interpretable Machine Learning”. From the abstract:\n\nUnfortunately, there is little consensus on what interpretability in machine learning is and how to evaluate it for benchmarking. Current interpretability evaluation typically falls into two categories. The first evaluates interpretability in the context of an application: if the system is useful in either a practical application or a simplified version of it, then it must be somehow interpretable. The second evaluates interpretability via a quantifiable proxy: a researcher might first claim that some model class—e.g. sparse linear models, rule lists, gradient boosted trees—are interpretable and then present algorithms to optimize within that class.\n\nThe paper raises a good point, which is that we don’t really have a definition of “interpretability”. We just know it when we see it. For the most part, there’s been some agreement that parametric models are “more interpretable” than other models, but that’s a relativey fuzzy statement.\nI’ve long heard that complex machine learning models that lack any real interpretability are okay because there are many situations where we don’t care “how things work”. When Netflix is recommending my next movie based on my movie history and perhaps other data, the only thing that matters is that the recommendation is something I like. In other words, the user experience defines the value to me. However, in other applications, such as when we’re assessing the relationship between air pollution and lung cancer, a more interpretable model may be required.\nI think the dichotomization between these two kinds of scenarios will eventually go away for a few reasons:\nFor some applications, lack of interpretability is fine…until it’s not. In other words, what happens when things go wrong? Interpretability can help us to decipher why things went wrong and how things can be modified to be fixed. In order to move the levers of a machine to fix it, we need to know exactly where the levers are. Yet another way to say this is that it’s possible to quickly jump from one situation (interpretability not needed) to another situation (what the heck just happened?) very quickly.\nI think interpretability will become the new reproducible research, transmogrified to the machine learning and AI world. In the scientific world, reproducibility took some time to catch on (and has not quite caught on completely), but it is not so controversial now and many people in many fields accept the notion that all studies should at least be reproducible (if not necessarily correct). There was a time when people differentiated between cases that needed reproducibility (big data, computational work), and cases where it wasn’t needed. But that differentiation is slowly going away. I believe interpretability in machine learning and statistical modeling wil go the same way as reproducibility in science.\nUltimately, I think it’s the success of machine learning that brings the requirement of interpretability on to the scene. Because machine learning has become ubiquitous, we as a society begin to develop expectations for what it is supposed to do. Thus, the value of the machine learning begins to be defined externally. It will no longer be good enough to simply provide a great user experience.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:28:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-03-07-time-series-model/",
    "title": "Model building with time series data",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-03-07",
    "categories": [],
    "contents": "\nA nice post by Alex Smolyanskaya over the Stitch Fix blog about some of the unique challenges of model building in a time series context:\n\nCross validation is the process of measuring a model’s predictive power by testing it on randomly selected data that was not used for training. However, autocorrelations in time series data mean that data points are not independent from each other across time, so holding out some data points from the training set doesn’t necessarily remove all their associated information. Further, time series models contain autoregressive components to deal with the autocorrelations. These models rely on having equally spaced data points; if we leave out random subsets of the data, the training and testing sets will have holes that destroy the autoregressive components.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:28:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-03-02-rr-glossy/",
    "title": "Reproducibility and replicability is a glossy science now so watch out for the hype",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2017-03-02",
    "categories": [],
    "contents": "\nReproducibility is the ability to take the code and data from a previous publication, rerun the code and get the same results. Replicability is the ability to rerun an experiment and get “consistent” results with the original study using new data. Results that are not reproducible are hard to verify and results that do not replicate in new studies are harder to trust. It is important that we aim for reproducibility and replicability in science.\nOver the last few years there has been increasing concern about problems with reproducibility and replicability in science. There are a number of suggestions for why this might be:\nPapers published by scientists with lack of training in statistics and computation\nTreating statistics as a second hand discipline that can be “tacked on” at the end of a science experiment\nFinancial incentives for companies and others to publish desirable results.\nAcademic incentives for scientists to publish desirable results so they can get their next grant.\nIncentives for journals to publish surprising/eye catching/interesting results.\nOver-hyped studies with limited statistical characteristics (small sample size, questionable study populations etc.)\nTED-style sound bytes of scientific results that are digested and repeated in the press despite limited scientific evidence.\nScientists who refuse to consider alternative explanations for their data\nUsually the targets of discussion about reproducibility and replicability are highly visible scientific studies. The targets are usually papers in what are considered “top journals” or the papers in journals like Science and Nature that seek to maximize visibility. Or, more recently, entire fields of science that are widely publicized - like psychology or cancer biology are targeted for reproducibility and replicability studies.\nThese studies have pointed out serious issues with the statistics, study designs, code availability and methods descriptions in papers they have studied. These are fundamental issues that deserve attention and should be taught to all scientists. As more papers have come out pointing out potential issues, they have merged into what is being called “a crisis of reproducibility”, “a crisis of replicability”, “a crisis of confidence in science” or other equally strong statements.\nAs the interest around reproducibility and replicability has built to a fever pitch in the scientific community it has morphed into a glossy scientific field in its own right. All of the characteristics are in place:\nA big central “positive” narrative that all science is not replicable, reproducible, or correct.\nIncentives to publish these types of results because they can appear in Nature/Science/other glossy journals. (I’m not immune to this)\nStrong and aggressive responses to papers that provide alternative explanations or don’t fit the narrative.\nResearchers whose careers depend on the narrative being true\nTED-style talks and sound bytes (“most published research is false”, “most papers don’t replicate”)\nPress hype, including for papers with statistical weaknesses (small sample sizes, weaker study designs)\nReproducibility and replicability has “arrived” and become a field in its own right. That has both positives and negatives. On the positive side it means critical statistical issues are now being talked about by a broader range of people. On the negative side, researchers now have to do the same sober evaluation of the claims in reproducibility and replicability papers that they do for any other scientific field. Papers on reproducibility and replicability must be judged with the same critical eye as we apply to any other scientific study. That way we can sift through the hype and move science forward.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:28:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-02-20-podcasting-setup/",
    "title": "My Podcasting Setup",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-02-20",
    "categories": [],
    "contents": "\nI’ve gotten a number of inquiries over the last 2 years about my podcasting setup and I’ve been meaning to write about it but….\nBut here it is! I actually wanted to write this because I felt like there actually wasn’t a ton of good information about this on the Internet that wasn’t for people who wanted to do it professionally but were rather more “casual” podcasters. So here’s what I’ve got.\nThere are two types of podcasts roughly: The kind you record with everyone in the same room and the kind you record where everyone is in different rooms. They both require slightly different setups so I’ll talk about both. For me, Elizabeth Matsui and I record The Effort Report locally because we’re both at Johns Hopkins. But Hilary Parker and I record Not So Standard Deviations remotely because she’s on the other side of the country most of the time.\nRecording Equipment\nWhen Hilary and I first started we just used the microphone attached to the headphones you get with your iPhone or whatever. That’s okay but the sound feels very “narrow” to me. That said, it’s a good way to get started and it likely costs you nothing.\nThe next level up for many people is the Blue Yeti USB Microphone which is perfectly fine microphone and not too expensive. Also, it uses USB (as opposed to more professional XLR) so it connects to any computer, which is nice. However, it typically retails for $120, which isn’t nothing, and there are probably cheaper microphones that are just as good. For example, Jason Snell recommends the Audio Technica ATR2100 which is only about $70.\nIf you’re willing to shell out a little more money, I’d highly recommend the Zoom H4n portable recorder. This is actually two things: a microphone and a recorder. It has a nice stero microphone built into the top along with two XLR inputs on the bottom that allow you to record from external mics. It records to SD cards so it’s great for a portable setup where you don’t want to carry a computer around with you. It retails for about $200 so it’s not cheap, but in my opinion it is worth every penny. I’ve been using my H4n for years now.\nBecause we do a lot or recording for our online courses here, we’ve actually got a bit more equipment in the office. So for in-person podcasts I sometimes record using a Sennheiser MKH416-P48US attached to an Auray MS-5230T microphone stand which is decidedly not cheap but is a great piece of hardware.\nBy the way, a microphone stand is great to have, if you can get one, so you don’t have to set the microphone on your desk or table. That way if you bump the table by accident or generally like to bang the table, it won’t get picked up on the microphone. It’s not something to get right away, but maybe later when you make the big time.\nRecording Software\nIf you’re recording by yourself, you can just hook up your microphone to your computer and record to any old software that records sound (on the Mac you can use Quicktime). If you have multiple people, you can either\nSpeak into the same mic and have both your voices recorded on the same audio file\nUse separate mics (and separate computers) and record separtely on to separate audio files. This requires synching the audio files in an editor, but that’s not too big a deal if you only have 2-3 people.\nFor local podcasts, I actually just use the H4n and record directly to the SD card. This creates separate WAV files for each microphone that are already synced so you can just plop them in the editor.\nFor remote podcasts, you’ll need some communication software. Hilary and I use Zencastr which has its own VoIP system that allows you to talk to anyone by just sending your guests a link. So I create a session in Zencastr, send Hilary the link for the session, she logs in (without needing any credentials) and we just start talking. The web site records the audio directly off of your microphone and then uploads the audio files (one for each guest) to Dropbox. The service is really nice and there are now a few just like it. Zencastr costs $20 a month right now but there is a limited free tier.\nThe other approach is to use something like Skype and then use something like ecamm call-recorder to record the conversation. The downside with this approach is that if you have any network trouble that messes up the audio, then you will also record that. However, Zencastr (and related services) do not work on iOS devices and other devices that use WebKit based browsers. So if you have someone calling in on a mobile device via Skype or something, then you’ll have to use this approach. Otherwise, I prefer the Zencastr approach and can’t really see any downside except for the cost.\nEditing Software\nThere isn’t a lot of software that’s specifically designed for editing podcasts. I actually started off editing podcasts in Final Cut Pro X (nonlinear video editor) because that’s what I was familiar with. But now I use Logic Pro X, which is not really designed for podcasts, but it’s a real digital audio workstation and has nice features (like strip silence). But I think something like Audacity would be fine for basic editing.\nThe main thing I need to do with editing is merge the different audio tracks together and cut off any extraneous material at the beginning or the end. I don’t usually do a lot of editing in the middle unless there’s a major mishap like a siren goes by or a cat jumps on the computer. Once the editing is done I bounce to an AAC or MP3 file for uploading.\nHosting\nYou’ll need a service for hosting your audio files if you don’t have your own server. You can technically host your audio files anywhere, but specific services have niceties like auto-generating the RSS feed. For Not So Standard Deviations I use SoundCloud and for The Effort Report I use Libsyn.\nOf the two services, I think I prefer Libsyn, because it’s specifically designed for podcasting and has somewhat better analytics. The web site feels a little bit like it was designed in 2003, but otherwise it works great. Libsyn also has features for things like advertising and subscriptions, but I don’t use any of those. SoundCloud is fine but wasn’t really designed for podcasting and sometimes feels a little unnatural.\nSummary\nIf you’re interested in getting started in podcasting, here’s my bottom line:\nGet a partner. It’s more fun that way!\nIf you and your partner are remote, use Zencastr or something similar.\nSplurge for the Zoom H4n if you can, otherwise get a reasonable cheap microphone like the Audio Technica or the Yeti.\nDon’t focus too much on editing. Just clip off the beginning and the end.\nHost on Libsyn.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:27:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-02-15-Data-Scientists-Clashing-at-Hedge-Funds/",
    "title": "Data Scientists Clashing at Hedge Funds",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-02-15",
    "categories": [],
    "contents": "\nThere’s an interesting article over at Bloomberg about how data scientists have struggled at some hedge funds:\n\nThe firms have been loading up on data scientists and coders to deliver on the promise of quantitative investing and lift their ho-hum returns. But they are discovering that the marriage of old-school managers and data-driven quants can be rocky. Managers who have relied on gut calls resist ceding control to scientists and their trading signals. And quants, emboldened by the success of computer-driven funds like Renaissance Technologies, bristle at their second-class status and vie for a bigger voice in investing.\n\nThere are some interesting tidbits in the article that I think hold lessons for any collaboration between a data scientist or analyst and a non-data scientist (for lack of a better word).\nAt Point72, the family office successor to SAC Capital, problems at the quant unit (known as Aperio):\n\nThe divide between Aperio quants and fundamental money managers was also intellectual. They struggled to communicate about the basics, like how big data could inform investment decisions. [Michael] Recce’s team, which was stacked with data scientists and coders, developed trading signals but didn’t always fully explain the margin of error in the analysis to make them useful to fund managers, the people said.\n\nIt’s hard to know the details of what actually happened, but for data scientists collaborating with others, there always needs to be an explanation of “what’s going on”. There’s a general feeling that it’s okay that machine learning techniques build complicated uninterpretable models because they work better. But in my experience that’s not enough. People want to know why they work better, when they work better, and when they don’t work.\nOn over-theorizing:\n\nHaynes, who joined Stamford, Connecticut-based Point72 in early 2014 after about two decades at McKinsey & Co., and other senior managers grew dissatisfied with Aperio’s progress and impact on returns, the people said. When the group obtained new data sets, it spent too much time developing theories about how to process them rather than quickly producing actionable results.\n\nI don’t necessarily agree with this “criticism”, but I only put it here because the land of hedge funds isn’t generally viewed on the outside as a place where lots of theorizing goes on.\nAt BlueMountain, another hedge fund:\n\nWhen quants showed their risk analysis and trading signals to fundamental managers, they sometimes were rejected as nothing new, the people said. Quants at times wondered if managers simply didn’t want to give them credit for their ideas.\n\nI’ve seen this quite a bit. When a data scientist presents results to collaborators, there’s often two responses:\n“I knew that already” and so you haven’t taught me anything new\n“I didn’t know that already” and so you must be wrong\nThe common link here, of course, is the inability to admit that there are things you don’t know. Whether this is an inherent character flaw or something that can be overcome through teaching is not yet clear to me. But it is common when data is brought to bear on a problem that previously lacked data. One of the key tasks that a data scientist in any industry must prepare for is the task of giving people information that will make them uncomfortable.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:27:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-02-13-nssd-episode-32/",
    "title": "Not So Standard Deviations Episode 32 - You Have to Reinvent the Wheel a Few Times",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-02-13",
    "categories": [],
    "contents": "\nHilary and I discuss training in PhD programs, estimating the variance vs. the standard deviation, the bias variance tradeoff, and explainable machine learning.\nWe’re also introducing a new level of support on our Patreon page, where you can get access to some of the outtakes from our episodes. Check out our Patreon page for details.\nShow notes:\nExplainable AI\nStitch Fix Blog NBA Rankings\nDavid Robinson’s Empirical Bayes book\nWar on the Rocks podcast\nRoger on Twitter\nHilary on Twitter\nGet the Not So Standard Deviations book\nSubscribe to the podcast on iTunes\nSubscribe to the podcast on Google Play\nFind past episodes\nDownload the audio for this episode\nListen here: \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:27:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-02-01-reproducible-research-limits/",
    "title": "Reproducible Research Needs Some Limiting Principles",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-02-01",
    "categories": [],
    "contents": "\nOver the past 10 years thinking and writing about reproducible research, I’ve come to the conclusion that much of the discussion is incomplete. While I think we as a scientific community have come a long way in changing people’s thinking about data and code and making them available to others, there are some key sticking points that keep coming up that are preventing further progress in the area.\nWhen I used to write about reproducibility, I felt that the primary challenge/roadblock was a lack of tooling. Much has changed in just the last five years though, and many new tools have been developed to make life a lot easier. Packages like knitr (for R), markdown, and iPython notebooks, have made writing reproducible data analysis documents a lot easier. Web sites like GitHub and many others have made distributing analyses a lot simpler because now everyone effectively has a free web site (this was NOT true in 2005).\nEven still, our basic definition of reproducibility is incomplete. Most people would say that a data analysis is reproducible if the analytic data and metadata are available and the code that did the analysis is available. Furthermore, it would be preferable to have some documentation to go along with both. But there are some key issues that need to be resolved to complete this general definition.\nReproducible for Whom?\nIn discussions about reproducibility with others, the topic of who should be able to reproduce the analysis only occasionally comes up. There’s a general sense, especially amongst academics, that anyone should be able to reproduce any analysis if they wanted to.\nThere is an analogy with free software here in the sense that free software can be free for some people and not for others. This made more sense in the days before the Internet when distribution was much more costly. The idea here was that I could write software for a client and give them the source code for that software (as they would surely demand). The software is free for them but not for anyone else. But free software ultimately only matters when it comes to distribution. Once I distribute a piece of software, that’s when all the restrictions come into play. However, if I only distribute it to a few people, I only need to guarantee that those few people have those freedoms.\nRichard Stallman once said that something like 90% of software was free software because almost all software being written was custom software for individual clients (I have no idea where he got this number). Even if the number is wrong, the point still stands that if I write software for a single person, it can be free for that person even if no one in the world has access to the software.\nOf course, now with the Internet, everything pretty much gets distributed to everyone because there’s nothing stopping someone from taking a piece of free software and posting it on a web site. But the idea still holds: Free software only needs to be free for the people who receive it.\nThat said, the analogy is not perfect. Software and research are not the same thing. They key difference is that you can’t call something research unless is generally available and disseminated. If Pfizer comes up with the cure for cancer and never tells anyone about it, it’s not research. If I discover that there’s a 9th planet and only tell my neighbor about it, it’s not research. Many companies might call those activities research (particularly from an tax/accounting point of view) but since society doesn’t get to learn about them, it’s not research.\nIf research is by definition disseminated to all, then it should therefore be reproducible by all. However, there are at least two circumstances in which we do not even pretend to believe this is possible.\nImbalance of resources: If I conduct a data analysis that requires the world’s largest supercomputer, I can make all the code and data available that I want–few people will be able to actually reproduce it. That’s an extreme case, but even if I were to make use of a dramatically smaller computing cluster it’s unlikely that anyone would be able to recreate those resources. So I can distribute something that’s reproducible in theory but not in reality by most people.\nProtected data: Numerous analyses in the biomedical sciences make use of protected health information that cannot easily be disseminated. Privacy is an important issue, in part, because in many cases it allows us to collect the data in the first place. However, most would agree we cannot simply post that data for all to see in the name of reproducibility. First, it is against the law, and second it would likely deter anyone from agreeing to participate in any study in the future.\nWe can pretend that we can make data analyses reproducible for all, but in reality it’s not possible. So perhaps it would make sense for us to consider whether a limiting principle should be applied. The danger of not considering it is that one may take things to the extreme—if it can’t be made reproducible for all, then why bother trying? A partial solution is needed here.\nFor How Long?\nAnother question that needs to be resolved for reproducibility to be a widely implemented and sustainable phenomenon is for how long should something be reproducible? Ultimately, this is a question about time and resources because ensuring that data and code can be made available and can run on current platforms in perpetuity requires substantial time and money. In the academic community, where projects are often funded off of grants or contracts with finite lifespans, often the money is long gone even though the data and code must be maintained. The question then is who pays for the maintainence and the upkeep of the data and code?\nI’ve never heard a satisfactory answer to this question. If the answer is that data analyses should be reproducible forever, then we need to consider a different funding model. This position would require a perpetual funds model, essentially an endowment, for each project that is disseminated and claims to be reproducible. The endowment would pay for things like servers for hosting the code and data and perhaps engineers to adapt and adjust the code as the surrounding environment changes. While there are a number of repositories that have developed scalable operating models, it’s not clear to me that the funding model is completely sustainable.\nIf we look at how scientific publications are sustained, we see that it’s largely private enterprise that shoulders the burden. Journals house most of the publications out there and they charge a fee for access (some for profit, some not for profit). Whether the reader pays or the author pays is not relevant, the point is that a decision has been made about who pays.\nThe author-pays model is interesting though. Here, an author pays a publication charge of ~$2,000, and the reader never pays anything for access (in perpetuity, presumably). The $2,000 payment by the author is like a one-time capital expense for maintaining that one publication forever (a mini-endowment, in a sense). It works for authors because grant/contract supported research often budget for one-time publication charges. There’s no need for continued payments after a grant/contract has expired.\nThe publication system is quite a bit simpler because almost all publications are the same size and require the same resources for access—basically a web site that can serve up PDF files and people to maintain it. For data analyses, one could see things potentially getting out of control. For a large analysis with terabytes of data, what would the one-time up-front fee be to house the data and pay for anyone to access it for free forever?\nUsing Amazon’s monthly cost estimator we can get a rough sense of what the pure data storage might cost. Suppose we have a 10GB dataset that we want to store and we anticipate that it might be downloaded 10 times per month. This would cost about $7.65 per month, or $91.80 per year. If we assume Amazon raises their prices about 3% per year and a discount rate of 5%, the total cost for the storage is $4,590. If we tack on 20% for other costs, that brings us to $5,508. This is perhaps not unreasonable, and the scenario would certainly include most people. For comparison a 1 TB dataset downloaded once a year, using the same formula gives us a one-time cost of about $40,000. This is real money when it comes to fixed research budgets and would likely require some discussion of trade-offs.\nSummary\nReproducibility is a necessity in science, but it’s high time that we start considering the practical implications of actually doing the job. There are still holdouts when it comes to the basic idea of reproducibiltiy, but they are fewer and farther between. If we do not seriously consider the details of how to implement reproducibility, perhaps by introducing some limiting principles, we may never be able to achieve any sort of widespread adoption.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:27:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-01-26-new-prototyping-class/",
    "title": "New class - Data App Prototyping for Public Health and Beyond",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2017-01-26",
    "categories": [],
    "contents": "\nAre you interested in building data apps to help save the world, start the next big business, or just to see if you can? We are running a data app prototyping class for people interested in creating these apps.\nThis will be a special topics class at JHU and is open to any undergrad student, grad student, postdoc, or faculty member at the university. We are also seeing if we can make the class available to people outside of JHU so even if you aren’t at JHU but are interested you should let us know below.\nOne of the principles of our approach is that anyone can prototype an app. Our class starts with some tutorials on Shiny and R. While we have no formal pre-reqs for the class you will have much more fun if you have the background equivalent to our Coursera classes:\nData Scientist’s Toolbox\nR programming\nBuilding R packages\nDeveloping Data Products\nIf you don’t have that background you can take the classes online starting now to get up to speed! To see some examples of apps we will be building check out our gallery.\nWe will mostly be able to support development with R and Shiny but would be pumped to accept people with other kinds of development background - we just might not be able to give a lot of technical assistance.\nAs part of the course we are also working with JHU’s Fast Forward program to streamline and ease the process of starting a company around the app you build for the class. So if you have entrepreneurial ambitions, this is the class for you!\nWe are in the process of setting up the course times, locations, and enrollment cap. The class will run from March to May (exact dates TBD). To sign up for announcements about the class please fill out your information here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:27:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-01-23-ux-value/",
    "title": "User Experience and Value in Products",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-01-23",
    "categories": [],
    "contents": "\nOver the past year, there have been a number of recurring topics in my global news feed that have a shared theme to them. Some examples of these topics are:\nFake news: Before and after the election in 2016, Facebook (or Facebook’s Trending News algorithm) was accused of promoting news stories that turned out to be completely false, promoted by dubious news sources in FYROM and elsewhere.\nTheranos: This diagnostic testing company promised to revolutionize the blood testing business and prevent disease for all by making blood testing simple and painless. This way people would not be afraid to get blood tests and would do them more often, presumably catching diseases while they were in the very early stages. Theranos lobbied to allow patients order their own blood tests so that they wouldn’t need a doctor’s order.\nHomeopathy: This a so-called alternative medical system developed in the late 18th century based on notions such as “like cures like” and “law of minimum dose.\nOnline education: New companies like Coursera and Udacity promised to revolutionize education by making it accessible to a broader audience than conventional universities were able.\nWhat exactly do these things have in common?\nFirst, consumers love them. Fake news played to people’s biases by confirming to them, from a seemingly trustworthy source, what they always “knew to be true”. The fact that the stories weren’t actually true was irrelevant given that users enjoyed the experience of seeing what they agreed with. Perhaps the best explanation of the entire Facebook fake news issue was from Kim-Mai Cutler:\n\n\nThe best way to have the stickiest and most lucrative product? Be a systematic tool for confirmation bias. https://t.co/8uOHZLomhX\n\n— Kim-Mai Cutler (@kimmaicutler) November 10, 2016\n\nTheranos promised to revolutionize blood testing and change the user experience behind the whole industry. Indeed the company had some fans (particularly amongst its investor base). However, after investigations by the Center for Medicare and Medicaid Services, the FDA, and an independent laboratory, it was found that Theranos’s blood testing machine was wildly inconsistent and variable, leading to Theranos ultimately retracting all of its blood test results and cutting half its workforce.\nHomeopathy is not company specific, but is touted by many as an “alternative” treatment for many diseases, with many claiming that it “works for them”. However, the NIH states quite clearly on its web site that “There is little evidence to support homeopathy as an effective treatment for any specific condition.”\nFinally, companies like Coursera and Udacity in the education space have indeed produced products that people like, but in some instances have hit bumps in the road. Udacity conducted a brief experiment/program with San Jose State University that failed due to the large differences between the population that took online courses and the one that took them in person. Coursera has massive offerings from major universities (including my own) but has run into continuing challenges with drop out and questions over whether the courses offered are suitable for job placement.\nUser Experience and Value\nIn each of these four examples there is a consumer product that people love, often because they provide a great user experience. Take the fake news example–people love to read headlines from “trusted” news sources that agree with what they believe. With Theranos, people love to take a blood test that is not painful (maybe “love” is the wrong word here). With many consumer products companies, it is the user experience that defines the value of a product. Often when describing the user experience, you are simultaneously describing the value of the product.\nTake for example Uber. With Uber, you open an app on your phone, click a button to order a car, watch the car approach you on your phone with an estimate of how long you will be waiting, get in the car and go to your destination, and get out without having to deal with paying. If someone were to ask me “What’s the value of Uber?” I would probably just repeat the description in the previous sentence. Isn’t it obvious that it’s better than the usual taxi experience? The same could be said for many companies that have recently come up: Airbnb, Amazon, Apple, Google. With many of the products from these companies, the description of the user experience is a description of its value.\nDisruption Through User Experience\nIn the example of Uber (and Airbnb, and Amazon, etc.) you could depict the relationship between the product, the user experience, and the value as such:\n\nAny changes that you can make to the product to improve the user experience will then improve the value that the product offers. Another way to say it is that the user experience serves as a surrogate outcome for the value. We can influence the UX and know that we are improving value. Furthermore, any measurements that we take on the UX (surveys, focus groups, app data) will serve as direct observations on the value provided to customers.\nNew companies in these kinds of consumer product spaces can disrupt the incumbents by providing a much better user experience. When incumbents have gotten fat and lazy, there is often a sizable segment of the customer base that feels underserved. That’s when new companies can swoop in to specifically serve that segment, often with a “worse” product overall (as in fewer features) and usually much cheaper. The Internet has made the “swooping in” much easier by dramatically reducing transaction and distribution costs. Once the new company has a foothold, they can gradually work their way up the ladder of customer segments to take over the market. It’s classic disruption theory a la Clayton Christensen.\nWhen Value Defines the User Experience and Product\nThere has been much talk of applying the classic disruption model to every space imaginable, but I contend that not all product spaces are the same. In particular, the four examples I described in the beginning of this post cover some of those different areas:\nMedicine (Theranos, homeopathy)\nNews (Facebook/fake news)\nEducation (Coursera/Udacity)\nOne thing you’ll notice about these areas, particularly with medicine and education, is that they are all heavily regulated. The reason is because we as a community have decided that there is a minimum level of value that is required to be provided by entities in this space. That is, the value that a product offers is defined first, before the product can come to market. Therefore, the value of the product actually constrains the space of products that can be produced. We can depict this relationship as such:\n\nIn classic regression modeling language, the value of a product must be “adjusted for” before examining the relationship between the product and the user experience. Naturally, as in any regression problem, when you adjust for a variable that is related to the product and the user experience, you reduce the overall variation in the product.\nIn situations where the value defines the product and the user experience, there is much less room to maneuver for new entrants in the market. The reason is because they, like everyone else, are constrained by the value that is agreed upon by the community, usually in the form of regulations.\nWhen Theranos comes in and claims that it’s going to dramatically improve the user experience of blood testing, that’s great, but they must be constrained by the value that society demands, which is a certain precision and accuracy in its testing results. Companies in the online education space are welcome to disrupt things by providing a better user experience. Online offerings in fact do this by allowing students to take classes according to their own schedule, wherever they may live in the world. But we still demand that the students learn an agreed-upon set of facts, skills, or lessons.\nNew companies will often argue that the things that we currently value are outdated or no longer valuable. Their incentive is to change the value required so that there is more room for new companies to enter the space. This is a good thing, but it’s important to realize that this cannot happen solely through changes in the product. Innovative features of a product may help us to understand that we should be valuing different things, but ultimately the change in what we preceive as value occurs independently of any given product.\nWhen I see new companies enter the education, medicine, or news areas, I always hesitate a bit because I want some assurance that they will still provide the value that we have come to expect. In addition, with these particular areas, there is a genuine sense that failing to deliver on what we value could cause serious harm to individuals. However, I think the discussion that is provoked by new companies entering the space is always welcome because we need to constantly re-evaluate what we value and whether it matches the needs of our time.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:27:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-01-18-data-prototyping-class/",
    "title": "Got a data app idea? Apply to get it prototyped by the JHU DSL!",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2017-01-18",
    "categories": [],
    "contents": "\nGet your app builtLast fall we ran the first iteration of a class at the Johns Hopkins Data Science Lab where we teach students to build data web-apps using Shiny, R, GoogleSheets and a number of other technologies. Our goals were to teach students to build data products, to reduce friction for students who want to build things with data, and to help people solve important data problems with web and SMS apps.\nWe are going to be running a second iteration of our program from March-June this year. We are looking for awesome projects for students to build that solve real world problems. We are particularly interested in projects that could have a positive impact on health but are open to any cool idea. We generally build apps that are useful for:\nData donation - if you have a group of people you would like to donate data to your project.\nData collection - if you would like to build an app for collecting data from people.\nData visualziation - if you have a data set and would like to have a web app for interacting with the data\nData interaction - if you have a statistical or machine learning model and you would like a web interface for it.\nBut we are interested in any consumer-facing data product that you might be interested in having built. We want you to submit your wildest, most interesting ideas and we’ll see if we can get them built for you.\nWe are hoping to solicit a large number of projects and then build as many as possible. The best part is that we will build the prototype for you for free! If you have an idea of something you’d like built please submit it to this Google form.\nStudents in the class will select projects they are interested in during early March. We will let you know if your idea was selected for the program by mid-March. If you aren’t selected you will have the opportunity to roll your submission over to our next round of prototyping.\nI’ll be writing a separate post targeted at students, but if you are interested in being a data app prototyper, sign up here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:27:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-01-17-effort-report-episode-23/",
    "title": "Interview with Al Sommer - Effort Report Episode 23",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-01-17",
    "categories": [],
    "contents": "\nMy colleage Elizabeth Matsui and I had a great opportunity to talk with Al Sommer on the latest episode of our podcast The Effort Report. Al is the former Dean of the Johns Hopkins Bloomberg School of Public Health and is Professor of Epidemiology and International Health at the School. He is (among other things) world reknown for his pioneering research in vitamin A deficiency and mortality in children.\nAl had some good bits of advice for academics and being successful in academia.\n\nWhat you are excited about and interested in at the moment, you’re much more likely to be succesful at—because you’re excited about it! So you’re going to get up at 2 in the morning and think about it, you’re going to be putting things together in ways that nobody else has put things together. And guess what? When you do that you’re more succesful [and] you actual end up getting academic promotions.\n\nOn the slow rate of progress:\n\nIt took ten years, after we had seven randomized trials already to show that you get this 1/3 reduction in child mortality by giving them two cents worth of vitamin A twice a year. It took ten years to convince the child survival Nawabs of the world, and there are still some that don’t believe it.\n\nOn working overseas:\n\nIt used to be true [that] it’s a lot easier to work overseas than it is to work here because the experts come from somewhere else. You’re never an expert in your own home.\n\nYou can listen to the entire episode here:\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:27:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-01-09-nssd-episode-30/",
    "title": "Not So Standard Deviations Episode 30 - Philately and Numismatology",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2017-01-09",
    "categories": [],
    "contents": "\nHilary and I follow up on open data and data sharing in government. They also discuss artificial intelligence, self-driving cars, and doing your taxes in R.\nIf you have questions you’d like Hilary and me to answer, you can send them to nssdeviations @ gmail.com or tweet us at [@NSSDeviations](https://twitter.com/nssdeviations).\nShow notes:\nLucy D’Agostino McGowan (@LucyStats) made a great translation of Hill’s criteria using XKCD comics\nLucy’s web page\nPreparing for the Future of Artificial Intelligence\nPartially Derivative White House Special – with DJ Patil, US Chief Data Scientist\nNot So Standard Deviations – Standards are Like Toothbrushes – with with Daniel Morgan, Chief Data Officer for the U.S. Department of Transportation and Terah Lyons, Policy Advisor to the Chief Technology Officer of the U.S.\nHenry Gitner Philatelists\nSome Pioneers of Modern Statistical Theory: A Personal Reflection by Sir David R. Cox\nDownload the audio for this episode\nListen here: \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:27:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-12-29-some-stress-reducers/",
    "title": "Some things I've found help reduce my stress around science",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-12-29",
    "categories": [],
    "contents": "\nBeing a scientist can be pretty stressful for any number of reasons, from the peer review process, to getting funding, to getting blown up on the internet.\nLike a lot of academics I suffer from a lot of stress related to my own high standards and the imposter syndrome that comes from not meeting them on a regular basis. I was just reading through the excellent material in Lorena Barba’s class on essential skills in reproducibility and came across this set of slides by Phillip Stark. The one that caught my attention said:\n\nIf I say just trust me and I’m wrong, I’m untrustworthy. If I say here’s my work and it’s wrong, I’m honest, human, and serving scientific progress.\n\nI love this quote because it shows how being open about both your successes and failures makes it less stressful to be a scientist. Inspired by this quote I decided to make a list of things that I’ve learned through hard experience do not help me with my own imposter syndrome and do help me to feel less stressed out about my science.\nPut everything out in the open. We release all of our software, data, and analysis scripts. This has led to almost exclusively positive interactions with people as they help us figure out good and bad things about our work.\nAdmit mistakes quickly. Since my code/data are out in the open I’ve had people find little bugs and big whoa this is bad bugs in my code. I used to freak out when that happens. But I found the thing that minimizes my stress is to just quickly admit the error and submit updates/changes/revisions to code and papers as necessary.\nRespond to requests for support at my own pace. I try to be as responsive as I can when people email me about software/data/code/papers of mine. I used to stress about doing this right away when I would get the emails. I still try to be prompt, but I don’t let that dominate my attention/time. I also prioritize things that are wrong/problematic and then later handle the requests for free consulting every open source person gets.\nTreat rejection as a feature not a bug. This one is by far the hardest for me but preprints have helped a ton. The academic system is designed to be critical. That is a good thing, skepticism is one of the key tenets of the scientific process. It took me a while to just plan on one or two rejections for each paper, one or two or more rejections for each grant, etc. But now that I plan on the rejection I find I can just focus on how to steadily move forward and constructively address criticism rather than taking it as a personal blow.\nDon’t argue with people on the internet, especially on Twitter. This is a new one for me and one I’m having to practice hard every single day. But I’ve found that I’ve had very few constructive debates on Twitter. I also found that this is almost purely negative energy for me and doesn’t help me accomplish much.\nRedefine success. I’ve found that if I recalibrate what success means to include accomplishing tasks like peer reviewing papers, getting letters of recommendation sent at the right times, providing support to people I mentor, and the submission rather than the success of papers/grants then I’m much less stressed out.\nDon’t compare myself to other scientists. It is very hard to get good evaluation in science and I’m extra bad at self-evaluation. Scientists are good in many different dimensions and so whenever I pick a one dimensional summary and compare myself to others there are always people who are “better” than me. I find I’m happier when I set internal, short term goals for myself and only compare myself to them.\nWhen comparing, at least pick a metric I’m good at. I’d like to claim I never compare myself to others, but the reality is I do it more than I’d like. I’ve found one way to not stress myself out for my own internal comparisons is to pick metrics I’m good at - even if they aren’t the “right” metrics. That way at least if I’m comparing I’m not hurting my own psyche.\nLet myself be bummed sometimes. Some days despite all of that I still get the imposter syndrome feels and can’t get out of the funk. I used to beat myself up about those days, but now I try to just build that into the rhythm of doing work.\nTry very hard to be positive in my interactions. This is another hard one, because it is important to be skeptical/critical as a scientist. But I also try very hard to do that in as productive a way as possible. I try to assume other people are doing the right thing and I try very hard to stay positive or neutral when writing blog posts/opinion pieces, etc.\nRealize that giving credit doesn’t take away from me. In my research career I have worked with some extremely generous mentors. They taught me to always give credit whenever possible. I also learned from Roger that you can give credit and not lose anything yourself, in fact you almost always gain. Giving credit is low cost but feels really good so is a nice thing to help me feel better.\nThe last thing I’d say is that having a blog has helped reduce my stress, because sometimes I’m having a hard time getting going on my big project for the day and I can quickly write a blog post and still feel like I got something done…\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:27:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-12-20-noncomprehensive-list-of-awesome/",
    "title": "A non-comprehensive list of awesome things other people did in 2016",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-12-20",
    "categories": [],
    "contents": "\nEditor’s note: For the last few years I have made a list of awesome things that other people did (2015, 2014, 2013). Like in previous years I’m making a list, again right off the top of my head. If you know of some, you should make your own list or add it to the comments! I have also avoided talking about stuff I worked on or that people here at Hopkins are doing because this post is supposed to be about other people’s awesome stuff. I write this post because a blog often feels like a place to complain, but we started Simply Stats as a place to be pumped up about the stuff people were doing with data.\nThomas Lin Pedersen created the tweenr package for interpolating graphs in animations. Check out this awesome logo he made with it.\nYihui Xie is still blowing away everything he does. First it was bookdown and then the yolo feature in xaringan package.\nJ Alammar built this great visual introduction to neural networks\nJenny Bryan is working literal world wonders with legos to teach functional programming. I loved her Data Rectangling talk. The analogy between exponential families and data frames is so so good.\nHadley Wickham’s book on R for data science is everything you’d expect. Super clear, great examples, just a really nice book.\nDavid Robinson is a machine put on this earth to create awesome data science stuff. Here is analyzing Trump’s tweets and here he is on empirical Bayes modeling explained with baseball.\nJulia Silge and David created the tidytext package. This is a holy moly big contribution to NLP in R. They also have a killer book on tidy text mining.\nJulia used the package to do this fascinating post on mining Reddit after the election.\nIt would be hard to pick just five different major contributions from JJ Allaire (great interview here), Joe Cheng, and the rest of the Rstudio folks. Rstudio is absolutely churning out awesome stuff at a rate that is hard to keep up with. I loved R notebooks and have used them extensively for teaching.\nKonrad Kording and Brett Mensh full on mike dropped on how to write a paper with their 10 simple rules piece Figure 1 from that paper should be affixed to the office of every student/faculty in the world permanently.\nYaniv Erlich just can’t stop himself from doing interesting things like seeq.io and dna.land.\nThomaz Berisa and Joe Pickrell set up a freaking Python API for genomics projects.\nDataCamp continues to do great things. I love their DataChats series and they have been rolling out tons of new courses.\nSean Rife and Michele Nuijten created statcheck.io for checking papers for p-value calculation errors. This was all over the press, but I just like the site as a dummy proofing for myself.\nThis was the artificial intelligence tweet of the year\nI loved seeing PLoS Genetics start a policy of looking for papers in biorxiv.\nMatthew Stephens post on his preprint getting pre-accepted and reproducibility is also awesome. Preprints are so hot right now!\nLorena Barba made this amazing reproducibility syllabus then won the Leamer-Rosenthal prize in open science.\nColin Dewey continues to do just stellar stellar work, this time on re-annotating genomics samples. This is one of the key open problems in genomics.\nI love FlowingData sooooo much. Here is one on the changing American diet.\nIf you like computational biology and data science and like super detailed reports of meetings/talks you MIchael Hoffman is your man. How he actually summarizes that much information in real time is still beyond me.\nI really really wish I had been at Alyssa Frazee’s talk at startup.ml but loved this review of it. Sampling, inverse probability weighting? Love that stats flavor!\nI have followed Cathy O’Neil for a long time in her persona as mathbabedotorg so it is no surprise to me that her new book Weapons of Math Descruction is so good. One of the best works on the ethics of data out there.\nA related and very important piece is on Machine bias in sentencing by Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner at ProPublica.\nDimitris Rizopolous created this stellar integrated Shiny app for his repeated measures class. I wish I could build things half this nice.\nDaniel Engber’s piece on Who will debunk the debunkers? at fivethirtyeight just keeps getting more relevant.\nI rarely am willing to watch a talk posted on the internet, but Amelia McNamara’s talk on seeing nothing was an exception. Plus she talks so fast #jealous.\nSherri Rose’s post on economic diversity in the academy focuses on statistics but should be required reading for anyone thinking about diversity. Everything about it is impressive.\nIf you like your data science with a side of Python you should definitely be checking out Jake Vanderplas’s data science handbook and the associated Jupyter notebooks.\nI love Thomas Lumley being snarky about the stats news. Its a guilty pleasure. If he ever collected them into a book I’d buy it (hint Thomas :)).\nDorothy Bishop’s blog is one of the ones I read super regularly. Her post on When is a replication a replication is just one example of her very clearly explaining a complicated topic in a sensible way. I find that so hard to do and she does it so well.\nBen Goldacre’s crowd is doing a bunch of interesting things. I really like their OpenPrescribing project.\nI’m really excited to see what Elizabeth Rhodes does with the experimental design for the Ycombinator Basic Income Experiment.\nLucy D’Agostino McGowan made this amazing explanation of Hill’s criterion using xckd.\nIt is hard to overstate how good Leslie McClure’s blog is. This post on biostatistics is public health should be read aloud at every SPH in the US.\nThe ASA’s statement on p-values is a really nice summary of all the issues around a surprisngly controversial topic. Ron Wasserstein and Nicole Lazar did a great job putting it together.\nI really liked this piece on the relationship between income and life expectancy by Raj Chetty and company.\nChristie Aschwanden continues to be the voice of reason on the statistical crises in science.\nThat’s all I have for now, I know I’m missing things. Maybe my New Year’s resolution will be to keep better track of the awesome things other people are doing :).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:27:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-12-16-the-four-eras-of-data/",
    "title": "The four eras of data",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-12-16",
    "categories": [],
    "contents": "\nI’m teaching a class in data science for our masters and PhD students here at Hopkins. I’ve been teaching a variation on this class since 2011 and over time I’ve introduced a number of new components to the class: high-dimensional data methods (2011), data manipulation and cleaning (2012), real, possibly not doable data analyses (2012,2013), peer reviews (2014), building swirl tutorials for data analysis techniques (2015), and this year building data analytic web apps/R packages.\nI’m the least efficient teacher in the world, probably because I’m very self conscious about my teaching. So I always feel like I have to completely re-do my lecture materials every year I teach the class (I know, I know I’m a dummy). This year I was reviewing my notes on high-dimensional data and I was looking at this breakdown of the three eras of statistics from Brad Efron’s book:\n\nThe age of Quetelet and his successors, in which huge census-level data sets were brought to bear on simple but important questions: Are there more male than female births? Is the rate of insanity rising?\nThe classical period of Pearson, Fisher, Neyman, Hotelling, and their successors, intellectual giants who developed a theory of optimal inference capable of wringing every drop of information out of a scientific experiment. The questions dealt with still tended to be simple — Is treatment A better than treatment B? — but the new methods were suited to the kinds of small data sets individual scientists might collect.\nThe era of scientific mass production, in which new technologies typi- fied by the microarray allow a single team of scientists to produce data sets of a size Quetelet would envy. But now the flood of data is accompanied by a deluge of questions, perhaps thousands of estimates or hypothesis tests that the statistician is charged with answering together; not at all what the classical masters had in mind.\n\nWhile I think this is a useful breakdown, I realized I think about it in a slightly different way as a statistician. My breakdown goes more like this:\nThe era of not much data This is everything prior to about 1995 in my field. The era when we could only collect a few measurements at a time. The whole point of statistics was to try to optimaly squeeze information out of a small number of samples - so you see methods like maximum likelihood and minimum variance unbiased estimators being developed.\nThe era of lots of measurements on a few samples This one hit hard in biology with the development of the microarray and the ability to measure thousands of genes simultaneously. This is the same statistical problem as in the previous era but with a lot more noise added. Here you see the development of methods for multiple testing and regularized regression to separate signals from piles of noise.\nThe era of a few measurements on lots of samples This era is overlapping to some extent with the previous one. Large scale collections of data from EMRs and Medicare are examples where you have a huge number of people (samples) but a relatively modest number of variables measured. Here there is a big focus on statistical methods for knowing how to model different parts of the data with hierarchical models and separating signals of varying strength with model calibration.\nThe era of all the data on everything. This is an era that currently we as civilians don’t get to participate in. But Facebook, Google, Amazon, the NSA and other organizations have thousands or millions of measurements on hundreds of millions of people. Other than just sheer computing I’m speculating that a lot of the problem is in segmentation (like in era 3) coupled with avoiding crazy overfitting (like in era 2).\nI’ve focused here on the implications of these eras from a statistical modeling perspective, but as we discussed in my class, era 4 coupled with advances in machine learning methods mean that there are social, economic, and behaviorial implications of these eras as well.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:27:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-12-15-nssd-episode-28/",
    "title": "Not So Standard Deviations Episode 28 - Writing is a lot Harder than Just Talking",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-12-15",
    "categories": [],
    "contents": "\nHilary and I talk about building data science products that provide a good user experience while adhering to some kind of ground truth, whether it’s in medicine, education, news, or elsewhere. Also Gilmore Girls.\nIf you have questions you’d like Hilary and me to answer, you can send them to nssdeviations @ gmail.com or tweet us at [@NSSDeviations](https://twitter.com/nssdeviations).\nShow notes:\nHill’s criteria for causation\nO’Reilly Bots Podcast\nNHTSA’s Federal Automated Vehicles Policy\nSubscribe to the podcast on iTunes or Google Play. And please leave us a review on iTunes.\nSupport us through our Patreon page.\nGet the Not So Standard Deviations book.\nDownload the audio for this episode\nListen here: \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-12-09-pisa-us-math/",
    "title": "What is going on with math education in the US?",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2016-12-09",
    "categories": [],
    "contents": "\nWhen colleagues with young children seeking information about schools ask me if I like the Massachusetts public school my children attend, my answer is always the same: “it’s great…except for math”. The fact is that in our household we supplement our kids’ math education with significant extra curricular work in order to ensure that they receive a math education comparable to what we received as children in the public system.\nThe latest results from the Program for International Student Assessment (PISA) results show that there is a general problem with math education in the US. Were it a country, Massachusetts would have been in second place in reading, sixth in science, but 20th in math, only ten points above the OECD average of 490. The US as a whole did not fair nearly as well as MA, and the same discrepancy between math and the other two subjects was present. In fact, among the top 30 performing countries ranked by their average of science and reading scores, the US has, by far, the largest discrepancy between math and the other two subjects tested by PISA. The difference of 27 was substantially greater than the second largest difference, which came from Finland at 17. Massachusetts had a difference of 28.\nPISA 2015 Math minus average of science and readingIf we look at the trend of this difference since PISA was started 16 years ago, we see a disturbing progression. While science and reading have remained stable, math has declined. In 2000 the difference between the results in math and the other subjects was only 8.5. Furthermore, the US is not performing exceptionally well in any subject:\nPISA 2015 Math versus average of science and readingSo what is going on? I’d love to read theories in the comment section. From my experience comparing my kids’ public schools now with those that I attended, I have one theory of my own. When I was a kid there was a math textbook. Even when a teacher was bad, it provided structure and an organized alternative for learning on your own. Today this approach is seen as being “algorithmic” and has fallen out of favor. “Project based learning” coupled with group activities have become popular replacements.\nProject based learning is great in principle. But, speaking from experience, I can say it is very hard to come up with good projects, even for highly trained mathematical minds. And it is certainly much more time consuming for the instructor than following a textbook. Teachers don’t have more time now than they did 30 years ago so it is no surprise that this new more open approach leads to improvisation and mediocre lessons. A recent example of a pointless math project involved 5th graders picking a number and preparing a colorful poster showing “interesting” facts about this number. To make things worse in terms of math skills, students are often rewarded for effort, while correctness is secondary and often disregarded.\nRegardless of the reason for the decline, given the trends we are seeing, we need to rethink the approach to math education. Math education may have had its problems in the past, but recent evidence suggests that the reforms of the past few decades seem to have only worsened the situation.\nNote: To make these plots I download and read-in the data into R as described here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-11-30-nssd-episode-27/",
    "title": "Not So Standard Deviations Episode 27 - Special Guest Amelia McNamara",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-11-30",
    "categories": [],
    "contents": "\nI had the pleasure of sitting down with Amelia McNamara, Visiting Assistant Professor of Statistical and Data Sciences at Smith College, to talk about data science, data journalism, visualization, the problems with R, and adult coloring books.\nIf you have questions you’d like Hilary and me to answer, you can send them to nssdeviations @ gmail.com or tweet us at [@NSSDeviations](https://twitter.com/nssdeviations).\nShow notes:\nAmelia McNamara’s web site\nMark Hansen\nListening Post\nMoveable Type\nAlan Kay\nHARC (Human Advancement Research Community)\nVPRI (Viewpoints Research Institute)\nInteractive essays\nGolden Ratio Coloring Book\nSubscribe to the podcast on iTunes or Google Play. And please leave us a review on iTunes.\nSupport us through our Patreon page.\nGet the Not So Standard Deviations book.\nDownload the audio for this episode\nListen here: \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-11-17-leekgroup-colors/",
    "title": "Help choose the Leek group color palette",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-11-17",
    "categories": [],
    "contents": "\nMy research group just recently finish a paper where several different teams within the group worked on different analyses. If you are interested the paper describes the recount resource which includes processed versions of thousands of human RNA-seq data sets.\nAs part of this project each group had to contribute some plots to the paper. One thing that I noticed is that each person used their own color palette and theme when building the plots. When we wrote the paper this made it a little harder for the figures to all fit together - especially when different group members worked on a single panel of a multi-panel plot.\nSo I started thinking about setting up a Leek group theme for both base R and ggplot2 graphics. One of the first problems was that every group member had their own opinion about what the best color palette would be. So we are running a little competition to determine what the official Leek group color palette for plots will be in the future.\nAs part of that process, one of my awesome postdocs, Shannon Ellis, decided to collect some data on how people perceive different color palettes. The survey is here:\nhttps://docs.google.com/forms/d/e/1FAIpQLSfHMXVsl7pxYGarGowJpwgDSf9lA2DfWJjjEON1fhuCh6KkRg/viewform?c=0&w=1\nIf you have a few minutes and have an opinion about colors (I know you do!) please consider participating in our little poll and helping to determine the future of Leek group plots!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-11-11-im-not-moving-to-canada/",
    "title": "Open letter to my lab: I am not \"moving to Canada\"",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2016-11-11",
    "categories": [],
    "contents": "\nDear Lab Members,\nI know that the results of Tuesday’s election have many of you concerned about your future. You are not alone. I am concerned about my future as well. But I want you to know that I have no plans of going anywhere and I intend to dedicate as much time to our projects as I always have. Meeting, discussing ideas and putting them into practice with you is, by far, the best part of my job.\nWe are all concerned that if certain campaign promises are kept many of our fellow citizens may need our help. If this happens, then we will pause to do whatever we can to help. But I am currently cautiously optimistic that we will be able to continue focusing on helping society in the best way we know how: by doing scientific research.\nThis week Dr. Francis Collins assured us that there is strong bipartisan support for scientific research. As an example consider this op-ed in which Newt Gingrich advocates for doubling the NIH budget. There also seems to be wide consensus in this country that scientific research is highly beneficial to society and an understanding that to do the best research we need the best of the best no matter their gender, race, religion or country of origin. Nothing good comes from creative, intelligent, dedicated people leaving science.\nI know there is much uncertainty but, as of now, there is nothing stopping us from continuing to work hard. My plan is to do just that and I hope you join me.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-11-09-not-all-forecasters-got-it-wrong/",
    "title": "Not all forecasters got it wrong: Nate Silver does it again (again)",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2016-11-09",
    "categories": [],
    "contents": "\nFour years ago we posted on Nate Silver’s, and other forecasters’, triumph over pundits. In contrast, after yesterday’s presidential election, results contradicted most polls and data-driven forecasters, several news articles came out wondering how this happened. It is important to point out that not all forecasters got it wrong. Statistically speaking, Nate Silver, once again, got it right.\nTo show this, below I include a plot showing the expected margin of victory for Clinton versus the actual results for the most competitive states provided by 538. It includes the uncertainty bands provided by 538 in this site (I eyeballed the band sizes to make the plot in R, so they are not exactly like 538’s).\n538-2016-electionNote that if these are 95% confidence/credible intervals, 538 got 1 wrong. This is exactly what we expect since 15/16 is about 95%. Furthermore, judging by the plot here, 538 estimated the popular vote margin to be 3.6% with a confidence/credible interval of about 5%. This too was an accurate prediction since Clinton is going to win the popular vote by about 1% 0.5% (note this final result is in the margin of error of several traditional polls as well). Finally, when other forecasters were giving Trump between 14% and 0.1% chances of winning, 538 gave him about a 30% chance which is slightly more than what a team has when down 3-2 in the World Series. In contrast, in 2012 538 gave Romney only a 9% chance of winning. Also, remember, if in ten election cycles you call it for someone with a 70% chance, you should get it wrong 3 times. If you get it right every time then your 70% statement was wrong.\nSo how did 538 outperform all other forecasters? First, as far as I can tell they model the possibility of an overall bias, modeled as a random effect, that affects every state. This bias can be introduced by systematic lying to pollsters or under sampling some group. Note that this bias can’t be estimated from data from one election cycle but it’s variability can be estimated from historical data. 538 appear to estimate the standard error of this term to be about 2%. More details on this are included here. In 2016 we saw this bias and you can see it in the plot above (more points are above the line than below). The confidence bands account for this source of variabilty and furthermore their simulations account for the strong correlation you will see across states: the chance of seeing an upset in Pennsylvania, Wisconsin, and Michigan is not the product of an upset in each. In fact it’s much higher. Another advantage 538 had is that they somehow were able to predict a systematic, not random, bias against Trump. You can see this by comparing their adjusted data to the raw data (the adjustment favored Trump about 1.5 on average). We can clearly see this when comparing the 538 estimates to The Upshots’:\n538-2016-electionThe fact that 538 did so much better than other forecasters should remind us how hard it is to do data analysis in real life. Knowing math, statistics and programming is not enough. It requires experience and a deep understanding of the nuances related to the specific problem at hand. Nate Silver and the 538 team seem to understand this more than others.\nUpdate: Jason Merkin points out (via Twitter) that 538 provides 80% credible intervals.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-11-08-chromebook-part2/",
    "title": "Data scientist on a chromebook take two",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-11-08",
    "categories": [],
    "contents": "\nMy friend Fernando showed me his collection of old Apple dongles that no longer work with the latest generation of Apple devices. This coupled with the announcement of the Macbook pro that promises way more dongles and mostly the same computing, had me freaking out about my computing platform for the future. I’ve been using cloudy tools for more and more of what I do and so it had me wondering if it was time to go back and try my Chromebook experiment again. Basically the question is whether I can do everything I need to do comfortably on a Chromebook.\nSo to execute the experience I got a brand new ASUS chromebook flip and the connector I need to plug it into hdmi monitors (there is no escaping at least one dongle I guess :(). Here is what that badboy looks like in my home office with Apple superfanboy Roger on the screen.\nchromebook2In terms of software there have been some major improvements since I last tried this experiment out. Some of these I talk about in my book How to be a modern scientist. As of this writing this is my current setup:\nMusic on Google Play\nLatex on Overleaf\nBlog/website/code on Github\nR programming on an Amazon AMI with Rstudio loaded although I hear there may be other options that are good there that I should try.\nEmail/Calendar/Presentations/Spreadsheets/Docs with Google products\nTwitter with Tweetdeck\nThat handles the vast majority of my workload so far (its only been a day :)). But I would welcome suggestions and I’ll report back when either I give up or if things are still going strong in a little while….\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-10-28-nssd-episode-25/",
    "title": "Not So Standard Deviations Episode 25 - How Exactly Do You Pronounce SQL?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-10-28",
    "categories": [],
    "contents": "\nHilary and I go through the overflowing mailbag to respond to listener questions! Topics include causal inference in trend modeling, regression model selection, using SQL, and data science certification.\nIf you have questions you’d like us to answer, you can send them to nssdeviations @ gmail.com or tweet us at [@NSSDeviations](https://twitter.com/nssdeviations).\nShow notes:\nProfessor Kobre’s Lightscoop Standard Version Bounce Flash Device\nSpeechpad\nSpeaking American by Josh Katz\nData Sets Are The New Server Rooms\nAre Datasets the New Server Rooms?\nSubscribe to the podcast on iTunes or Google Play. And please leave us a review on iTunes.\nSupport us through our Patreon page.\nGet the Not So Standard Deviations book.\nDownload the audio for this episode\nListen here:\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-10-26-datasets-new-server-rooms/",
    "title": "Are Datasets the New Server Rooms?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-10-26",
    "categories": [],
    "contents": "\nJosh Nussbaum has an interesting post over at Medium about whether massive datasets are the new server rooms of tech business.\nThe analogy comes from the “old days” where in order to start an Internet business, you had to buy racks and servers, rent server space, buy network bandwidth, license expensive server software, backups, and on and on. In order to do all that up front, it required a substantial amount of capital just to get off the ground. As inconvenient as this might have been, it provided an immediate barrier to entry for any other competitors who weren’t able to raise similar capital.\nOf course,\n\n…the emergence of open source software and cloud computing completely eviscerated the costs and barriers to starting a company, leading to deflationary economics where one or two people could start their company without the large upfront costs that were historically the hallmark of the VC industry.\n\nSo if startups don’t have huge capital costs in the beginning, what costs do they have? Well, for many new companies that rely on machine learning, they need to collect data.\n\nAs a startup collects the data necessary to feed their ML algorithms, the value the product/service provides improves, allowing them to access more customers/users that provide more data and so on and so forth.\n\nCollecting huge datasets ultimately costs money. The sooner a startup can raise money to get that data, the sooner they can defend themselves from competitors who may not yet have collected the huge datasets for training their algorithms.\nI’m not sure the analogy between datasets and server rooms quite works. Even back when you had to pay a lot of up front costs to setup servers and racks, a lot of that technology was already a commodity, and anyone could have access to it for a price.\nI see massive datasets used to train machine learning algorithms as more like the new proprietary software. The startups of yore spent a lot of time writing custom software for what we might now consider mundane tasks. This was a time-consuming activity but the software that was developed had value and was a differentiator for the company. Today, many companies write complex machine learning algorithms, but those algorithms and their implmentations are quickly becoming commodities. So the only thing that separates one company from another is the amount and quality of data that they have to train those algorithms.\nGoing forward, it will be interesting see what these companies will do with those massive datasets once they no longer need them. Will they “open source” them and make them available to everyone? Could there be an open data movement analogous to the open source movement?\nFor the most part, I doubt it. While I think many today would perhaps sympathize with the sentiment that software shouldn’t have owners, those same people I think would argue vociferously that data most certainly do have owners. I’m not sure how I’d feel if Facebook made all their data available to anyone. That said, many datasets are made available by various businesses, and as these datasets grow in number and in usefulness, we may see a day where the collection of data is not a key barrier to entry, and that you can train your machine learning algorithm on whatever is out there.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-10-20-distributed-masochism-as-a-pedagogical-model/",
    "title": "Distributed Masochism as a Pedagogical Model",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-10-20",
    "categories": [],
    "contents": "\nEditor’s note: This is a guest post by Sean Kross. Sean is a software developer in the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. Sean has contributed to several of our specializations including Data Science, Executive Data Science, and Mastering Software Development in R. He tweets [@seankross](https://twitter.com/seankross).\nOver the past few months I’ve been helping Jeff develop the Advanced Data Science class he’s teaching at the Johns Hopkins Bloomberg School of Public Health. We’ve been trying to identify technologies that we can teach to students which (we hope) will enable them to rapidly prototype data-based software applications which will serve a purpose in public health. We started with technologies that we’re familiar with (R, Shiny, static websites) but we’re also trying to teach ourselves new technologies (the Amazon Alexa Skills API, iOS and Swift). We’re teaching skills that we know intimately along with skills that we’re learning on the fly which is a style of teaching that we’ve practiced several times.\nJeff and I have come to realize that while building new courses with technologies that are new to us we experience particular pains and frustrations which, when documented, become valuable learning resources for our students. This process of documenting new-tech-induced pain is only a preliminary step. When we actually launch classes either online or in person our students run into new frustrations which we respond to with changes to either documentation or course content. This process of quickly iterating on course material is especially enhanced in online courses where the time span for a course lasts a few weeks compared to a full semester, so kinks in the course are ironed out at a faster rate compared to traditional in-person courses. All of the material in our courses is open-source and available on GitHub, and we teach our students how to use Git and GitHub. We can take advantage of improvements and contributions the students think we should make to our courses through pull requests that we recieve. Student contributions further reduce the overall start-up pain experienced by other students.\nWith students from all over the world participating in our online courses we’re unable to anticipate every technical need considering different locales, languages, and operating systems. Instead of being anxious about this reality we depend on a system of “distributed masochism” whereby documenting every student’s unique technical learning pains is an important aspect of improving the online learning experience. Since we only have a few months head start using some of these technologies compared to our students it’s likely that as instructors we’ve recently climbed a similar learning curve which makes it easier for us to help our students. We believe that this approach of teaching new technologies by allowing any student to contribute to open course material allows a course to rapidly adapt to students’ needs and to the inevitable changes and upgrades that are made to new technologies.\nI’m extremely interested in communicating with anyone else who is using similar techniques, so if you’re interested please contact me via Twitter (@seankross) or send me an email: sean at seankross.com.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-10-16-nssd-episode-24/",
    "title": "Not So Standard Deviations Episode 24 - 50 Minutes of Blathering",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-10-16",
    "categories": [],
    "contents": "\nAnother IRL episode! Hilary and I met at a Jimmy John’s to talk data science, like you do. Topics covered include RStudio Conf, polling, millennials, Karl Broman, and more!\nIf you have questions you’d like us to answer, you can send them to nssdeviations @ gmail.com or tweet us at [@NSSDeviations](https://twitter.com/nssdeviations).\nSubscribe to the podcast on iTunes or Google Play. And please leave us a review on iTunes.\nSupport us through our Patreon page.\nGet the Not So Standard Deviations book.\nShow notes:\nrstudio::conf\nWe Gave Four Good Pollsters the Same Raw Data. They Had Four Different Results\nMillenials\nKarl Broman\nInterview with J.J. Allaire\nOne Year at Stack Overflow\nDownload the audio for this episode\nListen here:\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-10-14-chatabot-or-faq/",
    "title": "Should I make a chatbot or a better FAQ?",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-10-14",
    "categories": [],
    "contents": "\nRoger pointed me to this interesting article (paywalled, sorry!) about Facebook’s chatbot service. I think the article made a couple of interesting points. The first thing I thought was interesting was their explicit acknowledgement of the process I outlined in a previous post for building an AI startup - (1) convince (or in this case pay) some humans to be your training set, and (2) collect the data on the humans and then use it to build your AI.\nThe other point that is pretty fascinating is that they realized how many data points they would need before they could reasonably replace a human with an AI chatbot. The original estimate was tens of thousands and the ultimate number was millions or more. I have been thinking a lot that the AI “revolution” is just a tradeoff between parameters and data points. If you have a billion parameter prediction algorithm it may work pretty awesome - as long as you have a few hundred billion data points to train it with.\nBut the theme of the article was that chatbots may have had some mis-steps/may not be ready for prime time. I think the main reason is that at the moment most AI efforts can only report facts, not intuit intention and alter the question for the user or go beyond the facts/state of the world.\nOne example I’ve run into recently was booking a ticket on an airline. I wanted to know if I could make a certain change to my ticket. The airline didn’t have any information about the change I wanted to make online. After checking thoroughly I clicked on the “Chat with an agent” button and was directed to what was clearly a chatbot. The chatbot asked a question or two and then sent me to the “make changes to a ticket” page of the website.\nI eventually had to call and get a person on the phone, because what I wanted to ask about didn’t apply to the public information. They set me straight and I booked the ticket. The chatbot wasn’t helpful because it could only respond with information it had available on the website. It couldn’t identify a new situation, realize it had to ask around, figure out there was an edge case, and then make a ruling/help out.\nI would guess that most of the time if a person interacts with a chatbot they are doing it only because they already looked at all the publicly available information on the FAQ, etc. and couldn’t find it. So an alternative solution, which would require a lot less work and a much smaller training set, is to just have a more complete FAQ.\nThe question to me is does anyone other than Facebook or Google have a big enough training set to make a chatbot worth it?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-10-12-weighting-survey/",
    "title": "The Dangers of Weighting Up a Sample",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-10-12",
    "categories": [],
    "contents": "\nThere’s a great story by Nate Cohn over at the New York Times’ Upshot about the dangers of “weighting up” a sample from a survey. In this case, it is in regards to a U.S.C/LA Times poll asking who people will vote for President:\n\nThe U.S.C./LAT poll weights for many tiny categories: like 18-to-21-year-old men, which U.S.C./LAT estimates make up around 3.3 percent of the adult citizen population. Weighting simply for 18-to-21-year-olds would be pretty bold for a political survey; 18-to-21-year-old men is really unusual.\n\nThe U.S.C./LA Times poll apparently goes even further:\n\nWhen you start considering the competing demands across multiple categories, it can quickly become necessary to give an astonishing amount of extra weight to particularly underrepresented voters — like 18-to-21-year-old black men. This wouldn’t be a problem with broader categories, like those 18 to 29, and there aren’t very many national polls that are weighting respondents up by more than eight or 10-fold. The extreme weights for the 19-year-old black Trump voter in Illinois are not normal.\n\nIt’s worth noting (as a good thing) that the U.S.C./LA Times poll data is completely open, thus allowing the NYT to reproduce this entire analysis.\nI haven’t done much in the way of survey analyses, but I’ve done some inverse probability weighting and in my experience it can be a tricky procedure in ways that are not always immediately obvious. The article discusses weight trimming, but also notes the dangers of that procedure. Overall, a good treatment of a complex issue.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-10-03-papr/",
    "title": "papr - it's like tinder, but for academic preprints",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-10-03",
    "categories": [],
    "contents": "\nAs part of the Johns Hopkins Data Science Lab we are setting up a web and mobile data product prototyping shop. As part of that process I’ve been working on different types of very cheap and easy to prototype apps. A few days ago I posted about creating a distributed data collection app with Google Sheets.\nSo for fun I built another kind of app. This one I’m calling papr and its sort of like “Tinder for preprints”. I scraped all of the papers out of the http://biorxiv.org/ database. When you open the app you see one at random and you can rate it according to two axes:\nIs the paper interesting? - a paper can be rated as exciting or boring. We leave the definitions of those terms up to you.\nIs the paper correct or questionable? - a paper can either be solidly correct or potentially questionable in its results. We leave the definitions of those terms up to you.\nWhen you click on your rating you are shown another randomly generated paper from bioRxiv. You can “level up” to different levels if you rate more papers. You can also download your ratings at any time.\nIf you have any feedback on the app I’d love to hear it and if anyone knows how to get custom domain names to work with shinyapps.io I’d also love to hear from you. I tried the instructions with no luck…\nTry the app here:\nhttps://jhubiostatistics.shinyapps.io/papr/\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-10-03-the-information-vc/",
    "title": "Information and VC Investing",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-10-03",
    "categories": [],
    "contents": "\nSam Lessin at The Information has a nice post (sorry, paywall, but it’s a great publication) about how increased measurement and analysis is changing the nature of venture capital investing.\n\nThis brings me back to what is happening at series A financings. Investors have always, obviously, tried to do diligence at all financing rounds. But series A investments used to be an exercise in a few top-level metrics a company might know, some industry interviews and analysis, and a whole lot of trust. The data that would drive capital market efficiency usually just wasn’t there, so capital was expensive and there were opportunities for financiers. Now, I am seeing more and more that after a seed round to boot up most companies, the backbone of a series A financing is an intense level of detail in reporting and analytics. It can be that way because the companies have the data\n\nI’ve seen this happen in other areas where data comes in to disrupt the way things are done. Good analysis only gives you an advantage if no one else is doing it. Once everyone accepts the idea and everyone has the data (and a good analytics team), there’s no more value left in the market.\nTime to search elsewhere.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:26:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-10-01-nssd-episode-23/",
    "title": "Not So Standard Deviations Episode 23 - Special Guest Walt Hickey",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-10-01",
    "categories": [],
    "contents": "\nHilary and Roger invite Walt Hickey of FiveThirtyEight.com on to the show to talk about polling, movies, and data analysis reproducibility (of course).\nIf you have questions you’d like us to answer, you can send them to nssdeviations @ gmail.com or tweet us at [@NSSDeviations](https://twitter.com/nssdeviations).\nSubscribe to the podcast on iTunes or Google Play.\nPlease leave us a review on iTunes.\nSupport us through our Patreon page.\nGet the Not So Standard Deviations book.\nShow Notes:\nFiveThirtyEight’s polling methodology\nWalt Hickey on Twitter\nThe 20 Most Extreme Cases Of ‘The Book Was Better Than The Movie’\nMatthew Butterick Typography\nHopp\nDownload the audio for this episode.\nListen here:\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-09-29-statistical-vitriol/",
    "title": "Statistical vitriol",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-09-29",
    "categories": [],
    "contents": "\nOver the last few months there has been a lot of vitriol around statistical ideas. First there were data parasites and then there were methodological terrorists. These epithets came from established scientists who have relatively little statistical training. There was the predictable backlash to these folks from their counterparties, typically statisticians or statistically trained folks who care about open source.\nI’m a statistician who cares about open source but I also frequently collaborate with scientists from different fields. It makes me sad and frustrated that statistics - which I’m so excited about and have spent my entire professional career working on - is something that is causing so much frustration, anxiety, and anger.\nI have been thinking a lot about the cause of this anger and division in the sciences. As a person who interacts with both groups pretty regularly I think that the reasons are some combination of the following.\nData is now everywhere, so every single publication involves some level of statistical modeling and analysis. It can’t be escaped.\nThe deluge of scientific papers means that only big claims get your work noticed, get you into fancy journals, and get you attention.\nMost senior scientists, the ones leading and designing studies, have little or no training in statistics. There is a structural reason for this: data was sparse when they were trained and there wasn’t any reason for them to learn statistics. So statistics and data science wasn’t (and still often isn’t) integrated into medical and scientific curricula.\nThere is an imbalance of power in the scientific process between statisticians/computational scientists and scientific investigators or clinicians. The clinicians/scientific investigators are “in charge” and the statisticians are often relegated to a secondary role. Statisticians with some control over their environment (think senior tenured professors of (bio)statistics) can avoid these imbalances and look for collaborators who respect statistical thinking, but not everyone can. There are a large number of lonely bioinformaticians out there.\nStatisticians and computational scientists are also frustrated because their is often no outlet for them to respond to these papers in the formal scientific literature - those outlets are controlled by scientists and rarely have statisticians in positions of influence within the journals.\nSince statistics is everywhere (1) and only flashy claims get you into journals (2) and the people leading studies don’t understand statistics very well (3), you get many publications where the paper makes a big claim based on shakey statistics but it gets through. This then frustrates the statisticians because they have little control over the process (4) and can’t get their concerns into the published literature (5).\nThis used to just result in lots of statisticians and computational scientists complaining behind closed doors. The internet changed all that, everyone is an internet scientist now. So the statisticians and statistically savvy take to blogs, f1000research, and other outlets to get their point across.\nSometimes to get attention, statisticians start to have the same problem as scientists; they need their complaints to get attention to have any effect. So they go over the top. They accuse people of fraud, or being statistically dumb, or nefarious, or intentionally doing things with data, or cast a wide net and try to implicate a large number of scientists in poor statistics. The ironic thing is that these things are the same thing that the scientists are doing to get attention that frustrated the statisticians in the first place.\nJust to be 100% clear here I am also guilty of this. I have definitely fallen into the hype trap - talking about the “replicability crisis”. I also made the mistake earlier in my blogging career of trashing the statistics of a paper that frustrated me. I am embarrassed I did that now, it wasn’t constructive and the author ended up being very responsive. I think if I had just emailed that person they would have resolved their problem.\nI just recently had an experience where a very prominent paper hadn’t made their data public and I was having trouble getting the data. I thought about writing a blog post to get attention, but at the end of the day just did the work of emailing the authors, explaining myself over and over and finally getting the data from them. The result is the same (I have the data) but it cost me time and frustration. So I understand when people don’t want to deal with that.\nThe problem is that scientists see the attention the statisticians are calling down on them - primarily negative and often over-hyped. Then they get upset and call the statisticians/open scientists names, or push back on entirely sensible policies because they are worried about being humiliated or discredited. While I don’t agree with that response, I also understand the feeling of “being under attack”. I’ve had that happen to me too and it doesn’t feel good.\nSo where do we go from here? How do we end statistical vitriol and make statistics a positive force? Here is my six part plan:\nWe should create continuining education for senior scientists and physicians in statistical and open data thinking so people who never got that training can understand the unique requirements of a data rich scientific world.\nWe should encourage journals and funders to incorporate statisticians and computational scientists at the highest levels of influence so that they can drive policy that makes sense in this new data driven time.\nWe should recognize that scientists and data generators have a lot more on the line when they produce a result or a scientific data set. We should give them appropriate credit for doing that even if they don’t get the analysis exactly right.\nWe should de-escalate the consequences of statistical mistakes. Right now the consequences are: retractions that hurt careers, blog posts that are aggressive and often too personal, and humiliation by the community. We should make it easy to acknowledge these errors without ruining careers. This will be hard - scientists careers often depend on the results they get (recall 2 above). So we need a way to pump up/give credit to/acknowledge scientists who are willing to sacrifice that to get the stats right.\nWe need to stop treating retractions/statistical errors/mistakes like a sport where there are winners and losers. Statistical criticism should be easy, allowable, publishable and not angry or personal.\nAny paper where statistical analysis is part of the paper must have both a statistically trained author or a statistically trained reviewer or both. I wouldn’t believe a paper on genomics that was performed entirely by statisticians with no biology training any more than I believe a paper with statistics in it performed entirely by physicians with no statistical training.\nI think scientists forget that statisticians feel un-empowered in the scientific process and statisticians forget that a lot is riding on any given study for a scientist. So being a little more sympathetic to the pressures we all face would go a long way to resolving statistical vitriol.\nI’d be eager to hear other ideas too. It makes me sad that statistics has become so political on both sides.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-09-28-mystery-palantir-continues/",
    "title": "The Mystery of Palantir Continues",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-09-28",
    "categories": [],
    "contents": "\nPalantir, the secretive data science/consulting/software company, continues to be a mystery to most people, but recent reports have not been great. Reuters reports that the U.S. Department of Labor is suing it for employment discrimination:\n\nThe lawsuit alleges Palantir routinely eliminated Asian applicants in the resume screening and telephone interview phases, even when they were as qualified as white applicants.\n\nInterestingly, the report indicates a statistical argument:\n\nIn one example cited by the Labor Department, Palantir reviewed a pool of more than 130 qualified applicants for the role of engineering intern. About 73 percent of applicants were Asian. The lawsuit, which covers Palantir’s conduct between January 2010 and the present, said the company hired 17 non-Asian applicants and four Asians. “The likelihood that this result occurred according to chance is approximately one in a billion,” said the lawsuit, which was filed with the department’s Office of Administrative Law Judges.\n\nUpdate: Thanks to David Robinson for point out that (a) I read the numbers incorrectly and (b) I should have used the hypergeometric distribution to account for the sampling without replacement. The paragraph below is corrected accordingly.\nNote the use of the phrase “qualified applicants” in reference to the 130. Presumably, there was a screening process that removed “unqualified applicants” and that led us to 130. Of the 130, 73% were Asian. Presumably, there was a follow up selection process (interview, exam) that led to 4 Asians being hired out of 21 (about 19%). Clearly there’s a difference between 19% and 73% but the reasons may not be nefarious. If you assume the number of Asians hired is proportional to the number in the qualified pool, then the p-value for the observed data is about 10^-8, which is not quite “1 in a billion” as the report claims but it’s indeed small. But my guess is the Labor Department has more than this test of binomial proportions in terms of evidence if they were to go through with a suit.\nAlfred Lee from The Information reports that a mutual fund run by Valic sold their shares of Palantir for below the recent valuation:\n\nThe Valic fund sold its stake at $4.50 per share, filings show, down from the $11.38 per share at which the company raised money in December. The value of the stake at the sale price was $621,000. Despite the price drop, Valic made money on the deal, as it had acquired stock in preferred fundraisings in 2012 and 2013 at between $3.06 and $3.51 per share.\n\nThe valuation suggested in the article by the recent sale is $8 billion. In my previous post on Palantir, I noted that while other large-scale consulting companies certainly make a lot of money, none have the sky-high valuation that Palantir commands. However, a more “down-to-Earth” valuation of $8 billion might be more or less in line with these other companies. It may be bad news for Palantir, but should the company ever have an IPO, it would be good for the public for market participants to realize the intrinsic value of the company.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-09-27-thinking-like-statistician-election-2016/",
    "title": "Thinking like a statistician: this is not the election for progressives to vote third party",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2016-09-27",
    "categories": [],
    "contents": "\nDemocratic elections permit us to vote for whomever we perceive has the highest expectation to do better with the issues we care about. Let’s simplify and assume we can quantify how satisfied we are with an elected official’s performance. Denote this quantity with X. Because when we cast our vote we still don’t know for sure how the candidate will perform, we base our decision on what we expect, denoted here with E(X). Thus we try to maximize E(X). However, both political theory and data tell us that in US presidential elections only two parties have a non-negligible probability of winning. This implies that E(X) is 0 for some candidates no matter how large X could potentially be. So what we are really doing is deciding if E(X-Y) is positive or negative with X representing one candidate and Y the other.\nIn past elections some progressives have argued that the difference between candidates is negligible and have therefore supported the Green Party ticket. The 2000 election is a notable example. The 2000 election was won by George W. Bush by just five electoral votes. In Florida, which had 25 electoral votes, Bush beat Al Gore by just 537 votes. Green Party candidate Ralph Nader obtained 97,488 votes. Many progressive voters were OK with this outcome because they perceived E(X-Y) to be practically 0.\nIn contrast, in 2016, I suspect few progressives think that E(X-Y) is anywhere near 0. In the figures below I attempt to quantify the progressive’s pre-election perception of consequences for the last five contests. The first figure shows E(X) and E(Y) and the second shows E(X-Y). Note despite E(X) being the lowest in the last past five elections, E(X-Y) is by far the largest. So if these figures accurately depict your perception and you think like a statistician, it becomes clear that this is not the election to vote third party.\nelection-2016election-diff-2016\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-09-26-facebook-left-censoring/",
    "title": "Facebook and left censoring",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-09-26",
    "categories": [],
    "contents": "\nFrom the Wall Street Journal:\n\nSeveral weeks ago, Facebook disclosed in a post on its “Advertiser Help Center” that its metric for the average time users spent watching videos was artificially inflated because it was only factoring in video views of more than three seconds. The company said it was introducing a new metric to fix the problem.\n\nA classic case of left censoring (in this case, by “accident”).\nAlso this:\n\nAd buying agency Publicis Media was told by Facebook that the earlier counting method likely overestimated average time spent watching videos by between 60% and 80%, according to a late August letter Publicis Media sent to clients that was reviewed by The Wall Street Journal.\n\nWhat does this information tell us about the actual time spent watching Facebook videos?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-09-19-msdr-launch-announcement/",
    "title": "Mastering Software Development in R",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-09-19",
    "categories": [],
    "contents": "\nToday I’m happy to announce that we’re launching a new specialization on Coursera titled Mastering Software Development in R. This is a 5-course sequence developed with Sean Kross and Brooke Anderson.\nThis sequence differs from our previous Data Science Specialization because it focuses primarily on using R for developing software. We’ve found that as the field of data science evolves, it is becoming ever more clear that software development skills are essential for producing useful data science results and products. In addition, there is a tremendous need for tooling in the data science universe and we want to train people to build those tools.\nThe first course, The R Programming Environment, launches today. In the following months, we will launch the remaining courses:\nAdvanced R Programming\nBuilding R Packages\nBuilding Data Visualization Tools\nIn addition to the course, we have a companion textbook that goes along with the sequence. The book is available from Leanpub and is currently in progress (if you get the book now, you will receive free updates as they are available). We will be releaseing new chapters of the book alongside the launches of the other courses in the sequence.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-09-19-nssd-episode-22/",
    "title": "Not So Standard Deviations Episode 22 - Number 1 Side Project",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-09-19",
    "categories": [],
    "contents": "\nHilary and I celebrate our one year anniversary doing the podcast together by discussing whether there are cities that are good for data scientists, reproducible research, and professionalizing data science.\nAlso, Hilary and I have just published a new book, Conversations on Data Science, which collects some of our episodes in an easy-to-read format. The book is available from Leanpub and will be updated as we record more episodes. If you’re new to the podcast, this is a good way to do some catching up!\nIf you have questions you’d like us to answer, you can send them to nssdeviations @ gmail.com or tweet us at [@NSSDeviations](https://twitter.com/nssdeviations).\nSubscribe to the podcast on iTunes or Google Play.\nPlease leave us a review on iTunes!\nSupport us through our Patreon page.\nShow Notes:\nRoger’s reproducible research workshop\nThere’s More Than One Kind of Data Scientist by Harlan Harris\nBillionaire’s row in San Francisco\nMindfulness-based stress reduction\nOSIRIS-REx\nDownload the audio for this episode.\nListen here:\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-09-07-interview-with-a-data-sucker/",
    "title": "Interview With a Data Sucker",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2016-09-07",
    "categories": [],
    "contents": "\nA few months ago Jill Sederstrom from ASH Clinical News interviewed me for this article on the data sharing editorial published by the The New England Journal of Medicine (NEJM) and the debate it generated. The article presented a nice summary, but I thought the original comprehensive set of questions was very good too. So, with permission from ASH Clinical News, I am sharing them here along with my answers.\nBefore I answer the questions below, I want to make an important remark. When writing these answers I am reflecting on data sharing in general. Nuances arise in different contexts that need to be discussed on an individual basis. For example, there are different considerations to keep in mind when sharing publicly funded data in genomics (my field) and sharing privately funded clinical trials data, just to name two examples.\nIn your opinion, what do you see as the biggest pros of data sharing?\nThe biggest pro of data sharing is that it can accelerate and improve the scientific enterprise. This can happen in a variety of ways. For example, competing experts may apply an improved statistical analysis that finds a hidden discovery the original data generators missed. Furthermore, examination of data by many experts can help correct errors missed by the analyst of the original project. Finally, sharing data facilitates the merging of datasets from different sources that allow discoveries not possible with just one study.\nNote that data sharing is not a radical idea. For example, thanks to an organization called The MGED Soceity, most journals require all published microarray gene expression data to be public in one of two repositories: GEO or ArrayExpress. This has been an incredible success, leading to new discoveries, new databases that combine studies, and the development of widely used statistical methods and software built with these data as practice examples.\nThe NEJM editorial expressed concern that a new generation of researchers will emerge, those who had nothing to do with collecting the research but who will use it to their own ends. It referred to these as “research parasites.” Is this a real concern?\nAbsolutely not. If our goal is to facilitate scientific discoveries that improve our quality of life, I would be much more concerned about “data hoarders” than “research parasites”. If an important nugget of knowledge is hidden in a dataset, don’t you want the best data analysts competing to find it? Restricting the researchers who can analyze the data to those directly involved with the generators cuts out the great majority of experts.\nTo further illustrate this, let’s consider a very concrete example with real life consequences. Imagine a loved one has a disease with high mortality rates. Finding a cure is possible but only after analyzing a very very complex genomic assay. If some of the best data analysts in the world want to help, does it make any sense at all to restrict the pool of analysts to, say, a freshly minted masters level statistician working for the genomics core that generated the data? Furthermore, what would be the harm of having someone double check that analysis?\nThe NEJM editorial also presented several other concerns it had with data sharing including whether researchers would compare data across clinical trials that is not in fact comparable and a failure to provide correct attribution. Do you see these as being concerns? What cons do you believe there may be to data sharing?\nIf such mistakes are made, good peer reviewers will catch the error. If it escapes peer review, we point it out in post publication discussions. Science is constantly self correcting.\nRegarding attribution, this is a legitimate, but in my opinion, minor concern. Developers of open source statistical methods and software see our methods used without attribution quite often. We survive. But as I elaborate below, we can do things to alleviate this concern.\nIs data stealing a real worry? Have you ever heard of it happening before?\nI can’t say I can recall any case of data being stolen. But let’s remember that most published data is paid for by tax payers. They are the actual owners. So there is an argument to be made that the public’s data is being held hostage.\nDoes data sharing need to happen symbiotically as the editorial suggests? Why or why not?\nI think symbiotic sharing is the most effective approach to the repurposing of data. But no, I don’t think we need to force it to happen this way. Competition is one of the key ingredients of the scientific enterprise. Having many groups competing almost always beats out a small group of collaborators. And note that the data generators won’t necessarily have time to collaborate with all the groups interested in the data.\nIn a recent blog post, you suggested several possible data sharing guidelines. What would the advantage be of having guidelines in place in help guide the data sharing process?\nI think you are referring to a post by Jeff Leek but I am happy to answer. For data to be generated, we need to incentivize the endeavor. Guidelines that assure patient privacy should of course be followed. Some other simple guidelines related to those mentioned by Jeff are:\nReward data generators when their data is used by others.\nPenalize those that do not give proper attribution.\nApply the same critical rigor on critiques of the original analysis as we apply to the original analysis.\nInclude data sharing ethics in scientific education\nOne of the guidelines suggested a new designation for leaders of major data collection or software generation projects. Why do you think this is important?\nAgain, this was Jeff, but I agree. This is important because we need an incentive other than giving the generators exclusive rights to publications emanating from said data.\nYou also discussed the need for requiring statistical/computational co-authors for papers written by experimentalists with no statistical/computational co-authors and vice versa. What role do you see the referee serving? Why is this needed?\nI think the same rule should apply to referees. Every paper based on the analysis of complex data needs to have a referee with statistical/computational expertise. I also think biomedical journals publishing data-driven research should start adding these experts to their editorial boards. I should mention that NEJM actually has had such experts on their editorial board for a while now.\nAre there certain guidelines would feel would be most critical to include?\nTo me the most important ones are:\nThe funding agencies and the community should reward data generators when their data is used by others. Perhaps more than for the papers they produce with these data.\nApply the same critical rigor on critiques of the original analysis as we apply to the original analysis. Bashing published results and talking about the “replication crisis” has become fashionable. Although in some cases it is very well merited (see Baggerly and Coombes work for example) in some circumstances critiques are made without much care mainly for the attention. If we are not careful about keeping a good balance, we may end up paralyzing scientific progress.\nYou mentioned that you think symbiotic data sharing would be the most effective approach. What are some ways in which scientists can work symbiotically?\nI can describe my experience. I am trained as a statistician. I analyze data on a daily basis both as a collaborator and method developer. Experience has taught me that if one does not understand the scientific problem at hand, it is hard to make a meaningful contribution through data analysis or method development. Most successful applied statisticians will tell you the same thing.\nMost difficult scientific challenges have nuances that only the subject matter expert can effectively describe. Failing to understand these usually leads analysts to chase false leads, interpret results incorrectly or waste time solving a problem no one cares about. Successful collaboration usually involve a constant back and forth between the data analysts and the subject matter experts.\nHowever, in many circumstances the data generator is not necessarily the only one that can provide such guidance. Some data analysts actually become subject matter experts themselves, others download data and seek out other collaborators that also understand the details of the scientific challenge and data generation process.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-09-06-a-short-guide-for-phd-applicants/",
    "title": "A Short Guide for Students Interested in a Statistics PhD Program",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2016-09-06",
    "categories": [],
    "contents": "\nThis summer I had several conversations with undergraduate students seeking career advice. All were interested in data analysis and were considering graduate school. I also frequently receive requests for advice via email. We have posted on this topic before, for example here and here, but I thought it would be useful to share this short guide I put together based on my recent interactions.\nIt’s OK to be confused\nWhen I was a college senior I didn’t really understand what Applied Statistics was nor did I understand what one does as a researcher in academia. Now I love being an academic doing research in applied statistics. But it is hard to understand what being a researcher is like until you do it for a while. Things become clearer as you gain more experience. One important piece of advice is to carefully consider advice from those with more experience than you. It might not make sense at first, but I can tell today that I knew much less than I thought I did when I was 22.\nShould I even go to graduate school?\nYes. An undergraduate degree in mathematics, statistics, engineering, or computer science provides a great background, but some more training greatly increases your career options. You may be able to learn on the job, but note that a masters can be as short as a year.\nA masters or a PhD?\nIf you want a career in academia or as a researcher in industry or government you need a PhD. In general, a PhD will give you more career options. If you want to become a data analyst or research assistant, a masters may be enough. A masters is also a good way to test out if this career is a good match for you. Many people do a masters before applying to PhD Programs. The rest of this guide focuses on those interested in a PhD.\nWhat discipline?\nThere are many disciplines that can lead you to a career in data science: Statistics, Biostatistics, Astronomy, Economics, Machine Learning, Computational Biology, and Ecology are examples that come to mind. I did my PhD in Statistics and got a job in a Department of Biostatistics. So this guide focuses on Statistics/Biostatistics.\nNote that once you finish your PhD you have a chance to become a postdoctoral fellow and further focus your training. By then you will have a much better idea of what you want to do and will have the opportunity to chose a lab that closely matches your interests.\nWhat is the difference between Statistics and Biostatistics?\nShort answer: very little. I treat them as the same in this guide. Long answer: read this.\nHow should I prepare during my senior year?\nMath\nGood grades in math and statistics classes are almost a requirement. Good GRE scores help and you need to get a near perfect score in the Quantitative Reasoning part of the GRE. Get yourself a practice book and start preparing. Note that to survive the first two years of a statistics PhD program you need to prove theorems and derive relatively complicated mathematical results. If you can’t easily handle the math part of the GRE, this will be quite challenging.\nWhen choosing classes note that the area of math most related to your stat PhD courses is Real Analysis. The area of math most used in applied work is Linear Algebra, specifically matrix theory including understanding eigenvalues and eigenvectors. You might not make the connection between what you learn in class and what you use in practice until much later. This is totally normal.\nIf you don’t feel ready, consider doing a masters first. But also, get a second opinion. You might be being too hard on yourself.\nProgramming\nYou will be using a computer to analyze data so knowing some programming is a must these days. At a minimum, take a basic programming class. Other computer science classes will help especially if you go into an area dealing with large datasets. In hindsight, I wish I had taken classes on optimization and algorithm design.\nKnow that learning to program and learning a computer language are different things. You need to learn to program. The choice of language is up for debate. If you only learn one, learn R. If you learn three, learn R, Python and C++.\nKnowing Linux/Unix is an advantage. If you have a Mac try to use the terminal as much as possible. On Windows get an emulator.\nWriting and Communicating\nMy biggest educational regret is that, as a college student, I underestimated the importance of writing. To this day I am correcting that mistake.\nYour success as a researcher greatly depends on how well you write and communicate. Your thesis, papers, grant proposals and even emails have to be well written. So practice as much as possible. Take classes, read works by good writers, and practice. Consider starting a blog even if you don’t make it public. Also note that in academia, job interviews will involve a 50 minute talk as well as several conversations about your work and future plans. So communication skills are also a big plus.\nBut wait, why so much math?\nThe PhD curriculum is indeed math heavy. Faculty often debate the possibility of changing the curriculum. But regardless of differing opinions on what is the right amount, math is the foundation of our discipline. Although it is true that you will not directly use much of what you learn, I don’t regret learning so much abstract math because I believe it positively shaped the way I think and attack problems.\nNote that after the first two years you are pretty much done with courses and you start on your research. If you work with an applied statistician you will learn data analysis via the apprenticeship model. You will learn the most, by far, during this stage. So be patient. Watch these two Karate Kid scenes for some inspiration.\nWhat department should I apply to?\nThe top 20-30 departments are practically interchangeable in my opinion. If you are interested in applied statistics make sure you pick a department with faculty doing applied research. Note that some professors focus their research on the mathematical aspects of statistics. By reading some of their recent papers you will be able to tell. An applied paper usually shows data (not simulated) and motivates a subject area challenge in the abstract or introduction. A theory paper shows no data at all or uses it only as an example.\nCan I take a year off?\nAbsolutely. Especially if it’s to work in a data related job. In general, maturity and life experiences are an advantage in grad school.\nWhat should I expect when I finish?\nYou will have many many options. The demand of your expertise is great and growing. As a result there are many high-paying options. If you want to become an academic I recommend doing a postdoc. Here is why. But there are many other options as we describe here and here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-08-26-googlesheets/",
    "title": "How to create a free distributed data collection \"app\" with R and Google Sheets",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-08-26",
    "categories": [],
    "contents": "\nJenny Bryan, developer of the google sheets R package, gave a talk at Use2015 about the package.\nOne of the things that got me most excited about the package was an example she gave in her talk of using the Google Sheets package for data collection at ultimate frisbee tournaments. One reason is that I used to play a little ultimate back in the day.\nAnother is that her idea is an amazing one for producing cool public health applications. One of the major issues with public health is being able to do distributed data collection cheaply, easily, and reproducibly. So I decided to write a little tutorial on how one could use Google Sheets and R to create a free distributed data collecton “app” for public health (or anything else really).\nWhat you will need\nA Google account and access to Google Sheets\nR and the googlesheets package.\nThe “app”\nWhat we are going to do is collect data in a Google Sheet or sheets. This sheet can be edited by anyone with the link using their computer or a mobile phone. Then we will use the googlesheets package to pull the data into R and analyze it.\nMaking the Google Sheet work with googlesheets\nAfter you have a first thing to do is to go to the Google Sheets I suggest bookmarking this page: https://docs.google.com/spreadsheets/u/0/ which skips the annoying splash screen.\nCreate a blank sheet and give it an appropriate title for whatever data you will be collecting.\nNext, we need to make the sheet public on the web so that the googlesheets package can read it. This is different from the sharing settings you set with the big button on the right. To make the sheet public on the web, go to the “File” menu and select “Publish to the web…”. Like this:\n\nthen it will ask you if you want to publish the sheet, just hit publish\n\nCopy the link it gives you and you can use it to read in the Google Sheet. If you want to see all the Google Sheets you can read in, you can load the package and use the gs_ls function.\nlibrary(googlesheets)\nsheets = gs_ls()\nsheets[1,]\n## # A tibble: 1 x 10\n##   sheet_title author  perm version             updated\n##         <chr>  <chr> <chr>   <chr>              <time>\n## 1 app_example jtleek    rw     new 2016-08-26 17:48:21\n## # ... with 5 more variables: sheet_key <chr>, ws_feed <chr>,\n## #   alternate <chr>, self <chr>, alt_key <chr>\nIt will pop up a dialog asking for you to authorize the googlesheets package to read from your Google Sheets account. Then you should see a list of spreadsheets you have created.\nIn my example I created a sheet called “app_example” so I can load the Google Sheet like this:\n## Identifies the Google Sheet\nexample_sheet = gs_title(\"app_example\")\n## Sheet successfully identified: \"app_example\"\n## Reads the data\ndat = gs_read(example_sheet)\n## Accessing worksheet titled 'Sheet1'.\n## No encoding supplied: defaulting to UTF-8.\nhead(dat)\n## # A tibble: 3 x 5\n##   who_collected at_work person  time       date\n##           <chr>   <chr>  <chr> <chr>      <chr>\n## 1          jeff      no   ingo 13:47 08/26/2016\n## 2          jeff     yes  roger 13:47 08/26/2016\n## 3          jeff     yes  brian 13:47 08/26/2016\nIn this case the data I’m collecting is about who is at work right now as I’m writing this post :). But you could collect whatever you want.\nDistributing the data collection\nNow that you have the data published to the web, you can read it into Google Sheets. Also, anyone with the link will be able to view the Google Sheet. But if you don’t change the sharing settings, you are the only one who can edit the sheet.\nThis is where you can make your data collection distributed if you want. If you go to the “Share” button, then click on advanced you will get a screen like this and have some options.\n\nPrivate data collection\nIn the example I’m using I haven’t changed the sharing settings, so while you can see the sheet, you can’t edit it. This is nice if you want to collect some data and allow other people to read it, but you don’t want them to edit it.\nControlled distributed data collection\nIf you just enter people’s emails then you can open the data collection to just those individuals you have shared the sheet with. Be careful though, if they don’t have Google email addresses, then they get a link which they could share with other people and this could lead to open data collection.\nUncontrolled distributed data collection\nAnother option is to click on “Change” next to “Private - Only you can access”. If you click on “On - Anyone with the link” and click on “Can View”.\n\nThen you can modify it to say “Can Edit” and hit “Save”. Now anyone who has the link can edit the Google Sheet. This means that you can’t control who will be editing it (careful!) but you can really widely distribute the data collection.\nCollecting data\nOnce you have distributed the link either to your collaborators or more widely it is time to collect data. This is where I think that the “app” part of this is so cool. You can edit the Google Sheet from a Desktop computer, but if you have the (free!) Google Sheets app for your phone then you can also edit the data on the go. There is even an offline mode if the internet connection isn’t available where you are working (more on this below).\nQuality control\nOne of the major issues with distributed data collection is quality control. If possible you want people to input data using (a) a controlled vocubulary/system and (b) the same controlled vocabulary/system. My suggestion here depends on whether you are using a controlled distributed system or an uncontrolled distributed system.\nFor the controlled distributed system you are specifically giving access to individual people - you can provide some training or a walk through before giving them access.\nFor the uncontrolled distributed system you should create a very detailed set of instructions. For example, for my sheet I would create a set of instructions like:\nEvery data point must have a label of who collected in in the who_collected column. You should pick a username that does not currently appear in the sheet and stick with it. Use all lower case for your username.\nYou should either report “yes” or “no” in lowercase in the at_work column.\nYou should report the name of the person in all lower case in the person column. You should search and make sure that the person you are reporting on doesn’t appear before introducing a new name. If the name already exists, use the name spelled exactly as it is in the sheet already.\nYou should report the time in the format hh:mm on a 24 hour clock in the eastern time zone of the United States.\nYou should report the date in the mm/dd/yyyy format.\nYou could be much more detailed depending on the case.\nOffline editing and conflicts\nOne of the cool things about Google Sheets is that they can even be edited without an internet connection. This is particularly useful if you are collecting data in places where internet connections may be spotty. But that may generate conflicts if you use only one sheet.\nThere may be different ways to handle this, but one I thought of is to just create one sheet for each person collecting data (if you are using controlled distributed data collection). Then each person only edits the data in their sheet, avoiding potential conflicts if multiple people are editing offline and non-synchronously.\nReading the data\nAnyone with the link can now read the most up-to-date data with the following simple code.\n## Identifies the Google Sheet\nexample_sheet = gs_url(\"https://docs.google.com/spreadsheets/d/177WyyzWOHGIQ9O5iUY9P9IVwGi7jL3f4XBY4d98CY_o/pubhtml\")\n## Sheet-identifying info appears to be a browser URL.\n## googlesheets will attempt to extract sheet key from the URL.\n## Putative key: 177WyyzWOHGIQ9O5iUY9P9IVwGi7jL3f4XBY4d98CY_o\n## Sheet successfully identified: \"app_example\"\n## Reads the data\ndat = gs_read(example_sheet, ws=\"Sheet1\")\n## Accessing worksheet titled 'Sheet1'.\n## No encoding supplied: defaulting to UTF-8.\ndat\n## # A tibble: 3 x 5\n##   who_collected at_work person  time       date\n##           <chr>   <chr>  <chr> <chr>      <chr>\n## 1          jeff      no   ingo 13:47 08/26/2016\n## 2          jeff     yes  roger 13:47 08/26/2016\n## 3          jeff     yes  brian 13:47 08/26/2016\nHere the url is the one I got when I went to the “File” menu and clicked on “Publish to the web…”. The argument ws in the gs_read command is the name of the worksheet. If you have multiple sheets assigned to different people, you can read them in one at a time and then merge them together.\nConclusion\nSo that’s it, its pretty simple. But as I gear up to teach advanced data science here at Hopkins I’m thinking a lot about Sean Taylor’s awesome post Real scientists make their own data\nI think this approach is a super cool/super lightweight system for collecting data either on your own or as a team. As I said I think it could be really useful in public health, but it could also be used for any data collection you want.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-08-26-nssd-episode-21/",
    "title": "Not So Standard Deviations Episode 21 - This Might be the Future!",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-08-26",
    "categories": [],
    "contents": "\nHilary and I are apart again and this time we’re talking about political polling. Also, they discuss Trump’s tweets, and the fact that Hilary owns a bowling ball.\nAlso, Hilary and I have just published a new book, Conversations on Data Science, which collects some of our episodes in an easy-to-read format. The book is available from Leanpub and will be updated as we record more episodes. If you’re new to the podcast, this is a good way to do some catching up!\nIf you have questions you’d like us to answer, you can send them to nssdeviations @ gmail.com or tweet us at [@NSSDeviations](https://twitter.com/nssdeviations).\nSubscribe to the podcast on iTunes or Google Play.\nPlease leave us a review on iTunes!\nSupport us through our Patreon page.\nShow Notes:\nFiveThirtyEight election dashboard\nThe Upshot’s election dashboard\nDavid Robinson’s post on Trump’s tweets\nJulia Silge’s Twitter account\nThe Katering Show\nOmni\nDownload the audio for this episode.\nListen here:\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-08-25-meinshausen-copss/",
    "title": "Interview with COPSS award winner Nicolai Meinshausen.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-08-25",
    "categories": [],
    "contents": "\nEditor’s Note: We are again pleased to interview the COPSS President’s award winner. The COPSS Award is one of the most prestigious in statistics, sometimes called the Nobel Prize in statistics. This year the award went to Nicolai Meinshausen from ETH Zurich. He is known for his work in causality, high-dimensional statistics, and machine learning. Also see our past COPSS award interviews with John Storey and Martin Wainwright.\nNicolai MeinshausenDo you consider yourself to be a statistician, data scientist, machine learner, or something else?\nPerhaps all of the above. If you forced me to pick one, then statistician but I hope we will soon come to a point where these distinctions do not matter much any more.\nHow did you find out you had won the COPSS award?\nJeremy Taylor called me. I know I am expected to say I did not expect it but that was indeed the case and it was a genuine surprise.\nHow do you see the fields of causal inference and high-dimensional statistics merging?\nCausal inference is already very challenging in the low-dimensional case - if understood as data for which the number of observations exceeds the number of variables. There are commonalities between high-dimensional statistics and the subfield of causal discovery, however, as we try to recover a sparse underlying structure from data in both cases (say when trying to reconstruct a gene network from observational and intervention data). The interpretations are just slightly different. A further difference is the implicit optimization. High-dimensional estimators can often be framed as convex optimization problems and the question is whether causal discovery can or should be pushed in this direction as well.\nCan you explain a little about how you can infer causal effects from inhomogeneous data?\nWhy do we want a causal model in the first place? In most cases the benefit of a causal over a regression model is that the predictions of a causal model continue to be valid even if we intervene on the variables we use for prediction.\nThe inference we proposed turns this around and is looking for all models that are invariant in the sense that they give the same prediction accuracy across a number of different settings or environments. If we just have observational data, then this invariance holds for all models but if the data are inhomogeneous, certain models can be discarded since they work better in one environment than in another and can thus not be causal. If all models that show invariance use a certain variable, we can claim that the variable in question has a causal effect (while controlling type I error rates) and construct confidence intervals for the strength of the effect.\nYou have worked on studying the effects of climate change - do you think statisticians can play an important role in this debate?\nTo a certain extent. I have worked on several projects with physicists and the general caveat is that physicists are in general quite advanced in their methodology already and have quite a good understanding of the relevant statistical concepts. Biology is thus maybe a field where even more external input is required. Then again, it saves one from having to calculate t-tests in collaborations with physicists and just the interestingand challenging problems are left.\nWhat advice would you give young statisticians getting into the discipline right now?\nFirst I would say that they have made a good choice since these are interesting times for the field with many challenging and relevant problems still open and unsolved (but not completely out of reach either). I think its important to keep an open mind and read as much literature as possible from neighbouring fields. My personal experience has been that it is very beneficial to get involved in some scientific collaborations.\nWhat sorts of things is your group working on these days?\nTwo general themes: the first is what people would call more classical machine learning. For example, how can we detect interactions in large-scale datasets in sub-quadratic time? Secondly, we are trying to extend the invariance approach to causal inference to more general settings, for example allowing for nonlinearities and hidden variables while at the same time improving the computational aspects.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-08-24-replication-crisis/",
    "title": "A Simple Explanation for the Replication Crisis in Science",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-08-24",
    "categories": [],
    "contents": "\nBy now, you’ve probably heard of the replication crisis in science. In summary, many conclusions from experiments done in a variety of fields have been found to not hold water when followed up in subsequent experiments. There are now any number of famous examples now, particularly from the fields of psychology and clinical medicine that show that the rate of replication of findings is less than the the expected rate.\nThe reasons proposed for this crisis are wide ranging, but typical center on the preference for “novel” findings in science and the pressure on investigators (especially new ones) to “publish or perish”. My favorite reason places the blame for the entire crisis on p-values.\nI think to develop a better understanding of why there is a “crisis”, we need to step back and look across differend fields of study. There is one key question you should be asking yourself: Is the replication crisis evenly distributed across different scientific disciplines? My reading of the literature would suggest “no”, but the reasons attributed to the replication crisis are common to all scientists in every field (i.e. novel findings, publishing, etc.). So why would there be any heterogeneity?\nAn Aside on Replication and Reproducibility\nAs Lorena Barba recently pointed out, there can be tremendous confusion over the use of the words replication and reproducibility, so it’s best that we sort that out now. Here’s how I use both words:\nreplication: This is the act of repeating an entire study, independently of the original investigator without the use of original data (but generally using the same methods).\nreproducibility: A study is reproducible if you can take the original data and the computer code used to analyze the data and reproduce all of the numerical findings from the study. This may initially sound like a trivial task but experience has shown that it’s not always easy to achieve this seemly minimal standard.\nFor more precise definitions of what I mean by these terms, you can take a look at my recent paper with Jeff Leek and Prasad Patil.\nOne key distinction between replication and reproducibility is that with replication, there is no need to trust any of the original findings. In fact, you may be attempting to replicate a study because you do not trust the findings of the original study. A recent example includes the creation of stem cells from ordinary cells, a finding that was so extraodinary that it led several laboratories to quickly try to replicate the study. Ultimately, seven separate laboratories could not replicate the finding and the original study was ultimately retracted.\nAstronomy and Epidemiology\nWhat do the fields of astronomy and epidemiology have in common? You might think nothing. Those two departments are often not even on the same campus at most universities! However, they have at least one common element, which is that the things that they study are generally reluctant to be controlled by human beings. As a result, both astronomers and epidemiologist rely heavily on one tools: the observational study. Much has been written about observational studies of late, and I’ll spare you the literature search by saying that the bottom line is they can’t be trusted (particularly observational studies that have not been pre-registered!).\nBut that’s fine—we have a method for dealing with things we don’t trust: It’s called replication. Epidemiologists actually codified their understanding of when they believe a causal claim (see Hill’s Criteria), which is typically only after a claim has been replicated in numerous studies in a variety of settings. My understanding is that astronomers have a similar mentality as well—no single study will result in anyone believe something new about the universe. Rather, findings need to be replicated using different approaches, instruments, etc.\nThe key point here is that in both astronomy and epidemiology expectations are low with respect to individual studies. It’s difficult to have a replication crisis when nobody believes the findings in the first place. Investigators have a culture of distrusting individual one-off findings until they have been replicated numerous times. In my own area of research, the idea that ambient air pollution causes health problems was difficult to believe for decades, until we started seeing the same associations appear in numerous studies conducted all around the world. It’s hard to imagine any single study “proving” that connection, no matter how well it was conducted.\nTheory and Experimentation in Science\nI’ve already described the primary mode of investigation in astronomy and epidemiology, but there are of course other methods in other fields. One large category of methods includes the controlled experiment. Controlled experiments come in a variety of forms, whether they are laboratory experiments on cells or randomized clinical trials with humans, all of them involve intentional manipulation of some factor by the investigator in order to observe how such manipulation affects an outcome. In clinical medicine and the social sciences, controlled experiments are considered the “gold standard” of evidence. Meta-analyses and literature summaries generally weight publications with controlled experiments more highly than other approaches like observational studies.\nThe other aspect I want to look at here is whether a field has a strong basic theoretical foundation. The idea here is that some fields, like say physics, have a strong set of basic theories whose predictions have been consistently validated over time. Other fields, like medicine, lack even the most rudimentary theories that can be used to make basic predictions. Granted, the distinction between fields with or without “basic theory” is a bit arbitrary on my part, but I think it’s fair to say that different fields of study fall on a spectrum in terms of how much basic theory they can rely on.\nGiven the theoretical nature of different fields and the primary mode of investigation, we can develop the following crude 2x2 table, in which I’ve inserted some representative fields of study.\nTheory vs. Experimentation in ScienceMy primary contention here is\n\nThe replication crisis in science is concentrated in areas where (1) there is a tradition of controlled experimentation and (2) there is relatively little basic theory underpinning the field.\n\nFurther, in general, I don’t believe that there’s anything wrong with the people tirelessly working in the upper right box. At least, I don’t think there’s anything more wrong with them compared to the good people working in the other three boxes.\nIn case anyone is wondering where the state of clinical science is relative to, say, particle physics with respect to basic theory, I only point you to the web site for the National Center for Complementary and Integrative Health. This is essentially a government agency with a budget of $124 million dedicated to advancing pseudoscience. This is the state of “basic theory” in clinical medicine.\nThe Bottom Line\nThe people working in the upper right box have an uphill battle for at least two reasons\nThe lack of strong basic theory makes it more difficult to guide investigation, leading to wider ranging efforts that may be less likely to replicate.\nThe tradition of controlled experimentation places high expectations that research produced here is “valid”. I mean, hey, they’re using the gold standard of evidence, right?\nThe confluence of these two factors leads to a much greater disappointment when findings from these fields do not replicate. This leads me to believe that the replication crisis in science is largely attributable to a mismatch in our expectations of how often findings should replicate and how difficult it is to actually discover true findings in certain fields. Further, the reliance of controlled experiements in certain fields has lulled us into believing that we can place tremendous weight on a small number of studies. Ultimately, when someone asks, “Why haven’t we cured cancer yet?” the answer is “Because it’s hard”.\nThe Silver Lining\nIt’s important to remember that, as my colleague Rafa Irizarry pointed out, findings from many of the fields in the upper right box, especially clinical medicine, can have tremendous positive impacts on our lives when they do work out. Rafa says\n\n…I argue that the rate of discoveries is higher in biomedical research than in physics. But, to achieve this higher true positive rate, biomedical research has to tolerate a higher false positive rate.\n\nIt is certainly possible to reduce the rate of false positives—one way would be to do no experiments at all! But is that what we want? Would that most benefit us as a society?\nThe Takeaway\nWhat to do? Here are a few thoughts:\nWe need to stop thinking that any single study is definitive or confirmatory, no matter if it was a controlled experiment or not. Science is always a cumulative business, and the value of a given study should be understood in the context of what came before it.\nWe have to recognize that some areas are more difficult to study and are less mature than other areas because of the lack of basic theory to guide us.\nWe need to think about what the tradeoffs are for discovering many things that may not pan out relative to discovering only a few things. What are we willing to accept in a given field? This is a discussion that I’ve not seen much of.\nAs Rafa pointed out in his article, we can definitely focus on things that make science better for everyone (better methods, rigorous designs, etc.).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-07-30-jsm2016/",
    "title": "A meta list of what to do at JSM 2016",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-07-30",
    "categories": [],
    "contents": "\nI’m going to be heading out tomorrow for JSM 2016. If you want to catch up I’ll be presenting in the 6-8PM poster session on The Extraordinary Power of Data on Sunday and on data visualization (and other things) in MOOCs at 8:30am on Monday. Here is a little sneak preview, the first slide from my talk:\nWas too scared to use GIFsThis year I am so excited that other people have done all the work of going through the program for me and picking out what talks to see. Here is a list of lists.\nKarl Broman - if you like open source software, data viz, and genomics.\nRstudio - if you like Rstudio\nMine Cetinkaya Rundel - if you like stat ed, data science, data viz, and data journalism.\nJulian Wolfson - if you like missing sessions and guilt.\nStephanie Hicks - if you like lots of sessions and can’t make up your mind (also stat genomics, open source software, stat computing, stats for social good…)\nIf you know about more lists, please feel free to tweet at me or send pull requests.\nI also saw the materials for this awesome tutorial on webscraping that I’m sorry I’ll miss.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-07-20-relativity-raw-data/",
    "title": "The relativity of raw data",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-07-20",
    "categories": [],
    "contents": "\n“Raw data” is one of those terms that everyone in statistics and data science uses but no one defines. For example, we all agree that we should be able to recreate results in scientific papers from the raw data and the code for that paper.\n\nBut what do we mean when we say raw data?\n\nWhen working with collaborators or students I often find myself saying - could you just give me the raw data so I can do the normalization or processing myself. To give a concrete example, I work in the analysis of data from high-throughput genomic sequencing experiments.\nThese experiments produce data by breaking up genomic molecules into short fragements of DNA - then reading off parts of those fragments to generate “reads” - usually 100 to 200 letters long per read. But the reads are just puzzle pieces that need to be fit back together and then quantified to produce measurements on DNA variation or gene expression abundances.\nHigh throughput sequencingImage from Hector Corrata Bravo’s lecture notes\nWhen I say “raw data” when talking to a collaborator I mean the reads that are reported from the sequencing machine. To me that is the rawest form of the data I will look at. But to generate those reads the sequencing machine first (1) created a set of images for each letter in the sequence of reads, (2) measured the color at the spots on that image to get the quantitative measurement of which letter, and (3) calculated which letter was there with a confidence measure. The raw data I ask for only includes the confidence measure and the sequence of letters itself, but ignores the images and the colors extracted from them (steps 1 and 2).\nSo to me the “raw data” is the files of reads. But to the people who produce the machine for sequencing the raw data may be the images or the color data. To my collaborator the raw data may be the quantitative measurements I calculate from the reads. When thinking about this I realized an important characteristics of raw data.\n\nRaw data is relative to your reference frame.\n\nIn other words the raw data is raw to you if you have done no processing, manipulation, coding, or analysis of the data. In other words, the file you received from the person before you is untouched. But it may not be the rawest version of the data. The person who gave you the raw data may have done some computations. They have a different “raw data set”.\nThe implication for reproducibility and replicability is that we need a “chain of custody” just like with evidence collected by the police. As long as each person keeps a copy and record of the “raw data” to them you can trace the provencance of the data back to the original source.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:25:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-07-18-nssd-episode-19/",
    "title": "Not So Standard Deviations Episode 18 - Divide by n-1, or n-2, or Whatever",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-07-18",
    "categories": [],
    "contents": "\nHilary and I talk about statistical software in fMRI analyses, the differences between software testing differences in proportions (a must listen!), and a preview of JSM 2016.\nAlso, Hilary and I have just published a new book, Conversations on Data Science, which collects some of our episodes in an easy-to-read format. The books is available from Leanpub and will be updated as we record more episodes.\nIf you have questions you’d like us to answer, you can send them to nssdeviations @ gmail.com or tweet us at [@NSSDeviations](https://twitter.com/nssdeviations).\nSubscribe to the podcast on iTunes.\nSubscribe to the podcast on Google Play.\nPlease leave us a review on iTunes!\nSupport us through our Patreon page.\nShow Notes:\nfMRI bugs could upend years of research\nEklund et al. PNAS Paper\nJSM 2016 Program\nConversations on Data Science\nDownload the audio for this episode.\nListen here:\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-07-11-tuesday-update/",
    "title": "Tuesday update",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-07-11",
    "categories": [],
    "contents": "\nIt Might All Be Wrong\nTom Nichols and colleagues have published a paper on the software used to analyze fMRI data:\n\nFunctional MRI (fMRI) is 25 years old, yet surprisingly its most common statistical methods have not been validated using real data. Here, we used resting-state fMRI data from 499 healthy controls to conduct 3 million task group analyses. Using this null data with different experimental designs, we estimate the incidence of significant results. In theory, we should find 5% false positives (for a significance threshold of 5%), but instead we found that the most common software packages for fMRI analysis (SPM, FSL, AFNI) can result in false-positive rates of up to 70%. These results question the validity of some 40,000 fMRI studies and may have a large impact on the interpretation of neuroimaging results.\n\nCriminal Justice Forecasts\nThe ongoing discussion over the use of prediction algorithms in the criminal justice system reminds me a bit of the introduction of DNA evidence decades ago. Ultimately, there is a technology that few people truly understand and there are questions as to whether the information they provide is fair or accurate.\nShameless Promotion\nI have a new book coming out with Hilary Parker, based on our Not So Standard Deviations podcast. Sign up to be notified of its release (which should be Real Soon Now).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-07-05-nssd-episode-18/",
    "title": "Not So Standard Deviations Episode 18 - Back on Planet Earth",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-07-05",
    "categories": [],
    "contents": "\nWith Hilary fresh from Use R! 2016, Hilary and I discuss some of the highlights from the conference. Also, some followup about a previous Free Advertising and the NSSD drinking game.\nIf you have questions you’d like us to answer, you can send them to nssdeviations @ gmail.com or tweet us at [@NSSDeviations](https://twitter.com/nssdeviations).\nSubscribe to the podcast on iTunes.\nSubscribe to the podcast on Google Play.\nPlease leave us a review on iTunes!\nSupport us through our Patreon page.\nShow notes:\nTheranos movie with Jennifer Lawrence and Adam McKay\nSnowden movie\nWhat3Words being used in Mongolia\nlintr package\n“The Electronic Coach” with Don Knuth\nExploring the data on gender and GitHub repo ownership\nJeff Atwood “Falling Into the Pit of Success”\nGoogle coLaboratory\n#rcatladies stickers\nKatie Mack time-lapse video\nDownload the audio for this episode.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-06-28-stack-overflow-drob/",
    "title": "A Year at Stack Overflow",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-06-28",
    "categories": [],
    "contents": "\nDavid Robinson ([@drob](https://twitter.com/drob)) has a great post on his blog about his first year as a data scientist at Stack Overflow. This section in particular stood out for me:\n\nI like using R to learn interesting things about our data, but my longer term goal is to make it easy for any of our engineers to do so….Towards this goal, I’ve been focusing on building reliable tools and frameworks that people can apply to a variety of problems, rather than “one-off” analysis scripts. (There’s an awesome post by Jeff Magnusson at StitchFix about some of these general challenges). My approach has been building internal R packages, similar to AirBnb’s strategy (though our data team is quite a bit younger and smaller than theirs). These internal packages can query databases and parsing our internal APIs, including making various security and infrastructure issues invisible to the user.\n\nThe world needs an army of David Robinsons.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-06-28-tuesday-update/",
    "title": "Tuesday Update",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-06-28",
    "categories": [],
    "contents": "\nIf you weren’t sick of Theranos yet….\nLooks like there will be a movie version of the Theranos saga which, as far as I can tell, isn’t over yet, but no matter. It will be done by Adam McKay, the writer-director of The Big Short (excellent film), and will star Jennifer Lawrence as Elizabeth Holmes. From Vanity Fair:\n\nLegendary Pictures snapped up rights to the hot-button biopic for a reported $3 million Thursday evening, after outbidding and outlasting a swarm of competition from Warner Bros., Twentieth Century Fox, STX Entertainment, Regency Enterprises, Cross Creek, Amazon Studios, AG Capital, the Weinstein Company, and, in the penultimate stretch, Paramount, among other studio suitors.\n\n\nBased on a book proposal by two-time Pulitzer Prize-winning journalist John Carreyrou titled Bad Blood: Secrets and Lies in Silicon Valley, the project (reported to be in the $40 million to $50 million budget range) has made the rounds to almost every studio in town. It’s been personally pitched by McKay, who won an Oscar for best adapted screenplay for last year’s rollicking financial meltdown procedural The Big Short.\n\nFrankly, I think we all know how this movie will end.\nThe People vs. OJ Simpson vs….Statistics\nI’m in the middle of watching The People vs. OJ Simpson and so far it is fantastic—I highly recommend it. One thing that is not represented in the show is the important role that statistics played in the trial. The trial was just in the early days of using DNA as evidence in criminal trials and there were many questions about how likely it was to find DNA matches in blood.\nTerry Speed ended up testifying for the defense (Simpson) and in this nice interview, he explains how that came to be:\n\nAt the beginning of the Simpson trial, there was going to be a pre-trial hearing and experts from both sides would argue in front of the judge as to what approaches should be accepted. Other pre-trial activities dragged on, and the one on DNA forensics was eventually scrapped. The DNA experts, including me were then asked whether they wanted to give evidence for the prosecution or defence, or leave. I did not initially plan to join the defence team, but wished to express my point of view in what was more or less a scientific environment before the trial started, but when the pre-trial DNA hearing was scrapped, I decided that I had no choice but to express my views in court on behalf of the defence, which I did.\n\nThe full interview is well worth the read.\nAI is the residual\nI just recently found out about the AI effect which I thought was interesting. Basically, “AI” is whatever can’t be explained, or in other words, the residuals of machine learning.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-06-14-ultimate-ai-battle/",
    "title": "Ultimate AI battle - Apple vs. Google",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-06-14",
    "categories": [],
    "contents": "\nYesterday, Apple launched its Worldwide Developer’s Conference (WWDC) and had its public keynote address. While many new things were announced, the one thing that caught my eye was the dramatic expansion of Apple’s use of artificial intelligence (AI) tools. I talked a bit about AI with Hilary Parker on the latest Not So Standard Deviations, particularly in the context of Amazon’s Echo/Alexa, and I think it’s definitely going to be an area of intense competition between the major tech companies.\nPretty much every major tech player is involved in AI—Google, Facebook, Amazon, Apple, Microsoft—the list goes on. Recently, a some commentators have suggested that Apple in particular will never catch up with the likes of Google with respect to AI because of Apple’s strict stance on privacy and unwillingness to gather/aggregate data from all its users. However, yesterday at WWDC, Apple revealed a few clues about what it was up to in the AI world.\nFirst, Apple mentioned deep learning more than a few times, including specifically calling out its use of LSTM in its Messages app and facial recognition in its Photos app. Previously, Apple had been rumored to be applying deep learning to its Siri assistant and its fingerprint sensor. At WWDC, Craig Federighi stressed Apple’s continued focus on privacy and how Apple does not need to develop “user profiles” server-side, but rather does most computation on-device (in this case, on the iPhone).\nHowever, it can’t be that Apple does all its deep learning computation on the iPhone. These models tend to be enormous and take advantage of reams of data that can only be reasonablly processed server-side. Unfortunately, because most companies (Apple in particular) release few details about what they do, we may never how this works. But we can definitely speculate!\nApple vs. Google\nI think the Apple/Google dichotomy provides an interesting opportunity to talk about how models can be learned using data in different ways. There are two approaches being represented here by Apple and Google:\nGoogle way - Collect lots of data from users and store them on a server in the Googleplex somewhere. Then use that data to fit an enormous model that can predict when you’ve taken a picture of a cat. As users generate more data, bring that data back to the Googleplex and update/refine the model.\nApple way - Build a “starter model” in the Apple Mothership. As users generate data on their phones, bring the model to the phone and update the model using just their data. Bring the updated model back to the Apple Mothership and leave the user’s data on the phone.\nPerhaps the easiest way to understand this difference is with the arithmetic mean, which is perhaps the simplest “model”. Suppose you have a bunch of users out there and you want to compute the average of some attribute that they have on their phones (or whatever device). The first approach would be to get all that data and compute the mean in the usual way.\nGoogle wayOnce all the data is in the Googleplex, we can just use the formula\nGoogle meanI’ll call this the “Google mean” because it requires that you get all the data X1 through Xn, then sum them up and divide by n. Here, each of the Xi’s represents the ith user’s data. The general principle here is to gather all the data and then estimate the model parameters server-side.\nWhat if you didn’t want to gather everyone’s data centrally? Can you still compute the mean?\nApple wayYes, because there’s a nice recurrence formula for the mean:\nApple meanWe can call this the “Apple mean”. With this strategy, we can send our current estimate of the mean to each user, update our estimate by taking the weighted average of the old value and the new value, and then move on to the next user. Here, you send the model parameters out to the users, update those parameters and then bring the parameters back.\nWhich method is better? Well, in this case, both give you the same answer. In general, for linear models (like the mean), you can usually rework the formulas to build out either “whole data” (Google) approaches or “streaming” (Apple) approaches and get pretty much the same answer. But for non-linear models, it’s not so simple and you usually cannot achieve this kind of equivalence.\nClients and Servers\nBalancing how much work is done on a server and how much is done on the client is an age-old computing problem and, over time, the balance of work between client and server seems to shift back and forth like a pendulum. When I was in grad school, we had so-called “dumb terminals” that were basically a screen that you used to login to the server. Today, I use my laptop for computing/work and that’s it. But I use the cloud for many other tasks.\nThe Apple approach definitely requires a “fatter” client because the work of integrating current model parameters with new user data has to happen on the phone. With the Google approach, all the phone has to do is be able to collect the data and send it over the network to Google.\nThe Apple approach is also closely related to what my colleagues Martin Lindquist and Brian Caffo refer to as “fusion science”, whereby Big Data and “Small Data” can be fused together via models to improve inference, but without ever having to actually combine the data. In a Bayesian context, you might think of the Big Data as making up the prior distribution and the Small Data as the likelihood. The Small Data can be used to update the model parameters and produce the posterior distribution, after which the Small Data can be thrown out.\nAnd the Winner is…\nIt’s not clear to me which approach is better in terms of building a better model for prediction or inference. Sadly, we may never have enough details to find out, and will only be ablle to evaluate which approach is better by the performance of the systems in the marketplace. But perhaps that’s the way things should be evaluated in this case?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-06-13-good-books/",
    "title": "Good list of good books",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-06-13",
    "categories": [],
    "contents": "\nThe MultiThreaded blog over at Stitch Fix (hat tip to Hilary Parker) has posted a really nice list of data science books (disclosure: one of my books is on the list).\n\nWe’ve queried our data science team for some of their favorite data science books. This list is by no means exhaustive, but should keep any data scientist/engineer new or old learning and entertained for many an evening.\n\nEnjoy!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-06-09-nssd-episode-17/",
    "title": "Not So Standard Deviations Episode 17 - Diurnal High Variance",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-06-09",
    "categories": [],
    "contents": "\nHilary and I talk about Amazon Echo and Alexa as AI as a service, the COMPAS algorithm, criminal justice forecasts, and whether algorithms can introduce or remove bias (or both).\nIf you have questions you’d like us to answer, you can send them to nssdeviations @ gmail.com or tweet us at [@NSSDeviations](https://twitter.com/nssdeviations).\nSubscribe to the podcast on iTunes.\nSubscribe to the podcast on Google Play.\nPlease leave us a review on iTunes!\nSupport us through our Patreon page.\nShow notes:\nIn Two Moves, AlphaGo and Lee Sedol Redefined the Future\nGoogle’s AI won the game Go by defying millennia of basic human instinct\nMachine Bias: There’s Software Used Across the Country to Predict Future Criminals. And it’s Biased Against Blacks\nProPublica analysis of COMPAS\nRichard Berk’s Criminal Justice Forecasts of Risk\nCathy O’Neill’s Weapons of Math Destruction\nI’ll stop calling algorithms racist when you stop anthropomorphizing AI\nRMS Fact package\nUse R! 2016\nDownload the audio for this episode.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-06-03-defining-success/",
    "title": "Defining success - Four secrets of a successful data science experiment",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-06-03",
    "categories": [],
    "contents": "\nEditor’s note: This post is excerpted from the book Executive Data Science: A Guide to Training and Managing the Best Data Scientists, written by myself, Brian Caffo, and Jeff Leek. This particular section was written by Brian Caffo.\nDefining success is a crucial part of managing a data science experiment. Of course, success is often context specific. However, some aspects of success are general enough to merit discussion. A list of hallmarks of success includes:\nNew knowledge is created.\nDecisions or policies are made based on the outcome of the experiment.\nA report, presentation, or app with impact is created.\nIt is learned that the data can’t answer the question being asked of it.\nSome more negative outcomes include: Decisions being made that disregard clear evidence from the data, equivocal results that do not shed light in one direction or another, uncertainty which prevents new knowledge from being created.\nLet’s discuss some of the successful outcomes first.\nNew knowledge seems ideal in many cases (especially since we are academics), but new knowledge doesn’t necessarily mean that it’s important. If this new knowledge produces actionable decisions or policies, that’s even better. The idea of having evidence-based policy, while perhaps newer than the analogous evidence-based medicine movement that has transformed medical practice, has the potential to similarly transform public policy. Finaly, that our data science products have great (positive) impact on an audience that is much wider than a group of data scientists, is of course ideal. Creating reusable code or apps is great way to increase the impact of a project and to disseminate its findings.\nThe fourth point is perhaps the most controversial. I view it as a success if we can show that the data can’t answer the questions being asked. I am reminded of a friend who told a story of the company he worked at. They hired many expensive prediction consultants to help use their data to inform pricing. However, the prediction results weren’t helping. They were able to prove that the data couldn’t answer the hypothesis under study. There was too much noise and the measurements just weren’t accurately measuring what was needed. Sure, the result wasn’t optimal, as they still needed to know how to price things, but it did save money on consultants. I have since heard this story repeated nearly identically by friends in different industries.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-05-31-barrier-to-medication/",
    "title": "Sometimes the biggest challenge is applying what we already know",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-05-31",
    "categories": [],
    "contents": "\nThere’s definitely a need to innovate and develop new treatments in the area of asthma, but it’s easy to underestimate the barriers to just doing what we already know, such as making sure that people are following existing, well-established guidelines on how to treat asthma. My colleague, Elizabeth Matsui, has written about the challenges in a study that we are collaborating on:\n\nOur group is currently conducting a study that includes implementation of national guidelines-based medical care for asthma, so that one process that we’ve had to get right is to prescribe an appropriate dose of medication and get it into the family’s hands. [emphasis added]\n\nSeems simple, right?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-05-24-somtimes-theres-friction-for-a-reason/",
    "title": "Sometimes there's friction for a reason",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-05-24",
    "categories": [],
    "contents": "\nThinking about my post on Theranos yesterday it occurred to me that one thing that’s great about all of the innovation and technology coming out of places like Silicon Valley is the tremendous reduction of friction in our lives. With Uber it’s much easier to get a ride because of improvement in communication and an increase in the supply of cars. With Amazon, I can get that jug of vegetable oil that I always wanted without having to leave the house, because Amazon.\nSo why is there all this friction? Sometimes it’s because of regulation, which may have made sense at an earlier time, but perhaps doesn’t make as much sense now. Sometimes, you need a company like Amazon to really master the logistics operation to be able to deliver anything anywhere. Otherwise, you’re just stuck driving to the grocery store to get that vegetable oil.\nBut sometimes there’s friction for a reason. For example, Ben Thompson talks about how previously there was quite a bit more friction involved before law enforcement could listen in on our communications. Although wiretapping had long been around (as noted by David Simon of…The Wire) the removal of all friction by the NSA made the situation quite different. Related to this idea is the massive data release from OkCupid a few weeks ago, as I discussed on the latest Not So Standard Deviations podcast episode. Sure, your OkCupid profile is visible to everyone with an account, but having someone vacuum up 70,000 profiles and dumping them on the web for anyone to view is not what people signed up for—there is a qualitative difference there.\nWhen it comes to Theranos and diagnostic testing in general, there is similarly a need for some friction in order to protect public health. John Ioannides notes in his commentary for JAMA:\n\nEven if the tests were accurate, when they are performed in massive scale and multiple times, the possibility of causing substantial harm from widespread testing is very real, as errors accumulate with multiple testing. Repeated testing of an individual is potentially a dangerous self-harm practice, and these individuals are destined to have some incorrect laboratory results and eventually experience harm, such as, for example, the anxiety of being labeled with a serious condition or adverse effects from increased testing and procedures to evaluate false-positive test results. Moreover, if the diagnostic testing process becomes dissociated from physicians, self-testing and self-interpretation could cause even more problems than they aim to solve.\n\nUnlike with the NSA, where the differences in scale may be difficult to quantify because the exact extent of the program is unknown to most people, with diagnostic testing, we can precisely quantify how a diagnostic test’s characteristics will change if we apply it to 1,000 people vs. 1,000,000 people. This is why organizations like the US Preventative Services Task Force so carefully considers recommendations for testing or screening (and why they have a really tough job).\nI’ll admit that a lot of the friction in our daily lives is pointless and it would be great to reduce it if possible. But in many cases, it was us that put the friction there for a reason, and it’s sometimes good to think about why before we move to eliminate it.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-05-23-nssd-episode-16/",
    "title": "Not So Standard Deviations Episode 16 - The Silicon Valley Episode",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-05-23",
    "categories": [],
    "contents": "\nRoger and Hilary are back, with Hilary broadcasting from the west coast. Hilary and Roger discuss the possibility of scaling data analysis and how that may or may not work for companies like Palantir. Also, the latest on Theranos and the release of data from OkCupid.\nIf you have questions you’d like us to answer, you can send them to nssdeviations @ gmail.com or tweet us at [@NSSDeviations](https://twitter.com/nssdeviations).\nSubscribe to the podcast on iTunes.\nSubscribe to the podcast on Google Play.\nPlease leave us a review on iTunes!\nSupport us through our Patreon page.\nShow notes:\nBuzzFeed Article on Palantir\nRoger’s Simply Statistics post on Palantir\nLooker\nData science done well looks easy\nLatest on Theranos\nOkCupid Data Release\nSecret history of Silicon Valley\nWealthfront blog\nDownload the audio for this episode.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-05-23-update-on-theranos/",
    "title": "Update on Theranos",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-05-23",
    "categories": [],
    "contents": "\nI think it’s fair to say that things for Theranos, the Silicon Valley blood testing company, are not looking up. From the Wall Street Journal (via The Verge):\n\nTheranos has voided two years of results from its Edison blood-testing machines, issuing tens of thousands of corrected reports to patients and doctors and raising the possibility that many health care decisions may have been made based on inaccurate data. The Wall Street Journal first reported the news, saying that many of the corrected tests have been run using traditional machinery. One doctor told the Journal that she sent a patient to the emergency room after seeing abnormal results from a Theranos test; the corrected report returned normal readings.\n\nFurthermore, this commentary in JAMA from John Ioannides emphasizes the need for caution when implementing testing on a massive scale. In particular, “The notion of patients and healthy people being repeatedly tested in supermarkets and pharmacies, or eventually in cafeterias or at home, sounds revolutionary, but little is known about the consequences” and the consequences really matter here. In addition, as the title of the commentary would indicate, research done in secret is not research at all. For there the be credibility for a company like this, data needs to be made public.\nI continue to maintain that the fundamental premise on which the company is built, as stated by its founder Elizabeth Holmes, is flawed. Two concepts are repeatedly made in the context of Theranos:\nMore testing is better. Anyone who stayed awake in their introduction to Bayesian statistics lecture knows this is very difficult to make true in all circumstances, no matter how accurate a test is. With rare diseases, the number of false positives is overwhelming and can have very real harmful effects on people. Combine testing on a massive scale, with repeated application over time, and you get a recipe for confusion.\nPeople do not get tested because they are afraid of needles. Elizabeth Holmes makes a big deal about her personal fear of needles and it’s impact on her (not) getting blood tests done. I have no doubt that many people share this fear, but I have serious doubt that this is the reason people don’t get the medical testing done. There are many barriers to people getting the medical care that they need, many that are non-financial in nature and do not include fear of needles. The problem of getting people the medical care that they need is one deserving of serious attention, but changing the manner in which blood is collected is not going to do it.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-12T07:59:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-05-18-software-engineering-data-science/",
    "title": "What is software engineering for data science?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-05-18",
    "categories": [],
    "contents": "\nEditor’s note: This post is a chapter from the book Executive Data Science: A Guide to Training and Managing the Best Data Scientists, written by myself, Brian Caffo, and Jeff Leek.\nSoftware is the generalization of a specific aspect of a data analysis. If specific parts of a data analysis require implementing or applying a number of procedures or tools together, software is the encompassing of all these tools into a specific module or procedure that can be repeatedly applied in a variety of settings. Software allows for the systematizing and the standardizing of a procedure, so that different people can use it and understand what it’s going to do at any given time.\nSoftware is useful because it formalizes and abstracts the functionality of a set of procedures or tools, by developing a well defined interface to the analysis. Software will have an interface, or a set of inputs and a set of outputs that are well understood. People can think about the inputs and the outputs without having to worry about the gory details of what’s going on underneath. Now, they may be interested in those details, but the application of the software at any given setting will not necessarily depend on the knowledge of those details. Rather, the knowledge of the interface to that software is important to using it in any given situation.\nFor example, most statistical packages will have a linear regression function which has a very well defined interface. Typically, you’ll have to input things like the outcome and the set of predictors, and maybe there will be some other inputs like the data set or weights. Most linear regression functions kind of work in that way. And importantly, the user does not have to know exactly how the linear regression calculation is done underneath the hood. Rather, they only need to know that they need to specify the outcome, the predictors, and a couple of other things. The linear regression function abstracts all the details that are required to implement linear regression, so that the user can apply the tool in a variety of settings.\nThere are three levels of software that are important to consider, going from kind of the simplest to the most abstract.\nAt the first level you might just have some code that you wrote, and you might want to encapsulate the automation of a set of procedures using a loop (or something similar) that repeats an operation multiple times.\nThe next step might be some sort of function. Regardless of what language you may be using, often there will be some notion of a function, which is used to encapsulate a set of instructions. And the key thing about a function is that you’ll have to define some sort of interface, which will be the inputs to the function. The function may also have a set of outputs or it may have some side effect for example, if it’s a plotting function. Now the user only needs to know those inputs and what the outputs will be. This is the first level of abstraction that you might encounter, where you have to actually define and interface to that function.\nThe highest level is an actual software package, which will often be a collection of functions and other things. That will be a little bit more formal because there’ll be a very specific interface or API that a user has to understand. Often for a software package there’ll be a number of convenience features for users, like documentation, examples, or tutorials that may come with it, to help the user apply the software to many different settings. A full on software package will be most general in the sense that it should be applicable to more than one setting.\nOne question that you’ll find yourself asking, is at what point do you need to systematize common tasks and procedures across projects versus recreating code or writing new code from scratch on every new project? It depends on a variety of factors and answering this question may require communication within your team, and with people outside of your team. You may need to develop an understanding of how often a given process is repeated, or how often a given type of data analysis is done, in order to weigh the costs and benefits of investing in developing a software package or something similar.\nWithin your team, you may want to ask yourself, “Is the data analysis you’re going to do something that you are going to build upon for future work, or is it just going to be a one shot deal?” In our experience, there are relatively few one shot deals out there. Often you will have to do a certain analysis more than once, twice, or even three times, at which point you’ve reached the threshold where you want to write some code, write some software, or at least a function. The point at which you need to systematize a given set of procedures is going to be sooner than you think it is. The initial investment for developing more formal software will be higher, of course, but that will likely pay off in time savings down the road.\nA basic rule of thumb is\nIf you’re going to do something once (that does happen on occasion), just write some code and document it very well. The important thing is that you want to make sure that you understand what the code does, and so that requires both writing the code well and writing documentation. You want to be able to reproduce it down later on if you ever come back to it, or if someone else comes back to it.\nIf you’re going to do something twice, write a function. This allows you to abstract a small piece of code, and it forces you to define an interface, so you have well defined inputs and outputs.\nIf you’re going to do something three times or more, you should think about writing a small package. It doesn’t have to be commercial level software, but a small package which encapsulates the set of operations that you’re going to be doing in a given analysis. It’s also important to write some real documentation so that people can understand what’s supposed to be going on, and can apply the software to a different situation if they have to.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-05-13-reproducible-research-language/",
    "title": "Disseminating reproducible research is fundamentally a language and communication problem",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-05-13",
    "categories": [],
    "contents": "\nJust about 10 years ago, I wrote my first of many articles about the importance of reproducible research. Since that article, one of the points I’ve made is that the key issue to resolve was one of tools and infrastructure. At the time, many people were concerned that people would not want to share data and that we had to spend a lot of energy finding ways to either compel or incentivize them to do so. But the reality was that it was difficult to answer the question “What should I do if I desperately want to make my work reproducible?” Back then, even if you could convince a clinical researcher to use R and LaTeX to create a Sweave document (!), it was not immediately obvious where they should host the document, code, and data files.\nMuch has happened since then. We now have knitr and Markdown for live documents (as well as iPython notebooks and the like). We also have git, GitHub, and friends, which provide free code sharing repositories in a distributed manner (unlike older systems like CVS and Subversion). With the recent announcement of the Journal of Open Source Software, posting code on GitHub can now be recognized within the current system of credits and incentives. Finally, the number of data repositories has grown, providing more places for researchers to deposit their data files.\nIs the tools and infrastructure problem solved? I’d say yes. One thing that has become clear is that disseminating reproducible research is no longer a software problem. At least in R land, building live documents that can be executed by others is straightforward and not too difficult to pick up (thank you John Gruber!). For other languages there many equivalent (if not better) tools for writing documents that mix code and text. For this kind of thing, there’s just no excuse anymore. Could things be optimized a bit for some edge cases? Sure, but the tools are prefectly fine for the vast majority of use cases.\nBut now there is a bigger problem that needs to be solved, which is that we do not have an effective way to communicate data analyses. One might think that publishing the full code and datasets is the perfect way to communicate a data analysis, but in a way, it is too perfect. That approach can provide too much information.\nI find the following analogy useful for discussing this problem. If you look at music, one way to communicate music is to provide an audio file, a standard WAV file or something similar. In a way, that is a near-perfect representation of the music—bit-for-bit—that was produced by the performer. If I want to experience a Beethoven symphony the way that it was meant to be experienced, I’ll listen to a recording of it.\nBut if I want to understand how Beethoven wrote the piece—the process and the details—the recording is not a useful tool. What I look at instead is the score. The recording is a serialization of the music. The score provides an expanded representation of the music that shows all of the different pieces and how they fit together. A person with a good ear can often reconstruct the score, but this is a difficult and time-consuming task. Better to start with what the composer wrote originally.\nOver centuries, classical music composers developed a language and system for communicating their musical ideas so that\nthere was enough detail that a 3rd party could interpret the music and perform it to a level of accuracy that satisfied the composer; but\nit was not so prescriptive or constraining so that different performers could not build on the work and incorporate their own ideas\nIt would seem that traditional computer code satisfies those criteria, but I don’t think so. Traditional computer code (even R code) is designed to communicate programming concepts and constructs, not to communicate data analysis constructs. For example, a for loop is not a data analysis concept, even though we may use for loops all the time in data analysis.\nBecause of this disconnect between computer code and data analysis, I often find it difficult to understand what a data analysis is doing, even if it is coded very well. I imagine this is what programmers felt like when programming in processor-specific assembly language. Before languages like C were developed, where high-level concepts could be expressed, you had to know the gory details of how each CPU operated.\nThe closest thing that I can see to a solution emerging is the work that Hadley Wickham is doing with packages like dplyr and ggplot2. The dplyr package’s verbs (filter, arrange, etc.) represent data manipulation concepts that are meaningful to analysts. But we still have a long way to go to cover all of data analysis in this way.\nReproducible research is important because it is fundamentally about communicating what you have done in your work. Right now we have a sub-optimal way to communicate what was done in a data analysis, via traditional computer code. I think developing a new approach to communicating data analysis could have a few benefits:\nIt would provide greater transparency\nIt would allow others to more easily build on what was done in an analysis by extending or modifying specific elements\nIt would make it easier to understand what common elements there were across many different data analyses\nIt would make it easier to teach data analysis in a systematic and scalable way\nSo, any takers?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-05-11-palantir-struggles/",
    "title": "The Real Lesson for Data Science That is Demonstrated by Palantir's Struggles",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-05-11",
    "categories": [],
    "contents": "\nBuzzfeed recently published a long article on the struggles of the secretive data science company, Palantir.\n\nOver the last 13 months, at least three top-tier corporate clients have walked away, including Coca-Cola, American Express, and Nasdaq, according to internal documents. Palantir mines data to help companies make more money, but clients have balked at its high prices that can exceed $1 million per month, expressed doubts that its software can produce valuable insights over time, and even experienced difficult working relationships with Palantir’s young engineers. Palantir insiders have bemoaned the “low-vision” clients who decide to take their business elsewhere.\n\nPalantir’s origins are with PayPal, and its founders are part of the PayPal Mafia. As Peter Thiel describes it in his book Zero to One, PayPal was having a lot of trouble with fraud and the FBI was getting on its case. So PayPal developed some software to monitor the millions of transacations going through its systems and to flag transactions that were suspicious. Eventually, they realized that this kind of software might be useful to government agencies in a variety of contexts and the idea for Palantir was born.\nMuch of the press reaction to Buzzfeed’s article amounts to schadenfreude over the potential fall of another so-called Silicon Valley unicorn. Indeed, Palentir is valued at $20 billion, a valuation only exceeded in the private markets by Airbnb and Uber. But to me, nothing in the article indicates that Palantir is necessarily more poorly run than your average startup. It looks like they are going through pretty standard growing pains, trying to scale the business and diversify the customer base. It’s not surprising to me that employees would leave at this point—going from startup to “real company” is often not that fun and just a lot of work.\nHowever, a key question that arises is that if Palantir is having trouble trying to scale the business, why might that be? The Buzzfeed article doesn’t contain any answers but in this post I will attempt to speculate.\nThe real message from the Buzzfeed article goes beyond just Palantir and is highly relevant to the data science world. It ultimately comes down to the question of what is the value of data analysis?, and secondarily, how do you communicate that value?\nThe Data Science Spectrum\nData science activities live on a spectrum with software on one end and highly customized consulting on another end (I lump a lot of things into consulting, including methods development, modeling, etc.).\nData Science SpectrumThe idea being that if someone comes to you with a data problem, there are two extremes that you might offer to them:\nGive them some software, some documentation, and maybe a brief tutorial on how to use the software, and then send them on their way. For example, if someone wants to see if two groups are different from each other, you could send them the t.test() function in R and explain how to use it. This could be done over email; you wouldn’t even have to talk to the person.\nMeet with the person, talk about their problem and the question they’re trying to answer, develop an analysis plan, and build a custom software solution that produces the exact output that they’re looking for.\nThe first option is cheap, simple, and if you had a good enough web site, the person probably wouldn’t even have to talk with you at all! For example, I use this web site for sample size calculations and I’ve never spoken with the author of the web site. Much of the labor is up front, for the development of the software, and then is amortized over the life of the product. Ultimately, a software product has zero marginal cost and so it can be easily replicated and is infinitely scalable.\nThe second option is labor intensive, time-consuming, ongoing in nature, and is only scalable to the extent that you are willing to forgo sleep and maybe bend the space-time continuum. By definition, a custom solution is unique and is difficult to replicate.\nSelling Data Science\nAn important question for Palantir and data scientists in general is “How do you communicate the value of data analysis?” Many people expect the result of a good data analysis to be something “surprising”, i.e. something that they didn’t already know. Because if they knew it already why bother looking at the data? Think Moneyball—if you can find that “diamond in the rough” it make spending the time to analyze the data worthwhile. But the success of a data analysis can’t depend on the results. What if you go through the data and find nothing? Is the data analysis a failure? We as data scientists can only show what the data show. Otherwise, it just becomes a recipe for telling people what they want to hear.\nIt’s tempting for a client to say “well, the data didn’t show anything surprising so there’s no value there.” Also, a data analysis may reveal something that is perhaps interesting but doesn’t necessarily lead to any sort of decision. For example, there may be an aspect of a business process that is inefficient but is nevertheless unmodifiable. There may be little perceived value in discovering this with data.\nWhat is Useful?\nPalantir apparently tried to develop a relationship with American Express, but ultimately failed.\n\nBut some major firms have not found Palantir’s products and services that useful. In April 2015, employees were informed that American Express (codename: Charlie’s Angels) had dumped Palantir after 18 months of cybersecurity work, including a six-month pilot, an email shows. “We struggled from day 1 to make Palantir a sticky product for users and generate wins,” Sid Rajgarhia, a Palantir business development employee, said in the email.\n\nWhat does it mean for a data analysis product to be useful? It’s not necessarily clear to me in this case. Did Palantir not reveal new information? Did they not highlight something that could be modified?\nLack of Deep Expertise\nA failed attempt attempt at working with Coke reveals some other challenges of the data science business model.\n\nThe beverage giant also had other concerns [in addition to the price]. Coke “wanted deeper industry expertise in a partner,” Jonty Kelt, a Palantir executive, told colleagues in the email. He added that Coca-Cola’s “working relationship” with the youthful Palantir employees was “difficult.” The Coke executive acknowledged that the beverage giant “needs to get better at working with millennials,” according to Kelt. Coke spokesperson Scott Williamson declined to comment.\n\nAnnoying millenials notwithstanding, it’s clear that Coke didn’t feel comfortable collaborating with Palantir’s personnel. Like any data science collaboration, it’s key that the data scientist have some familiarity with the domain. In many cases, having “deep expertise” in an area can give a collaborator confidence that you will focus on the things that matter to them. But developing that expertise costs money and time and it may prevent you from working with other types of clients where you will necessarily have less expertise. For example, Palantir’s long experience working with the US military and intelligence agencies gave them deep expertise in those areas, but how does that help them with a consumer products company?\nHarder Than It Looks\nThe final example of a client that backed out is Kimberly-Clark:\n\nBut Kimberly-Clark was getting cold feet by early 2016. In January, a year after the initial pilot, Kimberly-Clark executive Anthony J. Palmer said he still wasn’t ready to sign a binding contract, meeting notes show. Palmer also “confirmed our suspicion” that a primary reason Kimberly-Clark had not moved forward was that “they wanted to see if they could do it cheaper themselves,” Kelt told colleagues in January. [emphasis added]\n\nThis is a common problem confronted by anyone in the data science business. A good analysis often looks easy in retrospect—all you did was run some functions and put the data through some models! In fact, running the models probably is the easy part, but getting to the point where you can actually fit models can be incredibly hard. Many clients, not seeing the long and winding process leading to a model, will be tempted think they can do it themselves.\nPalantir’s Valuation\nUltimately, what makes Palantir interesting is its astounding valuation. But what is the driver of this valuation? I think the key to answering this question lies in the description of the company itself:\n\nThe company, based in Palo Alto, California, is essentially a hybrid software and consulting firm, placing what it calls “forward deployed engineers” on-site at client offices.\n\nWhat does it mean to be a “hybrid software and consulting firm”? And which one is the company more like? Consulting or software? Because ultimately, revealing which side of the spectrum Palantir is really on could have huge implications for its valuation and future prospects.\nConsulting companies can surely make a lot of money, but none to my knowledge have the kind of valuation that Palantir currently commands. If it turns out that every customer of Palantir’s requires a custom solution, then I think they’re likely overvalued, because that model scales poorly. On the other hand, if Palantir has genuinely figured out a way to “software-ize” data analysis and to turn it into a commodity, then they are very likely undervalued.\nGiven the tremendous difficulty of turning data analysis into a software problem, my guess is that they are more akin to a consulting company and are overvalued. This is not to say that they won’t make money—they will likely make plenty—but that they won’t be the Silicon Valley darling that everyone wants them to be.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:24:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-05-10-social-media/",
    "title": "A means not an end - building a social media presence as a junior scientist",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-05-10",
    "categories": [],
    "contents": "\nEditor’s note - This is a chapter from my book How to be a modern scientist where I talk about some of the tools and techniques that scientists have available to them now that they didn’t before. 50% of all royalties from the book go to support Data Carpentry to promote data science education.\nSocial media - what should I do and why?\nSocial media can serve a variety of roles for modern scientists. Here I am going to focus on the role of social media for working scientists whose primary focus is not on scientific communication. Something that is often missed by people who are just getting started with social media is that there are two separate components to developing a successful social media presence.\nThe first is to develop a following and connections to people in your community. This is achieved through being either a content curator, a content generator, or being funny/interesting in some other way. This often has nothing to do with your scientific output.\nThe second component is using your social media presence to magnify the audience for your scientific work. You can only do this if you have successfully developed a network and community in the first step. Then, when you post about your own scientific papers they will be shared.\nTo most effectively achieve all of these goals you need to identify relevant communities and develop a network of individuals who follow you and will help to share your ideas and work.\nSet up social media accounts and follow relevant people/journals\nOne of the largest academic communities has developed around Twitter, but some scientists are also using Facebook for professional purposes. If you set up a Twitter account, you should then find as many colleagues in your area of expertise that you can find and also any journals that are in your area.\nUse your social media account to promote the work of other people\nIf you just use your social media account to post links to any papers that you publish, it will be hard to develop much of a following. It is also hard to develop a following by constantly posting long form original content such as blog posts. Alternatively you can gain a large number of followers by being (a) funny, (b) interesting, or (c) being a content curator. This latter approach can be particularly useful for people new to social media. Just follow people and journals you find interesting and share anything that you think is important/creative/exciting.\nShare any work that you develop\nAny code, publications, data, or blog posts you create you can share from your social media account. This can help raise your profile as people notice your good work. But if you only post your own work it is rarely possible to develop a large following unless you are already famous for another reason.\nSocial media - what tools should I use?\nThere are a large number of social media platforms that are available to scientists. Creatively using any new social media platform if it has a large number of users can be a way to quickly jump into the consciousness of more people. That being said the two largest communities of scientists have organized around two of the largest social media platforms.\nTwitter - is a platform where you can post short (less than 140 character) messages. This is a great platform for both discovering science and engaging in conversations about topics at a superficial level. It is not particularly useful for in depth scientific discussions.\nFacebook - some scientists post longer form scientific discussions on Facebook, but the community there is somewhat less organized and people tend to use it less for professional reasons. However, sharing content on Facebook, particularly when it is of interest to a general audience, can lead to a broader engagement in your work.\nThere are also a large and growing number of academic-specific social networks. For the most part these social networks are not widely used by practicing scientists and so don’t represent the best use of your time.\nYou might also consider short videos on Vine, longer videos on Youtube, more image intensive social media on Tumblr or Instagram if you have content that regularly fits those outlets. But they tend to have smaller communities of scientists with less opportunity for back and forth.\nSocial media - further tips and issues\nYou do not need to develop original content\nSocial media can be a time suck, particularly if you are spending a lot of time engaging in conversations on your platform of choice. Generating long form content in particular can take up a lot of time. But you don’t need to do that to generate a decent following. Finding the right community and then sharing work within that community and adding brief commentary and ideas can often help you develop a large following which can then be useful for other reasons.\nAdd your own commentary\nOnce you are comfortable using the social media platform of your choice you can start to engage with other people in conversation or add comments when you share other people’s work. This will increase the interest in your social media account and help you develop followers. This can be as simple as one-liners copied straight from the text of papers or posts that you think are most important.\nMake online friends - then meet them offline\nOne of the biggest advantages of scientific social media is that it levels the playing ground. Don’t be afraid to engage with members of your scientific community at all levels, from members of the National Academy (if they are online!) all the way down to junior graduate students. Getting to know a diversity of people can really help you during scientific meetings and visits. If you spend time cultivating online friendships, you’ll often meet a “familiar handle” at any conference or meeting you go to.\nInclude images when you can\nIf you see a plot from a paper you think is particularly compelling, copy it and attach it when you post/tweet when you link to the paper. On social media, images are often better received than plain text.\nBe careful of hot button issues (unless you really care)\nOne thing to keep in mind on social media is the amplification of opinions. There are a large number of issues that are of extreme interest and generate really strong opinions on multiple sides. Some of these issues are common societal issues (e.g., racism, feminism, economic inequality) and some are specific to science (e.g., open access publishing, open source development). If you are starting a social media account to engage in these topics then you should definitely do that. If you are using your account primarily for scientific purposes you should consider carefully the consequences of wading into these discussions. The debates run very hot on social media and you may post what you consider to be a relatively tangential or light message on one of these topics and find yourself the center of a lot of attention (positive and negative).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-05-05-timeseries-biomedical/",
    "title": "Time Series Analysis in Biomedical Science - What You Really Need to Know",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-05-05",
    "categories": [],
    "contents": "\nFor a few years now I have given a guest lecture on time series analysis in our School’s Environmental Epidemiology course. The basic thrust of this lecture is that you should generally ignore what you read about time series modeling, either in papers or in books. The reason is because I find much of the time series literature is not particularly helpful when doing analyses in a biomedical or population health context, which is what I do almost all the time.\nPrediction vs. Inference\nFirst, most of the literature on time series models tends to assume that you are interested in doing prediction—forecasting future values in a time series. I almost am never doing this. In my work looking at air pollution and mortality, the goal is never to find the best model that predicts mortality. In particular, if our goal were to predict mortality, we would probably never include air pollution as a predictor. This is because air pollution has an inherently weak association with mortality at the population, whereas things like temperature and other seasonal factors tend to have a much stronger association.\nWhat I am interested in doing is estimating an association between changes in air pollution levels and mortality and making some sort of inference about that association, either to a broader population or to other time periods. The challenges in these types of analyses include estimating weak associations in the presence of many stronger signals and appropriately adjusting for any potential confounding variables that similarly vary over time.\nThe reason the distinction between prediction and inference is important is that focusing on one vs. the other can lead you to very different model building strategies. Prediction modeling strategies will always want you to include into the model factors that are strongly correlated with the outcome and explain a lot of the outcome’s variation. If you’re trying to do inference and use a prediction modeling strategy, you may make at least two errors:\nYou may conclude that your key predictor of interest (e.g. air pollution) is not important because the modeling strategy didn’t deem to include it\nYou may omit important potential confounders because they have a weak releationship with the outcome (but maybe have a strong relationship with your key predictor). For example, one class of potential confounders in air pollution studies is other pollutants, which tend to be weakly associated with mortality but may be strongly associated with your pollutant of interest.\nRandom vs. Fixed\nAnother area where I feel much time series literature differs from my practice is on the whether to focus on fixed effects or random effects. Most of what you might think of when you think of time series models (i.e. AR models, MA models, GARCH, etc.) focuses on modeling the random part of the model. Often, you end up treating time series data as random because you simply do not have any other data. But the reality is that in many biomedical and public health applications, patterns in time series data can be explained by clearly understood fixed patterns.\nFor example, take this time series here. It is lower at the beginning and at the end of the series, with higher level sin the middle of the period.\nTime series with seasonal pattern 1It’s possible that this time series could be modeled with an auto-regressive (AR) model or maybe an auto-regressive moving average (ARMA) model. Or it’s possible that the data are exhibiting a seasonal pattern. It’s impossible to tell from the data whether this is a random formulation of this pattern or whether it’s something you’d expect every time. The problem is that we usually onl have one observation from teh time series. That is, we observe the entire series only once.\nNow take a look at this time series. It exhibits some of the same properties as the first series: it’s low at the beginning and end and high in the middle.\nTime series with seasonal pattern 2Should we model this as a random process or as a process with a fixed pattern? That ultimately will depend on the what type of data this is and what we know about it. If it’s air pollution data, we might do one thing, but if it’s stock market data, we might do a totally different thing.\nIf one were to see replicates of the time series, we’d be able to resolve the fixed vs. random question. For example, because I simulated the data above, I can simulate another replicate and see what happens. In the plot below I show two replications from each of the processes.\nFixed and random time series patternsIt’s clear now that the time series on the top row has a fixed “seasonal” pattern while the time series on the bottom row is random (in fact it is simulated from an AR(1) model).\nThe point here is that I think very often we know things about the time series that we’re modeling that we know introduced fixed variation into the data: seasonal patterns, day-of-week effects, and long-term trends. Furthermore, there may be other time-varying covariates that can help predict whatever time series we’re modeling and can be put into the fixed part of the model (a.k.a regression modeling). Ultimately, when many of these fixed components are accounted for, there’s relatively little of interest left in the residuals.\nWhat to Model?\nSo the question remains: What should I do? The short answer is to try to incorporate everything that you know about the data into the fixed/regression part of the model. Then take a look at the residuals and see if you still care.\nHere’s a quick example from my work in air pollution and mortality. The data are all-cause mortality and PM10 pollution from Detroit for the years 1987–2000. The question is whether daily mortaliy is associated with daily changes in ambient PM10 levels. We can try to answer that with a simple linear regression model:\nCall:\nlm(formula = death ~ pm10, data = ds)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.978  -5.559  -0.386   5.109  34.022 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept) 46.978826   0.112284 418.394   <2e-16\npm10         0.004885   0.001936   2.523   0.0117\n\nResidual standard error: 8.03 on 5112 degrees of freedom\nMultiple R-squared:  0.001244,  Adjusted R-squared:  0.001049 \nF-statistic: 6.368 on 1 and 5112 DF,  p-value: 0.01165\nPM10 appears to be positively associated with mortality, but when we look at the autocorrelation function of the residuals, we see\nACF1If we see a seasonal-like pattern in the auto-correlation function, then that means there’s a seasonal pattern in the residuals as well. Not good.\nBut okay, we can just model the seasonal component with an indicator of the season.\nCall:\nlm(formula = death ~ season + pm10, data = ds)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.964  -5.087  -0.242   4.907  33.884 \n\nCoefficients:\n             Estimate Std. Error t value    Pr(>|t|)\n(Intercept) 50.830458   0.215679 235.676     < 2e-16\nseasonQ2    -4.864167   0.304838 -15.957     < 2e-16\nseasonQ3    -6.764404   0.304346 -22.226     < 2e-16\nseasonQ4    -3.712292   0.302859 -12.258     < 2e-16\npm10         0.009478   0.001860   5.097 0.000000358\n\nResidual standard error: 7.649 on 5109 degrees of freedom\nMultiple R-squared:  0.09411,   Adjusted R-squared:  0.09341 \nF-statistic: 132.7 on 4 and 5109 DF,  p-value: < 2.2e-16\nNote that the coefficient for PM10, the coefficient of real interest, gets a little bigger when we add the seasonal component.\nWhen we look at the residuals now, we see\nACF2The seasonal pattern is gone, but we see that there’s positive autocorrelation at seemingly long distances (~100s of days). This is usually an indicator that there’s some sort of long-term trend in the data. Since we only care about the day-to-day changes in PM10 and mortality, it would make sense to remove any such long-term trend. I can do that by just including the date as a linear predictor.\n\nCall:\nlm(formula = death ~ season + date + pm10, data = ds)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.407  -5.073  -0.375   4.718  32.179 \n\nCoefficients:\n               Estimate  Std. Error t value    Pr(>|t|)\n(Intercept) 60.04317325  0.64858433  92.576     < 2e-16\nseasonQ2    -4.76600268  0.29841993 -15.971     < 2e-16\nseasonQ3    -6.56826695  0.29815323 -22.030     < 2e-16\nseasonQ4    -3.42007191  0.29704909 -11.513     < 2e-16\ndate        -0.00106785  0.00007108 -15.022     < 2e-16\npm10         0.00933871  0.00182009   5.131 0.000000299\n\nResidual standard error: 7.487 on 5108 degrees of freedom\nMultiple R-squared:  0.1324,    Adjusted R-squared:  0.1316 \nF-statistic:   156 on 5 and 5108 DF,  p-value: < 2.2e-16\nNow we can look at the autocorrelation function one last time.\nACF3The ACF trails to zero reasonably quickly now, but there’s still some autocorrelation at short lags up to about 15 days or so.\nNow we can engage in some traditional time series modeling. We might want to model the residuals with an auto-regressive model over order p. What should p be? We can check by looking at the partial autocorrelation function (PACF).\nPACFThe PACF seems to suggest we should fit an AR(6) or AR(7) model. Let’s use an AR(6) model and see how things look. We can use the arima() function for that.\n\nCall:\narima(x = y, order = c(6, 0, 0), xreg = m, include.mean = FALSE)\n\nCoefficients:\n         ar1     ar2     ar3     ar4     ar5     ar6  (Intercept)\n      0.0869  0.0933  0.0733  0.0454  0.0377  0.0489      59.8179\ns.e.  0.0140  0.0140  0.0141  0.0141  0.0140  0.0140       1.0300\n      seasonQ2  seasonQ3  seasonQ4     date    pm10\n       -4.4635   -6.2778   -3.2878  -0.0011  0.0096\ns.e.    0.4569    0.4624    0.4546   0.0001  0.0018\n\nsigma^2 estimated as 53.69:  log likelihood = -17441.84,  aic = 34909.69\nNote that the coefficient for PM10 hasn’t changed much from our initial models. The usual concern with not accounting for residual autocorrelation is that the variance/standard error of the coefficient of interest will be affected. In this case, there does not appear to be much of a difference between using the AR(6) to account for the residual autocorrelation and ignoring it altogether. Here’s a comparison of the standard errors for each coefficient.\n               Naive AR model\n(Intercept) 0.648584 1.030007\nseasonQ2    0.298420 0.456883\nseasonQ3    0.298153 0.462371\nseasonQ4    0.297049 0.454624\ndate        0.000071 0.000114\npm10        0.001820 0.001819\nThe standard errors for the pm10 variable are almost identical, while the standard errors for the other variables are somewhat bigger in the AR model.\nConclusion\nUltimately, I’ve found that in many biomedical and public health applications, time series modeling is very different from what I read in the textbooks. The key takeaways are:\nMake sure you know if you’re doing prediction or inference. Most often you will be doing inference, in which case your modeling strategies will be quite different.\nFocus separately on the fixed and random parts of the model. In particular, work with the fixed part of the model first, incorporating as much information as you can that will explain variability in your outcome.\nModel the random part appropriately, after incorporating as much as you can into the fixed part of the model. Classical time series models may be of use here, but also simple robust variance estimators may be sufficient.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-05-04-nssd-episode-15/",
    "title": "Not So Standard Deviations Episode 15 - Spinning Up Logistics",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-05-04",
    "categories": [],
    "contents": "\nThis is Hilary’s and my last New York-Baltimore episode! In future episodes, Hilary will be broadcasting from California. In this episode we discuss collaboration tools and workflow management for data science projects. To date, I have not found a project management tool that I can actually use (besides email), but am open to suggestions (from students).\nIf you have questions you’d like us to answer, you can send them to nssdeviations @ gmail.com or tweet us at [@NSSDeviations](https://twitter.com/nssdeviations).\nSubscribe to the podcast on iTunes.\nPlease leave us a review on iTunes!\nSupport us through our Patreon page.\nShow notes:\nHilary’s tweet on cats\nAwesome vs. cats mug\nJohn Urschel’s web page\nProfile of John Urschel by the AMS\nThe other NFL player/mathematician)\nGitHub flow\nProblems with Slack\nRelativity and GPS\nThe Information is looking for a Data Storyteller\nStitch Fix is looking for Data Scientists\nDownload the audio for this episode.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-04-28-r-intimidated/",
    "title": "High school student builds interactive R class for the intimidated with the JHU DSL",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-04-28",
    "categories": [],
    "contents": "\nAnnika Salzberg is currently a biology undergraduate at Haverford College majoring in biology. While in high-school here in Baltimore she developed and taught an R class to her classmates at the Park School. Her interest in R grew out of a project where she and her fellow students and teachers went to the Canadian sub-Arctic to collect data on permafrost depth and polar bears. When analyzing the data she learned R (with the help of a teacher) to be able to do the analyses, some of which she did on her laptop while out in the field.\nLater she worked on developing a course that she felt was friendly and approachable enough for her fellow high-schoolers to benefit. With the help of Steven Salzberg and the folks here at the JHU DSL, she built a class she calls R for the intimidated which just launched on DataCamp and you can take for free!\nThe class is a great introduction for people who are just getting started with R. It walks through R/Rstudio, package installation, data visualization, data manipulation, and a final project. We are super excited about the content that Annika created working here at Hopkins and think you should go check it out!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-04-27-georgia-tech-mooc-program/",
    "title": "An update on Georgia Tech's MOOC-based CS degree",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-04-27",
    "categories": [],
    "contents": "\nThis article in Inside Higher Ed discusses next steps for Georgia Tech’s ground-breaking low-cost CS degree based on MOOCs run by Udacity. With Sebastian Thrun stepping down as CEO at Udacity, it seems both Georgia Tech and Udacity might be moving into a new phase.\nOne fact that surprised me about the Georgia Tech program concerned the demographics:\n\nOnce the first applications for the online program arrived, Georgia Tech was surprised by how the demographics differed from the applications to the face-to-face program. The institute’s face-to-face cohorts tend to have more men than women and international students than U.S. citizens or residents. Applications to the online program, however, came overwhelmingly from students based in the U.S. (80 percent). The gender gap was even larger, with nearly nine out of 10 applications coming from men.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-04-21-writing/",
    "title": "Write papers like a modern scientist (use Overleaf or Google Docs + Paperpile)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-04-21",
    "categories": [],
    "contents": "\nEditor’s note - This is a chapter from my book How to be a modern scientist where I talk about some of the tools and techniques that scientists have available to them now that they didn’t before.\nWriting - what should I do and why?\nWrite using collaborative software to avoid version control issues.\nOn almost all modern scientific papers you will have co-authors. The traditional way of handling this was to create a single working document and pass it around. Unfortunately this system always results in a long collection of versions of a manuscript, which are often hard to distinguish and definitely hard to synthesize.\nAn alternative approach is to use formal version control systems like Git and Github. However, the overhead for using these systems can be pretty heavy for paper authoring. They also require all parties participating in the writing of the paper to be familiar with version control and the command line. Alternative paper authoring tools are now available that provide some of the advantages of version control without the major overhead involved in using base version control systems.\nThe usual result of file naming by a group (image via https://xkcd.com/1459/)Make figures the focus of your writing\nOnce you have a set of results and are ready to start writing up the paper the first thing is not to write. The first thing you should do is create a set of 1-10 publication-quality plots with 3-4 as the central focus (see Chapter 10 here for more information on how to make plots). Show these to someone you trust to make sure they “get” your story before proceeding. Your writing should then be focused around explaining the story of those plots to your audience. Many people, when reading papers, read the title, the abstract, and then usually jump to the figures. If your figures tell the whole story you will dramatically increase your audience. It also helps you to clarify what you are writing about.\nWrite clearly and simply even though it may make your papers harder to publish.\nLearn how to write papers in a very clear and simple style. Whenever you can write in plain English and make the approach you are using understandable and clear. This can (sometimes) make it harder to get your papers into journals. Referees are trained to find things to criticize and by simplifying your argument they will assume that what you have done is easy or just like what has been done before. This can be extremely frustrating during the peer review process. But the peer review process isn’t the end goal of publishing! The point of publishing is to communicate your results to your community and beyond so they can use them. Simple, clear language leads to much higher use/reading/citation/impact of your work.\nInclude links to code, data, and software in your writing\nNot everyone recognizes the value of re-analysis, scientific software, or data and code sharing. But it is the fundamental cornerstone of the modern scientific process to make all of your materials easily accessible, re-usable and checkable. Include links to data, code, and software prominently in your abstract, introduction and methods and you will dramatically increase the use and impact of your work.\nGive credit to others\nIn academics the main currency we use is credit for publication. In general assigning authorship and getting credit can be a very tricky component of the publication process. It is almost always better to err on the side of offering credit. A very useful test that my advisor John Storey taught me is if you are embarrassed to explain the authorship credit to anyone who was on the paper or not on the paper, then you probably haven’t shared enough credit.\nWriting - what tools should I use?\nWYSIWYG software: Google Docs and Paperpile.\nThis system uses Google Docs for writing and Paperpile for reference management. If you have a Google account you can easily create documents and share them with your collaborators either privately or publicly. Paperpile allows you to search for academic articles and insert references into the text using a system that will be familiar if you have previously used Endnote and Microsoft Word.\nThis system has the advantage of being a what you see is what you get system - anyone with basic text processing skills should be immediately able to contribute. Google Docs also automatically saves versions of your work so that you can flip back to older versions if someone makes a mistake. You can also easily see which part of the document was written by which person and add comments.\nGetting started\nSet up accounts with Google and with Paperpile. If you are an academic the Paperpile account will cost $2.99 a month, but there is a 30 day free trial.\nGo to Google Docs and create a new document.\nSet up the Paperpile add-on for Google Docs\nIn the upper right hand corner of the document, click on the Share link and share the document with your collaborators\nStart editing\nWhen you want to include a reference, place the cursor where you want the reference to go, then using the Paperpile menu, choose insert citation. This should give you a search box where you can search by Pubmed ID or on the web for the reference you want.\nOnce you have added some references use the Citation style option under Paperpile to pick the citation style for the journal you care about.\nThen use the Format citations option under Paperpile to create the bibliography at the end of the document\nThe nice thing about using this system is that everyone can easily directly edit the document simultaneously - which reduces conflict and difficulty of use. A disadvantage is getting the formatting just right for most journals is nearly impossible, so you will be sending in a version of your paper that is somewhat generic. For most journals this isn’t a problem, but a few journals are sticklers about this.\nTypesetting software: Overleaf or ShareLatex\nAn alternative approach is to use typesetting software like Latex. This requires a little bit more technical expertise since you need to understand the Latex typesetting language. But it allows for more precise control over what the document will look like. Using Latex on its own you will have many of the same issues with version control that you would have for a word document. Fortunately there are now “Google Docs like” solutions for editing latex code that are readily available. Two of the most popular are Overleaf and ShareLatex.\nIn either system you can create a document, share it with collaborators, and edit it in a similar manner to a Google Doc, with simultaneous editing. Under both systems you can save versions of your document easily as you move along so you can quickly return to older versions if mistakes are made.\nI have used both kinds of software, but now primarily use Overleaf because they have a killer feature. Once you have finished writing your paper you can directly submit it to some preprint servers like arXiv or biorXiv and even some journals like Peerj or f1000research.\nGetting started\nCreate an Overleaf account. There is a free version of the software. Paying $8/month will give you easy saving to Dropbox.\nClick on New Project to create a new document and select from the available templates\nOpen your document and start editing\nShare with colleagues by clicking on the Share button within the project. You can share either a read only version or a read and edit version.\nOnce you have finished writing your document you can click on the Publish button to automatically submit your paper to the available preprint servers and journals. Or you can download a pdf version of your document and submit it to any other journal.\nWriting - further tips and issues\nWhen to write your first paper\nAs soon as possible! The purpose of graduate school is (in some order):\nFreedom\nTime to discover new knowledge\nTime to dive deep\nOpportunity for leadership\nOpportunity to make a name for yourself\nR packages\nPapers\nBlogs\n\nGet a job\nThe first couple of years of graduate school are typically focused on (1) teaching you all the technical skills you need and (2) data dumping as much hard-won practical experience from more experienced people into your head as fast as possible.\nAfter that one of your main focuses should be on establishing your own program of research and reputation. Especially for Ph.D. students it can not be emphasized enough no one will care about your grades in graduate school but everyone will care what you produced. See for example, Sherri’s excellent guide on CV’s for academic positions.\nI firmly believe that R packages and blog posts can be just as important as papers, but the primary signal to most traditional academic communities still remains published peer-reviewed papers. So you should get started on writing them as soon as you can (definitely before you feel comfortable enough to try to write one).\nEven if you aren’t going to be in academics, papers are a great way to show off that you can (a) identify a useful project, (b) finish a project, and (c) write well. So the first thing you should be asking when you start a project is “what paper are we working on?”\nWhat is an academic paper?\nA scientific paper can be distilled into four parts:\nA set of methodologies\nA description of data\nA set of results\nA set of claims\nWhen you (or anyone else) writes a paper the goal is to communicate clearly items 1-3 so that they can justify the set of claims you are making. Before you can even write down 4 you have to do 1-3. So that is where you start when writing a paper.\nHow do you start a paper?\nThe first thing you do is you decide on a problem to work on. This can be a problem that your advisor thought of or it can be a problem you are interested in, or a combination of both. Ideally your first project will have the following characteristics:\nConcrete\nSolves a scientific problem\nGives you an opportunity to learn something new\nSomething you feel ownership of\nSomething you want to work on\nPoints 4 and 5 can’t be emphasized enough. Others can try to help you come up with a problem, but if you don’t feel like it is your problem it will make writing the first paper a total slog. You want to find an option where you are just insanely curious to know the answer at the end, to the point where you just have to figure it out and kind of don’t care what the answer is. That doesn’t always happen, but that makes the grind of writing papers go down a lot easier.\nOnce you have a problem the next step is to actually do the research. I’ll leave this for another guide, but the basic idea is that you want to follow the usual data analytic process:\nDefine the question\nGet/tidy the data\nExplore the data\nBuild/borrow a model\nPerform the analysis\nCheck/critique results\nWrite things up\nThe hardest part for the first paper is often knowing where to stop and start writing.\nHow do you know when to start writing?\nSometimes this is an easy question to answer. If you started with a very concrete question at the beginning then once you have done enough analysis to convince yourself that you have the answer to the question. If the answer to the question is interesting/surprising then it is time to stop and write.\nIf you started with a question that wasn’t so concrete then it gets a little trickier. The basic idea here is that you have convinced yourself you have a result that is worth reporting. Usually this takes the form of between 1 and 5 figures that show a coherent story that you could explain to someone in your field.\nIn general one thing you should be working on in graduate school is your own internal timer that tells you, “ok we have done enough, time to write this up”. I found this one of the hardest things to learn, but if you are going to stay in academics it is a critical skill. There are rarely deadlines for paper writing (unless you are submitting to CS conferences) so it will eventually be up to you when to start writing. If you don’t have a good clock, this can really slow down your ability to get things published and promoted in academics.\nOne good principle to keep in mind is “the perfect is the enemy of the very good” Another one is that a published paper in a respectable journal beats a paper you just never submit because you want to get it into the “best” journal.\nA note on “negative results”\nIf the answer to your research problem isn’t interesting/surprising but you started with a concrete question it is also time to stop and write. But things often get more tricky with this type of paper as most journals when reviewing papers filter for “interest” so sometimes a paper without a really “big” result will be harder to publish. This is ok!! Even though it may take longer to publish the paper, it is important to publish even results that aren’t surprising/novel. I would much rather that you come to an answer you are comfortable with and we go through a little pain trying to get it published than you keep pushing until you get an “interesting” result, which may or may not be justifiable.\nHow do you start writing?\nOnce you have a set of results and are ready to start writing up the paper the first thing is not to write. The first thing you should do is create a set of 1-4 publication-quality plots (see Chapter 10 here). Show these to someone you trust to make sure they “get” your story before proceeding.\nStart a project on Overleaf or Google Docs.\nWrite up a story around the four plots in the simplest language you feel you can get away with, while still reporting all of the technical details that you can.\nGo back and add references in only after you have finished the whole first draft.\nAdd in additional technical detail in the supplementary material if you need it.\nWrite up a reproducible version of your code that returns exactly the same numbers/figures in your paper with no input parameters needed.\nWhat are the sections in a paper?\nKeep in mind that most people will read the title of your paper only, a small fraction of those people will read the abstract, a small fraction of those people will read the introduction, and a small fraction of those people will read your whole paper. So make sure you get to the point quickly!\nThe sections of a paper are always some variation on the following:\nTitle: Should be very short, no colons if possible, and state the main result. Example, “A new method for sequencing data that shows how to cure cancer”. Here you want to make sure people will read the paper without overselling your results - this is a delicate balance.\nAbstract: In (ideally) 4-5 sentences explain (a) what problem you are solving, (b) why people should care, (c) how you solved the problem, (d) what are the results and (e) a link to any data/resources/software you generated.\nIntroduction: A more lengthy (1-3 pages) explanation of the problem you are solving, why people should care, and how you are solving it. Here you also review what other people have done in the area. The most critical thing is never underestimate how little people know or care about what you are working on. It is your job to explain to them why they should.\nMethods: You should state and explain your experimental procedures, how you collected results, your statistical model, and any strengths or weaknesses of your proposed approach.\nComparisons (for methods papers): Compare your proposed approach to the state of the art methods. Do this with simulations (where you know the right answer) and data you haven’t simulated (where you don’t know the right answer). If you can base your simulation on data, even better. Make sure you are simulating both the easy case (where your method should be great) and harder cases where your method might be terrible.\nYour analysis: Explain what you did, what data you collected, how you processed it and how you analysed it.\nConclusions: Summarize what you did and explain why what you did is important one more time.\nSupplementary Information: If there are a lot of technical computational, experimental or statistical details, you can include a supplement that has all of the details so folks can follow along. As far as possible, try to include the detail in the main text but explained clearly.\nThe length of the paper will depend a lot on which journal you are targeting. In general the shorter/more concise the better. But unless you are shooting for a really glossy journal you should try to include the details in the paper itself. This means most papers will be in the 4-15 page range, but with a huge variance.\nNote: Part of this chapter appeared in the Leek group guide to writing your first paper\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-04-20-data-repositories/",
    "title": "As a data analyst the best data repositories are the ones with the least features",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-04-20",
    "categories": [],
    "contents": "\nLately, for a range of projects I have been working on I have needed to obtain data from previous publications. There is a growing list of data repositories where data is made available. General purpose data sharing sites include:\nThe open science framework\nThe Harvard Dataverse\nFigshare\nDatadryad\nThere are also a host of field-specific data sharing sites.One thing that I find a little frustrating about these sites is that they add a lot of bells and whistles. For example I wanted to download a p-value data set from Dataverse (just to pick on one, but most repositories have similar issues). I go to the page and click Download on the data set I want.\nDownloading a dataverse paperThen I have to accept terms:\nThen I have to \nThen the data set is downloaded. But it comes from a button that doesn’t allow me to get the direct link. There is an R package that you can use to download dataverse data, but again not with direct links to the data sets.\nThis is a similar system to many data repositories where there is a multi-step process to downloading data rather than direct links.\nBut as a data analyst I often find that I want:\nTo be able to find a data set with some minimal search terms\nFind the data set in .csv or tab delimited format via a direct link\nHave the data set be available both as raw and processed versions\nThe processed version will either be one or many tidy data sets.\nAs a data analyst I would rather have all of the data stored as direct links and ideally as csv files. Then you don’t need to figure out a specialized package, an API, or anything else. You just use read.csv directly using the URL in R and you are off to the races. It also makes it easier to point to which data set you are using. So I find the best data repositories are the ones with the least features.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-04-11-publishing/",
    "title": "Junior scientists - you don't have to publish in open access journals to be an open scientist.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-04-11",
    "categories": [],
    "contents": "\nEditor’s note - This is a chapter from my book How to be a modern scientist where I talk about some of the tools and techniques that scientists have available to them now that they didn’t before.\nPublishing - what should I do and why?\nA modern scientific writing process goes as follows.\nYou write a paper\nYou post a preprint\nEveryone can read and comment\n\nYou submit it to a journal\nIt is peer reviewed privately\nThe paper is accepted or rejected\nIf rejected go back to step 2 and start over\nIf accepted it will be published\n\nYou can take advantage of modern writing and publishing tools to handle several steps in the process.\nPost preprints of your work\nOnce you have finished writing you paper, you want to share it with others. Historically, this involved submitting the paper to a journal, waiting for reviews, revising the paper, resubmitting, and eventually publishing it. There is now very little reason to wait that long for your paper to appear in print. Generally you can post a paper to a preprint server and have it appear in 1-2 days. This is a dramatic improvement on the weeks or months it takes for papers to appear in peer reviewed journals even under optimal conditions. There are several advantages to posting preprints.\nPreprints establish precedence for your work so it reduces your risk of being scooped.\nPreprints allow you to collect feedback on your work and improve it quickly.\nPreprints can help you to get your work published in formal academic journals.\nPreprints can get you attention and press for your work.\nPreprints give junior scientists and other researchers gratification that helps them handle the stress and pressure of their first publications.\nThe last point is underappreciated and was first pointed out to me by Yoav Gilad It takes a really long time to write a scientific paper. For a student publishing their first paper, the first feedback they get is often (a) delayed by several months and (b) negative and in the form of a referee report. This can have a major impact on the motivation of those students to keep working on projects. Preprints allow students to have an immediate product they can point to as an accomplishment, allow them to get positive feedback along with constructive or negative feedback, and can ease the pain of difficult referee reports or rejections.\nChoose the journal that maximizes your visibility\nYou should try to publish your work in the best journals for your field. There are a couple of reasons for this. First, being a scientist is both a calling and a career. To advance your career, you need visibilty among your scientific peers and among the scientists who will be judging you for grants and promotions. The best place to do this is by publishing in the top journals in your field. The important thing is to do your best to do good work and submit it to these journals, even if the results aren’t the most “sexy”. Don’t adapt your workflow to the journal, but don’t ignore the career implications either. Do this even if the journals are closed source. There are ways to make your work accessible and you will both raise your profile and disseminate your results to the broadest audience.\nShare your work on social media\nAcademic journals are good for disseminating your work to the appropriate scientific community. As a modern scientist you have other avenues and other communities - like the general public - that you would like to reach with your work. Once your paper has been published in a preprint or in a journal, be sure to share your work through appropriate social media channels. This will also help you develop facility in coming up with one line or one figure that best describes what you think you have published so you can share it on social media sites like Twitter.\nPreprints and criticism\nSee the section on scientific blogging for how to respond to criticism of your preprints online.\nPublishing - what tools should I use?\nPreprint servers\nHere are a few preprint servers you can use.\narXiv (free) - primarily takes math/physics/computer science papers. You can submit papers and they are reviewed and posted within a couple of days. It is important to note that once you submit a paper here, you can not take it down. But you can submit revisions to the paper which are tracked over time. This outlet is followed by a large number of journalists and scientists.\nbiorXiv (free) - primarily takes biology focused papers. They are pretty strict about which categories you can submit to. You can submit papers and they are reviewed and posted within a couple of days. biorxiv also allows different versions of manuscripts, but some folks have had trouble with their versioning system, which can be a bit tricky for keeping your paper coordinated with your publication. bioXiv is pretty carefully followed by the biological and computational biology communities.\nPeerj (free) - takes a wide range of different types of papers. They will again review your preprint quickly and post it online. You can also post different versions of your manuscript with this system. This system is newer and so has fewer followers, you will need to do your own publicity if you publish your paper here.\nJournal preprint policies\nThis list provides information on which journals accept papers that were first posted as preprints. However, you shouldn’t\nPublishing - further tips and issues\nOpen vs. closed access\nOnce your paper has been posted to a preprint server you need to submit it for publication. There are a number of considerations you should keep in mind when submitting papers. One of these considerations is closed versus open access. Closed access journals do not require you to pay to submit or publish your paper. But then people who want to read your paper either need to pay or have a subscription to the journal in question.\nThere has been a strong push for open access journals over the last couple of decades. There are some very good reasons justifying this type of publishing including (a) moral arguments based on using public funding for research, (2) each of access to papers, and (3) benefits in terms of people being able to use your research. In general, most modern scientists want their work to be as widely accessible as possible. So modern scientists often opt for open access publishing.\nOpen access publishing does have a couple of disadvantages. First it is often expensive, with fees for publication ranging between $1,000 and $4,000 depending on the journal. Second, while science is often a calling, it is also a career. Sometimes the best journals in your field may be closed access. In general, one of the most important components of an academic career is being able to publish in journals that are read by a lot of people in your field so your work will be recognized and impactful.\nHowever, modern systems make both closed and open access journals reasonable outlets.\nClosed access + preprints\nIf the top journals in your field are closed access and you are a junior scientist then you should try to submit your papers there. But to make sure your papers are still widely accessible you can use preprints. First, you can submit a preprint before you submit the paper to the journal. Second, you can update the preprint to keep it current with the published version of your paper. This system allows you to make sure that your paper is read widely within your field, but also allows everyone to freely read the same paper. On your website, you can then link to both the published and preprint version of your paper.\nOpen access\nIf the top journal in your field is open access you can submit directly to that journal. Even if the journal is open access it makes sense to submit the paper as a preprint during the review process. You can then keep the preprint up-to-date, but this system has the advantage that the formally published version of your paper is also available for everyone to read without constraints.\nResponding to referee comments\nAfter your paper has been reviewed at an academic journal you will receive referee reports. If the paper has not been outright rejected, it is important to respond to the referee reports in a timely and direct manner. Referee reports are often maddening. There is little incentive for people to do a good job refereeing and the most qualified reviewers will likely be those with a conflict of interest.\nThe first thing to keep in mind is that the power in the refereeing process lies entirely with the editors and referees. The first thing to do when responding to referee reports is to eliminate the impulse to argue or respond with any kind of emotion. A step-by-step process for responding to referee reports is the following.\nCreate a Google Doc. Put in all referee and editor comments in italics.\nBreak the comments up into each discrete criticism or request.\nIn bold respond to each comment. Begin each response with “On page xx we did yy to address this comment”\nPerform the analyses and experiments that you need to fill in the yy\nEdit the document to reflect all of the experiments that you have performed\nBy actively responding to each comment you will ensure you are responsive to the referees and give your paper the best chance of success. If a comment is incorrect or non-sensical, think about how you can edit the paper to remove this confusion.\nFinishing\nWhile I have advocated here for using preprints to disseminate your work, it is important to follow the process all the way through to completion. Responding to referee reports is drudgery and no one likes to do it. But in terms of career advancement preprints are almost entirely valueless until they are formally accepted for publication. It is critical to see all papers all the way through to the end of the publication cycle.\nYou aren’t done!\nPublication of your paper is only the beginning of successfully disseminating your science. Once you have published the paper, it is important to use your social media, blog, and other resources to disseminate your results to the broadest audience possible. You will also give talks, discuss the paper with colleagues, and respond to requests for data and code. The most successful papers have a long half life and the responsibilities linger long after the paper is published. But the most successful scientists continue to stay on top of requests and respond to critiques long after their papers are published.\nNote: Part of this chapter appeared in the Simply Statistics blog post: “Preprints are great, but post publication peer review isn’t ready for prime time”\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-04-08-eecom/",
    "title": "A Natural Curiosity of How Things Work, Even If You're Not Responsible For Them",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-04-08",
    "categories": [],
    "contents": "\nI just read Karl’s great post on what it means to be a data scientist. I can’t really add much to it, but reading it got me thinking about the Apollo 12 mission, the second moon landing.\nThis mission is actually famous because of its launch, where the Saturn V was struck by lightning and John Aaron (played wonderfully by Loren Dean in the movie Apollo 13), the flight controller in charge of environmental, electrical, and consumables (EECOM), had to make a decision about whether to abort the launch.\nIn this great clip from the movie Failure is Not An Option, the real John Aaron describes what makes for a good EECOM flight controller. The bottom line is that\n\nA good EECOM has a natural curiosity for how things work, even if you…are not responsible for them\n\nI think a good data scientist or statistician also has that property. They key part of that line is the “even if you are not responsible for them” part. I’ve found that a lot of being a statistician involves nosing around in places where you’re not supposed to be, seeing how data are collected, handled, managed, analyzed, and reported. Focusing on the development and implementation of methods is not enough.\nHere’s the clip, which describes the famous “SCE to AUX” call from John Aaron:\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-04-07-nssd-episode-13/",
    "title": "Not So Standard Deviations Episode 13 - It's Good that Someone is Thinking About Us",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-04-07",
    "categories": [],
    "contents": "\nIn this episode, Hilary and I talk about the difficulties of separating data analysis from its context, and Feather, a new file format for storing tabular data. Also, we respond to some listener questions and Hilary announces her new job.\nIf you have questions you’d like us to answer, you can send them to nssdeviations @ gmail.com or tweet us at @NSSDeviations.\nSubscribe to the podcast on iTunes.\nPlease leave us a review on iTunes!\nShow notes:\nNSSD Patreon page\nFeather git repository\nApache Arrow\nFlatBuffers\nRoger’s blog post on feather\nNausicaaDistribution\nNew York R Conference\nEvery Frame a Painting\nDownload the audio for this episode.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-04-05-corporations-academia/",
    "title": "Companies are Countries, Academia is Europe",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-04-05",
    "categories": [],
    "contents": "\nI’ve been thinking a lot recently about the practice of data analysis in different settings and how the environment in which you work can affect the view you have on how things should be done. I’ve been working in academia for over 12 years now. I don’t have any industry data science experience, but long ago I worked as a software engineer at two companies. Obviously, my experience is biased on the academic side.\nI’ve see an interesting divergence between what I see being written from data scientists in industry and my personal experience doing data science in academia. From the industry side, I see a lot of stuff about tooling/software and processes. This makes sense to me. Often, a company needs/wants to move quickly and doing so requires making decisions on a reasonable time scale. If decisions are made with data, then the process of collecting, organizing, analyzing, and communicating data needs to be well thought-out, systematized, rigorous, and streamlined. If everytime someone at the company had a question the data science team developed some novel custom coded-from-scratch solution, decisions would be made at a glacial pace, which is probably not good for business. In order to handle this type of situation you need solid tools and flexible workflows. You also need agreement within the company about how things are down and the processes that are followed.\nNow, I don’t mean to imply that life at a company is easy, that there isn’t politics or bureacracy to deal with. But I see companies as much like individual countries, with a clear (hierarchical) leadership structure and decision-making process (okay, maybe ideal companies). Much like in a country, it might take some time to come to a decision about a policy or problem (e.g. health insurance), with much negotiation and horse-trading, but once consensus is arrived at, often the policy can be implemented across the country at a reasonable timescale. In a company, if a certain workflow or data process can be shown to be beneficial and perhaps improve profitability down the road, then a decision could be made to implement it. Ultimately, everyone within a company is in the same boat and is interested in seeing the company succeed.\nWhen I worked at a company as a software developer, I’d sometimes run into a problem that was confusing or difficult to code. So I’d walk down to the systems engineer’s office (they guy who wrote the specification) and talk to him about it. We’d hash things out for a while and then figure out a way to go forward. Often the technical writers who wrote the documentation would come and ask me what exactly a certain module did and I’d explain it to them. Communication was usually quick and efficient because it usually occurred person-to-person and because we were all on the same team.\nAcademia is more like Europe, a somewhat loose federation of states that only communicates with each other because they have to. Each principal investigator is a country and s/he has to engage in constant (sometimes contentious) negotiations with other investigators (“countries”). As a data scientist, this can be tricky because unless I collect/generate my own data (which sometimes, I do), I have to negotiate with another investigator to obtain the data. Even if I were collaborating with that investigator from the very beginning of a study, I typically have very little direct control over the data collection process because those people don’t work for me. The result is often, the data come to me in some format over which I had little input, and I just have to deal with it. Sometimes this is a nice CSV file, but often it is not.\nIn good situations, I can talk with the investigator collecting the data and we can hash out a plan to put the data into a certain format. But even if we can agree on that, often the expertise will not be available on their end to get the data into that format, so I’ll end up having to do it myself anyway. In not-so-good situations, I can make all the arguments I want for an organized data collection and analysis workflow, but if the investigator doesn’t want to do it, can’t afford it, or doesn’t see any incentive, then it’s not going to happen. Ever.\nHowever, even in the good situations, every investigator works in their own personal way. I mean, that’s why people go into academia, because you can “be your own boss” and work on problems that interest you. Most people develop a process for running their group/lab that most suits their personality. If you’re a data scientist, you need to figure out a way to mesh with each and every investigator you collaborate with. In addition, you need to adapt yourself to whatever data process each investigator has developed for their group. So if you’re working with a genomics person, you might need to learn about BAM files. For a neuroimaging collaborator, you’ll need to know about SPM. If one person doesn’t like tidy data, then that’s too bad. You need to deal with it (or don’t work with them). As a result, it’s difficult to develop a useful “system” for data science because any system that works for one collaborator is unlikely to work for another collaborator. In effect, each collaboration often results in a custom coded-from-scratch solution.\nThis contrast between companies and academia got me thinking about the Theory of the Firm. This is an economic theory that tries to explain why firms/companies develop at all, as opposed to individuals or small groups negotiating over an open market. My understanding is that it all comes down to how well you can write and enforce a contract between two parties. For example, if I need to manufacture iPhones, I can go to a contract manufacturer, given them the designs and the precise specifications/tolerances and they can just produce millions of them. However, if I need to design the iPhone, it’s a bit harder for me to go to another company and just say “Design an awesome new phone!” That kind of contract is difficult to write down, much less enforce. That other company will be operating off of different incentives from me and will likely not produce what I want. It’s probably better if I do the design work in-house. Ultimately, once the transaction costs of having two different companies work together become too high, it makes more sense for a company to do the work in-house.\nI think collaborating on data analysis is a high transaction cost activity. Companies have an advantage in this realm to the extent that they can hire lots of data scientists to work in-house. Academics that are well-funded and have large labs can often hire a data analyst to work for them. This is good because it makes a well-trained person available at low transaction cost, but this setup is the exception. PIs with smaller labs barely have enough funding to do their experiments and so either have to analyze the data themselves (for which they may not be appropriately trained) or collaborate with someone willing to do it. Large academic centers often have research cores that provide data analysis services, but this doesn’t change the fact that data analysis that occurs “outside the company” dramatically increases the transaction costs of doing the research. Because data analysis is a highly iterative process, each time you have to go back in forth with an outside entity, the costs go up.\nI think it’s possible to see a time when data analysis can effectively be made external. I mean, Apple used to manufacture all its products, but has shifted to contract manufacturing to great success. But I think we will have to develop a much better understanding of the data analysis process before we see the transaction costs start to go down.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-03-31-feather/",
    "title": "New Feather Format for Data Frames",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-03-31",
    "categories": [],
    "contents": "\nThis past Tuesday, Hadley Wickham and Wes McKinney announced a new binary file format specifically for storing data frames.\n\nOne thing that struck us was that, while R’s data frames and Python’s pandas data frames utilize different internal memory representations, the semantics of their user data types are mostly the same. In both R and pandas, data frames contain lists of named, equal-length columns, which can be numeric, boolean, and date-and-time, categorical (factors), or string. Additionally, these columns must support missing (null) values.\n\nTheir work builds on the Apache Arrow project, which specifies a format for tabular data. There is currently a Python and R implementation for reading/writing these files but other implementations could easily be built as the file format looks pretty straightforward. The git repository is here.\nInitial thoughts:\nThe possibilities for passing data between languages is I think the main point here. The potential for passing data through a pipeline without worrying about the specifics of different languages could make for much more powerful analyses where different tools are used for whatever they tend to do best. Essentially, as long as data can be made tidy going in and coming out, there should not be a communication issue between languages.\nR users might be wondering what the big deal is–we already have a binary serialization format (XDR). But R’s serialization format is meant to cover all possible R objects. Feather’s focus on data frames allows for the removal of many of the annoying (but seldom used) complexities of R objects and optimizing a very commonly used data format.\nIn my testing, there’s a noticeable speed difference between reading a feather file and reading an (uncompressed) R workspace file (feather seems about 2x faster). I didn’t time writing files, but the difference didn’t seem as noticeable there. That said, it’s not clear to me that performance on files is the main point here.\nGiven the underlying framework and representation, there seem to be some interesting possibilities for low-memory environments.\nI’ve only had a chance to quickly look at the code but I’m excited to see what comes next.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-03-30-humans-as-training-set/",
    "title": "How to create an AI startup - convince some humans to be your training set",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-03-30",
    "categories": [],
    "contents": "\nThe latest trend in data science is artificial intelligence. It has been all over the news for tackling a bunch of interesting questions. For example:\nAlphaGo beat one of the top Go players in the world in what has been called a major advance for the field.\nMicrosoft created a chatbot Tay that ultimately went very very wrong.\nGoogle and a number of others are working on self driving cars.\nFacebook is creating an artificial intellgence based virtual assistant called M\nAlmost all of these applications are based (at some level) on using variations on neural networks and deep learning. These models are used like any other statistical or machine learning model. They involve a prediction function that is based on a set of parameters. Using a training data set, you estimate the parameters. Then when you get a new set of data, you push it through the prediction function using those estimated parameters and make your predictions.\nSo why does deep learning do so well on problems like voice recognition, image recognition, and other complicated tasks? The main reason is that these models involve hundreds of thousands or millions of parameters, that allow the model to capture even very subtle structure in large scale data sets. This type of model can be fit now because (a) we have huge training sets (think all the pictures on Facebook or all voice recordings of people using Siri) and (b) we have fast computers that allow us to estimate the parameters.\nAlmost all of the high-profile examples of “artificial intelligence” we are hearing about involve this type of process. This means that the machine is “learning” from examples of how humans behave. The algorithm itself is a way to estimate subtle structure from collections of human behavior.\nThe result is that the typical trajectory for an AI business is.\nGet a large collection of humans to perform some repetitive but possibly complicated behavior (play thousands of games of Go, or answer requests from people on Facebook messenger, or label pictures and videos, or drive cars.)\nRecord all of the actions the humans perform to create a training set.\nFeed these data into a statistical model with a huge number of parameters - made possible by having a huge training set collected from the humans in steps 1 and 2.\nApply the algorithm to perform the repetitive task and cut the humans out of the process.\nThe question is how do you get the humans to perform the task for you? One option is to collect data from humans who are using your product (think Facebook image tagging). The other, more recent phenomenon, is to farm the task out to a large number of contractors (think gig economy jobs like driving for Uber, or responding to queries on Facebook).\nThe interesting thing about the latter case is that in the short term it produces a market for gigs for humans. But in the long term, by performing those tasks, the humans are putting themselves out of a job. This played out in a relatively public way just recently with a service called GoButler that used its employees to train a model and then replaced them with that model.\nIt will be interesting to see how many areas of employment this type of approach takes over. It is also interesting to think about how much value each task you perform for a company like that is worth to the training set. It will also be interesting if there is a legal claim for the gig workers at these companies to make that their labor helped “create the value” at the companies that replace them.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-03-26-nssd-episode-12/",
    "title": "Not So Standard Deviations Episode 12 - The New Bayesian vs. Frequentist",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-03-26",
    "categories": [],
    "contents": "\nIn this episode, Hilary and I discuss the new direction for the journal Biostatistics, the recent fracas over ggplot2 and base graphics in R, and whether collecting more data is always better than collecting less (fewer?) data. Also, Hilary and Roger respond to some listener questions and more free advertising.\nIf you have questions you’d like us to answer, you can send them to nssdeviations @ gmail.com or tweet us at @NSSDeviations.\nSubscribe to the podcast on iTunes.\nPlease leave us a review on iTunes!\nShow notes:\nJeff Leek on why he doesn’t use ggplot2\nDavid Robinson on why he uses ggplot2\nNathan Yau’s post comparing ggplot2 and base graphics\nBiostatistics Medium post\nPhotoviz\nPigeonAir\nI just want to plot()\nHilary and Rush Limbaugh\nDeep learning training set\nNSSD Patreon Page\nDownload the audio for this episode.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-03-24-the-future-of-biostatistics/",
    "title": "The future of biostatistics",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-03-24",
    "categories": [],
    "contents": "\nStarting in January my colleague Dimitris Rizopoulos and I took over as co-editors of the journal Biostatistics. We are pretty fired up to try some new things with the journal and to make sure that the most important advances in statistical methodology and application have a good home.\nWe started a blog for the journal and our first post is here: The future of Biostatistics. Thanks to Karl Broman and his famiy we also have the twitter handle [@biostatistics](https://twitter.com/biostatistics). Follow us there to hear about all the new stuff we are rolling out.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-03-21-dataScientistEvo-jaffe/",
    "title": "The Evolution of a Data Scientist",
    "description": {},
    "author": [
      {
        "name": "Andrew Jaffe",
        "url": {}
      }
    ],
    "date": "2016-03-21",
    "categories": [],
    "contents": "\nEditor’s note: This post is a guest post by Andrew Jaffe\n“How do you get to Carnegie Hall? Practice, practice, practice.” (“The Wit Parade” by E.E. Kenyon on March 13, 1955)\n“..an extraordinarily consistent answer in an incredible number of fields … you need to have practiced, to have apprenticed, for 10,000 hours before you get good.” (Malcolm Gladwell, on Outliers)\nI have been a data scientist for the last seven or eight years, probably before “data science” existed as a field. I work almost exclusively in the R statistical environment which I first toyed with as a sophomore in college, which ramped up through graduate school. I write all of my code in Notepad++ and make all of my plots with base R graphics, over newer and probably easier approaches, like R Studio, ggplot2, and R Markdown. Every so often, someone will email asking for code used in papers for analysis or plots, and I dig through old folders to track it down. Every time this happens, I come to two realizations: 1) I used to write fairly inefficient and not-so-great code as an early PhD student, and 2) I write a lot of R code.\nI think there are some pretty good ways of measuring success and growth as a data scientist – you can count software packages and their user-bases, projects and papers, citations, grants, and promotions. But I wanted to calculate one more metric to add to the list – how much R code have I written in the last 8 years? I have been using the Joint High Performance Computing Exchange (JHPCE) at Johns Hopkins University since I started graduate school, so all of my R code was pretty much all in one place. I therefore decided to spend my Friday night drinking some Guinness and chronicling my journey using R and evolution as a data scientist.\nI found all of the .R files across my four main directories on the computing cluster (after copying over my local scripts), and then removed files that came with packages, that belonged to other users, and that resulted from poorly designed simulation and permutation analyses (perm1.R,…,perm100.R) before I learned how to use array jobs, and then extracted the creation date, last modified date, file size, and line count for each R script. Based on this analysis, I have written 3257 R scripts across 13.4 megabytes and 432,753 lines of code (including whitespace and comments) since February 22, 2009.\nI found that my R coding output has generally increased over time when tabulated by month (number of scripts: p=6.3e-7, size of files: p=3.5x10-9, and number of lines: p=5.0e-9). These metrics of coding – number, size, and lines - also suggest that, on average, I wrote the most code during my PhD (p-value range: 1.7e-4-1.8e-7). Interestingly, the changes in output over time surprisingly consistent across the three phases of my academic career: PhD, postdoc, and faculty (see Figure 1) – you can see the initial dropoff in production during the first one or two months as I transitioned to a postdoc at the Lieber Institute for Brain Development after finishing my PhD. My output rate has dropped slightly as a faculty member as I started working with doctoral students who took over the analyses of some projects (month-by-output interaction p-value: 5.3e-4, 0.002, and 0.03, respectively, for number, size, and lines). The mean coding output – on average, how much code it takes for a single analysis – were also increased over time and slightly decreased at LIBD, although to lesser extents (all p-values were between 0.01-0.05). I was actually surprised that coding output increased – rather than decreased – over time, as any gains in coding efficiency were probably canceled out my often times more modular analyses at LIBD.\nFigure 1: Coding output over time. Vertical bars separate my PhD, postdoc, and faculty jobsI also looked at coding output by hour of the day to better characterize my working habits – the output per hour is shown stratified by the two eras each about ~3 years (Figure 2). As expected, I never really work much in the morning – very little work get done before 8AM – and little has changed since a second year PhD student. As a faculty member, I have the highest output between 9AM-3PM. The trough between 4PM and 7PM likely corresponds to walking the dog we got three years ago, working out, and cooking (and eating) dinner. The output then increases steadily from 8PM-12AM, where I can work largely uninterrupted from meetings and people dropping by my office, with occasional days (or nights) working until 1AM.\nFigure 2: Coding output by hour of day. X-axis starts at 5AM to divide the day into a more temporal order.Lastly, I examined R coding output by day of the week. As expected, the lowest output occurred over the weekend, especially on Saturdays. Interestingly, I tended to increase output later in the work week as a faculty member, and also work a little more on Sundays and Mondays, compared to a PhD student.\nFigure 3: Coding output by day of week.Looking at the code itself, of the 432,753 lines, 84,343 were newlines (19.5%), 66,900 were lines that were exclusively comments (15.5%), and an additional 6,994 lines contained comments following R code (1.6%). Some of my most used syntax and symbols, as line counts containing at least one symbol, were pretty much as expected (dropping commas and requiring whitespace between characters):\nCode | Count | Code | Count |\n= | 175604| == | 5542 |\n# | 48763 | < | 5039 |\n<- | 16492 | for(i | 5012 |\n{ | 11879 | & | 4803 |\n} | 11612 | the | 4734 |\nin | 10587 | function(x) | 4591 |\n## | 8508 | ### | 4105 |\n~ | 6948 | - | 4034 |\n> | 5621 | %in% | 3896 |\nMy code is available on GitHub: https://github.com/andrewejaffe/how-many-lines (after removing file paths and names, as many of the projects are currently unpublished and many files are placed in folders named by collaborator), so feel free to give it a try and see how much R code you’ve written over your career. While there are probably a lot more things to play around with and explore, this was about all the time I could commit to this, given other responsibilities (I’m not on sabbatical like Jeff Leek…). All in all, this was a pretty fun experience and largely reflected, with data, how my R skills and experience have progressed over the years.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:23:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-03-14-nssd-episode-11/",
    "title": "Not So Standard Deviations Episode 11 - Start and Stop",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-03-14",
    "categories": [],
    "contents": "\nWe’ve started a Patreon page! Now you can support the podcast directly by going to our page and making a pledge. This will help Hilary and me build the podcast, add new features, and get some better equipment.\nEpisode 11 is an all craft episode of Not So Standard Deviations, where Hilary and Roger discuss starting and ending a data analysis. What do you do at the very beginning of an analysis? Hilary and Roger talk about some of the things that seem to come up all the time. Also up for discussion is the American Statistical Association’s statement on p values, famous statisticians on Twitter, and evil data scientists on TV. Plus two new things for free advertising.\nSubscribe to the podcast on iTunes.\nPlease leave us a review on iTunes!\nShow notes:\nNSSD Patreon Page\nJan de Leeuw\nDouglas Bates\nSports Night\nASA’s statement on p values\nBasic and Applied Psychology Editorial banning p values\nJ. Kenji Alt’s Vegan Experience\nfieldworkfail\nDownload the audio for this episode.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-03-02-nssd-episode-10/",
    "title": "Not So Standard Deviations Episode 10 - It's All Counterexamples",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-03-02",
    "categories": [],
    "contents": "\nIn the latest episode of Not So Standard Deviations Hilary and I talk about the motivation behind the explainr package and the general usefulness of automated reporting and interpretation of statistical tests. Also, Roger struggles to come up with a quick and easy way to explain why statistics is useful when it sometimes doesn’t produce any different results.\nSubscribe to the podcast on iTunes.\nPlease leave us a review on iTunes!\nShow notes:\nThe explainr package\nGoogle’s CausalImpact package\nSoftware is Eating the World\nMany Rules of Statistics are Wrong\nDownload the audio for this episode.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-02-26-preprints-and-pppr/",
    "title": "Preprints are great, but post publication peer review isn't ready for prime time",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-02-26",
    "categories": [],
    "contents": "\nThe current publication system works something like this:\nCoupled review and publication\nYou write a paper\nYou submit it to a journal\nIt is peer reviewed privately\nThe paper is accepted or rejected\nIf rejected go back to step 2 and start over\nIf accepted it will be published\n\nIf published then people can read it\nThis system has several major disadvantages that bother scientists. It means all research appears on a lag (whatever the time in peer review is). It can be a major lag time if the paper is sent to “top tier journals” and rejected then filters down to “lower tier” journals before ultimate publication. Another disadvantage is that there are two options for most people to publish their papers: (a) in closed access journals where it doesn’t cost anything to publish but then the articles are beyind paywalls and (b) in open access journals where anyone can read them but it costs money to publish. Especially for junior scientists or folks without resources, this creates a difficult choice because they might not be able to afford open access fees.\nFor a number of years some fields like physics (with the arxiv) and economics (with NBER) have solved this problem by decoupling peer review and publication. In these fields the system works like this:\nDecoupled review and publication\nYou write a paper\nYou post a preprint\nEveryone can read and comment\n\nYou submit it to a journal\nIt is peer reviewed privately\nThe paper is accepted or rejected\nIf rejected go back to step 2 and start over\nIf accepted it will be published\n\nLately there has been a growing interest in this same system in molecular and computational biology. I think this is a really good thing, because it makes it easier to publish papers more quickly and doesn’t cost researchers to publish. That is why the papers my group publishes all show up on biorxiv or arxiv first.\nWhile I think this decoupling is great, there seems to be a push for this decoupling and at the same time a move to post publication peer review. I used to argue pretty strongly for post-publication peer review but Rafa set me straight and pointed out that at least with peer review every paper that gets submitted gets evaluated by someone even if the paper is ultimately rejected.\nOne of the risks of post publication peer review is that there is no incentive to peer review in the current system. In a paper a few years ago I actually showed that under an economic model for closed peer review the Nash equilibrium is for no one to peer review at all. We showed in that same paper that under open peer review there is an increase in the amount of time spent reviewing, but the effect was relatively small. Moreover the dangers of open peer review are clear (junior people reviewing senior people and being punished for it) while the benefits (potentially being recognized for insightful reviews) are much hazier. Even the most vocal proponents of post publication peer review don’t do it that often when given the chance.\nThe reason is that everyone in academics already have a lot of things they are asked to do. Many review papers either out of a sense of obligation or because they want to be in the good graces of a particular journal. Without this system in place there is a strong chance that peer review rates will drop and only a few papers will get reviewed. This will ultimately decrease the accuracy of science. In our (admittedly contrived/simplified) experiment on peer review accuracy went from 39% to 78% after solutions were reviewed. You might argue that only “important” papers should be peer reviewed but then you are back in the camp of glamour. Say waht you want about glamour journals. They are for sure biased by the names of the people submitting the papers there. But it is possible for someone to get a paper in no matter who they are. If we go to a system where there is no curation through a journal-like mechanism then popularity/twitter followers/etc. will drive readers. I’m not sure that is better than where we are now.\nSo while I think pre-prints are a great idea I’m still waiting to see a system that beats pre-publication review for maintaining scientific quality (even though it may just be an impossible problem)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-02-23-spreadsheets-the-original-analytics-dashboard/",
    "title": "Spreadsheets: The Original Analytics Dashboard",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-02-23",
    "categories": [],
    "contents": "\nSoon after my discussion with Hilary Parker and Jenny Bryan about spreadsheets on Not So Standard Deviations, Brooke Anderson forwarded me this article written by Steven Levy about the original granddaddy of spreadsheets, VisiCalc. Actually, the real article was written back in 1984 as so-called microcomputers were just getting their start. VisiCalc was originally written for the Apple II computer and notable competitors at the time included Lotus 1-2-3 and Microsoft Multiplan, all since defunct.\nIt’s interesting to see Levy’s perspective on spreadsheets back then and to compare it to the current thinking about data, data science, and reproducibility in science. The problem back then was “ledger sheets” (what we might now call a spreadsheet), which contained numbers and calculations related to businesses, were tedious to make and keep up to date.\n\nMaking spreadsheets, however necessary, was a dull chore best left to accountants, junior analysts, or secretaries. As for sophisticated “modeling” tasks – which, among other things, enable executives to project costs for their companies – these tasks could be done only on big mainframe computers by the data-processing people who worked for the companies Harvard MBAs managed.\n\nYou can see one issue here: Spreadsheets/Ledgers were a “dull chore”, and best left to junior people. However, the “real” computation was done by the people the “data processing” center on big mainframes. So what exactly does that leave for the business executive to do?\nNote that the way of doing things back then was effectively reproducible, because the presentation (ledger sheets printed on paper) and the computation (data processing on mainframes) was separated.\nThe impact of the microcomputer-based spreadsheet program appears profound.\n\n\nAlready, the spreadsheet has redefined the nature of some jobs; to be an accountant in the age of spreadsheet program is — well, almost sexy. And the spreadsheet has begun to be a forceful agent of decentralization, breaking down hierarchies in large companies and diminishing the power of data processing.\n\n\nThere has been much talk in recent years about an “entrepreneurial renaissance” and a new breed of risk-taker who creates businesses where none previously existed. Entrepreneurs and their venture-capitalist backers are emerging as new culture heroes, settlers of another American frontier. Less well known is that most of these new entrepreneurs depend on their economic spreadsheets as much as movie cowboys depend on their horses.\n\n\n\n If you replace “accountant” with “statistician” and “spreadsheet” with “big data” and you are magically teleported into 2016.\n\n\nThe way I see it, in the early 80’s, spreadsheets satisfied the never-ending desire that people have to interact with data. Now, with things like tablets and touch-screen phones, you can literally “touch” your data. But it took microcomputers to get to a certain point before interactive data analysis could really be done in a way that we recognize today. Spreadsheets tightened the loop between question and answer by cutting out the Data Processing department and replacing it with an Apple II (or an IBM PC, if you must) right on your desk.\n\n\nOf course, the combining of presentation with computation comes at a cost of reproducibility and perhaps quality control. Seeing the description of how spreadsheets were originally used, it seems totally natural to me. It is not unlike today’s analytic dashboards that give you a window into your business and allow you to “model” various scenarios by tweaking a few numbers of formulas. Over time, people took spreadsheets to all sorts of extremes, using them for purposes for which they were not originally designed, and problems naturally arose.\n\n\nSo now, we are trying to separate out the computation and presentation bits a little. Tools like knitr and R and shiny allow us to do this and to bring them together with a proper toolchain. The loss in interactivity is only slight because of the power of the toolchain and the speed of computers nowadays. Essentially, we’ve brought back the Data Processing department, but have staffed it with robots and high speed multi-core computers.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-02-17-non-tidy-data/",
    "title": "Non-tidy data",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-02-17",
    "categories": [],
    "contents": "\nDuring the discussion that followed the ggplot2 posts from David and I last week we started talking about tidy data and the man himself noted that matrices are often useful instead of “tidy data” and I mentioned there might be other data that are usefully “non tidy”. Here I will be using tidy/non-tidy according to Hadley’s definition. So tidy data have:\nOne variable per column\nOne observation per row\nEach type of observational unit forms a table\nI push this approach in my guide to data sharing and in a lot of my personal work. But note that non-tidy data can definitely be already processed, cleaned, organized and ready to use.\n\n\n@hadleywickham @drob @mark_scheuerell I’m saying that not all data are usefully tidy (and not just matrices) so I care more abt flexibility\n\n\n— Jeff Leek (@jtleek) February 12, 2016\n\n\nThis led to a very specific blog request:\n\n\n@jtleek @drob I want a blog post on non-tidy data!\n\n\n— Hadley Wickham (@hadleywickham) February 12, 2016\n\n\nSo I thought I’d talk about a couple of reasons why data are usefully non-tidy. The basic reason is that I usually take a problem first, not solution backward approach to my scientific research. In other words, the goal is to solve a particular problem and the format I chose is the one that makes it most direct/easy to solve that problem, rather than one that is theoretically optimal.   To illustrate these points I’ll use an example from my area.\nExample data\nOften you want data in a matrix format. One good example is gene expression data or data from another high-dimensional experiment. David talks about one such example in his post here. He makes the (valid) point that for students who aren’t going to do genomics professionally, it may be more useful to learn an abstract tool such as tidy data/dplyr. But for those working in genomics, this can make you do unnecessary work in the name of theory/abstraction.\nHe analyzes the data in that post by first tidying the data.\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(readr)\nlibrary(broom)\n \noriginal_data %\n  separate(NAME, c(\"name\", \"BP\", \"MF\", \"systematic_name\", \"number\"), sep = \"\\\\|\\\\|\") %>%\n  mutate_each(funs(trimws), name:systematic_name) %>%\n  select(-number, -GID, -YORF, -GWEIGHT) %>%\n  gather(sample, expression, G0.05:U0.3) %>%\n  separate(sample, c(\"nutrient\", \"rate\"), sep = 1, convert = TRUE)\n\n\nIt isn’t 100% tidy as data of different types are in the same data frame (gene expression and metadata/phenotype data belong in different tables). But its close enough for our purposes. Now suppose that you wanted to fit a model and test for association between the “rate” variable and gene expression for each gene. You can do this with David’s tidy data set, dplyr, and the broom package like so:\n\n\nrate_coeffs = cleaned_data %>% group_by(name) %>%\n     do(fit = lm(expression ~ rate + nutrient, data = .)) %>%\n     tidy(fit) %>% \n     dplyr::filter(term==\"rate\")\n\n\nOn my computer we get something like:\n\n\nsystem.time( cleaned_data %>% group_by(name) %>%\n+               do(fit = lm(expression ~ rate + nutrient, data = .)) %>%\n+                tidy(fit) %>% \n+                dplyr::filter(term==\"rate\"))\n|==========================================================|100% ~0 s remaining \nuser  system elapsed \n 12.431   0.258  12.364\n\n\nLet’s now try that analysis a little bit differently. As a first step, lets store the data in two separate tables. A table of “phenotype information” and a matrix of “expression levels”. This is the more common format used for these type of data. Here is the code to do that:\n\n\nexpr = original_data %>% \n  select(grep(\"[0:9]\",names(original_data)))\n \nrownames(expr) = original_data %>%\n  separate(NAME, c(\"name\", \"BP\", \"MF\", \"systematic_name\", \"number\"), sep = \"\\\\|\\\\|\") %>%\n  select(systematic_name) %>% mutate_each(funs(trimws),systematic_name) %>% as.matrix()\n \nvals = data.frame(vals=names(expr))\npdata = separate(vals,vals,c(\"nutrient\", \"rate\"), sep = 1, convert = TRUE)\n \nexpr = as.matrix(expr)\n\n\nIf we leave the data in this format we can get the model fits and the p-values using some simple linear algebra\n\n\nexpr = as.matrix(expr)\n \nmod = model.matrix(~ rate +  as.factor(nutrient),data=pdata)\nrate_betas = expr %*% mod %*% solve(t(mod) %*% mod)\n\n\nThis gives the same answer after re-ordering\n\n\nall(abs(rate_betas[,2]- rate_coeffs$estimate[ind]) < 1e-5,na.rm=T)\n[1] TRUE\n\n\nBut this approach is much faster.\n\n\n system.time(expr %*% mod %*% solve(t(mod) %*% mod))\n   user  system elapsed \n  0.015   0.000   0.015\n\n\nThis requires some knowledge of linear algebra and isn’t pretty. But it brings us to the first general point: you might not use tidy data because some computations are more efficient if the data is in a different format. \nMany examples from graphical models, to genomics, to neuroimaging, to social sciences rely on some kind of linear algebra based computations (matrix multiplication, singular value decompositions, eigen decompositions, etc.) which are all optimized to work on matrices, not tidy data frames. There are ways to improve performance with tidy data for sure, but they would require an equal amount of custom code to take advantage of say C, or vectorization properties in R.\nOk now the linear regressions here are all treated independently, but it is very well known that you get much better performance in terms of the false positive/true positive tradeoff if you use an empirical Bayes approach for this calculation where you pool variances.\nIf the data are in this matrix format you can do it with R like so:\n\n\nlibrary(limma)\nfit_limma = lmFit(expr,mod)\nebayes_limma = eBayes(fit_limma)\ntopTable(ebayes_limma)\n\n\nThis approach is again very fast, optimized for the calculations being performed and performs much better than the one-by-one regression approach. But it requires the data in matrix or expression set format. Which brings us to the second general point: ****you might not use tidy data because many functions require a different, also very clean and useful data format, and you don’t want to have to constantly be switching back and forth. ****Again, this requires you to be more specific to your application, but the potential payoffs can be really big as in the case of limma.\nI’m showing an example here with expression sets and matrices, but in NLP the data are often input in the form of lists, in graphical analyses as matrices, in genomic analyses as GRanges lists, etc. etc. etc. One option would be to rewrite all infrastructure in your area of interest to accept tidy data formats but that would be going against conventions of a community and would ultimately cost you a lot of work when most of that work has already been done for you.\nThe final point, which I won’t discuss here is that data are often usefully represented in a non-tidy way. Examples include the aforementioned GRanges list which consists of (potentially) ragged arrays of intervals and quantitative measurements about them. You could force these data to be tidy by the definition above, but again most of the infrastructure is built around a different format that is much more intuitive for that type of data. Similarly data from other applications may be more suited to application specific formats.\nIn summary, tidy data is a useful conceptual idea and is often the right way to go for general, small data sets, but may not be appropriate for all problems. Here are some examples of data formats (biased toward my area, but there are others) that have been widely adopted, have a ton of useful software, but don’t meet the tidy data definition above. I will define these as “processed data” as opposed to “tidy data”.\nExpression sets for expression data\nSummarized experiments for a variety of genomic experiments\nGranges Lists for genomic intervals\nCorpus objects for corpora of texts.\nigraph objects for graphs\nI’m sure there are a ton more I’m missing and would be happy to get some suggestions on Twitter too.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-02-16-when-it-comes-to-science-its-the-economy-stupid/",
    "title": "When it comes to science - its the economy stupid.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-02-16",
    "categories": [],
    "contents": "\nI read a lot of articles about what is going wrong with science:\nThe reproducibility/replicability crisis\nLack of jobs for PhDs\nThe pressure on the families (or potential families) of scientists\nHype around specific papers and a more general abundance of BS\nConsortia and their potential evils\nPeer review not working well\nResearch parasites\nNot enough room for applications/public good\nPress releases that do evil\nScientists don’t release enough data\nThese articles always point to the “incentives” in science and how they don’t align with how we’d like scientists to work. These discussions often frustrate me because they almost always boil down to asking scientists (especially and often junior scientists) to make some kind of change for public good without any guarantee that they are going to be ok. I’ve seen an acceleration/accumulation of people who are focusing on these issues, I think largely  because it is now possible to make a very nice career by pointing out how other people are doing science wrong.\nThe issue I have is that the people who propose unilateral moves seem to care less that science is both (a) a calling and (b) a career for most people. I do science because I love it. I do science because I want to discover new things about the world. It is a direct extension of the wonder and excitement I had about the world when I was a little kid. But science is also a career for me. It matters if I get my next grant, if I get my next paper. Why? Because I want to be able to support myself and my family.\nThe issue with incentives is that talking about them costs nothing, but actually changing them is expensive. Right now our system, broadly defined, rewards (a) productivity - lots of papers, (b) cleverness - coming up with an idea first, and (c) measures of prestige - journal titles, job titles, etc. This is because there are tons of people going for a relatively small amount of grant money. More importantly, that money is decided on by processes that are both peer reviewed and political.\nSuppose that you wanted to change those incentives to something else. Here is a small list of things I would like:\nPeople can have stable careers and live in a variety of places without massive two body problems\nScientists shouldn’t have to move every couple of years 2-3 times right at the beginning of their career\nWe should distribute our money among the largest number of scientists possible \nIncentivizing long term thinking\nIncentivizing objective peer review\nIncentivizing openness and sharing\n\nThe key problem isn’t publishing, or code, or reproducibility, or even data analysis.\n\n\n\n\n\nThe key problem is that the fundamental model by which we fund science is completely broken. \n\n\n\n\n\nThe model now is that you have to come up with an idea every couple of years then “sell” it to funders, your peers, etc. This is the source of the following problems:\n\n\n\n\nAn incentive to publish only positive results so your ideas look good\nAn incentive to be closed so people don’t discover flaws in your analysis\n An incentive to publish in specific “big name” journals that skews the results (again mostly in the positive direction)\n Pressure to publish quickly which leads to cutting corners\nPressure to stay in a single area and make incremental changes so you know things will work.\n\nIf we really want to have any measurable impact on science we need to solve the funding model. The solution is actually pretty simple. We need to give out 20+ year grants to people who meet minimum qualifications. These grants would cover their own salary plus one or two people and the minimum necessary equipment.\n\n\n\n\n\nThe criteria for getting or renewing these grants should not be things  like Nature papers or number of citations. It has to be designed to incentivize the things that we want to (mine are listed above). So if I was going to define the criteria for meeting the standards people would have to be:\n\n\n\n\nWorking on a scientific problem and trained as a scientist\nPublishing all results immediately online as preprints/free code\nResponding to queries about their data/code\nAgreeing to peer review a number of papers per year\nMore importantly these grants should be given out for a very long term (20+ years) and not be tied to a specific institution. This would allow people to have flexible careers and to target bigger picture problems. We saw the benefits of people working on problems they weren’t originally funded to work on with research on the Zika virus.\nThese grants need to be awarded using a rigorous peer review system just like the NIH, HHMI, and other organizations use to ensure we are identifying scientists with potential early in their careers and letting them flourish. But they’d be given out in a different matter. I’m very confident in a peer review to detect the difference between psuedo-science and real science, or complete hype and realistic improvement. But I’m much less confident in the ability of peer review to accurately distinguish “important” from “not important” research. So I think we should consider seriously the lottery for these grants.\nEach year all eligible scientists who meet some minimum entry requirements submit proposals for what they’d like to do scientifically. Each year those proposals are reviewed to make sure they meet the very minimum bar (are they scientific? do they have relevant training at all?). Among all the (very large) class of people who pass that bar we hold a lottery. We take the number of research dollars and divide it up to give the maximum number of these grants possible.  These grants might be pretty small - just enough to fund the person’s salary and maybe one or two students/postdocs. To make this works for labs that required equipment there would have to be cooperative arrangements between multiple independent indviduals to fund/sustain equipment they needed. Renewal of these grants would happen as long as you were posting your code/data online, you were meeting peer review requirements, and responding to inquires about your work.\nOne thing we’d do to fund this model is eliminate/reduce large-scale projects and super well funded labs. Instead of having 30 postdocs in a well funded lab, you’d have some fraction of those people funded as independent investigators right from the get-go. If we wanted to run a massive large scale program that would be out of a very specific pot of money that would have to be saved up and spent, completely outside of the pot of money for investigator-initiated grants. That would reduce the hierarchy in the system, reduce pressure that leads to bad incentive, and give us the best chance to fund creative, long term thinking science.\nRegardless of whether you like my proposal or not, I hope that people will start focusing on how to change the incentives, even when that means doing something big or potentially costly.\n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-02-12-not-so-standard-deviations-episode-9-spreadsheet-drama/",
    "title": "Not So Standard Deviations Episode 9 - Spreadsheet Drama",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-02-12",
    "categories": [],
    "contents": "\nFor this episode, special guest Jenny Bryan (@jennybryan) joins us from the University of British Columbia! Jenny, Hilary, and I talk about spreadsheets and why some people love them and some people despise them. We also discuss blogging as part of scientific discourse.\nSubscribe to the podcast on iTunes.\nShow notes:\nJenny’s Stat 545\nCoding is not the new literacy\nGoldman Sachs spreadsheet error\nJingmai O’Connor episode\nDe-weaponizing reproducibility\nVintage Space\nTabby Cats\nDownload the audio for this episode.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-02-11-why-i-dont-use-ggplot2/",
    "title": "Why I don't use ggplot2",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-02-11",
    "categories": [],
    "contents": "\nSome of my colleagues think of me as super data-sciencey compared to other academic statisticians. But one place I lose tons of street cred in the data science community is when I talk about ggplot2. For the 3 data type people on the planet who still don’t know what that is, ggplot2 is an R package/phenomenon for data visualization. It was created by Hadley Wickham, who is (in my opinion) perhaps the most important statistician/data scientist on the planet. It is one of the best maintained, most important, and really well done R packages. Hadley also supports R software like few other people on the planet.\nBut I don’t use ggplot2 and I get nervous when other people do.\nI get no end of grief for this from Hilary and Roger and especially from drob, among many others. So I thought I would explain why and defend myself from the internet hordes. To understand why I don’t use it, you have to understand the three cases where I use data visualization.\nWhen creating exploratory graphics - graphs that are fast, not to be shown to anyone else and help me to explore a data set\nWhen creating expository graphs - graphs that i want to put into a publication that have to be very carefully made.\nWhen grading student data analyses.\nLet’s consider each case.\nExploratory graphs\nExploratory graphs don’t have to be pretty. I’m going to be the only one who looks at 99% of them. But I have to be able to make them quickly and I have to be able to make a broad range of plots with minimal code. There are a large number of types of graphs, including things like heatmaps, that don’t neatly fit into ggplot2 code and therefore make it challenging to make those graphs. The flexibility of base R comes at a price, but it means you can make all sorts of things you need to without struggling against the system. Which is a huge advantage for data analysts. There are some graphs (like this one) that are pretty straightforward in base, but require quite a bit of work in ggplot2. In many cases qplot can be used sort of interchangably with plot, but then you really don’t get any of the advantages of the ggplot2 framework.\nExpository graphs\nWhen making graphs that are production ready or fit for publication, you can do this with any system. You can do it with ggplot2, with lattice, with base R graphics. But regardless of which system you use it will require about an equal amount of code to make a graph ready for publication. One perfect example of this is the comparison of different plotting systems for creating Tufte-like graphs. To create this minimal barchart:\n\n \nThe code they use in base graphics is this (super blurry sorry, you can also go to the website for a better view).\n\nin ggplot2 the code is:\n\n \nBoth require a significant amount of coding. The ggplot2 plot also takes advantage of the ggthemes package here. Which means, without that package for some specific plot, it would require more coding.\nThe bottom line is for production graphics, any system requires work. So why do I still use base R like an old person? Because I learned all the stupid little tricks for that system, it was a huge pain, and it would be a huge pain to learn it again for ggplot2, to make very similar types of plots. This is one where neither system is particularly better, but the time-optimal solution is to stick with whichever system you learned first.\nGrading student work\nPeople I seriously respect suggest teaching ggplot2 before base graphics as a way to get people up and going quickly making pretty visualizations. This is a good solution to the little data scientist’s predicament. The tricky thing is that the defaults in ggplot2 are just pretty enough that they might trick you into thinking the graph is production ready using defaults. Say for example you make a plot of the latitude and longitude of quakes data in R, colored by the number of stations reporting. This is one case where ggplot2 crushes base R for simplicity because of the automated generation of a color scale. You can make this plot with just the line:\nggplot() + geom_point(data=quakes,aes(x=lat,y=long,colour=stations))\nAnd get this out:\n\nThat is a pretty amazing plot in one line of code! What often happens with students in a first serious data analysis class is they think that plot is done. But it isn’t even close. Here are a few things you would need to do to make this plot production ready: (1) make the axes bigger, (2) make the labels bigger, (3) make the labels be full names (latitude and longitude, ideally with units when variables need them), (4) make the legend title be number of stations reporting. Those are the bare minimum. But a very common move by a person who knows a little R/data analysis would be to leave that graph as it is and submit it directly. I know this from lots of experience.\nThe one nice thing about teaching base R here is that the base version for this plot is either (a) a ton of work or (b) ugly. In either case, it makes the student think very hard about what they need to do to make the plot better, rather than just assuming it is ok.\nWhere ggplot2 is better for sure\nggplot2 being compatible with piping, having a simple system for theming, having a good animation package, and in general being an excellent platform for developers who create Some of my colleagues think of me as super data-sciencey compared to other academic statisticians. But one place I lose tons of street cred in the data science community is when I talk about ggplot2. For the 3 data type people on the planet who still don’t know what that is, [ggplot2](https://cran.r-project.org/web/packages/ggplot2/index.html) is an R package/phenomenon for data visualization. It was created by Hadley Wickham, who is (in my opinion) perhaps the most important statistician/data scientist on the planet. It is one of the best maintained, most important, and really well done R packages. Hadley also supports R software like few other people on the planet. are all huge advantages. It is also great for getting absolute newbies up and making medium-quality graphics in a huge hurry. This is a great way to get more people engaged in data science and I’m psyched about the reach and power ggplot2 has had. Still, I probably won’t use it for my own work, even thought it disappoints my data scientist friends.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-02-10-data-handcuffs/",
    "title": "Data handcuffs",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-02-10",
    "categories": [],
    "contents": "\nA few years ago, if you asked me what the top skills I got asked about for students going into industry, I’d definitely have said things like data cleaning, data transformation, database pulls, and other non-traditional statistical tasks. But as companies have progressed from the point of storing data to actually wanting to do something with it, I would say one of the hottest skills is understanding and dealing with data from randomized trials.\nIn particular I see data scientists talking more about A/B testing, sequential stopping rules, hazard regression and other ideas  that are really common in Biostatistics, which has traditionally focused on the analysis of data from designed experiments in biology.\nI think it is great that companies are choosing to do experiments, as this still remains the gold standard for how to generate knowledge about causal effects. One interesting new development though is the extreme lengths it appears some organizations are going to to be “data-driven”.  They make all decisions based on data they have collected or experiments they have performed.\nBut data mostly tell you about small scale effects and things that happened in the past. To be able to make big discoveries/improvements requires (a) having creative ideas that are not data supported and (b) trying them in experiments to see if they work. If you get too caught up in experimenting on the same set of conditions you will inevitably asymptote to a maximum and quickly reach diminishing returns. This is where the data handcuffs come in. Data can only tell you about the conditions that existed in the past, they often can’t predict conditions in the future or ideas that may work out or might not.\nIn an interesting parallel to academic research a good strategy appears to be: (a) trying a bunch of things, including some things that have only a pretty modest chance of success, (b) doing experiments early and often when trying those things, and (c) getting very good at recognizing failure quickly and moving on to ideas that will be fruitful. The challenges are that in part (a) it is often difficult to generate really knew ideas, especially if you are already doing something that has had any level of success. There will be extreme pressure not to change what you are doing. In part (c) the challenge is that if you discard ideas too quickly you might miss a big opportunity, but if you don’t discard them quickly enough you will sink a lot of time/cost into utlimately not very fruitful projects.\nRegardless, almost all of the most interesting projects I’ve worked on in my life were not driven by data that suggested they would be successful. They were often risks where the data either wasn’t in, or the data supported not doing at all. But as a statistician I decided to straight up ignore the data and try anyway. Then again, these ideas have also been the sources of my biggest flameouts.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-02-09-leek-group-guide-to-reading-scientific-papers/",
    "title": "Leek group guide to reading scientific papers",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-02-09",
    "categories": [],
    "contents": "\nThe other day on Twitter Amelia requested a guide for reading papers\n\n\nI love @jtleek’s github guides to reviewing papers, writing R packages, giving talks, etc. Would love one on reading papers, for students.\n\n\n— Amelia McNamara (@AmeliaMN) February 5, 2016\n\n\n \nSo I came up with a guide which you can find here: Leek group guide to reading papers. I actually found this to be one that I had the hardest time with. I described how I tend to read a paper but I’m not sure that is really the optimal (or even a very good) way. I’d really appreciate pull requests if you have ideas on how to improve the guide.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-02-01-a-menagerie-of-messed-up-data-analyses-and-how-to-avoid-them/",
    "title": "A menagerie of messed up data analyses and how to avoid them",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-02-01",
    "categories": [],
    "contents": "\nUpdate: I realize this may seem like I’m picking on people. I really don’t mean to, I have for sure made all of these mistakes and many more. I can give many examples, but the one I always remember is the time Rafa saved me from “I got a big one here” when I made a huge mistake as a first year assistant professor.\nIn any introductory statistics or data analysis class they might teach you the basics, how to load a data set, how to munge it, how to do t-tests, maybe how to write a report. But there are a whole bunch of ways that a data analysis can be screwed up that often get skipped over. Here is my first crack at creating a “menagerie” of messed up data analyses and how you can avoid them. Depending on interest I could probably list a ton more, but as always I’m doing the non-comprehensive list :).\n \n \nOutcome switching\n_What it is: _Outcome switching is where you collect data looking at say, the relationship between exercise and blood pressure. Once you have the data, you realize that blood pressure isn’t really related to exercise. So you change the outcome and ask if HDL levels are related to exercise and you find a relationship. It turns out that when you do this kind of switch you have now biased your analysis because you would have just stopped if you found the original relationship.\n\nAn example: In this article they discuss how Paxil, an anti-depressant, was originally studied for several main outcomes, none of which showed an effect - but some of the secondary outcomes did. So they switched the outcome of the trial and used this result to market the drug.\n\n\nWhat you can do: Pre-specify your analysis plan, including which outcomes you want to look at. Then very clearly state when you are analyzing a primary outcome or a secondary analysis. That way people know to take the secondary analyses with a grain of salt. You can even get paid $$ to pre-specify with the OSF’s pre-registration challenge.\n\n\nGarden of forking paths\n_What it is: _In this case you may or may not have specified your outcome and stuck with it. Let’s assume you have, so you are still looking at blood pressure and exercise. But it turns out a bunch of people had apparently erroneous measures of blood pressure. So you dropped those measurements and did the analysis with the remaining values. This is a totally sensible thing to do, but if you didn’t specify in advance how you would handle bad measurements, you can make a bunch of different choices here (the forking paths). You could drop them, impute them, multiply impute them, weight them, etc. Each of these gives a different result and you can accidentally pick the one that works best even if you are being “sensible”\nAn example: This article gives several examples of the forking paths. One is where authors report that at peak fertility women are more likely to wear red or pink shirts. They made several inclusion/exclusion choices (which women to include in which comparison group) for who to include that could easily have gone a different direction or were against stated rules.\n_What you can do: _Pre-specify every part of your analysis plan, down to which observations you are going to drop, transform, etc. To be honest this is super hard to do because almost every data set is messy in a unique way. So the best thing here is to point out steps in your analysis where you made a choice that wasn’t pre-specified and you could have made differently. Or, even better, try some of the different choices and make sure your results aren’t dramatically different.\n \nP-hacking\n_What it is: _The nefarious cousin of the garden of forking paths. Basically here the person outcome switches, uses the garden of forking paths, intentionally doesn’t correct for multiple testing, or uses any of these other means to cheat and get a result that they like.\nAn example: This one gets talked about a lot and there is some evidence that it happens. But it is usually pretty hard to ascribe purely evil intentions to people and I’d rather not point the finger here. I think that often the garden of forking paths results in just as bad an outcome without people having to try.\nWhat to do: Know how to do an analysis well and don’t cheat.\nUpdate:  Some Update: I realize this may seem like I’m picking on people. I really don’t mean to, I have for sure made all of these mistakes and many more. I can give many examples, but the one I always remember is the time Rafa saved me from “I got a big one here” when I made a huge mistake as a first year assistant professor. “when honest researchers face ambiguity about what analyses to run, and convince themselves those leading to better results are the correct ones (see e.g., Gelman & Loken, 2014; John, Loewenstein, & Prelec, 2012; Simmons, Nelson, & Simonsohn, 2011; Vazire, 2015).” This coincides with the definition of “garden of forking paths”. I have been asked to point this out on Twitter. It was never my intention to accuse anyone of accusing people of fraud. That being said, I still think that the connotation that many people think of when they think “p-hacking” corresponds to my definition above, although I agree with folks that isn’t helpful - which is why I prefer we call the non-nefarious version the garden of forking paths.\n \nUncorrected multiple testing \n_What it is: _This one is related to the garden of forking paths and outcome switching. Most statistical methods for measuring the potential for error assume you are only evaluating one hypothesis at a time. But in reality you might be measuring a ton either on purpose (in a big genomics or neuroimaging study) or accidentally (because you consider a bunch of outcomes). In either case, the expected error rate changes a lot if you consider many hypotheses.\nAn example:  The most famous example is when someone did an fMRI on a dead fish and showed that there were a bunch of significant regions at the P < 0.05 level. The reason is that there is natural variation in the background of these measurements and if you consider each pixel independently ignoring that you are looking at a bunch of them, a few will have P < 0.05 just by chance.\nWhat you can do: Correct for multiple testing. When you calculate a large number of p-values make sure you know what their distribution is expected to be and you use a method like Bonferroni, Benjamini-Hochberg, or q-value to correct for multiple testing.\n \nI got a big one here\nWhat it is: One of the most painful experiences for all new data analysts. You collect data and discover a huge effect. You are super excited so you write it up and submit it to one of the best journals or convince your boss to be the farm. The problem is that huge effects are incredibly rare and are usually due to some combination of experimental artifacts and biases or mistakes in the analysis. Almost no effects you detect with statistics are huge. Even the relationship between smoking and cancer is relatively weak in observational studies and requires very careful calibration and analysis.\nAn example: In a paper authors claimed that 78% of genes were differentially expressed between Asians and Europeans. But it turns out that most of the Asian samples were measured in one sample and the Europeans in another. Update: I realize this may seem like I’m picking on people. I really don’t mean to, I have for sure made all of these mistakes and many more. I can give many examples, but the one I always remember is the time Rafa saved me from “I got a big one here” when I made a huge mistake as a first year assistant professor. a large fraction of these differences.\nWhat you can do: Be deeply suspicious of big effects in data analysis. If you find something huge and counterintuitive, especially in a well established research area, spend a lot of time trying to figure out why it could be a mistake. If you don’t, others definitely will, and you might be embarrassed.\nDouble complication\nWhat it is: When faced with a large and complicated data set, beginning analysts often feel compelled to use a big complicated method. Imagine you have collected data on thousands of genes or hundreds of thousands of voxels and you want to use this data to predict some health outcome. There is a severe temptation to use deep learning or blend random forests, boosting, and five other methods to perform the prediction. The problem is that complicated methods fail for complicated reasons, which will be extra hard to diagnose if you have a really big, complicated data set.\nAn example: There are a large number of examples where people use very small training sets and complicated methods. One example (there were many other problems with this analysis, too) is when people tried to use complicated prediction algorithms to predict which chemotherapy would work best using genomics. Ultimately this paper was retracted for may problems, but the complication of the methods plus the complication of the data made it hard to detect.\nWhat you can do: When faced with a big, messy data set, try simple things first. Use linear regression, make simple scatterplots, check to see if there are obvious flaws with the data. If you must use a really complicated method, ask yourself if there is a reason it is outperforming the simple methods because often with large data sets even simple things work.\n \n \n \n \n \nImage credits:\nOutcome switching. Icon made by Hanan from www.flaticon.com is licensed under CC BY 3.0\nForking paths. Icon made by Popcic from www.flaticon.com is licensed under CC BY 3.0\nP-hacking.Icon made by Icomoon from www.flaticon.com is licensed under CC BY 3.0\nUncorrected multiple testing.Icon made by Freepik from www.flaticon.com is licensed under CC BY 3.0\nBig one here. Icon made by Freepik from www.flaticon.com is licensed under CC BY 3.0\nDouble complication. Icon made by Freepik from www.flaticon.com is licensed under CC BY 3.0\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-01-26-exactly-how-risky-is-breathing/",
    "title": "Exactly how risky is breathing?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-01-26",
    "categories": [],
    "contents": "\nThis article by by George Johnson in the NYT describes a study by Kamen P. Simonov​​ and Daniel S. Himmelstein​ that examines the hypothesis that people living at higher altitudes experience lower rates of lung cancer than people living at lower altitudes.\n\nAll of the usual caveats apply. Studies like this, which compare whole populations, can be used only to suggest possibilities to be explored in future research. But the hypothesis is not as crazy as it may sound. Oxygen is what energizes the cells of our bodies. Like any fuel, it inevitably spews out waste — a corrosive exhaust of substances called “free radicals,” or “reactive oxygen species,” that can mutate DNA and nudge a cell closer to malignancy.\n\nI’m not so much focused on the science itself, which is perhaps intriguing, but rather on the way the article was written. First, George Johnson links to the paper itself, already a major victory. Also, I thought he did a very nice job of laying out the complexity of doing a population-level study like this one–all the potential confounders, selection bias, negative controls, etc.\nI remember particulate matter air pollution epidemiology used to have this feel. You’d try to do all these different things to make the effect go away, but for some reason, under every plausible scenario, in almost every setting, there was always some association between air pollution and health outcomes. Eventually you start to believe it….\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-01-25-on-research-parasites-and-internet-mobs-lets-try-to-solve-the-real-problem/",
    "title": "On research parasites and internet mobs - let's try to solve the real problem.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2016-01-25",
    "categories": [],
    "contents": "\nA couple of days ago one of the editors of the New England Journal of Medicine posted an editorial showing some moderate level of support for data sharing but also introducing the term “research parasite”:\n\nA second concern held by some is that a new class of research person will emerge — people who had nothing to do with the design and execution of the study but use another group’s data for their own ends, possibly stealing from the research productivity planned by the data gatherers, or even use the data to try to disprove what the original investigators had posited. There is concern among some front-line researchers that the system will be taken over by what some researchers have characterized as “research parasites.”\n\nWhile this is obviously the most inflammatory statement in the article, I think that there are several more important and overlooked misconceptions. The biggest problems are:\n“****The first concern is that someone not involved in the generation and collection of the data may not understand the choices made in defining the parameters.****” This almost certainly would be the fault of the investigators who published the data. If the authors adhere to good [A couple of days ago one of the editors of the New England Journal of Medicine posted an editorial showing some moderate level of support for data sharing but also introducing the term “research parasite”:\n\nA second concern held by some is that a new class of research person will emerge — people who had nothing to do with the design and execution of the study but use another group’s data for their own ends, possibly stealing from the research productivity planned by the data gatherers, or even use the data to try to disprove what the original investigators had posited. There is concern among some front-line researchers that the system will be taken over by what some researchers have characterized as “research parasites.”\n\nWhile this is obviously the most inflammatory statement in the article, I think that there are several more important and overlooked misconceptions. The biggest problems are:\n“****The first concern is that someone not involved in the generation and collection of the data may not understand the choices made in defining the parameters.****” This almost certainly would be the fault of the investigators who published the data. If the authors adhere to good](https://github.com/jtleek/datasharing) policies and respond to queries from people using their data promptly then this should not be a problem at all.\n“… but use another group’s data for their own ends, possibly stealing from the research productivity planned by the data gatherers, or even use the data to try to disprove what the original investigators had posited.” The idea that no one should be able to try to disprove ideas with the authors data has been covered in other blogs/on Twitter. One thing I do think is worth considering here is the concern about credit. I think that the traditional way credit has accrued to authors has been citations. But if you get a major study funded, say for 50 million dollars, run that study carefully, sit on a million conference calls, and end up with a single major paper, that could be frustrating. Which is why I think that a better policy would be to have the people who run massive studies get credit in a way that is not papers. They should get some kind of formal administrative credit. But then the data should be immediately and publicly available to anyone to publish on. That allows people who run massive studies to get credit and science to proceed normally.\n“****The new investigators arrived on the scene with their own ideas and worked symbiotically, rather than parasitically, with the investigators holding the data, moving the field forward in a way that neither group could have done on its own.”  The story that follows about a group of researchers who collaborated with the NSABP to validate their gene expression signature is very encouraging. But it isn’t the only way science should work. Researchers shouldn’t be constrained to one model or another. Sometimes collaboration is necessary, sometimes it isn’t, but in neither case should we label the researchers “symbiotic” or “parasitic”, terms that have extreme connotations.\n“How would data sharing work best? We think it should happen symbiotically, not parasitically.” I think that it should happen automatically. If you generate a data set with public funds, you should be required to immediately make it available to researchers in the community. But you should get credit for generating the data set and the hypothesis that led to the data set. The problem is that people who generate data will almost never be as fast at analyzing it as people who know how to analyze data. But both deserve credit, whether they are working together or not.\n“Start with a novel idea, one that is not an obvious extension of the reported work. Second, identify potential collaborators whose collected data may be useful in assessing the hypothesis and propose a collaboration. Third, work together to test the new hypothesis. Fourth, report the new findings with relevant coauthorship to acknowledge both the group that proposed the new idea and the investigative group that accrued the data that allowed it to be tested.” The trouble with this framework is that it preferentially accrues credit to data generators and doesn’t accurately describe the role of either party. To flip this argument around,  you could just as easily say that anyone who uses Steven Salzberg’s software for aligning or assembling short reads should make him a co-author. I think Dr. Drazen would agree that not everyone who aligned reads should add Steven as co-author, despite his contribution being critical for the completion of their work.\nAfter the piece was posted there was predictable internet rage from data parasites, a dedicated hashtag, and half a dozen angry blog posts written about the piece. These inspired a follow up piece from Drazen. I recognize why these folks were upset - the “research parasites” thing was unnecessarily inflammatory. But I also sympathize with data creators who are also subject to a tough environment - particularly when they are junior scientists.\nI think the response to the internet outrage also misses the mark and comes off as a defense of people with angry perspectives on data sharing. I would have much rather seen a more pro-active approach from a leading journal of medicine. I’d like to see something that acknowledges different contributions appropriately and doesn’t slow down science. Something like:\nWe will require all data, including data from clinical trials, to be made public immediately on publication as long as it poses minimal risk to the patients involved or the patients have been consented to broad sharing.\nWhen data are not made publicly available they are still required to be deposited with a third party such as the NIH or Figshare to be held available for request from qualified/approved researchers.\nWe will require that all people who use data give appropriate credit to the original data generators in terms of data citations.\nWe will require that all people who use software/statistical analysis tools give credit to the original tool developers in terms of software citations.\nWe will include a new designation for leaders of major data collection or software generation projects that can be included to demonstrate credit for major projects undertaken and completed.\nWhen reviewing papers written by experimentalists with no statistical/computational co-authors we will require no fewer than 2 statistical/computational referees to ensure there has not been a mistake made by inexperienced researchers.\nWhen reviewing papers written by statistical/computational authors with no experimental co-authors we will require no fewer than 2 experimental referees to ensure there has not been a mistake made by inexperienced researchers.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-01-24-not-so-standard-deviations-episode-8-snow-day/",
    "title": "Not So Standard Deviations Episode 8 - Snow Day",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-01-24",
    "categories": [],
    "contents": "\nHilary and I were snowed in over the weekend, so we recorded Episode 8 of Not So Standard Deviations. In this episode, Hilary and I talk about how to get your foot in the door with data science, the New England Journal’s view on data sharing, Google’s “Cohort Analysis”, and trying to predict a movie’s box office returns based on the movie’s script.\nSubscribe to the podcast on iTunes.\nFollow [@NSSDeviations](https://twitter.com/nssdeviations) on Twitter!\nShow notes:\nRemembrances of Peter Hall\nResearch Parasites (NEJM editorial by Dan Longo and Jeffrey Drazen)\nAmazon review/data analysis of Fifty Shades of Grey\nTime-lapse cats\nPocket\nApologies for my audio on this episode. I had a bit of a problem calibrating my microphone. I promise to figure it out for the next episode!\nDownload the audio for this episode.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-01-21-parallel-blas-in-r/",
    "title": "Parallel BLAS in R",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-01-21",
    "categories": [],
    "contents": "\nI’m working on a new chapter for my R Programming book and the topic is parallel computation. So, I was happy to see this tweet from David Robinson (@drob) yesterday:\n\n\nHow fast is this #rstats code? x <- replicate(5e3, rnorm(5e3)) x %*% t(x) For me, w/Microsoft R Open, 2.5sec. Wow. https://t.co/0SbijNxxVa\n\n\n— David Robinson (@drob) January 20, 2016\n\n\nWhat does this have to do with parallel computation? Briefly, the code generates 5,000 standard normal random variates, repeats this 5,000 times and stores them in a 5,000 x 5,000 matrix (`x’). Then it computes x x’. The second part is key, because it involves a matrix multiplication.\nMatrix multiplication in R is handled, at a very low level, by the library that implements the Basic Linear Algebra Subroutines, or BLAS. The stock R that you download from CRAN comes with what’s known as a reference implementation of BLAS. It works, it produces what everyone agrees are the right answers, but it is in no way optimized. Here’s what I get when I run this code on my Mac using Studio and the CRAN version of R for Mac OS X:\nsystem.time({ x <- replicate(5e3, rnorm(5e3)); tcrossprod(x) })\n   user  system elapsed \n 59.622   0.314  59.927 \n\nNote that the “user” time and the “elapsed” time are roughly the same. Note also that I use the tcrossprod() function instead of the otherwise equivalent expression x %*% t(x). Both crossprod() and tcrossprod() are generally faster than using the %*% operator.\nNow, when I run the same code on my built-from-source version of R (version 3.2.3), here’s what I get:\nsystem.time({ x <- replicate(5e3, rnorm(5e3)); tcrossprod(x) })\n   user  system elapsed \n 14.378   0.276   3.344 \n\nOverall, it’s faster when I don’t run the code through RStudio (14s vs. 59s). Also on this version the elapsed time is about 1/4 the user time. Why is that?\nThe build-from-source version of R is linked to Apple’s Accelerate framework, which is a large library that includes an optimized BLAS library for Intel chips. This optimized BLAS, in addition to being optimized with respect to the code itself, is designed to be multi-threaded so that it can split work off into chunks and run them in parallel on multi-core machines. Here, the tcrossprod() function was run in parallel on my machine, and so the elapsed time was about a quarter of the time that was “charged” to the CPU(s).\nDavid’s tweet indicated that when using Microsoft R Open, which is a custom built binary of R, that the (I assume?) elapsed time is 2.5 seconds. Looking at the attached link, it appears that Microsoft’s R Open is linked against Intel’s Math Kernel Library (MKL) which contains, among other things, an optimized BLAS for Intel chips. I don’t know what kind of computer David was running on, but assuming it was similarly high-powered as mine, it would suggest Intel’s MKL sees slightly better performance. But either way, both Accelerate and MKL achieve that speed up through custom-coding of the BLAS routines and multi-threading on multi-core systems.\nIf you’re going to be doing any linear algebra in R (and you will), it’s important to link to an optimized BLAS. Otherwise, you’re just wasting time unnecessarily. Besides Accelerate (Mac) and Intel MKL, theres AMD’s ACML library for AMD chips and the ATLAS library which is a general purpose tunable library. Also Goto’s BLAS is optimized but is not under active development.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:22:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-01-14-profile-of-hilary-parker/",
    "title": "Profile of Hilary Parker",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-01-14",
    "categories": [],
    "contents": "\nIf you’ve ever wanted to know more about my Not So Standard Deviations co-host (and Johns Hopkins graduate) Hilary Parker, you can go check out the great profile of her on the American Statistical Association’s This Is Statistics web site.\n\nWhat advice would you give to high school students thinking about majoring in statistics?\nIt’s such a great field! Not only is the industry booming, but more importantly, the disciplines of statistics teaches you to think analytically, which I find helpful for just about every problem I run into. It’s also a great field to be interested in as a generalist– rather than dedicating yourself to studying one subject, you are deeply learning a set of tools that you can apply to any subject that you find interesting. Just one glance at the topics covered on The Upshot or 538 can give you a sense of that. There’s politics, sports, health, history… the list goes on! It’s a field with endless possibility for growth and exploration, and as I mentioned above, the more I explore the more excited I get about it.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-01-12-not-so-standard-deviations-episode-7-statistical-royalty/",
    "title": "Not So Standard Deviations Episode 7 - Statistical Royalty",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-01-12",
    "categories": [],
    "contents": "\nThe latest episode of Not So Standard Deviations is out, and boy does Hilary have a story to tell.\nWe also talk about Theranos and the pitfalls of diagnostic testing, Spotify’s Discover Weekly playlist generation algorithm (and the need for human product managers), and of course, a little Star Wars. Also, Hilary and I start a new segment where we each give some “free advertising” to something interesting that they think other people should know about.\nShow Notes:\nGosset Icterometer\nThe dangers of entertainment medicine\nSpotify’s Discover Weekly solves human curation?\nDavid Robinson’s Variance Explained\nWhat3Words\nDownload the audio for this episode.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-01-11-jeff-roger-and-brian-caffo-are-doing-a-reddit-ama-at-3pm-est-today/",
    "title": "Jeff, Roger and Brian Caffo are doing a Reddit AMA at 3pm EST Today",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2016-01-11",
    "categories": [],
    "contents": "\nJeff Leek, Brian Caffo, and I are doing a Reddit AMA TODAY at 3pm EST. We’re happy to answer questions about…anything…including our roles as Co-Directors of the Johns Hopkins Data Science Specialization as well as the Executive Data Science Specialization.\nThis is one of the few pictures of the three of us together.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-12-21-a-non-comprehensive-list-of-awesome-things-other-people-did-in-2015/",
    "title": "A non-comprehensive list of awesome things other people did in 2015",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-12-21",
    "categories": [],
    "contents": "\nEditor’s Note: This is the third year I’m making a list of awesome things other people did this year. Just like the lists for 2013 and 2014 I am doing this off the top of my head.   I have avoided talking about stuff I worked on or that people here at Hopkins are doing because this post is supposed to be about other people’s awesome stuff. I wrote this post because a blog often feels like a place to complain, but we started Simply Stats as a place to be pumped up about the stuff people were doing with data. This year’s list is particularly “off the cuff” so I’d appreciate additions if you have ’em. I have surely missed awesome things people have done.\nI hear the Tukey conference put on by my former advisor John S. was amazing. Out of it came this really good piece by David Donoho on 50 years of Data Science.\nSherri Rose wrote really accurate and readable guides on academic CVs, academic cover letters, and how to be an effective PhD researcher.\nI am not 100% sold on the deep learning hype, but Michael Nielson wrote this awesome book on deep learning and neural networks. I like how approachable it is and how un-hypey it is. I also thought Andrej Karpathy’s blog post on whether you have a good selfie or not was fun.\nThomas Lumley continues to be must read regardless of which blog he writes for with a ton of snarky fun posts debunking the latest ridiculous health headlines on statschat and more in depth posts like this one on pre-filtering multiple tests on notstatschat.\nDavid Robinson is making a strong case for top data science blogger with his series of awesome posts on empirical Bayes.\nHadley Wickham doing Hadley Wickham things again. readr is the biggie for me this year.\nI’ve been really enjoying the solid coverage of science/statistics from the (not entirely statistics focused as the name would suggest) STAT.\nBen Goldacre and co. launched OpenTrials for aggregating all the clinical trial data in the world in an open repository.\nChristie Aschwanden’s piece on why Science Isn’t Broken  is a must read and one of the least polemic treatments of the reproducibility/replicability issue I’ve read. The p-hacking graphic is just icing on the cake.\nI’m excited about the new R Consortium and the idea of having more organizations that support folks in the R community.\nEmma Pierson’s blog and writeups in various national level news outlets continue to impress. I thought this one on changing the incentives for sexual assault surveys was particularly interesting/good.\nAmanda Cox an co. created this [Editor’s Note: This is the third year I’m making a list of awesome things other people did this year. Just like the lists for 2013 and 2014 I am doing this off the top of my head.   I have avoided talking about stuff I worked on or that people here at Hopkins are doing because this post is supposed to be about other people’s awesome stuff. I wrote this post because a blog often feels like a place to complain, but we started Simply Stats as a place to be pumped up about the stuff people were doing with data. This year’s list is particularly “off the cuff” so I’d appreciate additions if you have ’em. I have surely missed awesome things people have done.\nI hear the Tukey conference put on by my former advisor John S. was amazing. Out of it came this really good piece by David Donoho on 50 years of Data Science.\nSherri Rose wrote really accurate and readable guides on academic CVs, academic cover letters, and how to be an effective PhD researcher.\nI am not 100% sold on the deep learning hype, but Michael Nielson wrote this awesome book on deep learning and neural networks. I like how approachable it is and how un-hypey it is. I also thought Andrej Karpathy’s blog post on whether you have a good selfie or not was fun.\nThomas Lumley continues to be must read regardless of which blog he writes for with a ton of snarky fun posts debunking the latest ridiculous health headlines on statschat and more in depth posts like this one on pre-filtering multiple tests on notstatschat.\nDavid Robinson is making a strong case for top data science blogger with his series of awesome posts on empirical Bayes.\nHadley Wickham doing Hadley Wickham things again. readr is the biggie for me this year.\nI’ve been really enjoying the solid coverage of science/statistics from the (not entirely statistics focused as the name would suggest) STAT.\nBen Goldacre and co. launched OpenTrials for aggregating all the clinical trial data in the world in an open repository.\nChristie Aschwanden’s piece on why Science Isn’t Broken  is a must read and one of the least polemic treatments of the reproducibility/replicability issue I’ve read. The p-hacking graphic is just icing on the cake.\nI’m excited about the new R Consortium and the idea of having more organizations that support folks in the R community.\nEmma Pierson’s blog and writeups in various national level news outlets continue to impress. I thought this one on changing the incentives for sexual assault surveys was particularly interesting/good.\nAmanda Cox an co. created this ](http://www.nytimes.com/interactive/2015/05/28/upshot/you-draw-it-how-family-income-affects-childrens-college-chances.html) , which is an amazing way to teach people about pre-conceived biases in the way we think about relationships and correlations. I love the crowd-sourcing view on data analysis this suggests.\nAs usual Philip Guo was producing gold over on his blog. I appreciate this piece on twelve tips for data driven research.\nI am really excited about the new field of adaptive data analysis. Basically understanding how we can let people be “real data analysts” and still get reasonable estimates at the end of the day. This paper from Cynthia Dwork and co was one of the initial salvos that came out this year.\nDatacamp incorporated Python into their platform. The idea of interactive education for R/Python/Data Science is a very cool one and has tons of potential.\nI was really into the idea of Cross-Study validation that got proposed this year. With the growth of public data in a lot of areas we can really start to get a feel for generalizability.\nThe Open Science Foundation did this incredible replication of 100 different studies in psychology with attention to detail and care that deserves a ton of attention.\nFlorian’s piece “You are not working for me; I am working with you.” should be required reading for all students/postdocs/mentors in academia. This is something I still hadn’t fully figured out until I read Florian’s piece.\nI think Karl Broman’s post on why reproducibility is hard is a great introduction to the real issues in making data analyses reproducible.\nThis was the year of the f1000 post-publication review paper. I thought this one from Yoav and the ensuing fallout was fascinating.\nI love pretty much everything out of Di Cook/Heike Hoffman’s groups. This year I liked the paper on visual statistical inference in high-dimensional low sample size settings.\nThis is pretty recent, but Nathan Yau’s day in the life graphic is mesmerizing.\nThis was a year where open source data people described their pain from people being demanding/mean to them for their contributions. As the year closes I just want to give a big thank you to everyone who did awesome stuff I used this year and have completely ungraciously failed to acknowledge.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-12-18-not-so-standard-deviations-episode-6-google-is-the-new-fisher/",
    "title": "Not So Standard Deviations: Episode 6 - Google is the New Fisher",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-12-18",
    "categories": [],
    "contents": "\nEpisode 6 of Not So Standard Deviations is now posted. In this episode Hilary and I talk about the analytics of our own podcast, and analyses that seem easy but are actually hard.\nIf you haven’t already, you can subscribe to the podcast through iTunes.\nThis will be our last episode for 2015 so see you in 2016!\nNotes\nRoger’s books on Leanpub\nKPIs\nReply All, a great podcast\nUse R! 2016 conference where Don Knuth is an invited speaker!\nLiz Stuart’s directory of propensity score software\nA/B testing\niid\nR 3.2.3 release notes\npqR\nJohn Myles White’s tweet\nDownload the audio file for this episode.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-12-11-instead-of-research-on-reproducibility-just-do-reproducible-research/",
    "title": "Instead of research on reproducibility, just do reproducible research",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-12-11",
    "categories": [],
    "contents": "\nRight now reproducibility, replicability, false positive rates, biases in methods, and other problems with science are the hot topic. As I mentioned in a previous post pointing out a flaw with a scientific study is way easier to do correctly than generating a new scientific study. Some folks have noticed that right now there is a huge market for papers pointing out how science is flawed. The combination of the relative ease of pointing out flaws and the huge payout for writing these papers is helping to generate the hype around the “reproducibility crisis”.\nI gave a talk a little while ago at an NAS workshop where I stated that all the tools for reproducible research exist (the caveat being really large analyses - although that is changing as well). To make a paper completely reproducible, open, and available for post publication review you can use the following approach with no new tools/frameworks needed.\nUse Github for version control.\nUse rmarkdown or iPython notebooks for your analysis code\nWhen your paper is done post it to arxiv or biorxiv.\nPost your data to an appropriate repository like SRA or a general purpose site like figshare.\nSend any software you develop to a controlled repository like CRAN or Bioconductor.\nParticipate in the post publication discussion on Twitter and with a Blog\nThis is also true of open science, open data sharing, reproducibility, replicability, post-publication peer review and all the other issues forming the “reproducibility crisis”. There is a lot of attention and heat that has focused on the “crisis” or on folks who make a point to take a stand on reproducibility or open science or post publication review. But in the background, outside of the hype, there are a large group of people that are quietly executing solid, open, reproducible science.\nI wish that this group would get more attention so I decided to point out a few of them. Next time somebody asks me about the research on reproducibility or open science I’ll just point them here and tell them to just follow the lead of people doing it.\nKarl Broman - posts all of his talks online , generates many widely used open source packages, writes free/open tutorials on everything from knitr to making webpages, makes his papers highly reproducible.\nJessica Li - posts her data online and writes open source software for her analyses.\nMark Robinson - posts many of his papers as preprints on biorxiv, makes his analyses reproducible, writes open source software \nFlorian Markowetz - writes open source software, provides Bioconductor data for major projects, links his papers with his code nicely on his publications page.\nRaphael Gottardo - writes/maintains many open source software packages, makes his analyses reproducible and available via Github, posts preprints of his papers.\nGenevera Allen - writes](https://cran.r-project.org/web/packages/TCGA2STAT/index.html) to make data easier to access, posts preprints on biorxiv and on arxiv\nLorena Barba - teaches open source moocs, with lessons as open source iPython modules, and reproducible code for her analyses.\nAlicia Oshlack  - writes papers with completely reproducible analyses, publishes lots of open source software and publishes preprints for her papers.\nBaggerly and Coombs - although they are famous for a highly public reproducible piece of research they have also quietly implemented policies like making all  reports reproducible for their consulting center.\nThis list was made completely haphazardly as all my lists are, but just to indicate there are a ton of people out there doing this. One thing that is clear too is that grad students and postdocs are adopting the approach I described at a very high rate.\nMoreover there are people that have been doing parts of this for a long time (like the physics or biostatistics communities with preprints, or how people have used Sweave for a long time) . I purposely left people off the list like Titus and Ethan who have gone all in, even posting their grants online. I did this because they are very loud advocates of open science, but I wanted to highlight quieter contributors and point out that while there is a lot of noise going on over in one corner, many people are quietly doing really good science in another.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-12-09-by-opposing-tracking-well-meaning-educators-are-hurting-disadvantaged-kids/",
    "title": "By opposing  tracking well-meaning educators are hurting disadvantaged kids",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-12-09",
    "categories": [],
    "contents": "\n\n\n<div class=\"column\">\n  <p>\n    An unfortunate fact about the US K-12 system is that the education gap between poor and rich is growing. One manifestation of this trend is that we rarely see US kids from disadvantaged backgrounds become tenure track faculty, especially in the STEM fields. In my experience, the ones that do make it, when asked how they overcame the suboptimal math education their school district provided, often respond \"I was <a href=\"https://en.wikipedia.org/wiki/Tracking_(education)\">tracked<\/a>\" or \"I went to a <a href=\"https://en.wikipedia.org/wiki/Magnet_school\">magnet school<\/a>\". Magnet schools filter students with admission tests and then teach at a higher level than an average school, so essentially the entire school is an advanced track.\n  <\/p>\n<\/div>\n\n\nTwenty years of classroom instruction experience has taught me that classes with diverse academic abilities present one of the most difficult teaching challenges. Typically, one is forced to focus on only a sub-group of students, usually the second quartile. As a consequence the lower and higher quartiles are not properly served. At the university level, we minimize this problem by offering different levels: remedial math versus math for engineers, probability for the Masters program versus probability for PhD students, co-ed intramural sports versus the varsity basketball team, intro to World Music versus a spot in the orchestra, etc. In K-12, tracking seems like the obvious solution to teaching to an array of student levels.\nUnfortunately, there has been a trend recently to move away from tracking and several school districts now forbid it. The motivation seems to be a series of observational studies that note that “low-track classes tend to be primarily composed of low-income students, usually minorities, while upper-track classes are usually dominated by students from socioeconomically successful groups.” Tracking opponents infer that this unfortunate reality is due to bias (conscious or unconscious) in the the informal referrals that are typically used to decide which students are advanced. However, this is a critique of the referral system, not of tracking itself. A simple fix is to administer an objective test or use the percentiles from state assessment tests. In fact, such exams have been developed and implemented. A recent study (summarized here) examined the data from a district that for a period of time implemented an objective assessment and found that\n\n[t]he number of Hispanic students [in the advanced track increased] by 130 percent and the number of black students by 80 percent.\n\nUnfortunately, instead of maintaining the placement criteria, which benefited underrepresented minorities without relaxing standards, these school districts reverted to the old, flawed system due to budget cuts.\nAnother argument against tracking is that students benefit more from being in classes with higher-achieving peers, rather than being in a class with students with similar subject mastery and a teacher focused on their level. However a  (and the only one of which I am aware) finds that tracking helps all students:\n\nWe find that tracking students by prior achievement raised scores for all students, even those assigned to lower achieving peers. On average, after 18 months, test scores were 0.14 standard deviations higher in tracking schools than in non-tracking schools (0.18 standard deviations higher after controlling for baseline scores and other control variables). After controlling for the baseline scores, students in the top half of the pre-assignment distribution gained 0.19 standard deviations, and those in the bottom half gained 0.16 standard deviations. Students in all quantiles benefited from tracking. \n\nI believe that without tracking, the achievement gap between disadvantaged children and their affluent peers will continue to widen since involved parents will seek alternative educational opportunities, including private schools or subject specific extracurricular acceleration programs. With limited or no access to advanced classes in the public system, disadvantaged students will be less prepared to enter the very competitive STEM fields. Note that competition comes not only from within the US, but from other countries including many with educational systems that track.\nTo illustrate the extreme gap, the following exercises are from a 7th grade public school math class (in a high performing school district):\n\n\n\n\n\n\n(Click to enlarge). There is no tracking so all students must work on these problems. Meanwhile, in a 7th grade advanced, private math class, that same student can be working on problems like these:Let me stress that there is nothing wrong with the first example if it is the appropriate level of the student.  However, a student who can work at the level of the second example, should be provided with the opportunity to do so notwithstanding their family’s ability to pay. Poorer kids in districts which do not offer advanced classes will not only be less equipped to compete with their richer peers, but many of the academically advanced ones may, I suspect,  dismiss academics due to lack of challenge and boredom.  Educators need to consider evidence when making decisions regarding policy. Tracking can be applied unfairly, but that aspect can be remedied. Eliminating tracking all together takes away a crucial tool for disadvantaged students to move into the STEM fields and, according to the empirical evidence, hurts all students.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-12-03-not-so-standard-deviations-episode-5-irl-roger-is-totally-with-it/",
    "title": "Not So Standard Deviations: Episode 5 - IRL Roger is Totally With It",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-12-03",
    "categories": [],
    "contents": "\nI just posted Episode 5 of Not So Standard Deviations so check your feeds! Sorry for the long delay since the last episode but we got a bit tripped up by the Thanksgiving holiday.\nIn this episode, Hilary and I open up the mailbag and go through some of the feedback we’ve gotten on the previous episodes. The rest of the time is spent talking about the importance of reproducibility in data analysis both in academic research and in industry settings.\nIf you haven’t already, you can subscribe to the podcast through iTunes. Or you can use the SoundCloud RSS feed directly.\nNotes:\nHilary’s talk on reproducible analysis in production at the New York R Conference\nHilary’s Ignite presentation at Strata 2013\nRoger’s talk on “Computational and Policy Tools for Reproducible Research” at the Applied Mathematics Perspectives Workshop in Vancouver, 2011\nDuke Scandal Starter Set\nKeith Baggerly’s talk on Duke Scandal\nThe Web of Trust\ntestdat R package\nDownload the audio file for this episode.\nOr you can listen right here:\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-12-01-thinking-like-a-statistician-fund-more-investigator-initiated-grants/",
    "title": "Thinking like a statistician: the importance of investigator-initiated grants",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-12-01",
    "categories": [],
    "contents": "\nA substantial amount of scientific research is funded by investigator-initiated grants. A researcher has an idea, writes it up and sends a proposal to a funding agency. The agency then elicits help from a group of peers to evaluate competing proposals. Grants are awarded to the most highly ranked ideas. The percent awarded depends on how much funding gets allocated to these types of proposals. At the NIH, the largest funding agency of these types of grants, the success rate recently fell below 20% from a high above 35%. Part of the reason these percentages have fallen is to make room for large collaborative projects. Large projects seem to be increasing, and not just at the NIH. In Europe, for example, the Human Brain Project has an estimated cost of over 1 billion US$ over 10 years. To put this in perspective, 1 billion dollars can fund over 500 NIH R01s. R01 is the NIH mechanism most appropriate for investigator initiated proposals.\nThe merits of big science has been widely debated (for example here and here). And most agree that some big projects have been successful. However, in this post I present a statistical argument highlighting the importance of investigator-initiated awards. The idea is summarized in the graph below.\n\n\nThe two panes above represent two different funding strategies: fund-many-R01s (left) or reduce R01s to fund several large projects (right). The grey crosses represent investigators and the gold dots represent potential paradigm-shifting geniuses. Location on the Cartesian plane represent research areas, with the blue circles denoting areas that are prime for an important scientific advance. The largest scientific contributions occur when a gold dot falls in a blue circle. Large contributions also result from the accumulation of incremental work produced by grey crosses in the blue circles.\nAlthough not perfect, the peer review approach implemented by most funding agencies appears to work quite well at weeding out unproductive researchers and unpromising ideas. They also seem to do well at spreading funds across general areas. For example NIH spreads funds across diseases and public health challenges (for example cancer, mental health, heart, genomics, heart and lung disease.) as well as general medicine, genomics and information. However, precisely predicting who will be a gold dot or what specific area will be a blue circle seems like an impossible endeavor. Increasing the number of tested ideas and researchers therefore increases our chance of success. When a funding agency decides to invest big in a specific area (green dollar signs) they are predicting the location of a blue circle. As funding flows into these areas, so do investigators (note the clusters). The total number of funded lead investigators also drops. The risk here is that if the dollar sign lands far from a blue dot, we pull researchers away from potentially fruitful areas. If after 10 years of funding, the Human Brain Project doesn’t “achieve a multi-level, integrated understanding of brain structure and function” we will have missed out on trying out 500 ideas by hundreds of different investigators. With a sample size this large, we expect at least a  handful of these attempts to result in the type of impactful advance that justifies funding scientific research.\nThe simulation presented (code below) here is clearly an over simplification, but it does depict the statistical reason why I favor investigator-initiated grants.  The simulation clearly depicts that the strategy of funding many investigator-initiated grants is key for the continued success of scientific research.\nset.seed(2)\nlibrary(rafalib)\nthecol=\"gold3\"\nmypar(1,2,mar=c(0.5,0.5,2,0.5))\n## Start with the many R01s model\n###\n##generate location of 2,000 investigators\nN = 2000\nx = runif(N)\ny = runif(N)\n## 1% are geniuses\nNg = N*0.01\ng = rep(4,N);g[1:Ng]=16\n## generate location of important areas of research\nM0 = 10\nx0 = runif(M0)\ny0 = runif(M0)\nr0 = rep(0.03,M0)\n##Make the plot\nnullplot(xaxt=\"n\",yaxt=\"n\",main=\"Many R01s\")\nsymbols(x0,y0,circles=r0,fg=\"black\",bg=\"blue\",lwd=3,add=TRUE,inches=FALSE)\npoints(x,y,pch=g,col=ifelse(g==4,\"grey\",thecol))\npoints(x,y,pch=g,col=ifelse(g==4,NA,thecol))\n### Generate the location of 5 big projects\nM1 = 5\nx1 = runif(M1)\ny1 = runif(M1)\n##make initial plot\nnullplot(xaxt=\"n\",yaxt=\"n\",main=\"A Few Big Projects\")\nsymbols(x0,y0,circles=r0,fg=\"black\",bg=\"blue\",lwd=3,add=TRUE,inches=FALSE)\n### Generate location of investigators attracted\n### to location of big projects. There are 1000 total\n### investigators\nSigma = diag(2)*0.005\nN1 = 200\nNg1 = round(N1*0.01)\ng1 = rep(4,N);g1[1:Ng1]=16\nlibrary(MASS)\nfor(i in 1:M1){\n   xy = mvrnorm(N1,c(x1[i],y1[i]),Sigma)\n   points(xy[,1],xy[,2],pch=g1,col=ifelse(g1==4,\"grey\",thecol))\n}\n### generate location of investigators that ignore big projects\n### note now 500 instead of 200. Note overall total\n## is also less because large projects result in less\n## lead investigators\nN = 500\nx = runif(N)\ny = runif(N)\nNg = N*0.01\ng = rep(4,N);g[1:Ng]=16\npoints(x,y,pch=g,col=ifelse(g==4,\"grey\",thecol))\npoints(x1,y1,pch=\"$\",col=\"darkgreen\",cex=2,lwd=2)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-11-25-a-thanksgiving-dplyr-rubiks-cube-puzzle-for-you/",
    "title": "A thanksgiving dplyr Rubik's cube puzzle for you",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-11-25",
    "categories": [],
    "contents": "\nNick Carchedi is back visiting from DataCamp and for fun we came up with a [Nick Carchedi](http://nickcarchedi.com/) is back visiting from [DataCamp](https://www.datacamp.com/) and for fun we came up with a Rubik’s cube puzzle. Here is how it works. To solve the puzzle you have to make a 4 x 3 data frame that spells Thanksgiving like this:\n\n View the code on Gist. \n\nTo solve the puzzle you need to pipe this data frame in \n\n View the code on Gist. \n\nand pipe out the Thanksgiving data frame using only the dplyr commands arrange, mutate, slice, filter and select. For advanced users you can try our slightly more complicated puzzle:\n\n View the code on Gist. \n\nSee if you can do it this fast. Post your solutions in the comments and Happy Thanksgiving!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-11-24-20-years-of-data-science-and-data-driven-discovery-from-music-to-genomics/",
    "title": "20 years of Data Science: from Music to Genomics",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-11-24",
    "categories": [],
    "contents": "\nI finally got around to reading David Donoho’s 50 Years of Data Science paper.  I highly recommend it. The following quote seems to summarize the sentiment that motivated the paper, as well as why it has resonated among academic statisticians:\n\n\n<div class=\"column\">\n  <blockquote>\n    <p>\n      The statistics profession is caught at a confusing moment: the activities which preoccupied it over centuries are now in the limelight, but those activities are claimed to be bright shiny new, and carried out by (although not actually invented by) upstarts and strangers.\n    <\/p>\n  <\/blockquote>\n<\/div>\n\n\nThe reason we started this blog over four years ago was because, as Jeff wrote in his inaugural post, we were “fired up about the new era where data is abundant and statisticians are scientists”. It was clear that many disciplines were becoming data-driven and  that interest in data analysis was growing rapidly. We were further motivated because, despite this new found interest in our work, academic statisticians were, in general, more interested in the development of context free methods than in leveraging applied statistics to take leadership roles in data-driven projects. Meanwhile, great and highly visible applied statistics work was occurring in other fields such as astronomy, computational biology, computer science, political science and economics. So it was not completely surprising that some (bio)statistics departments were being left out from larger university-wide data science initiatives. Some of our posts exhorted academic departments to embrace larger numbers of applied statisticians:\n\n[M]any of the giants of our discipline were very much interested in solving specific problems in genetics, agriculture, and the social sciences. In fact, many of today’s most widely-applied methods were originally inspired by insights gained by answering very specific scientific questions. I worry that the balance between application and theory has shifted too far away from applications. An unfortunate consequence is that our flagship journals, including our applied journals, are publishing too many methods seeking to solve many problems but actually solving none.  By shifting some of our efforts to solving specific problems we will get closer to the essence of modern problems and will actually inspire more successful generalizable methods.\n\nDonoho points out that John Tukey had a similar preoccupation 50 years ago:\n\n\n<div class=\"column\">\n  <blockquote>\n    <p>\n      For a long time I have thought I was a statistician, interested in inferences from the particular to the general. But as I have watched mathematical statistics evolve, I have had cause to wonder and to doubt. ... All in all I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data\n    <\/p>\n  <\/blockquote>\n  \n  <p>\n    Many applied statisticians do the things Tukey mentions above. In the blog we have encouraged them to <a href=\"http://simplystatistics.org/2014/09/15/applied-statisticians-people-want-to-learn-what-we-do-lets-teach-them/\">teach the gory details of what what they do<\/a>, along with the general methodology we currently teach. With all this in mind, several months ago, when I was invited to give a talk at a department that was, at the time, deciphering their role in their university's data science initiative, I gave a talk titled<em> 20 years of Data Science: from Music to Genomics. <\/em>The goal was to explain why <em>applied statistician<\/em> is not considered synonymous with <em>data scientist <\/em>even when we focus on the same goal: <a href=\"https://en.wikipedia.org/wiki/Data_science\">extract knowledge or insights from data.<\/a>\n  <\/p>\n  \n  <p>\n    The first example in the talk related to how academic applied statisticians tend to emphasize the parts that will be most appreciated by our math stat colleagues and ignore the aspects that are today being heralded as the linchpins of data science. I used my thesis papers as examples. <a href=\"http://archive.cnmat.berkeley.edu/Research/1998/Rafael/tesis.pdf\">My dissertation work<\/a> was about finding meaningful parametrization of musical sound signals that<img class=\"wp-image-4449 alignright\" src=\"http://www.biostat.jhsph.edu/~ririzarr/Demo/img7.gif\" alt=\"Spectrogram\" width=\"380\" height=\"178\" /> my collaborators could use to manipulate sounds to create new ones. To do this, I prepared a database of sounds, wrote code to extract and import the digital representations from CDs into S-plus (yes, I'm that old), visualized the data to motivate models, wrote code in C (or was it Fortran?) to make the analysis go faster, and tested these models with residual analysis by ear (you can listen to them <a href=\"http://www.biostat.jhsph.edu/~ririzarr/Demo/\">here<\/a>). None of these data science aspects were highlighted in the <a href=\"http://www3.stat.sinica.edu.tw/statistica/oldpdf/A10n42.pdf\">papers<\/a> <a href=\"http://www.tandfonline.com/doi/abs/10.1198/000313001300339969#.Vk4_ht-rQUE\">I<\/a> <a href=\"http://www.tandfonline.com/doi/abs/10.1198/016214501750332875#.Vk4_mN-rQUE\">wrote <\/a><a href=\"http://www.tandfonline.com/doi/abs/10.1198/016214501753168082#.Vk4_qt-rQUE\">about<\/a> my <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/1467-9892.01515/abstract?userIsAuthenticated=false&deniedAccessCustomisedMessage=\">thesis<\/a>. Here is a screen shot from <a href=\"http://onlinelibrary.wiley.com/doi/10.1111/1467-9892.01515/abstract\">this paper<\/a>:\n  <\/p>\n<\/div>\n\n\n\nI am actually glad I wrote out and published all the technical details of this work.  It was great training. My point was simply that based on the focus of these papers, this work would not be considered data science.\nThe rest of my talk described some of the work I did once I transitioned into applications in Biology. I was fortunate to have a department chair that appreciated lead-author papers in the subject matter journals as much as statistical methodology papers. This opened the door for me to become a full fledged applied statistician/data scientist. In the talk I described how developing software packages, planning the gathering of data to aid method development, developing web tools to assess data analysis techniques in the wild, and facilitating data-driven discovery in biology has been very gratifying and, simultaneously, helped my career. However, at some point, early in my career, senior members of my department encouraged me to write and submit a methods paper to a statistical journal to go along with every paper I sent to the subject matter journals. Although I do write methods papers when I think the ideas add to the statistical literature, I did not follow the advice to simply write papers for the sake of publishing in statistics journals. Note that if (bio)statistics departments require applied statisticians to do this, then it becomes harder to have an impact as data scientists. Departments that are not producing widely used methodology or successful and visible applied statistics projects (or both), should not be surprised when they are not included in data science initiatives. So, applied statistician, read that Tukey quote again, listen to President Obama, and go do some great data science.\n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-11-19-some-links-related-to-randomized-controlled-trials-for-policymaking/",
    "title": "Some Links Related to Randomized Controlled Trials for Policymaking",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-11-19",
    "categories": [],
    "contents": "\n\n\nIn response to my previous post, Avi Feller sent me these links related to efforts promoting the use of RCTs  and evidence-based approaches for policymaking:\n\n\n The theme of this year’s just-concluded APPAM conference (the national public policy research organization) was “evidence-based policymaking,” with a headline panel on using experiments in policy (see here and here).\n\n\nJeff Liebman has written extensively about the use of randomized experiments in policy (see here for a recent interview).\n\n\nThe White House now has an entire office devoted to running randomized trials to improve government performance (the so-called “nudge unit”). Check out their recent annual report here.\n\n\nJPAL North America just launched a major initiative to help state and local governments run randomized trials (see here).\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-11-17-why-are-randomized-trials-not-used-by-policymakers/",
    "title": "Given the history of medicine, why are randomized trials not used for social policy?",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-11-17",
    "categories": [],
    "contents": "\nPolicy changes can have substantial societal effects. For example, clean water and  hygiene policies have saved millions, if not billions, of lives. But effects are not always positive. For example, prohibition, or the “noble experiment”, boosted organized crime, slowed economic growth and increased deaths caused by tainted liquor. Good intentions do not guarantee desirable outcomes.\nThe medical establishment is well aware of the danger of basing decisions on the good intentions of doctors or biomedical researchers. For this reason, randomized controlled trials (RCTs) are the standard approach to determining if a new treatment is safe and effective. In these trials an objective assessment is achieved by assigning patients at random to a treatment or control group, and then comparing the outcomes in these two groups. Probability calculations are used to summarize the evidence in favor or against the new treatment. Modern RCTs are considered one of the greatest medical advances of the 20th century.\nDespite their unprecedented success in medicine, RCTs have not been fully adopted outside of scientific fields. In this post, Ben Goldcare advocates for politicians to learn from scientists and base policy decisions on RCTs. He provides several examples in which results contradicted conventional wisdom. In this TED talk Esther Duflo convincingly argues that RCTs should be used to determine what interventions are best at fighting poverty. Although some RCTs  are being conducted, they are still rare and oftentimes ignored by policymakers. For example, despite at least two RCTs finding that universal pre-K programs are not effective, polymakers in New York are implementing a $400 million a year program. Supporters of this noble endeavor defend their decision by pointing to observational studies and “expert” opinion that support their preconceived views. Before the 1950s, indifference to RCTs was common among medical doctors as well, and the outcomes were at times devastating.\nToday, when we compare conclusions from non-RCT studies to RCTs, we note the unintended strong effects that preconceived notions can have. The first chapter in this book provides a summary and some examples. One example comes from a study of 51 studies on the effectiveness of the portacaval shunt. Here is table summarizing the conclusions of the 51 studies:\n\nDesign\n\n\nMarked Improvement\n\n\nModerate Improvement\n\n\nNone\n\n\nNo control\n\n\n24\n\n\n7\n\n\n1\n\n\nControls; but no randomized\n\n\n10\n\n\n3\n\n\n2\n\n\nRandomized\n\n\n\n\n1\n\n\n3\n\nCompare the first and last column to appreciate the importance of the randomized trials.\nA particularly troubling example relates to the studies on Diethylstilbestrol (DES). DES is a drug that was used to prevent spontaneous abortions. Five out of five studies using historical controls found the drug to be effective, yet all three randomized trials found the opposite. Before the randomized trials convinced doctors to stop using this drug , it was given to thousands of women. This turned out to be a tragedy as later studies showed DES has terrible side effects. Despite the doctors having the best intentions in mind, ignoring the randomized trials resulted in unintended consequences.\nWell meaning experts are regularly implementing policies without really testing their effects. Although randomized trials are not always possible, it seems that they are rarely considered, in particular when the intentions are noble. Just like well-meaning turn-of-the-20th-century doctors, convinced that they were doing good, put their patients at risk by providing ineffective treatments, well intentioned policies may end up hurting society.\nUpdate: A reader pointed me to these preprints which point out that the control group in one of the cited early education RCTs included children that receive care in a range of different settings, not just staying at home. This implies that the signal is attenuated if what we want to know is if the program is effective for children that would otherwise stay at home. In this preprint they use statistical methodology (principal stratification framework) to obtain separate estimates: the effect for children that would otherwise go to other center-based care and the effect for children that would otherwise stay at home. They find no effect for the former group but a significant effect for the latter. Note that in this analysis the effect being estimated is no longer based on groups assigned at random. Instead, model assumptions are used to infer the two effects. To avoid dependence on these assumptions we will have to perform an RCT with better defined controls. Also note that the RCT data facilitated the principal stratification framework analysis. I also want to restate what I’ve posted before, “I am not saying that observational studies are uninformative. If properly analyzed, observational data can be very valuable. For example, the data supporting smoking as a cause of lung cancer is all observational. Furthermore, there is an entire subfield within statistics (referred to as causal inference) that develops methodologies to deal with observational data. But unfortunately, observational data are commonly misinterpreted.”\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-11-16-so-you-are-getting-crushed-on-the-internet-the-new-normal-for-academics/",
    "title": "So you are getting crushed on the internet? The new normal for academics.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-11-16",
    "categories": [],
    "contents": "\nRoger and I were just talking about all the discussion around the Case and Deaton paper on death rates for middle class people. Andrew Gelman discussed it among many others. They noticed a potential bias in the analysis and did some re-analysis. Just yesterday Noah Smith wrote a piece about academics versus blogs and how many academics are taken by surprise when they see their paper being discussed so rapidly on the internet. Much of the debate comes down to the speed, tone, and ferocity of internet discussion of academic work - along with the fact that sometimes it isn’t fully fleshed out.\nI have been seeing this play out not just in the case of this specific paper, but many times that folks have been confronted with blogs or the quick publication process of f1000Research. I think it is pretty scary for folks who aren’t used to “internet speed” to see this play out and I thought it would be helpful to make a few points.\nEveryone is an internet scientist now. The internet has arrived as part of academics and if you publish a paper that is of interest (or if you are a Nobel prize winner, or if you dispute a claim, etc.) you will see discussion of that paper within a day or two on the blogs. This is now a fact of life.\nThe internet loves a fight. The internet responds best to personal/angry blog posts or blog posts about controversial topics like p-values, errors, and bias. Almost certainly if someone writes a blog post about your work or an f1000 paper it will be about an error/bias/correction or something personal.\nTakedowns are easier than new research and happen faster. It is much, much easier to critique a paper than to design an experiment, collect data, figure out what question to ask, ask it quantitatively, analyze the data, and write it up. This doesn’t mean the critique won’t be good/right it just means it will happen much much faster than it took you to publish the paper because it is easier to do. All it takes is noticing one little bug in the code or one error in the regression model. So be prepared for speed in the response.\nIn light of these three things, you have a couple of options about how to react if you write an interesting paper and people are discussing it - which they will certainly do (point 1), in a way that will likely make you uncomfortable (point 2), and faster than you’d expect (point 3). The first thing to keep in mind is that the internet wants you to “fight back” and wants to declare a “winner”. Reading about amicable disagreements doesn’t build audience. That is why there is reality TV. So there will be pressure for you to score points, be clever, be fast, and refute every point or be declared the loser. I have found from my own experience that is what I feel like doing too. I think that resisting this urge is both (a) very very hard and (b) the right thing to do. I find the best solution is to be proud of your work, but be humble, because no paper is perfect and thats ok. If you do the best you can , sensible people will acknowledge that.\nI think these are the three ways to respond to rapid internet criticism of your work.\nOption 1: Respond on internet time. This means if you publish a big paper that you think might be controversial  you should block off a day or two to spend time on the internet responding. You should be ready to do new analysis quickly, be prepared to admit mistakes quickly if they exist, and you should be prepared to make it clear when there aren’t. You will need social media accounts and you should probably have a blog so you can post longer form responses. Github/Figshare accounts make it better for quickly sharing quantitative/new analyses. Again your goal is to avoid the personal and stick to facts, so I find that Twitter/Facebook are best for disseminating your more long form responses on blogs/Github/Figshare. If you are going to go this route you should try to respond to as many of the major criticisms as possible, but usually they cluster into one or two specific comments, which you can address all in one.\nOption2 : Respond in academic time. You might have spent a year writing a paper to have people respond to it essentially instantaneously. Sometimes they will have good points, but they will rarely have carefully thought out arguments given the internet-speed response (although remember point 3 that good critiques can be faster than good papers). One approach is to collect all the feedback, ignore the pressure for an immediate response, and write a careful, scientific response which you can publish in a journal or in a fast outlet like f1000Research. I think this route can be the most scientific and productive if executed well. But this will be hard because people will treat that like “you didn’t have a good answer so you didn’t respond immediately”. The internet wants a quick winner/loser and that is terrible for science. Even if you choose this route though, you should make sure you have a way of publicizing your well thought out response - through blogs, social media, etc. once it is done.\nOption 3: Do not respond. This is what a lot of people do and I’m unsure if it is ok or not. Clearly internet facing commentary can have an impact on you/your work/how it is perceived for better or worse. So if you ignore it, you are ignoring those consequences. This may be ok, but depending on the severity of the criticism may be hard to deal with and it may mean that you have a lot of questions to answer later. Honestly, I think as time goes on if you write a big paper under a lot of scrutiny Option 3 is going to go away.\nAll of this only applies if you write a paper that a ton of people care about/is controversial. Many technical papers won’t have this issue and if you keep your claims small, this also probably won’t apply. But I thought it was useful to try to work out how to act under this “new normal”.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-11-10-prediction-markets-for-science-what-problem-do-they-solve/",
    "title": "Prediction Markets for Science: What Problem Do They Solve?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-11-10",
    "categories": [],
    "contents": "\nI’ve recently seen a bunch of press on this paper, which describes an experiment with developing a prediction market for scientific results. From FiveThirtyEight:\n\nAlthough replication is essential for verifying results, the current scientific culture does little to encourage it in most fields. That’s a problem because it means that misleading scientific results, like those from the “shades of gray” study, could be common in the scientific literature. Indeed, a 2005 study claimed that most published research findings are false.\n[…]\nThe researchers began by selecting some studies slated for replication in the Reproducibility Project: Psychology — a project that aimed to reproduce 100 studies published in three high-profile psychology journals in 2008. They then recruited psychology researchers to take part in two prediction markets. These are the same types of markets that people use to bet on who’s going to be president. In this case, though, researchers were betting on whether a study would replicate or not.\n\nThere are all kinds of prediction markets these days–for politics, general ideas–so having one for scientific ideas is not too controversial. But I’m not sure I see exactly what problem is solved by having a prediction market for science. In the paper, they claim that the market-based bets were better predictors of the general survey that was administrated to the scientists. I’ll admit that’s an interesting result, but I’m not yet convinced.\nFirst off, it’s worth noting that this work comes out of the massive replication project conducted by the Center for Open Science, where I believe they have a fundamentally flawed definition of replication. So I’m not sure I can really agree with the idea of basing a prediction market on such a definition, but I’ll let that go for now.\nThe purpose of most markets is some general notion of “price discovery”. One popular market is the stock market and I think it’s instructive to see how that works. Basically, people continuously bid on the shares of certain companies and markets keep track of all the bids/offers and the completed transactions. If you are interested in finding out what people are willing to pay for a share of Apple, Inc., then it’s probably best to look at…what people are willing to pay. That’s exactly what the stock market gives you. You only run into trouble when there’s no liquidity, so no one shows up to bid/offer, but that would be a problem for any market.\nNow, suppose you’re interested in finding out what the “true fundamental value” of Apple, Inc. Some people think the stock market gives you that at every instance, while others think that the stock market can behave irrationally for long periods of time. Perhaps in the very long run, you get a sense of the fundamental value of a company, but that may not be useful information at that point.\nWhat does the market for scientific hypotheses give you? Well, it would be one thing if granting agencies participated in the market. Then, we would never have to write grant applications. The granting agencies could then signal what they’d be willing to pay for different ideas. But that’s not what we’re talking about.\nHere, we’re trying to get at whether a given hypothesis is true or not. The only real way to get information about that is to conduct an experiment. How many people betting in the markets will have conducted an experiment? Likely the minority, given that the whole point is to save money by not having people conduct experiments investigating hypotheses that are likely false.\nBut if market participants aren’t contributing real information about an hypothesis, what are they contributing? Well, they’re contributing their opinion about an hypothesis. How is that related to science? I’m not sure. Of course, participants could be experts in the field (although not necessarily) and so their opinions will be informed by past results. And ultimately, it’s consensus amongst scientists that determines, after repeated experiments, whether an hypothesis is true or not. But at the early stages of investigation, it’s not clear how valuable people’s opinions are.\nIn a way, this reminds me of a time a while back when the EPA was soliciting “expert opinion” about the health effects of outdoor air pollution, as if that were a reasonable substitute for collecting actual data on the topic. At least it cost less money–just the price of a conference call.\nThere’s a version of this playing out in the health tech market right now. Companies like Theranos and 23andMe are selling health products that they claim are better than some current benchmark. In particular, Theranos claims its blood tests are accurate when only using a tiny sample of blood. Is this claim true or not? No one outside Theranos knows for sure, but we can look to the financial markets.\nTheranos can point to the marketplace and show that people are willing to pay for its products. Indeed, the $9 billion valuation of the private company is another indicator that people…highly value the company. But ultimately, we still don’t know if their blood tests are accurate because we don’t have any data. If we were to go by the financial markets alone, we would necessarily conclude that their tests are good, because why else would anyone invest so much money in the company?\nI think there may be a role to play for prediction markets in science, but I’m not sure discovering the truth about nature is one of them.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:21:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-11-09-biostatistics-its-not-what-you-think-it-is/",
    "title": "Biostatistics: It's not what you think it is",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-11-09",
    "categories": [],
    "contents": "\nMy department recently sent me on a recruitment trip for our graduate program. I had the opportunity to chat with undergrads interested in pursuing a career related to data analysis. I found that several did not know about the existence of Departments of Biostatistics and most of the rest thought Biostatistics was the study of clinical trials. We have posted on the need for better marketing for Statistics, but Biostatistics needs it even more. So this post is for students considering a career as applied statisticians or data science and are considering PhD programs.\nThere are dozens of Biostatistics departments and most run PhD programs. As an undergraduate, you may have never heard of it because they are usually in schools that undergrads don’t regularly frequent: Public Health and Medicine.  However, they are very active in research and teaching graduate students. In fact, the 2014 US News & World Report ranking of Statistics Departments includes three Biostat departments in the top five spots. Although clinical trials are a popular area of interest in these departments, there are now many other areas of research. With so many fields of science shifting to data intensive research, Biostatistics has adapted to work in these areas. Today pretty much any Biostat department will have people working on projects related to genetics, genomics, computational biology, electronic medical records, neuroscience, environmental sciences, and epidemiology, health-risk analysis, and clinical decision making. Through collaborations, academic biostatisticians have early access to the cutting edge datasets produced by public health scientists and biomedical researchers. Our research usually revolves in either developing statistical methods that are used by researchers working in these fields or working directly with a collaborator in data-driven discovery.\nHow is it different from Statistics? In the grand scheme of things, they are not very different. As implied by the name, Biostatisticians focus on data related to biology while statisticians tend to be more general. However, the underlying theory and skills we learn are similar. In my view, the major difference is that Biostatisticians, in general, tend to be more interested in data and the subject matter, while in Statistics Departments more emphasis is given to the mathematical theory.\nWhat type of job can I get with a Phd In Biostatistics? A well paying one. And you will have many options to chose from. Our graduates tend to go to academia, industry or government. Also, the Bio in the name does not keep our graduates for landing non-bio related jobs, such as in high tech. The reason for this is that the training our students receive and the what they learn from research experiences can be widely applied to data analysis challenges.\nHow should I prepare if I want to apply to a PhD program? First you need to decide if you are going to like it. One way to do this is to participate in one of the summer programs where you get a glimpse of what we do. My department runs one of these as well.  However, as an undergrad I would mainly focus on courses. Undergraduate research experiences are a good way to get an idea of what it’s like, but it is difficult to do real research unless you can set aside several hours a week for several consecutive months. This is difficult as an undergrad because you have to make sure to do well in your courses, prepare for the GRE, and get a solid mathematical and computing foundation in order to conduct research later. This is why these programs are usually in the summer. If you decide to apply to a PhD program, I recommend you take advanced math courses such as Real Analysis and Matrix Algebra. If you plan to develop software for complex datasets, I  recommend CS courses that cover algorithms and optimization. Note that programming skills are not the same thing as the theory taught in these CS courses. Programming skills in R will serve you well if you plan to analyze data regardless of what academic route you follow. Python and a low-level language such as C++ are more powerful languages that many biostatisticians use these days.\nI think the demand for well-trained researchers that can make sense of data will continue to be on the rise. If you want a fulfilling job where you analyze data for a living, you should consider a PhD in Biostatistics.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-11-07-not-so-standard-deviations-episode-4-a-gajillion-time-series/",
    "title": "Not So Standard Deviations: Episode 4 - A Gajillion Time Series",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-11-07",
    "categories": [],
    "contents": "\nEpisode 4 of Not So Standard Deviations is hot off the audio editor. In this episode Hilary first explains to me what heck is DevOps and then we talk about the statistical challenges in detecting rare events in an enormous set of time series data. There’s also some discussion of Ben and Jerry’s and the t-test, so you’ll want to hang on for that.\nNotes:\nNobody Loves Graphite Anymore\nA response\nWhy Gosset is awesome\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-11-06-how-i-decide-when-to-trust-an-r-package/",
    "title": "How I decide when to trust an R package",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-11-06",
    "categories": [],
    "contents": "\nOne thing that I’ve given a lot of thought to recently is the process that I use to decide whether I trust an R package or not. Kasper Hansen took a break from trolling me on Twitter to talk about how he trusts packages on Github less than packages that are on CRAN and particularly Bioconductor.  A couple of points he makes that I think are very relevant. First, that having a package on CRAN/Bioconductor raises trust in that package:\n\n\n.@michaelhoffman But it’s not on Bioconductor or CRAN. This decreases trust substantially.\n\n\n— Kasper Daniel Hansen (@KasperDHansen) October 29, 2015\n\n\nThe primary reason is because Bioc/CRAN demonstrate something about the developer’s willingness to do the boring but critically important parts of package development like documentation, vignettes, minimum coding standards, and being sure that their code isn’t just a rehash of something else. The other big point Kasper made was the difference between a repository - which is user oriented and should provide certain guarantees and Github - which is a developer platform and makes things easier/better for developers but doesn’t have a user guarantee system in place.\n\n\n.@StrictlyStat CRAN is a repository, not a development platform. It is user oriented, not developer oriented. GH is the reverse.\n\n\n— Kasper Daniel Hansen (@KasperDHansen) November 4, 2015\n\n\nThis discussion got me thinking about when/how I depend on R packages and how I make that decision. The scenarios where I depend on R packages are:\nQuick and dirty analyses for myself\nShareable data analyses that I hope are reproducible\nAs dependencies of R packages I maintain\nAs you move from 1-3 it is more and more of a pain if the package I’m depending on breaks. If it is just something I was doing for fun, its not that big of a deal. But if it means I have to rewrite/recheck/rerelease my R package than that is a much bigger headache.\nSo my scale for how stringent I am about relying on packages varies by the type of activity, but what are the criteria I use to measure how trustworthy a package is? For me, the criteria are in this order:\nPeople prior \nForced competence\nIndirect data\nI’ll explain each criteria in a minute, but the main purpose of using these criteria is (a) to ensure that I’m using a package that works and (b) to ensure that if the package breaks I can trust it will be fixed or at least I can get some help from the developer.\nPeople prior\nThe first thing I do when I look at a package I might depend on is look at who the developer is. If that person is someone I know has developed widely used, reliable software and who quickly responds to requests/feedback then I immediately trust the package. I have a list of people like Brian, or Hadley, or Jenny, or Rafa, who could post their package just as a link to their website and I would trust it. It turns out almost all of these folks end up putting their packages on CRAN/Bioconductor anyway. But even if they didn’t I assume that the reason is either (a) the package is very new or (b) they have a really good reason for not distributing it through the normal channels.\nForced competence\nFor people who I don’t know about or whose software I’ve never used, then I have very little confidence in the package a priori. This is because there are a ton of people developing R packages now with highly variable levels of commitment to making them work. So as a placeholder for all the variables I don’t know about them, I use the repository they choose as a surrogate. My personal prior on the trustworthiness of a package from someone I don’t know goes something like:\n\nThis prior is based on the idea of forced competence. In general, you have to do more to get a package approved on Bioconductor than on CRAN (for example you have to have a good vignette) and you have to do more to get a package on CRAN (pass R CMD CHECK and survive the review process) than to put it on Github.\nThis prior isn’t perfect, but it does tell me something about how much the person cares about their package. If they go to the work of getting it on CRAN/Bioc, then at least they cared enough to document it. They are at least forced to be minimally competent - at least at the time of submission and enough for the packages to still pass checks.\nIndirect data\nAfter I’ve applied my priors I then typically look at the data. For Bioconductor I look at the badges, like how downloaded it is, whether it passes the checks, and how well it is covered by tests. I’m already inclined to trust it a bit since it is on that platform, but I use the data to adjust my prior a bit. For CRAN I might look at the download stats provided by Rstudio. The interesting thing is that as John Muschelli points out, Github actually has the most indirect data available for a package:\n\n\n.@KasperDHansen Flipside: CRAN has no issue pages, stars/ratings, outdated limits on size, and limited development cycle/turnover.\n\n\n— John Muschelli (@StrictlyStat) November 4, 2015\n\n\nIf I’m going to use a package that is on Github from a person who isn’t on my prior list of people to trust then I look at a few things. The number of stars/forks/watchers is one thing that is a quick and dirty estimate of how used a package is. I also look very carefully at how many commits the person has submitted to both the package in question and in general all other packages over the last couple of months. If the person isn’t actively developing either the package or anything else on Github, that is a bad sign. I also look to see how quickly they have responded to issues/bug reports on the package in the past if possible. One idea I haven’t used but I think is a good one is to submit an issue for a trivial change to the package and see if I get a response very quickly. Finally I look and see if they have some demonstration their package works across platforms (say with a travis badge). If the package is highly starred, frequently maintained, all issues are responded to and up-to-date, and passes checks on all platform then that data might overwhelm my prior and I’d go ahead and trust the package.\nSummary\nIn general one of the best things about the R ecosystem is being able to rely on other packages so that you don’t have to write everything from scratch. But there is a hard balance to strike with keeping the dependency list small. One way I maintain this balance is using the strategy I’ve outlined to worry less about trustworthy dependencies.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-10-30-facultypostdoc-job-opportunities-in-genomics-across-johns-hopkins/",
    "title": "Faculty/postdoc job opportunities in genomics across Johns Hopkins",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-10-30",
    "categories": [],
    "contents": "\nIt’s pretty exciting to be in genomics at Hopkins right now with three new Bloomberg professors in genomics areas, a ton of stellar junior faculty, and a really fun group of students/postdocs. If you want to get in on the action here is a non-comprehensive list of great opportunities.\nFaculty Jobs\nJob: Multiple tenure track faculty positions in all areas including in genomics\nDepartment:  Biostatistics\nTo apply: http://www.jhsph.edu/departments/biostatistics/_docs/faculty-ad-2016-combined-large-final.pdf\nDeadline: Review ongoing\nJob: Tenure track position in data intensive biology\nDepartment:  Biology\nTo apply: http://apply.interfolio.com/31146\nDeadline: Nov 1st and ongoing\nJob: Tenure track positions in bioinformatics, with focus on proteomics or sequencing data analysis\nDepartment:  Oncology Biostatistics\nTo apply: https://www.research-it.onc.jhmi.edu/DBB/PhD_Statistician.pdf\nDeadline: Review ongoing\n \nPostdoc Jobs\nJob: Postdoc(s) in statistical methods/software development for RNA-seq\nEmployer:  Jeff Leek\nTo apply: email Jeff (http://jtleek.com/jobs/)\nDeadline: Review ongoing\nJob: Data scientist for integrative genomics in the human brain (MS/PhD)\nEmployer:  Andrew Jaffe\nTo apply: email Andrew (http://www.aejaffe.com/jobs.html)\nDeadline: Review ongoing\nJob: Research associate for genomic data processing and analysis (BA+)\nEmployer:  Andrew Jaffe\nTo apply: email Andrew (http://www.aejaffe.com/jobs.html)\nDeadline: Review ongoing\nJob: PhD developing scalable software and algorithms for analyzing sequencing data\nEmployer:  Ben Langmead\nTo apply:  http://www.cs.jhu.edu/graduate-studies/phd-program/\nDeadline: See site\nJob: Postdoctoral researcher developing scalable software and algorithms for analyzing sequencing data\nEmployer:  Ben Langmead\nTo apply:  email Ben (http://www.langmead-lab.org/open-positions/)\nDeadline: Review ongoing\nJob: Postdoctoral researcher developing algorithms for challenging problems in large-scale genomics whole-genome assenbly, RNA-seq analysis, and microbiome analysis\nEmployer:  Steven Salzberg\nTo apply:  email Steven (http://salzberg-lab.org/)\nDeadline: Review ongoing\nJob: Research associate for genomic data processing and analysis (BA+) in cancer\nEmployer:  Luigi Marchionni (with Don Geman)\nTo apply:  email Luigi (http://luigimarchionni.org/)\nDeadline: Review ongoing\nJob: Postdoctoral researcher developing algorithms for biomarkers development and precision medicine application in cancer\nEmployer:  Luigi Marchionni (with Don Geman)\nTo apply:  email Luigi (http://luigimarchionni.org/)\nDeadline: Review ongoing\nJob:Postdoctoral researcher developing methods in machine learning, genomics, and regulatory variation\nEmployer:  Alexis Battle\nTo apply:  email Alexis (http://battlelab.jhu.edu/join_us.html)\nDeadline: Review ongoing\nJob: Postdoctoral fellow with interests in biomarker discovery for Alzheimer’s disease\nEmployer:  Madhav Thambisetty / Ingo Ruczinski\nTo apply:  http://www.alzforum.org/jobs/postdoctoral-research-fellow-alzheimers-disease-biomarkers\nDeadline: Review ongoing\nJob: Postdoctoral positions for research in the interface of statistical genetics, precision medicine and big data\nEmployer:  Nilanjan Chatterjee\nTo apply:  http://www.jhsph.edu/departments/biostatistics/_docs/postdoc-ad-chatterjee.pdf\nDeadline: Review ongoing\nJob: Postdoctoral research developing algorithms and software for time course pattern detection in genomics data\nEmployer:  Elana Fertig\nTo apply:  email Elana (ejfertig@jhmi.edu)\nDeadline: Review ongoing\nJob: Postdoctoral fellow to develop novel methods for large-scale DNA and RNA sequence analysis related to human and/or plant genetics, such as developing methods for discovering structural variations in cancer or for assembling and analyzing large complex plant genomes.\nEmployer:  Mike Schatz\nTo apply:  email Mike (http://schatzlab.cshl.edu/apply/)\nDeadline: Review ongoing\nStudents\nWe are all always on the hunt for good Ph.D. students. At Hopkins students are admitted to specific departments. So if you find a faculty member you want to work with, you can apply to their department. Here are the application details for the various departments admitting students to work on genomics: https://ccb.jhu.edu/students.shtml\n \n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-10-30-the-statistics-identity-crisis-am-i-a-data-scientist/",
    "title": "The Statistics Identity Crisis: Am I a Data Scientist",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-10-30",
    "categories": [],
    "contents": "\nThe joint ASA/Simply Statistics webinar on the statistics identity crisis is now live!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-10-29-the-statistics-identity-crisis-am-i-really-a-data-scientist/",
    "title": "The statistics identity crisis: am I really a data scientist?",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-10-29",
    "categories": [],
    "contents": "\n \n \n \n\n \nTl;dr: We will host a Google Hangout of our popular JSM session October 30th 2-4 PM EST. \n \nI organized a session at JSM 2015 called “The statistics identity crisis: am I really a data scientist?” The session turned out to be pretty popular:\n\n\nPacked room of statisticians with identity crises at #JSM2015 session: are we really data scientists? pic.twitter.com/eLsGosoTCt\n\n\n— Dr Ruth Etzioni (@retzioni) August 11, 2015\n\n\nbut it turns out not everyone fit in the room:\n\n\nThis is the closest I can get to @statpumpkin’s talk. #jsm2015 still had no clue how to predict session attendance. pic.twitter.com/gTb4OqdAo3\n\n\n— sandy griffith (@sgrifter) August 11, 2015\n\n\nThankfully, Steve Pierson at the ASA had the awesome idea to re-run the session for people who couldn’t be there. So we will be hosting a Google Hangout with the following talks:\n\n\n\n‘Am I a Data Scientist?’: The Applied Statistics Student’s Identity Crisis — Alyssa Frazee, Stripe\n\n\n\n\nHow Industry Views Data Science Education in Statistics Departments — Chris Volinsky, AT&T\n\n\n\n\nEvaluating Data Science Contributions in Teaching and Research — Lance Waller, Emory University\n\n\n\n\nTeach Data Science and They Will Come — Jennifer Bryan, The University of British Columbia\n\nYou can watch it on Youtube or Google Plus. Here is the link:\nhttps://plus.google.com/events/chuviltukohj2inbqueap9h7228\nThe session will be held October 30th (tomorrow!) from 2-4PM EST. You can watch it live and discuss the talks using the hashtag   or you can watch later as the video will remain on Youtube.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-10-28-discussion-of-the-theranos-controversy-with-elizabeth-matsui/",
    "title": "Discussion of the Theranos Controversy with Elizabeth Matsui",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-10-28",
    "categories": [],
    "contents": "\nTheranos is a Silicon Valley diagnostic testing company that has been in the news recently. The story of Theranos has fascinated me because I think it represents a perfect collision of the tech startup culture and the health care culture and how combining them together can generate unique problems.\nI talked with Elizabeth Matsui, a Professor of Pediatrics in the Division of Allergy and Immunology here at Johns Hopkins, to discuss Theranos, the realities of diagnostic testing, and the unique challenges that a health-tech startup faces with respect to doing good science and building products people want to buy.\nNotes:\nOriginal Wall Street Journal story on Theranos (paywalled)\nRelated stories in Wired and NYT’s Dealbook (not paywalled)\nTheranos response to WSJ story\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-10-24-not-so-standard-deviations-episode-3-gilmore-girls/",
    "title": "Not So Standard Deviations: Episode 3 - Gilmore Girls",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-10-24",
    "categories": [],
    "contents": "\nI just uploaded Episode 3 of Not So Standard Deviations so check your feeds. In this episode Hilary and I talk about our jobs and the life of the data scientist in both academia and the tech industry. It turns out that they’re not as different as I would have thought.\nDownload the audio file for this episode.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-10-20-we-need-a-statistically-rigorous-and-scientifically-meaningful-definition-of-replication/",
    "title": "We need a statistically rigorous and scientifically meaningful definition of replication",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-10-20",
    "categories": [],
    "contents": "\nReplication and confirmation are indispensable concepts that help define scientific facts.  However, the way in which we reach scientific consensus on a given finding is rather complex. Although some press releases try to convince us otherwise, rarely is one publication enough. In fact, most published results go unnoticed and no attempts to replicate them are made.  These are not debunked either; they simply get discarded to the dustbin of history. The very few results that garner enough attention for others to spend time and energy on them are assessed by an ad-hoc process involving a community of peers. The assessments are usually a combination of deductive reasoning, direct attempts at replication, and indirect checks obtained by attempting to build on the result in question.  This process eventually leads to a result either being accepted by consensus or not. For particularly important cases, an official scientific consensus report may be commissioned by a national academy or an established scientific society. Examples of results that have become part of the scientific consensus in this way include smoking causing lung cancer, HIV causing AIDS, and climate change being caused by humans.  In contrast, the published result that vaccines cause autism has been thoroughly debunked by several follow up studies. In none of these four cases a simple definition of replication was used to confirm or falsify a result. The same is true for most results for which there is consensus. Yet science moves on, and continues to be an incomparable force at improving our quality of life.\nRegulatory agencies, such as the FDA, are an exception since they clearly spell out a definition of replication. For example, to approve a drug they may require two independent clinical trials, adequately powered, to show statistical significance at some predetermined level. They also require a large enough effect size to justify the cost and potential risks associated with treatment. This is not to say that FDA approval is equivalent to scientific consensus, but they do provide a clearcut definition of replication.\nIn response to a growing concern over a reproducibility crisis, projects such as the Open Science Collaboration have commenced to systematically try to replicate published results. In a recent post, Jeff described one of their recent papers on estimating the reproducibility of psychological science (they really mean replicability; see note below). This Science paper led to lay press reports with eye-catching headlines such as “only 36% of psychology experiments replicate”. Note that the 36% figure comes from a definition of replication that mimics the definition used by regulatory agencies: results are considered replicated if a p-value < 0.05 was reached in both the original study and the replicated one. Unfortunately, this definition ignores both effect size and statistical power. If power is not controlled, then the expected proportion of correct findings that replicate can be quite small. For example, if I try to replicate the smoking-causes-lung-cancer result with a sample size of 5, there is a good chance it will not replicate. In his post, Jeff notes that for several of the studies that did not replicate, the 95% confidence intervals intersected. So should intersecting confidence intervals be our definition of replication? This too has a flaw since it favors imprecise studies with very large confidence intervals. If effect size is ignored, we may waste our time trying to replicate studies reporting practically meaningless findings. Generally defining replication for published studies is not as easy as for highly controlled clinical trials. However, one clear improvement from what is currently being done is to consider statistical power and effect sizes.\nTo further illustrate this, let’s consider a very concrete example with real life consequences. Imagine a loved one has a disease with high mortality rates and asks for your help in evaluating the scientific evidence on treatments. Four experimental drugs are available all with promising clinical trials resulting in p-values <0.05. However, a replication project redoes the experiments and finds that only the drug A and drug B studies replicate (p<0.05). So which drug do you take? Let’s give a bit more information to help you decide. Here are the p-values for both original and replication trials:\n\nDrug\n\n\nOriginal\n\n\nReplication\n\n\nReplicated\n\n\nA\n\n\n0.0001\n\n\n0.001\n\n\nYes\n\n\nB\n\n\n<0.000001\n\n\n0.03\n\n\nYes\n\n\nC\n\n\n0.03\n\n\n0.06\n\n\nNo\n\n\nD\n\n\n<0.000001\n\n\n0.10\n\n\nNo\n\n\n\nWhich drug would you take now? The information I have provided is based on p-values and therefore is missing a key piece of information: the effect sizes. Below I show the confidence intervals for all four studies (left) and four replication studies (right). Note that except for drug B, all confidence intervals intersect. In light of the figure below, which one would you chose?\n\nI would be inclined to go with drug D because it has a large effect size, a small p-value, and the replication experiment effect estimate fell inside a 95% confidence interval. I would definitely not go with A since it provides marginal benefits, even if the trial found a statistically significant effect and was replicated. So the p-value based definition of replication is practically worthless from a practical standpoint.\nIt seems that before continuing the debate over replication, and certainly before declaring that we are in a reproducibility crisis, we need a statistically rigorous and scientifically meaningful definition of replication.  This definition does not necessarily need to be dichotomous (replicated or not) and it will probably require more than one replication experiment and more than one summary statistic: one for effect size and one for uncertainty. In the meantime, we should be careful not to dismiss the current scientific process, which seems to be working rather well at either ignoring or debunking false positive results while producing useful knowledge and discovery.\nFootnote on reproducible versus replication: As Jeff pointed out, the cited Open Science Collaboration paper is about replication, not reproducibility. A study is considered reproducible if an independent researcher can recreate the tables and figures from the original raw data. Replication is not nearly as simple to define because it involves probability. To replicate the experiment it has to be performed again, with a different random sample and new set of measurement errors.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-10-16-thorns-runs-head-first-into-the-realities-of-diagnostic-testing/",
    "title": "Theranos runs head first into the realities of diagnostic testing",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-10-16",
    "categories": [],
    "contents": "\nThe Wall Street Journal has published a lengthy investigation into the diagnostic testing company Theranos.\n\nThe company offers more than 240 tests, ranging from cholesterol to cancer. It claims its technology can work with just a finger prick. Investors have poured more than $400 million into Theranos, valuing it at $9 billion and her majority stake at more than half that. The 31-year-old Ms. Holmes’s bold talk and black turtlenecks draw comparisons to Apple Inc. cofounder Steve Jobs.\n\nIf ever there were a warning sign, the comparison to Steve Jobs has got to be it.\n\nBut Theranos has struggled behind the scenes to turn the excitement over its technology into reality. At the end of 2014, the lab instrument developed as the linchpin of its strategy handled just a small fraction of the tests then sold to consumers, according to four former employees.\n\nOne former senior employee says Theranos was routinely using the device, named Edison after the prolific inventor, for only 15 tests in December 2014. Some employees were leery about the machine’s accuracy, according to the former employees and emails reviewed by The Wall Street Journal.\n\n\n\n\n\nIn a complaint to regulators, one Theranos employee accused the company of failing to report test results that raised questions about the precision of the Edison system. Such a failure could be a violation of federal rules for laboratories, the former employee said.\n\n\n\nWith these kinds of stories, it’s always hard to tell whether there’s reality here or it’s just a bunch of axe grinding. But one thing that’s for sure is that people are talking, and probably not for good reasons.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-10-14-minimal-r-package-check-list/",
    "title": "Minimal R Package Check List",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-10-14",
    "categories": [],
    "contents": "\nA little while back I had the pleasure of flying in a small Cessna with a friend and for the first time I got to see what happens in the cockpit with a real pilot. One thing I noticed was that basically you don’t lift a finger without going through some sort of check list. This starts before you even roll the airplane out of the hangar. It makes sense because flying is a pretty dangerous hobby and you want to prevent problems from occurring when you’re in the air.\nThat experience got me thinking about what might be the minimal check list for building an R package, a somewhat less dangerous hobby. First off, much has changed (for the better) since I started making R packages and I wanted to have some clean documentation of the process, particularly with using RStudio’s tools. So I wiped off my installations of both R and RStudio and started from scratch to see what it would take to get someone to build their first R package.\nThe list is basically a “pre-flight” list-–the presumption here is that you actually know the important details of building packages, but need to make sure that your environment is setup correctly so that you don’t run into errors or problems. I find this is often a problem for me when teaching students to build packages because I focus on the details of actually making the packages (i.e. DESCRIPTION files, Roxygen, etc.) and forget that way back when I actually configured my environment to do this.\nPre-flight Procedures for R Packages\nInstall most recent version of R\nInstall most recent version of RStudio\nOpen RStudio\nInstall devtools package\nClick on Project –> New Project… –> New Directory –> R package\nEnter package name\nDelete boilerplate code and “hello.R” file\nGoto “man” directory an delete “hello.Rd” file\nIn File browser, click on package name to go to the top level directory\nClick “Build” tab in environment browser\nClick “Configure Build Tools…”\nCheck “Generate documentation with Roxygen”\nCheck “Build & Reload” when Roxygen Options window opens –> Click OK\nClick OK in Project Options window\nAt this point, you’re clear to build your package, which obviously involves writing R code, Roxygen documentation, writing package metadata, and building/checking your package.\nIf I’m missing a step or have too many steps, I’d like to hear about it. But I think this is the minimum number of steps you need to configure your environment for building R packages in RStudio.\nUPDATE: I’ve made some changes to the check list and will be posting future updates/modifications to my GitHub repository.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-10-03-profile-of-data-scientist-shannon-cebron/",
    "title": "Profile of Data Scientist Shannon Cebron",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-10-03",
    "categories": [],
    "contents": "\nThe “This is Statistics” campaign has a nice profile of Shannon Cebron, a data scientist working at the Baltimore-based Pegged Software.\n\nWhat advice would you give to someone thinking of a career in data science?\nTake some advanced statistics courses if you want to see what it’s like to be a statistician or data scientist. By that point, you’ll be familiar with enough statistical methods to begin solving real-world problems and understanding the power of statistical science.  I didn’t realize I wanted to be a data scientist until I took more advanced statistics courses, around my third year as an undergraduate math major.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-10-02-not-so-standard-deviations-episode-2-we-got-it-under-40-minutes/",
    "title": "Not So Standard Deviations: Episode 2 - We Got it Under 40 Minutes",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-10-02",
    "categories": [],
    "contents": "\nEpisode 2 of my podcast with Hilary Parker, Not So Standard Deviations, is out! In this episode, we talk about user testing for statistical methods, navigating the Hadleyverse, the crucial significance of rename(), and the secret reason for creating the podcast (hint: it rhymes with “bee”). Also, I erroneously claim that Bill Cleveland is way older than he actually is. Sorry Bill.\nIn other news, we are finally on iTunes so you can subscribe from there directly if you want (just search for “Not So Standard Deviations” or paste the link directly into your podcatcher.\nDownload the audio file for this episode.\nNotes:\nBill Cleveland’s paper in Science, on graphical perception, published in 1985\nTomFest\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-10-01-a-glass-half-full-interpretation-of-the-replicability-of-psychological-science/",
    "title": "A glass half full interpretation of the replicability of psychological science",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-10-01",
    "categories": [],
    "contents": "\n\ntl;dr: 77% of replication effects from the psychology replication study were in (or above) the 95% prediction interval based on the original effect size. This isn’t perfect and suggests (a) there is still room for improvement, (b) the scientists who did the replication study are pretty awesome at replicating, (c) we need a better definition of replication that respects uncertainty but (d) the scientific sky isn’t falling. We wrote this up in a paper on arxiv; the code is here. \n\n\nA week or two ago a paper came out in Science on Estimating the reproducibility of psychological science. The basic behind the study was to take a sample of studies that appeared in a particular journal in 2008 and try to replicate each of these studies. Here I’m using the definition that reproducibility is the ability to recalculate all results given the raw data and code from a study and replicability is the ability to re-do the study and get a consistent result. \n\n\nThe paper is pretty incredible and the authors did an amazing job of going back to the original sources and trying to be faithful to the original study designs. I have to admit when I first heard about the study design I was incredibly pessimistic about the results (I suppose grouchy is a natural default state for many statisticians –especially those with sleep deprivation). I mean 2008 was well before the push toward reproducibility had really taken off (Biostatistics was one of the first journals to adopt a policy on reproducible research and that didn’t happen until 2009). More importantly, the student researchers from those studies had possibly moved on, study populations may change, there could be any number of minor variations in the study design and so forth. I thought the chances of getting any effects in the same range was probably pretty low. \n\n\nSo when the results were published I was pleasantly surprised. I wasn’t the only one:\n\n\n\nSomeone has to say it, but this plot shows that science is, in fact, working. http://t.co/JUy10xHfbH pic.twitter.com/lJSx6IxPw2\n\n\n— Roger D. Peng (@rdpeng) August 27, 2015\n\n\n\n\nLooks like psychologists are in a not-too-bad spot on the ROC curves of science (http://t.co/fPsesCn2yK) http://t.co/9rAOdZWvzv\n\n\n— Joe Pickrell (@joe_pickrell) August 28, 2015\n\n\nBut that was definitely not the prevailing impression that the paper left on social and mass media. A lot of the discussion around the paper focused on the idea that only 36% of the studies had a p-value less than 0.05 in both the original and replication study. But many of the sample sizes were small and the effects were modest. So the first question I asked myself was, “Well what would we expect to happen if we replicated these studies?” The original paper measured replicability in several ways and tried hard to calibrate expected coverage of confidence intervals for the measured effects.\nWith Roger and Prasad we tried a little different approach. We estimated the 95% prediction interval for the replication effect given the original effect size.\n\n \n72% of the replication effects were within the 95% prediction interval and 2 were above the interval (showed a stronger signal in replication in than predicted from original study). This definitely shows that there is still room for improvement in replication of these studies - we would expect 95% of the effects to fall into the 95% prediction interval. But at least my opinion is that 72% (or 77% if you count the 2 above the P.I.) of studies falling in the prediction interval is (a) not bad and (b) a testament to the authors of the reproducibility paper and their efforts to get the studies right.\nAn important point here is that replication and reproducibility aren’t the same thing. When reproducing a study we expect the numbers and figures to be _exactly the same. _But a replication involves recollection of data and is subject to variation and so we don’t expect the answer to be exactly the same in the replication. This is of course made more confusing by regression to the mean, publication bias, and the garden of forking paths.  Our use of a prediction interval measures both the variation expected in the original study and in the replication. One thing we noticed when re-analyzing the data is how many of the studies had very low sample sizes. \n \nSample sizes were generally bigger in the replication, but often very low regardless. This makes it more difficult to disentangle what didn’t replicate from what is just expected variation for a small sample size study.  The point remains whether those small studies should be trusted in general, but for the purposes of measuring replication it makes the problem more difficult.\nOne thing I have been thinking about a lot and this study drove home is that if we are measuring replication we need a definition that incorporates uncertainty directly. Suppose that you collect a data set D0 from an original study and  D1 from a replication. Then replication means that the data from a study replicates if D0 ~ F and D1 ~ F. Informally, if the data are generated from the same distribution in both experiments then the study replicates. To get an estimate you apply a pipeline to the data set to get an estimate e0 = p(D0). If the study is also reproducible than p() is the same for both studies and p(D0) ~ G and p(D1) ~ G, subject to some conditions on p(). \nOne interesting consequence of this definition is that each complete replication data set represents only a single data point for measuring replication. To measure replication with this definition you either need to make assumptions about the data generating distribution for D0 and D1 or you need to perform a complete replication of a study many times to determine if it replicates. However, it does mean that we can define replication even for studies with very small number of replicates as the data generating distribution may be arbitrarily variable in each case.\nRegardless of this definition I was excited that the OSF folks did the study and pulled it off as well as they did and was a bit bummed about the most common  reaction. I think there is an easy narrative that “science is broken” which I think isn’t a positive thing for a number of reasons. I love the way that {reproducibility/replicability/open science/open publication} are becoming more and more common, but often think we fall into the same trap in wanting to report these results as clear cut as we do when reporting exaggerations or oversimplifications of scientific discoveries in headlines. I’m excited to see how these kinds of studies look in 10 years when Github/open science/pre-prints/etc. are all the standards.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-09-30-apple-musics-moment-of-truth/",
    "title": "Apple Music's Moment of Truth",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-09-30",
    "categories": [],
    "contents": "\nToday is the day when Apple, Inc. learns whether it’s brand new streaming music service, Apple Music, is going to be a major contributor to the bottom line or just another streaming service (JASS?). Apple Music launched 3 months ago and all new users are offered a 3-month free trial. Today, that free trial ends and the big question is how many people will start to pay for their subscription, as opposed to simply canceling it. My guess is that most people (> 50%) will opt to pay, but that’s a complete guess. For what it’s worth, I’ll be paying for my subscription. After adding all this music to my library, I’d hate to see it all go away.\nBack on August 18, 2015, consumer market research firm MusicWatch released a study that claimed, among other things, that\n\nAmong people who had tried Apple Music, 48 percent reported they are not currently using the service.\n\nThis would suggest that almost half of people who had signed up for the free trial period of Apple Music were not interested in using it further and would likely not pay for it once the trial ended. If it were true, it would be a blow to the newly launched service.\nBut how did MusicWatch arrive at its number? It claimed to have surveyed 5,000 people in its study. Shortly before the survey by MusicWatch was released, Apple claimed that about 11 million people had signed up for their new Apple Music service (because the service had just launched, everyone who had signed up was in the free trial period). Clearly, 5,000 people do not make up the entire population, so we have but a small sample of users.\nWhat is the target that MusicWatch was trying to answer? It seems that they wanted to know the percentage of all people who had signed up for Apple Music that were still using the service. Can they make inference about the entire population from the sample of 5,000?\nIf the sample is representative and the individuals are independent, we could use the number 48% as an estimate of the percentage in the population who no longer use the service. The press release from MusicWatch did not indicate any measure of uncertainty, so we don’t know how reliable the number is.\nInterestingly, soon after the MusicWatch survey was released, Apple released a statement to the publication The Verge, stating that 79% of users who had signed up were still using the service (i.e. only 21% had stopped using it, as opposed to 48% reported by MusicWatch). In other words, Apple just came out and gave us the truth! This was unusual because Apple typically does not make public statements about newly launched products. I just found this amusing because I’ve never been in a situation where I was trying to estimate a parameter and then someone later just told me what its value was.\nIf we believe that Apple and MusicWatch were measuring the same thing in their analyses (and it’s not clear that they were), then it would suggest that MusicWatch’s estimate of the population percentage (48%) was quite far off from the true value (21%). What would explain this large difference?\nRandom variation. It’s true that MusicWatch’s survey was a small sample relative to the full population, but the sample was still big with 5,000 people. Furthermore, the analysis was fairly simple (just taking the proportion of users still using the service), so the uncertainty associated with that estimate is unlikely to be that large.\nSelection bias. Recall that it’s not clear how MusicWatch sampled its respondents, but it’s possible that the way that they did it led them to capture a set of respondents who were less inclined to use Apple Music. Beyond this, we can’t really say more without knowing the details of the survey process.\nRespondents are not independent. It’s possible that the survey respondents are not independent of each other. This would primiarily affect the uncertainty about the estimate, making it larger than we might expect if the respondents were all independent. However, since we do not know what MusicWatch’s uncertainty about their estimate was in the first place, it’s difficult to tell if dependence between respondents could play a role. Apple’s number, of course, has no uncertainty.\nMeasurement differences. This is the big one, in my opinion. We don’t know is how either MusicWatch or Apple defined “still using the service”. You could imagine a variety of ways to determine whether a person was still using the service. You could ask “Have you used it in the last week?” or perhaps “Did you use it yesterday?” Responses to these questions would be quite different and would likely lead to different overall percentages of usage.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-09-29-we-used-data-to-improve-our-harvardx-courses-new-versions-start-oct-15/",
    "title": "We Used Data to Improve our HarvardX Courses: New Versions Start Oct 15",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-09-29",
    "categories": [],
    "contents": "\nYou can sign up following links here\nLast semester we successfully You can sign up following links [here](http://genomicsclass.github.io/book/pages/classes.html) of my Data Analysis course. To create the second version, the first was split into eight courses. Over 2,000 students successfully completed the first of these, but, as expected, the numbers were lower for the more advanced courses. We wanted to remove any structural problems keeping students from maximizing what they get from our courses, so we studied the assessment questions data, which included completion rate and time, and used the findings to make improvements. We also used qualitative data from the discussion board. The major changes to version 3 are the following:\nWe no longer use R packages that Microsoft Windows users had trouble installing in the first course.\nAll courses are now designed to be completed in 4 weeks.\nWe added new assessment questions.\nWe improved the assessment questions determined to be problematic.\nWe split the two courses that students took the longest to complete into smaller modules. Students now have twice as much time to complete these.\nWe consolidated the case studies into one course.\nWe combined the materials from the statistics courses into a book, which you can download here. The material in the book match the materials taught in class so you can use it to follow along.\nYou can enroll into any of the seven courses following the links below. We will be on the discussion boards starting October 15, and we hope to see you there.\nStatistics and R for the Life Sciences starts October 15.\nIntroduction to Linear Models and Matrix Algebra starts November 15.\nStatistical Inference and Modeling for High-throughput Experiments starts December 15.\nHigh-Dimensional Data Analysis starts January 15.\nIntroduction to Bioconductor: Annotation and Analysis of Genomes and Genomic Assays starts February 15.\nHigh-performance Computing for Reproducible Genomics starts March 15.\nCase Studies in Functional Genomics start April 15.\nThe landing page for the series continues to be here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:20:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-09-23-data-analysis-for-the-life-sciences-a-book-completely-written-in-r-markdown/",
    "title": "Data Analysis for the Life Sciences - a book completely written in R markdown",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-09-23",
    "categories": [],
    "contents": "\nThe book Data Analysis for the Life Sciences is now available on Leanpub.\n\nData analysis is now part of practically every research project in the life sciences. In this book we use data and computer code to teach the necessary statistical concepts and programming skills to become a data analyst. Following in the footsteps of Stat Labs, instead of showing theory first and then applying it to toy examples, we start with actual applications and describe the theory as it becomes necessary to solve specific challenges. We use simulations and data analysis examples to teach statistical concepts. The book includes links to computer code that readers can use to program along as they read the book.\nIt includes the following chapters: Inference, Exploratory Data Analysis, Robust Statistics, Matrix Algebra, Linear Models, Inference for High-Dimensional Data, Statistical Modeling, Distance and Dimension Reduction, Practical Machine Learning, and Batch Effects.\nThe text was completely written in R markdown and every section contains a link to the document that was used to create that section. This means that you can use knitr to reproduce any section of the book on your own computer. You can also access all these markdown documents directly from GitHub. Please send a pull request if you fix a typo or other mistake! For now we are keeping the R markdowns for the exercises private since they contain the solutions. But you can see the solutions if you take our online course quizzes. If we find that most readers want access to the solutions, we will open them up as well.\nThe material is based on the online courses I have been teaching with Mike Love. As we created the course, Mike and I wrote R markdown documents for the students and put them on GitHub. We then usedjekyll to create a webpage with html versions of the markdown documents. Jeff then convinced us to publish it on LeanbupLeanpub. So we wrote a shell script that compiled the entire book into a Leanpub directory, and after countless hours of editing and tinkering we have a 450+ page book with over 200 exercises. The entire book compiles from scratch in about 20 minutes. We hope you like it.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-09-18-the-leek-group-guide-to-writing-your-first-paper/",
    "title": "The Leek group guide to writing your first paper",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-09-18",
    "categories": [],
    "contents": "\n\n\nThe @jtleek guide to writing your first academic paper https://t.co/APLrEXAS46\n\n\n— Stephen Turner (@genetics_blog) September 17, 2015\n\n\nI have written guides on reviewing papers, sharing data,  and writing R packages. One thing I haven’t touched on until now has been writing papers. Certainly for me, and I think for a lot of students, the hardest transition in graduate school is between taking classes and doing research.\nThere are several hard parts to this transition including trying to find a problem, trying to find an advisor, and having a ton of unstructured time. One of the hardest things I’ve found is knowing (a) when to start writing your first paper and (b) how to do it. So I wrote a guide for students in my group:\nhttps://github.com/jtleek/firstpaper\nOn how to write your first paper. It might be useful for other folks as well so I put it up on Github. Just like with the other guides I’ve written this is a very opinionated (read: doesn’t apply to everyone) guide. I also would appreciate any feedback/pull requests people have.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-09-17-not-so-standard-deviations-the-podcast/",
    "title": "Not So Standard Deviations: The Podcast",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-09-17",
    "categories": [],
    "contents": "\nI’m happy to announce that I’ve started a brand new podcast called Not So Standard Deviations with Hilary Parker at Etsy. Episode 1 “RCatLadies Origin Story” is available through SoundCloud. In this episode we talk about the origins of RCatLadies, evidence-based data analysis, my new book, and the Python vs. R debate.\nYou can subscribe to the podcast using the RSS feed from SoundCloud. We’ll be getting it up on iTunes hopefully very soon.\nDownload the audio file.\nShow Notes:\nRCatLadies Twitter account\nHilary’s analysis of the name Hilary\nThe Art of Data Science\nWhat is JSM?\nA rising tide lifts all boats\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-08-25-interview-with-copss-award-winner-john-storey/",
    "title": "Interview with COPSS award Winner John Storey",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-08-25",
    "categories": [],
    "contents": "\nJohn Storey PictureEditor’s Note: We are again pleased to interview the COPSS President’s award winner. The COPSS Award is one of the most prestigious in statistics, sometimes called the Nobel Prize in statistics. This year the award went to John Storey who also won the Mortimer Spiegelman award for his outstanding contribution to public health statistics. This interview is a particular pleasure since John was my Ph.D. advisor and has been a major role model and incredibly supportive mentor for me throughout my career. He also did the whole interview in markdown and put it under version control at Github so it is fully reproducible.\nSimplyStats: Do you consider yourself to be a statistician, data scientist, machine learner, or something else?\nJS: For the most part I consider myself to be a statistician, but I’m also very serious about genetics/genomics, data analysis, and computation. I was trained in statistics and genetics, primarily statistics. I was also exposed to a lot of machine learning during my training since Rob Tibshirani was my PhD advisor. However, I consider my research group to be a data science group. We have the Venn diagram reasonably well covered: experimentalists, programmers, data wranglers, and developers of theory and methods; biologists, computer scientists, and statisticians.\nSimplyStats: How did you find out you had won the COPSS Presidents’ Award?\nJS: I received a phone call from the chairperson of the awards committee while I was visiting the Department of Statistical Science at Duke University to give a seminar. It was during the seminar reception, and I stepped out into the hallway to take the call. It was really exciting to get the news!\nSimplyStats: One of the areas where you have had a big impact is inference in massively parallel problems. How do you feel high-dimensional inference is different from more traditional statistical inference?\nJS: My experience is that the most productive way to approach high-dimensional inference problems is to first think about a given problem in the scenario where the parameters of interest are random, and the joint distribution of these parameters is incorporated into the framework. In other words, I first gain an understanding of the problem in a Bayesian framework. Once this is well understood, it is sometimes possible to move in a more empirical and nonparametric direction. However, I have found that I can be most successful if my first results are in this Bayesian framework.\nAs an example, Theorem 1 from Storey (2003) Annals of Statistics was the first result I obtained in my work on false discovery rates. This paper first appeared as a technical report in early 2001, and the results spawned further work on a point estimation approach to false discovery rates, the local false discovery rate, q-value and its application to genomics, and a unified theoretical framework.\nBesides false discovery rates, this approach has been useful in my work on the optimal discovery procedure as well as surrogate variable analysis (in particular, Desai and Storey 2012 for surrogate variable analysis).  For high-dimensional inference problems, I have also found it is important to consider whether there are any plausible underlying causal relationships among variables, even if causal inference in not the goal. For example, causal model considerations provided some key guidance in a recent paper of ours on testing for genetic associations in the presence of arbitrary population structure. I think there is a lot of insight to be gained by considering what is the appropriate approach for a high-dimensional inference problem under different causal relationships among the variables.\nSimplyStats: Do you have a process when you are tackling a hard problem or working with students on a hard problem?\nJS: I like to work on statistics research that is aimed at answering a specific scientific problem (usually in genomics). My process is to try to understand the why in the problem as much as the how. The path to success is often found in the former. I try first to find solutions to research problems by using simple tools and ideas. I like to get my hands dirty with real data as early as possible in the process. I like to incorporate some theory into this process, but I prefer methods that work really well in practice over those that have beautiful theory justifying them without demonstrated success on real-world applications. In terms of what I do day-to-day, listening to music is integral to my process, for both concentration and creative inspiration: typically King Crimson or some variant of metal or ambient – which Simply Statistics co-founder](http://jtleek.com/) got to endure enjoy for years during his PhD in my lab.\nSimplyStats: You are the founding Director of the Center for Statistics and Machine Learning at Princeton. What parts of the new gig are you most excited about?\nJS: Princeton closed its Department of Statistics in the early 1980s. Because of this, the style of statistician and machine learner we have here today is one who’s comfortable being appointed in a field outside of statistics or machine learning. Examples include myself in genomics, Kosuke Imai in political science, Jianqing Fan in finance and economics, and Barbara Engelhardt in computer science. Nevertheless, statistics and machine learning here is strong, albeit too small at the moment (which will be changing soon). This is an interesting place to start, very different from most universities.\nWhat I’m most excited about is that we get to answer the question: “What’s the best way to build a faculty, educate undergraduates, and create a PhD program starting now, focusing on the most important problems of today?”\nFor those who are interested, we’ll be releasing a public version of our strategic plan within about six months. We’re trying to do something unique and forward-thinking, which will hopefully make Princeton an influential member of the statistics, machine learning, and data science communities.\nSimplyStats: You are organizing the Tukey conference at Princeton (to be held September 18, details here). Do you think Tukey’s influence will affect your vision for re-building statistics at Princeton?\nJS: Absolutely, Tukey has been and will be a major influence in how we re-build. He made so many important contributions, and his approach was extremely forward thinking and tied into real-world problems. I strongly encourage everyone to read Tukey’s 1962 paper titled The Future of Data Analysis. Here he’s 50 years into the future, foreseeing the rise of data science. This paper has truly amazing insights, including:\n\nFor a long time I have thought I was a statistician, interested in inferences from the particular to the general. But as I have watched mathematical statistics evolve, I have had cause to wonder and to doubt.\nAll in all, I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.\nData analysis is a larger and more varied field than inference, or incisive procedures, or allocation.\nBy and large, the great innovations in statistics have not had correspondingly great effects upon data analysis. . . . Is it not time to seek out novelty in data analysis?\n\nIn this regard, another paper that has been influential in how we are re-building is Leo Breiman’s titled Statistical Modeling: The Two Cultures. We’re building something at Princeton that includes both cultures and seamlessly blends them into a bigger picture community concerned with data-driven scientific discovery and technology development.\nSimplyStats: What advice would you give young statisticians getting into the discipline now?\nJS: My most general advice is don’t isolate yourself within statistics. Interact with and learn from other fields. Work on problems that are important to practitioners of science and technology development. I recommend that students should master both “traditional statistics” and at least one of the following: (1) computational and algorithmic approaches to data analysis, especially those more frequently studied in machine learning or data science; (2) a substantive scientific area where data-driven discovery is extremely important (e.g., social sciences, economics, environmental sciences, genomics, neuroscience, etc.). I also recommend that students should consider publishing in scientific journals or computer science conference proceedings, in addition to traditional statistics journals. I agree with a lot of the constructive advice and commentary given on the Simply Statistics blog, such as encouraging students to learn about reproducible research, problem-driven research, software development, improving data analyses in science, and outreach to non-statisticians. These things are very important for the future of statistics.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-08-24-the-next-national-library-of-medicine-director-can-help-define-the-future-of-data-science/",
    "title": "The Next National Library of Medicine Director Can Help Define the Future of Data Science",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-08-24",
    "categories": [],
    "contents": "\nThe main motivation for starting this blog was to share our enthusiasm about the increased importance of data and data analysis in science, industry, and society in general. Based on recent initiatives, such as BD2k, it is clear that the NIH is also enthusiastic and very much interested in supporting data science. For those that don’t know, the National Institutes of Health (NIH) is the largest public funder of biomedical research in the world. This federal agency has an annual budget of about $30 billion.\nThe NIH has several institutes, each with its own budget and capability to guide funding decisions. Currently, the missions of most of these institutes relate to a specific disease or public health challenge.  Many of them fund research in statistics and computing because these topics are important components of achieving their specific mission. Currently, however, there is no institute directly tasked with supporting data science per se. This is about to change.\nThe National Library of Medicine (NLM) is one of the few NIH institutes that is not focused on a particular disease or public health challenge. Apart from the important task of maintaining an actual library, it supports, among many other initiatives, indispensable databases such as PubMed, GeneBank and GEO. After over 30 years of successful service as NLM director, Dr. Donald Lindberg stepped down this year and, as is customary, an advisory board was formed to advice the NIH on what’s next for NLM. One of the main recommendations of the report is the following:\n\nNLM  should be the intellectual and programmatic epicenter for data science at NIH and stimulate its advancement throughout biomedical research and application.\n\nData science features prominently throughout the report making it clear the NIH is very much interested in further supporting this field. The next director can therefore have an enormous influence in the futre of data science. So, if you love data, have administrative experience, and a vision about the future of data science as it relates to the medical and related sciences, consider this exciting opportunity.\nHere is the ad.\n \n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-08-21-interview-with-sherri-rose-and-laura-hatfied/",
    "title": "Interview with Sherri Rose and Laura Hatfield",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-08-21",
    "categories": [],
    "contents": "\n \n\n\nRose/Hatfield © Savannah Bergquist\n\nLaura Hatfield and Sherri Rose are Assistant Professors specializing in biostatistics at Harvard Medical School in the Department of Health Care Policy. Laura received her PhD in Biostatistics from the University of Minnesota and Sherri completed her PhD in Biostatistics at UC Berkeley. They are developing novel statistical methods for health policy problems.\nSimplyStats: Do you consider yourselves statisticians, data scientists, machine learners, or something else?\nRose: I’d definitely say a statistician. Even when I’m working on things that fall into the categories of data science or machine learning, there’s underlying statistical theory guiding that process, be it for methods development or applications. Basically, there’s a statistical foundation to everything I do.\nHatfield: When people ask what I do, I start by saying that I do research in health policy. Then I say I’m a statistician by training and I work with economists and physicians. People have mistaken ideas about what a statistician or professor does, so describing my context and work seems more informative. If I’m at a party, I usually wrap it up in a bow as, “I crunch numbers to study how Obamacare is working.” [laughs]\n \nSimplyStats: What is the Health Policy Data Science Lab? How did you decide to start that?\nHatfield: We wanted to give our trainees a venue to promote their work and get feedback from their peers. And it helps me keep up on the cool projects Sherri and her students are working on.\nRose: This grew out of us starting to jointly mentor trainees. It’s been a great way for us to make intellectual contributions to each other’s work through Lab meetings. Laura and I approach statistics from completely different frameworks, but work on related applications, so that’s a unique structure for a lab.\n \nSimplyStats: What kinds of problems are your groups working on these days? Are they mostly focused on health policy?\nRose: One of the fun things about working in health policy is that it is quite expansive. Statisticians can have an even bigger impact on science and public health if we take that next step: thinking about the policy implications of our research. And then, who needs to see the work in order to influence relevant policies. A couple projects I’m working on that demonstrate this breadth include a machine learning framework for risk adjustment in insurance plan payment and a new estimator for causal effects in a complex epidemiologic study of chronic disease. The first might be considered more obviously health policy, but the second will have important policy implications as well.\nHatfield: When I start an applied collaboration, I’m also thinking, “Where is the methods paper?” Most of my projects use messy observational data, so there is almost always a methods paper. For example, many studies here need to find a control group from an administrative data source. I’ve been keeping track of challenges in this process. One of our Lab students is working with me on a pathological case of a seemingly benign control group selection method gone bad. I love the creativity required in this work; my first 10 analysis ideas may turn out to be infeasible given the data, but that’s what makes this fun!\n \nSimplyStats: What are some particular challenges of working with large health data?\nHatfield: When I first heard about the huge sample sizes, I was excited! Then I learned that data not collected for research purposes…\nRose: This was going to be my answer!\nHatfield: …are very hard to use for research! In a recent project, I’ve been studying how giving people a tool to look up prices for medical services changes their health care spending. But the data set we have leaves out [painful pause] a lot of variables we’d like to use for control group selection and… a lot of the prices. But as I said, these gaps in the data are begging to be filled by new methods.\nRose: I think the fact that we have similar answers is important. I’ve repeatedly seen “big data” not have a strong signal for the research question, since they weren’t collected for that purpose. It’s easy to get excited about thousands of covariates in an electronic health record, but so much of it is noise, and then you end up with an R2 of 10%. It can be difficult enough to generate an effective prediction function, even with innovative tools, let alone try to address causal inference questions. It goes back to basics: what’s the research question and how can we translate that into a statistical problem we can answer given the limitations of the data.\nSimplyStats: You both have very strong data science skills but are in academic positions. Do you have any advice for students considering the tradeoff between academia and industry?\nHatfield: I think there is more variance within academia and within industry than between the two.\nRose: Really? That’s surprising to me…\nHatfield: I had stereotypes about academic jobs, but my current job defies those.\nRose: What if a larger component of your research platform included programming tools and R packages? My immediate thought was about computing and its role in academia. Statisticians in genomics have navigated this better than some other areas. It can surely be done, but there are still challenges folding that into an academic career.\nHatfield: I think academia imposes few restrictions on what you can disseminate compared to industry, where there may be more privacy and intellectual property concerns. But I take your point that R packages do not impress most tenure and promotion committees.\nRose: You want to find a good match between how you like spending your time and what’s rewarded. Not all academic jobs are the same and not all industry jobs are alike either. I wrote a more detailed guest post on this topic for Simply Statistics.\nHatfield: I totally agree you should think about how you’d actually spend your time in any job you’re considering, rather than relying on broad ideas about industry versus academia. Do you love writing? Do you love coding? etc.\n \nSimplyStats: You are both adopters of social media as a mechanism of disseminating your work and interacting with the community. What do you think of social media as a scientific communication tool? Do you find it is enhancing your careers?\nHatfield: Sherri is my social media mentor!\nRose: I think social media can be a useful tool for networking, finding and sharing neat articles and news, and putting your research out there to a broader audience. I’ve definitely received speaking invitations and started collaborations because people initially “knew me from Twitter.” It’s become a way to recruit students as well. Prospective students are more likely to “know me” from a guest post or Twitter than traditional academic products, like journal articles.\nHatfield: I’m grateful for our Lab’s new Twitter because it’s a purely academic account. My personal account has been awkwardly transitioning to include professional content; I still tweet silly things there.\nRose: My timeline might have a cat picture or two.\nHatfield: My very favorite thing about academic Twitter is discovering things I wouldn’t have even known to search for, especially packages and tricks in R. For example, that’s how I got converted to tidy data and dplyr.\nRose: I agree. I think it’s a fantastic place to become exposed to work that’s incredibly related to your own but in another field, and you wouldn’t otherwise find it preparing a typical statistics literature review.\n \nSimplyStats: What would you change in the statistics community?\nRose: Mentoring. I was tremendously lucky to receive incredible mentoring as a graduate student and now as a new faculty member. Not everyone gets this, and trainees don’t know where to find guidance. I’ve actively reached out to trainees during conferences and university visits, erring on the side of offering too much unsolicited help, because I feel there’s a need for that. I also have a resources page on my website that I continue to update. I wish I had a more global solution beyond encouraging statisticians to take an active role in mentoring not just your own trainees. We shouldn’t lose good people because they didn’t get the support they needed.\nHatfield: I think we could make conferences much better! Being in the same physical space at the same time is very precious. I would like to take better advantage of that at big meetings to do work that requires face time. Talks are not an example of this. Workshops and hackathons and panels and working groups – these all make better use of face-to-face time. And are a lot more fun!\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-08-20-if-you-ask-different-quetions-you-get-different-asnwers-one-more-way-science-isnt-broken-it-is-just-really-hard/",
    "title": "If you ask different questions you get different answers - one more way science isn't broken it is just really hard",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-08-20",
    "categories": [],
    "contents": "\nIf you haven’t already read the amazing piece by Christie Aschwanden on why Science isn’t Broken you should do so immediately. It does an amazing job of capturing the nuance of statistics as applied to real data sets and how that can be misconstrued as science being “broken” without falling for the easy “everything is wrong” meme.\nOne thing that caught my eye was how the piece highlighted a crowd-sourced data analysis of soccer red cards. The key figure for that analysis is this one:\n \n\nI think the figure and underlying data for this figure are fascinating in that they really highlight the human behavioral variation in data analysis and you can even see some data analysis subcultures emerging from the descriptions of how people did the analysis and justified or not the use of covariates.\nOne subtlety of the figure that I missed on the original reading is that not all of the estimates being reported are measuring the same thing. For example, if some groups adjusted for the country of origin of the referees and some did not, then the estimates for those two groups are measuring different things (the association conditional on country of origin or not, respectively). In this case the estimates may be different, but entirely consistent with each other, since they are just measuring different things.\nIf you ask two people to do the analysis and you only ask them the simple question: Are referees more likely to give  red cards to dark skinned players? then you may get a different answer based on those two estimates. But the reality is the answers the analysts are reporting are actually to the questions:\nAre referees more likely to give  red cards to dark skinned players holding country of origin fixed?\nAre referees more likely to give  red cards to dark skinned players averaging over country of origin (and everything else)?\nThe subtlety lies in the fact that changes to covariates in the analysis are actually changing the hypothesis you are studying.\nSo in fact the conclusions in that figure may all be entirely consistent after you condition on asking the same question. I’d be interested to see the same plot, but only for the groups that conditioned on the same set of covariates, for example. This is just one more reason that science is really hard and why I’m so impressed at how well the FiveThirtyEight piece captured this nuance.\n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-08-19-p-0-05-i-can-make-any-p-value-statistically-significant-with-adaptive-fdr-procedures/",
    "title": "P > 0.05? I can make any p-value statistically significant with adaptive FDR procedures",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-08-19",
    "categories": [],
    "contents": "\nEveryone knows now that you have to correct for multiple testing when you calculate many p-values otherwise this can happen:\n\n\n\nhttp://xkcd.com/882/\n\n\n \nOne of the most popular ways to correct for multiple testing is to estimate or control the false discovery rate. The false discovery rate attempts to quantify the fraction of made discoveries that are false. If we call all p-values less than some threshold t significant, then borrowing notation from this great introduction to false discovery rates \n\n \nSo F(t) is the (unknown) total number of null hypotheses called significant and S(t) is the total number of hypotheses called significant. The FDR is the expected ratio of these two quantities, which, under certain assumptions can be approximated by the ratio of the expectations.\n \n\n \nTo get an estimate of the FDR we just need an estimate for  E[_F(t)] _ and _E[S(t)]. _The latter is pretty easy to estimate as just the total number of rejections (the number of p < t). If you assume that the p-values follow the expected distribution then E[_F(t)]  _can be approximated by multiplying the fraction of null hypotheses, multiplied by the total number of hypotheses and multiplied by t since the p-values are uniform. To do this, we need an estimate for , the proportion of null hypotheses. There are a large number of ways to estimate this quantity but it is almost always estimated using the full distribution of computed p-values in an experiment. The most popular estimator compares the fraction of p-values greater than some cutoff to the number you would expect if every single hypothesis were null. This fraction is about the fraction of null hypotheses.\nCombining the above equation with our estimates for E[_F(t)] _ and _E[S(t)] _we get:\n \n\n \nThe q-value is a multiple testing analog of the p-value and is defined as:\n\n \nThis is of course a very loose version of this and you can get a more technical description here. But the main thing to notice is that the q-value depends on the estimated proportion of null hypotheses, which depends on the distribution of the observed p-values. The smaller the estimated fraction of null hypotheses, the smaller the FDR estimate and the smaller the q-value. This suggests a way to make any p-value significant by altering its “testing partners”. Here is a quick example. Suppose that we have done a test and have a p-value of 0.8. Not super significant. Suppose we perform this test in conjunction with a number of hypotheses that are null generating a p-value distribution like this.\n\nThen you get a q-value greater than 0.99 as you would expect. But if you test that exact same p-value with a ton of other non-null hypotheses that generate tiny p-values in a distribution that looks like this:\n\n \nThen you get a q-value of 0.0001 for that same p-value of 0.8. The reason is that the estimate of the fraction of null hypotheses goes essentially to zero, which drives down the q-value. You can do this with any p-value, if you make its testing partners have sufficiently low p-values then the q-value will also be as small as you like.\nA couple of things to note:\nObviously doing this on purpose to change the significance of a calculated p-value is cheating and shouldn’t be done.\nFor correctly calculated p-values on a related set of hypotheses this is actually a sensible property to have - if you have almost all very small p-values and one very large p-value, you are doing a set of tests where almost everything appears to be alternative and you should weight that in some sensible way.\nThis is the reason that sometimes a “multiple testing adjusted” p-value (or q-value) is smaller than the p-value itself.\nThis doesn’t affect non-adaptive FDR procedures - but those procedures still depend on the “testing partners” of any p-value through the total number of tests performed. This is why people talk about the so-called “multiple testing burden”. But that is a subject for a future post. It is also the reason non-adaptive procedures can be severely underpowered compared to adaptive procedures when the p-values are correct.\nI’ve appended the code to generate the histograms and calculate the q-values in this post in the following gist.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-08-12-correlation-is-not-a-measure-of-reproducibility/",
    "title": "Correlation is not a measure of reproducibility",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-08-12",
    "categories": [],
    "contents": "\nBiologists make wide use of correlation as a measure of reproducibility. Specifically, they quantify reproducibility with the correlation between measurements obtained from replicated experiments. For example, the ENCODE data standards document states\n\nA typical R2 (Pearson) correlation of gene expression (RPKM) between two biological replicates, for RNAs that are detected in both samples using RPKM or read counts, should be between 0.92 to 0.98. Experiments with biological correlations that fall below 0.9 should be either be repeated or explained.\n\nHowever, for reasons I will explain here, correlation is not necessarily informative with regards to reproducibility. The mathematical results described below are not inconsequential theoretical details, and understanding them will help you assess new technologies, experimental procedures and computation methods.\nSuppose you have collected data from an experiment\n\nx1, x2,…, xn\nand want to determine if a second experiment replicates these findings. For simplicity, we represent data from the second experiment as adding unbiased (averages out to 0) and statistically independent measurement error d to the first:\n\ny1=x1+d1, y2=x2+d2, … yn=xn+dn.\n\nFor us to claim reproducibility we want the differences\n\nd1=y1-x1, d2=y2-x2,… ,dn=yn-xn\nto be “small”. To give this some context, imagine the x and y are log scale (base 2) gene expression measurements which implies the d represent log fold changes. If these differences have a standard deviation of 1, it implies that fold changes of 2 are typical between replicates. If our replication experiment produces measurements that are typically twice as big or twice as small as the original, I am not going to claim the measurements are reproduced. However, as it turns out, such terrible reproducibility can still result in correlations higher than 0.92.\nTo someone basing their definition of correlation on the current common language usage this may seem surprising, but to someone basing it on math, it is not. To see this, note that the mathematical definition of correlation tells us that because d and x are independent:\n\nThis tells us that correlation summarizes the variability of d relative to the variability of x. Because of the wide range of gene expression values we observe in practice, the standard deviation of x can easily be as large as 3 (variance is 9). This implies we expect to see correlations as high as 1/sqrt(1+1/9) = 0.95, despite the lack of reproducibility when comparing x to y.\nNote that using Spearman correlation does not fix this problem. A Spearman correlation of 1 tells us that the ranks of x and y are preserved, yet doest not summarize the actual differences. The problem comes down to the fact that we care about the variability of d and correlation, Pearson or Spearman, does not provide an optimal summary. While correlation relates to the preservation of ranks, a much more appropriate summary of reproducibly is the distance between x and y which is related to the standard deviation of the differences d. A very simple R command you can use to generate this summary statistic is:\nsqrt(mean(d^2))\nor the robust version:\nmedian(abs(d)) ##multiply by 1.4826 for unbiased estimate of true sd\n\nThe equivalent suggestion for plots it to make an MA-plot instead of a scatterplot.\nBut aren’t correlations and distances directly related? Sort of, and this actually brings up another problem. If the x and y are standardized to have average 0 and standard deviation 1 then, yes, correlation and distance are directly related:\n\nHowever, if instead x and y have different average values, which would put into question reproducibility, then distance is sensitive to this problem while correlation is not. If the standard devtiation is 1, the formula is:\n \n\nOnce we consider units (standard deviations different from 1) then the relationship becomes even more complicated. Two advantages of distance you should be aware of are:\nit is in the same units as the data, while correlations have no units making it hard to interpret and select thresholds, and\ndistance accounts for bias (differences in average), while correlation does not.\nA final important point relates to the use of correlation with data that is not approximately normal. The useful interpretation of correlation as a summary statistic stems from the bivariate normal approximation: for every standard unit increase in the first variable, the second variable increased r standard units, with r the correlation. A summary of this is here. However, when data is not normal this interpretation no longer holds. Furthermore, heavy tail distributions, which are common in genomics, can lead to instability. Here is an example of uncorrelated data with a single pointed added that leads to correlations close to 1. This is quite common with RNAseq data.\n\n \n\n\n\n",
    "preview": "https://raw.githubusercontent.com/simplystats/simplystats.github.io/master/wp-content/uploads/2015/08/pearsonformula.png",
    "last_modified": "2021-11-11T14:19:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-08-12-ucla-statistics-2015-commencement-address/",
    "title": "UCLA Statistics 2015 Commencement Address",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-08-12",
    "categories": [],
    "contents": "\nI was asked to speak at the UCLA Department of Statistics Commencement Ceremony this past June. As one of the first graduates of that department back in 2003, I was tremendously honored to be invited to speak to the graduates. When I arrived I was just shocked at how much the department had grown. When I graduated I think there were no more than 10 of us between the PhD and Master’s programs. Now they have ~90 graduates per year with undergrad, Master’s and PhD. It was just stunning.\nHere’s the text of what I said, which I think I mostly stuck to in the actual speech.\n \nUCLA Statistics Graduation: Some thoughts on a career in statistics\nWhen I asked Rick [Schoenberg] what I should talk about, he said to ’talk for 95 minutes on asymptotic properties of maximum likelihood estimators under nonstandard conditions”. I thought this is a great opportunity! I busted out Tom Ferguson’s book and went through my old notes. Here we go. Let X be a complete normed vector space….\nI want to thank the department for inviting me here today. It’s always good to be back. I entered the UCLA stat department in 1999, only the second entering class, and graduated from UCLA Stat in 2003. Things were different then. Jan was the chair and there were not many classes so we could basically do whatever we wanted. Things are different now and that’s a good thing. Since 2003, I’ve been at the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health, where I was first a postdoctoral fellow and then joined the faculty. It’s been a wonderful place for me to grow up and I’ve learned a lot there.\nIt’s just an incredible time to be a statistician. You guys timed it just right. I’ve been lucky enough to witness two periods like this, the first time being when I graduated from college at the height of the dot come boom. Today, it’s not computer programming skills that the world needs, but rather it’s statistical skills. I wish I were in your shoes today, just getting ready to startup. But since I’m not, I figured the best thing I could do is share some of the things I’ve learned and talk about the role that these things have played in my own life.\nKnow your edge: What’s the one thing that you know that no one else seems to know? You’re not a clone—you have original ideas and skills. You might think they’re not valuable but you’re wrong. Be proud of these ideas and use them to your advantage. As an example, I’ll give you my one thing. Right now, I believe the greatest challenge facing the field of statistics today is getting the entire world to know what we in this room already know. Data are everywhere today and the biggest barrier to progress is our collective inability to process and analyze those data to produce useful information. The need for the things that we know has absolutely exploded and we simply have not caught up. That’s why I created, along with Jeff Leek and Brian Caffo, the Johns Hopkins Data Science Specialization, which is currently the most successful massive open online course program ever. Our goal is to teach the entire world statistics, which we think is an essential skill. We’re not quite there yet, but—assuming you guys don’t steal my idea—I’m hopeful that we’ll get there sometime soon.\nAt some point the edge you have will no longer work: That sounds like a bad thing, but it’s actually good. If what you’re doing really matters, then at some point everyone will be doing it. So you’ll need to find something else. I’ve been confronted with this problem at least 3 times in my life so far. Before college, I was pretty good at the violin, and it opened a lot of doors for me. It got me into Yale. But when I got to Yale, I quickly realized that there were a lot of really good violinists here. Suddenly, my talent didn’t have so much value. This was when I started to pick up computer programming and in 1998 I learned an obscure little language called R. When I got to UCLA I realized I was one of the only people who knew R. So I started a little brown bag lunch series where I’d talk about some feature of R to whomever would show up (which wasn’t many people usually). Picking up on R early on turned out to be really important because it was a small community back then and it was easy to have a big impact. Also, as more and more people wanted to learn R, they’d usually call on me. It’s always nice to feel needed. Over the years, the R community exploded and R’s popularity got to the point where it was being talked about in the New York Times. But now you see the problem. Saying that you know R doesn’t exactly distinguish you anymore, so it’s time to move on again. These days, I’m realizing that the one useful skill that I have is the ability to make movies. Also, my experience being a performer on the violin many years ago is coming in handy. My ability to quickly record and edit movies was one of the key factors that enabled me to create an entire online data science program in 2 months last year.\nFind the right people, and stick with them forever. Being a statistician means working with other people. Choose those people wisely and develop a strong relationship. It doesn’t matter how great the project is or how famous or interesting the other person is, if you can’t get along then bad things will happen. Statistics and data analysis is a highly verbal process that requires constant and very clear communication. If you’re uncomfortable with someone in any way, everything will suffer. Data analysis is unique in this way—our success depends critically on other people. I’ve only had a few collaborators in the past 12 years, but I love them like family. When I work with these people, I don’t necessarily know what will happen, but I know it will be good. In the end, I honestly don’t think I’ll remember the details of the work that I did, but I’ll remember the people I worked with and the relationships I built.\nSo I hope you weren’t expecting a new asymptotic theorem today, because this is pretty much all I’ve got. As you all go on to the next phase of your life, just be confident in your own ideas, be prepared to change and learn new things, and find the right people to do them with. Thank you.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-08-10-rafalib-package-now-on-cran/",
    "title": "rafalib package now on CRAN",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-08-10",
    "categories": [],
    "contents": "\nFor the last several years I have been collecting functions I routinely use during exploratory data analysis in a private R package. Mike Love and I used some of these in our HarvardX course and now, due to popular demand, I have created man pages and added the rafalib package to CRAN. Mike has made several improvements and added some functions of his own. Here is quick descriptions of the rafalib functions I most use:\nmypar - Before making a plot in R I almost always type mypar(). This basically gets around the suboptimal defaults of par. For example, it makes the margins (mar, mpg) smaller and defines RColorBrewer colors as defaults.  It is optimized for the RStudio window. Another advantage is that you can type mypar(3,2) instead of par(mfrow=c(3,2)). bigpar() is optimized for R presentations or PowerPoint slides.\nas.fumeric - This function turns characters into factors and then into numerics. This is useful, for example, if you want to plot values x,y with colors defined by their corresponding categories saved in a character vector labsplot(x,y,col=as.fumeric(labs)).\nshist (smooth histogram, pronounced shitz) - I wrote this function because I have a hard time interpreting the y-axis of density. The height of the curve drawn by shist can be interpreted as the height of a histogram if you used the units shown on the plot. Also, it automatically draws a smooth histogram for each entry in a matrix on the same plot.\nsplot (subset plot) - The datasets I work with are typically large enough that\nplot(x,y) involves millions of points, which is a problem. Several solution are available to avoid over plotting, such as alpha-blending, hexbinning and 2d kernel smoothing. For reasons I won’t explain here, I generally prefer subsampling over these solutions. splot automatically subsamples. You can also specify an index that defines the subset.\nsboxplot (smart boxplot) - This function draws points, boxplots or outlier-less boxplots depending on sample size. Coming soon is the kaboxplot (Karl Broman box-plots) for when you have too many boxplots.\ninstall_bioc - For Bioconductor users, this function simply does the source(“http://www.bioconductor.org/biocLite.R”) for you and then uses BiocLite to install.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-08-09-interested-in-analyzing-images-of-brains-get-started-with-open-access-data/",
    "title": "Interested in analyzing images of brains? Get started with open access data.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-08-09",
    "categories": [],
    "contents": "\n\nEditor’s note: This is a guest post by Ani Eloyan. She is an Assistant Professor of Biostatistics at Brown University. Dr. Eloyan’s work focuses on semi-parametric likelihood based methods for matrix decompositions, statistical analyses of brain images, and the integration of various types of complex data structures for analyzing health care data. She received her PhD in statistics from North Carolina State University and subsequently completed a postdoctoral fellowship in the Department of Biostatistics at Johns Hopkins University. Dr. Eloyan and her team won the ADHD200 Competition discussed in this article. She tweets @eloyan_ani.\n\n\n \n\n\n\nNeuroscience is one of the exciting new fields for biostatisticians interested in real world applications where they can contribute novel statistical approaches. Most research in brain imaging has historically included studies run for small numbers of patients. While justified by the costs of data collection, the claims based on analyzing data for such small numbers of subjects often do not hold for our populations of interest. As discussed in <a href=\"http://www.huffingtonpost.com/american-statistical-association/wanted-neuroquants_b_3749363.html\" target=\"_blank\">this<\/a> article, there is a huge demand for biostatisticians in the field of quantitative neuroscience; so called neuroquants or neurostatisticians. However, while more statisticians are interested in the field, we are far from competing with other substantive domains. For instance, a quick search of abstract keywords in the online program of the upcoming <a href=\"https://www.amstat.org/meetings/jsm/2015/\" target=\"_blank\">JSM2015<\/a> conference of “brain imaging” and “neuroscience” results in 15 records, while a search of the words “genomics” and “genetics” generates 76 <a>records<\/a>.\n\n\n\n\n\nAssuming you are trained in statistics and an aspiring neuroquant, how would you go about working with brain imaging data? As a graduate student in the <a href=\"http://www.stat.ncsu.edu/\" target=\"_blank\">Department of Statistics at NCSU<\/a> several years ago, I was very interested in working on statistical methods that would be directly applicable to solve problems in neuroscience. But I had this same question: “Where do I find the data?” I soon learned that to <i>really<\/i>approach substantial relevant problems I also needed to learn about the subject matter underlying these complex data structures.\n\n\n\n\n\nIn recent years, several leading groups have uploaded their lab data with the common goal of fostering the collection of high dimensional brain imaging data to build powerful models that can give generalizable results. <a href=\"http://www.nitrc.org/\" target=\"_blank\">Neuroimaging Informatics Tools and Resources Clearinghouse (NITRC)<\/a> founded in 2006 is a platform for public data sharing that facilitates streamlining data processing pipelines and compiling high dimensional imaging datasets for crowdsourcing the analyses. It includes data for people with neurological diseases and neurotypical children and adults. If you are interested in Alzheimer’s disease, you can check out <a href=\"http://adni.loni.usc.edu/\" target=\"_blank\">ADNI<\/a>. <a href=\"http://fcon_1000.projects.nitrc.org/indi/abide/\" target=\"_blank\">ABIDE<\/a> provides data for people with Autism Spectrum Disorder and neurotypical peers. <a href=\"http://fcon_1000.projects.nitrc.org/indi/adhd200/\" target=\"_blank\">ADHD200<\/a> was released in 2011 as a part of a competition to motivate building predictive methods for disease diagnoses using functional magnetic resonance imaging (MRI) in addition to demographic information to predict whether a child has attention deficit hyperactivity disorder (ADHD). While the competition ended in 2011, the dataset has been widely utilized afterwards in studies of ADHD.  According to Google Scholar, the <a href=\"http://www.nature.com/mp/journal/v19/n6/abs/mp201378a.html\" target=\"_blank\">paper<\/a> introducing the ABIDE set has been cited 129 times since 2013 while the <a href=\"http://journal.frontiersin.org/article/10.3389/fnsys.2012.00062/full\" target=\"_blank\">paper<\/a> discussing the ADHD200 has been cited 51 times since <span style=\"font-family: Arial;\">2012. These are only a few examples from the list of open access datasets that could of utilized by statisticians. <\/span>\n\n\n\n\n\nAnyone can download these datasets (you may need to register and complete some paperwork in some cases), however, there are several data processing and cleaning steps to perform before the final statistical analyses. These preprocessing steps can be daunting for a statistician new to the field, especially as the tools used for preprocessing may not be available in R. <a href=\"https://hopstat.wordpress.com/2014/08/27/statisticians-in-neuroimaging-need-to-learn-preprocessing/\" target=\"_blank\">This<\/a> discussion makes the case as to why statisticians need to be involved in every step of preprocessing the data, while <u><a href=\"https://hopstat.wordpress.com/2014/06/17/fslr-an-r-package-interfacing-with-fsl-for-neuroimaging-analysis/\" target=\"_blank\">this R package<\/a><\/u> contains new tools linking R to a commonly used platform <a href=\"http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/\" target=\"_blank\">FSL<\/a>. However, as a newcomer, it can be easier to start with data that are already processed. <a href=\"http://projecteuclid.org/euclid.ss/1242049389\" target=\"_blank\">This<\/a> excellent overview by Dr. Martin Lindquist provides an introduction to the different types of analyses for brain imaging data from a statisticians point of view, while our<a href=\"http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0089470\" target=\"_blank\">paper<\/a> provides tools in R and example datasets for implementing some of these methods. At least one course on Coursera can help you get started with <a href=\"https://www.coursera.org/course/fmri\" target=\"_blank\">functional MRI<\/a> data. Talking to and reading the papers of biostatisticians working in the field of quantitative neuroscience and scientists in the field of neuroscience is the key.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-08-09-statistical-theory-is-our-write-once-run-anywhere/",
    "title": "Statistical Theory is our \"Write Once, Run Anywhere\"",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-08-09",
    "categories": [],
    "contents": "\nHaving followed the software industry as a casual bystander, I periodically see the tension flare up between the idea of writing “native apps”, software that is tuned to a particular platform (Windows, Mac, etc.) and more cross-platform apps, which run on many platforms without too much modification. Over the years it has come up in many different forms, but they fundamentals are the same. Back in the day, there was Java, which was supposed to be the platform that ran on any computing device. Sun Microsystems originated the phrase “Write Once, Run Anywhere” to illustrate the cross-platform strengths of Java. More recently, Steve Jobs famously banned Flash from any iOS device. Apple is also moving away from standards like OpenGL and towards its own Metal platform.\nWhat’s the problem with “write once, run anywhere”, or of cross-platform development more generally, assuming it’s possible? Well, there are a number of issues: often there are performance penalties, it may be difficult to use the native look and feel of a platform, and you may be reduced to using the “lowest common denominator” of feature sets. It seems to me that anytime a new meta-platform comes out that promises to relieve programmers of the burden of having to write for multiple platforms, it eventually gets modified or subsumed by the need to optimize apps for a given platform as much as possible. The need to squeeze as much juice out of an app seems to be too important an opportunity to pass up.\nIn statistics, theory and theorems are our version of “write once, run anywhere”. The basic idea is that theorems provide an abstract layer (a “virtual machine”) that allows us to reason across a large number of specific problems. Think of the central limit theorem, probably our most popular theorem. It could be applied to any problem/situation where you have a notion of sample size that could in principle be increasing.\nBut can it be applied to every situation, or even any situation? This might be more of a philosophical question, given that the CLT is stated asymptotically (maybe we’ll find out the answer eventually). In practice, my experience is that many people attempt to apply it to problems where it likely is not appropriate. Think, large-scale studies with a sample size of 10. Many people will use Normal-based confidence intervals in those situations, but they probably have very poor coverage.\nBecause the CLT doesn’t apply in many situations (small sample, dependent data, etc.), variations of the CLT have been developed, as well as entirely different approaches to achieving the same ends, like confidence intervals, p-values, and standard errors (think bootstrap, jackknife, permutation tests). While the CLT an provide beautiful insight in a large variety of situations, in reality, one must often resort to a custom solution when analyzing a given dataset or problem. This should be a familiar conclusion to anyone who analyzes data. The promise of “write once, run anywhere” is always tantalizing, but the reality never seems to meet that expectation.\nIronically, if you look across history and all programming languages, probably the most “cross-platform” language is C, which was originally considered to be too low-level to be broadly useful. C programs run on basically every existing platform and the language has been completely standardized so that compilers can be written to produce well-defined output. The keys to C’s success I think are that it’s a very simple/small language which gives enormous (sometimes dangerous) power to the programmer, and that an enormous toolbox (compiler toolchains, IDEs) has been developed over time to help developers write applications on all platforms.\nIn a sense, we need “compilers” that can help us translate statistical theory for specific data analysis problems. In many cases, I’d imagine the compiler would “fail”, meaning the theory was not applicable to that problem. This would be a Good Thing, because right now we have no way of really enforcing the appropriateness of a theorem for specific problems.\nMore practically (perhaps), we could develop data analysis pipelines that could be applied to broad classes of data analysis problems. Then a “compiler” could be employed to translate the pipeline so that it worked for a given dataset/problem/toolchain.\nThe key point is to recognize that there is a “translation” process that occurs when we use theory to justify certain data analysis actions, but this translation process is often not well documented or even thought through. Having an explicit “compiler” for this would help us to understand the applicability of certain theorems and may serve to prevent bad data analysis from occurring.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-07-30-autonomous-killing-machines-wont-look-like-the-terminator-and-that-is-why-they-are-so-scary/",
    "title": "Autonomous killing machines won't look like the Terminator...and that is why they are so scary",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-07-30",
    "categories": [],
    "contents": "\nJust a few days ago many of the most incredible minds in science and technology urged governments to avoid using artificial intelligence to create autonomous killing machines. One thing that always happens when such a warning is put into place is you see the inevitable Terminator picture:\n \n\n \nThe reality is that robots that walk and talk are getting better but still have a ways to go:\n \n \nDoes this mean that I think all those really smart people are silly for making this plea about AI now though? No, I think they are probably just in time.\nThe reason is that the first autonomous killing machines will definitely not look anything like the Terminator. They will more likely than not be drones, that are already in widespread use by the military, and will soon be flying over our heads delivering Amazon products.\n \n\n \nI also think that when people think about “artificial intelligence” they also think about robots that can mimic the behaviors of a human being, including the ability to talk, hold a conversation, or pass the Turing test. But it turns out that the “artificial intelligence” you would need to create an automated killing system is much much simpler than that and is mostly some basic data science. The things you would need are:\nA drone with the ability to fly on its own\nThe ability to make decisions about what people to target\nThe ability to find those people and attack them\n \nThe first issue, being able to fly on autopilot, is something that has existed for a while. You have probably flown on a plane that has used autopilot for at least some of the flight. I won’t get into the details on this one because I think it is the least interesting - it has been around a while and we didn’t get the dire warnings about autonomous agents.\nThe second issue, about deciding which people to target is already in existence as well. We have already seen programs like PRISM and others that collect individual level metadata and presumably use those to make predictions. We have already seen programs like PRISM and others that collect individual level metadata and presumably use those to make predictions. While the true and false positive rates are probably messed up by the fact that there are very very few “true positives” these programs are being developed and even relatively simple statistical models can be used to build a predictor - even if those don’t work.\nThe second issue is being able to find people to attack them. This is where the real “artificial intelligence” comes in to play. But it isn’t artificial intelligence like you might think about. It could be just as simple as having the drone fly around and take people’s pictures. Then we could use those pictures to match up with the people identified through metadata and attack them. Facebook has a Just a few days ago many of the most incredible minds in science and technology [urged governments to avoid using artificial intelligence](http://www.theguardian.com/technology/2015/jul/27/musk-wozniak-hawking-ban-ai-autonomous-weapons) to create autonomous killing machines. One thing that always happens when such a warning is put into place is you see the inevitable Terminator picture: that demonstrates an algorithm that can identify people with near human level accuracy. This approach is based on something called deep neural nets, which sounds very intimidating, but is actually just a set of nested nonlinear logistic regression models. These models have gotten very good because (a) we are getting better at fitting them mathematically and computationally but mostly (b) we have much more data to train them with than we ever did before. The speed that this part of the process is developing is (I think) why there is so much recent concern about potentially negative applications like autonomous killing machines.\nThe scary thing is that these technologies could be combined *right now* to create such a system that was not controlled directly by humans but made automated decisions and flew drones to carry out those decisions. The technology to shrink these type of deep neural net systems to identify people is so good it can even be made simple enough to run on a phone for things like language translation and could easily be embedded in a drone.\nSo I am with Musk, Hawking, and others who would urge caution by governments in developing these systems. Just because we can make it doesn’t mean it will do what we want. Just look at how well Facebook/Amazon/Google make suggestions for “other things you might like” to get an idea about how potentially disastrous automated killing systems could be.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-07-28-announcing-the-jhu-data-science-hackathon-2015/",
    "title": "Announcing the JHU Data Science Hackathon 2015",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-07-28",
    "categories": [],
    "contents": "\nWe are pleased to announce that the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health will be hosting the first ever JHU Data Science Hackathon (DaSH) on September 21-23, 2015 at the Baltimore Marriott Waterfront.\nThis event will be an opportunity for data scientists and data scientists-in-training to get together and hack on real-world problems collaboratively and to learn from each other. The DaSH will feature data scientists from government, academia, and industry presenting problems and describing challenges in their respective areas. There will also be a number of networking opportunities where attendees can get to know each other. We think this will be  fun event and we encourage people from all areas, including students (graduate and undergraduate), to attend.\nTo get more details and to sign up for the hackathon, you can go to the DaSH web site. We will be posting more information as the event nears.\nOrganizers:\nJeff Leek\nBrian Caffo\nRoger Peng\nLeah Jager\nFunding:\nNational Institutes of Health\nJohns Hopkins University\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-07-24-stringsasfactors-an-unauthorized-biography/",
    "title": "stringsAsFactors: An unauthorized biography",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-07-24",
    "categories": [],
    "contents": "\nRecently, I was listening in on the conversation of some colleagues who were discussing a bug in their R code. The bug was ultimately traced back to the well-known phenomenon that functions like ‘read.table()’ and ‘read.csv()’ in R convert columns that are detected to be character/strings to be factor variables. This lead to the spontaneous outcry from one colleague of\n\nWhy does stringsAsFactors not default to FALSE????\n\nThe argument ‘stringsAsFactors’ is an argument to the ‘data.frame()’ function in R. It is a logical that indicates whether strings in a data frame should be treated as factor variables or as just plain strings. The argument also appears in ‘read.table()’ and related functions because of the role these functions play in reading in table data and converting them to data frames. By default, ‘stringsAsFactors’ is set to TRUE.\nThis argument dates back to May 20, 2006 when it was originally introduced into R as the ‘charToFactor’ argument to ‘data.frame()’. Soon afterwards, on May 24, 2006, it was changed to ‘stringsAsFactors’ to be compatible with S-PLUS by request from Bill Dunlap.\nMost people I talk to today who use R are completely befuddled by the fact that ‘stringsAsFactors’ is set to TRUE by default. First of all, it should be noted that before the ‘stringsAsFactors’ argument even existed, the behavior of R was to coerce all character strings to be factors in a data frame. If you didn’t want this behavior, you had to manually coerce each column to be character.\nSo here’s the story:\nIn the old days, when R was primarily being used by statisticians and statistical types, this setting strings to be factors made total sense. In most tabular data, if there were a column of the table that was non-numeric, it almost certainly encoded a categorical variable. Think sex (male/female), country (U.S./other), region (east/west), etc. In R, categorical variables are represented by ‘factor’ vectors and so character columns got converted factor.\nWhy do we need factor variables to begin with? Because of modeling functions like ‘lm()’ and ‘glm()’. Modeling functions need to treat expand categorical variables into individual dummy variables, so that a categorical variable with 5 levels will be expanded into 4 different columns in your modeling matrix. There’s no way for R to know it should do this unless it has some extra information in the form of the factor class. From this point of view, setting ‘stringsAsFactors = TRUE’ when reading in tabular data makes total sense. If the data is just going to go into a regression model, then R is doing the right thing.\nThere’s also a more obscure reason. Factor variables are encoded as integers in their underlying representation. So a variable like “disease” and “non-disease” will be encoded as 1 and 2 in the underlying representation. Roughly speaking, since integers only require 4 bytes on most systems, the conversion from string to integer actually saved some space for long strings. All that had to be stored was the integer levels and the labels. That way you didn’t have to repeat the strings “disease” and “non-disease” for as many observations that you had, which would have been wasteful.\nAround June of 2007, R introduced hashing of CHARSXP elements in the underlying C code thanks to Seth Falcon. What this meant was that effectively, character strings were hashed to an integer representation and stored in a global table in R. Anytime a given string was needed in R, it could be referenced by its underlying integer. This effectively put in place, globally, the factor encoding behavior of strings from before. Once this was implemented, there was little to be gained from an efficiency standpoint by encoding character variables as factor. Of course, you still needed to use ‘factors’ for the modeling functions.\nThe difference nowadays is that R is being used a by a very wide variety of people doing all kinds of things the creators of R never envisioned. This is, of course, wonderful, but it introduces lots of use cases that were not originally planned for. I find that most often, the people complaining about ‘stringsAsFactors’ not being FALSE are people who are doing things that are not the traditional statistical modeling things (things that old-time statisticians like me used to do). In fact, I would argue that if you’re upset about ‘stringsAsFactors = TRUE’, then it’s a pretty good indicator that you’re either not a statistician by training, or you’re doing non-traditional statistical things.\nFor example, in genomics, you might have the names of the genes in one column of data. It really doesn’t make sense to encode these as factors because they won’t be used in any modeling function. They’re just labels, essentially. And because of CHARSXP hashing, you don’t gain anything from an efficiency standpoint by converting them to factors either.\nBut of course, given the long-standing behavior of R, many people depend on the default conversion of characters to factors when reading in tabular data. Changing this default would likely result in an equal number of people complaining about ‘stringsAsFactors’.\nI fully expect that this blog post will now make all R users happy. If you think I’ve missed something from this unauthorized biography, please let me know on Twitter (@rdpeng).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-07-17-the-statistics-department-moneyball-opportunity/",
    "title": "The statistics department Moneyball opportunity",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-07-17",
    "categories": [],
    "contents": "\nMoneyball is a book and a movie about Billy Bean.It makes statisticians look awesome and I loved the movie. I loved it so much I’m putting the movie trailer right here:\nThe basic idea behind Moneyball was that the Oakland Athletics were able to build a very successful baseball team on a tight budget by valuing skills that many other teams undervalued. In baseball those skills were things like on-base percentage and slugging percentage. By correctly valuing these skills and their impact on a teams winning percentage, the A’s were able to build one of the most successful regular season teams on a minimal budget. This graph shows what an outlier they were, from a nice fivethirtyeight analysis.\n \nOakland \nI think that the data science/data analysis revolution that we have seen over the last decade has created a similar moneyball opportunity for statistics and biostatistics departments. Traditionally in these departments the highest value activities have been publishing a select number of important statistics journals (JASA, JRSS-B, Annals of Statistics, Biometrika, Biometrics and more recently journals like Biostatistics and Annals of Applied Statistics). But there are some hugely valuable ways to contribute to statistics/data science that don’t necessarily end with papers in those journals like:\nCreating good, well-documented, and widely used software\nBeing primarily an excellent collaborator who brings in grant money and is a major contributor to science through statistics\nPublishing in top scientific journals rather than statistics journals\nBeing a good scientific communicator who can attract talent\nBeing a statistics educator who can build programs\nAnother thing that is undervalued is not having a Ph.D. in statistics or biostatistics. The fact that these skills are undervalued right now means that up and coming departments could identify and recruit talented people that might be missed by other departments and have a huge impact on the world. One tricky thing is that the rankings of department are based on the votes of people from other departments who may or may not value these same skills. Another tricky thing is that many industry data science positions put incredibly high value on these skills and so you might end up competing with them for people - a competition that will definitely drive up the market value of these data scientist/statisticians. But for the folks that want to stay in academia, now is a prime opportunity.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:19:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-07-10-the-mozilla-fellowship-for-science/",
    "title": "The Mozilla Fellowship for Science",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-07-10",
    "categories": [],
    "contents": "\nThis looks like an interesting opportunity for grad students, postdocs, and early career researchers:\n\nWe’re looking for researchers with a passion for open source and data sharing, already working to shift research practice to be more collaborative, iterative and open. Fellows will spend 10 months starting September 2015 as community catalysts at their institutions, mentoring the next generation of open data practitioners and researchers and building lasting change in the global open science community.\nThroughout their fellowship year, chosen fellows will receive training and support from Mozilla to hone their skills around open source and data sharing. They will also craft code, curriculum and other learning resources that help their local communities learn open data practices, and teach forward to their peers.\n\nHere’s what you get:\n\nFellows will receive:\nA stipend of $60,000 USD, paid in 10 monthly installments.\nOne-time health insurance supplement for Fellows and their families, ranging from $3,500 for single Fellows to $7,000 for a couple with two or more children.\nOne-time childcare allotment for families with children of up to $6,000.\nAllowance of up to $3,000 towards the purchase of laptop computer, digital cameras, recorders and computer software; fees for continuing studies or other courses, research fees or payments, to the extent related to the fellowship.\nAll approved fellowship trips – domestic and international – are covered in full.\n\nDeadline is August 14.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-07-08-jhu-umd-researchers-are-getting-a-really-big-big-data-center/",
    "title": "JHU, UMD researchers are getting a really big Big Data center",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-07-08",
    "categories": [],
    "contents": "\nFrom Technical.ly Baltimore:\n\nA nondescript, 3,700-square-foot building on Johns Hopkins’ Bayview campus will house a new data storage and computing center for university researchers. The $30 million Maryland Advanced Research Computing Center (MARCC) will be available to faculty from JHU and the University of Maryland, College Park.\n\nThe web site has a pretty cool time-lapse video of the construction of the computing center. There’s also a bit more detail at the JHU Hub site.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-07-03-the-massive-future-of-statistics-education/",
    "title": "The Massive Future of Statistics Education",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-07-03",
    "categories": [],
    "contents": "\nNOTE: This post was written as a chapter for the not-yet-released Handbook on Statistics Education. \nData are eating the world, but our collective ability to analyze data is going on a starvation diet.\n\n\nEverywhere you turn, data are being generated somehow. By the time you read this piece, you’ll probably have collected some data. (For example this piece has 2,072 words). You can’t avoid data—it’s coming from all directions.\n\n\nSo what do we do with it? For the most part, nothing. There’s just too much data being spewed about. But for the data that we are interested in, we need to know the appropriate methods for thinking about and analyzing them. And by “we”, I mean pretty much everyone.\n\n\nIn the future, everyone will need some data analysis skills. People are constantly confronted with data and the need to make choices and decisions from the raw data they receive. Phones deliver information about traffic, we have ratings about restaurants or books, and even rankings of hospitals. High school students can obtain complex and rich information about the colleges to which they’re applying while admissions committees can get real-time data on applicants’ interest in the college.\n\n\nMany people already have heuristic algorithms to deal with the data influx—and these algorithms may serve them well—but real statistical thinking will be needed for situations beyond choosing which restaurant to try for dinner tonight.\n\n\nLimited Capacity\n\n\nThe McKinsey Global Institute, in a highly cited report, predicted that there would be a shortage of “data geeks” and that by 2018 there would be between 140,000 and 190,000 unfilled positions in data science. In addition, there will be an estimated 1.5 million people in managerial positions who will need to be trained to manage data scientists and to understand the output of data analysis. If history is any guide, it’s likely that these positions will get filled by people, regardless of whether they are properly trained. The potential consequences are disastrous as untrained analysts interpret complex big data coming from myriad sources of varying quality.\n\n\nWho will provide the necessary training for all these unfilled positions? The field of statistics’ current system of training people and providing them with master’s degrees and PhDs is woefully inadequate to the task. In 2013, the top 10 largest statistics master’s degree programs in the U.S. graduated a total of 730 people. At this rate we will never train the people needed. While statisticians have greatly benefited from the sudden and rapid increase in the amount of data flowing around the world, our capacity for scaling up the needed training for analyzing those data is essentially nonexistent.\n\n\nOn top of all this, I believe that the McKinsey report is a gross underestimation of how many people will need to be trained in some data analysis skills in the future. Given how much data is being generated every day, and how critical it is for everyone to be able to intelligently interpret these data, I would argue that it’s necessary for everyone to have some data analysis skills. Needless to say, it’s foolish to suggest that everyone go get a master’s or even bachelor’s degrees in statistics. We need an alternate approach that is both high-quality and scalable to a large population over a short period of time.\n\n\nEnter the MOOCs\n\n\nIn April of 2014, Jeff Leek, Brian Caffo, and I launched the Johns Hopkins Data Science Specialization on the Coursera platform. This is a sequence of nine courses that intends to provide a “soup-to-nuts” training in data science for people who are highly motivated and have some basic mathematical and computing background. The sequence of the nine courses follow what we believe is the essential “data science process”, which is\n\n\nFormulating a question that can be answered with data\n\n\nAssembling, cleaning, tidying data relevant to a question\n\n\nExploring data, checking, eliminating hypotheses\n\n\nDeveloping a statistical model\n\n\nMaking statistical inference\n\n\nCommunicating findings\n\n\nMaking the work reproducible\n\n\nWe took these basic steps and designed courses around each one of them.\n\n\nEach course is provided in a massive open online format, which means that many thousands of people typically enroll in each course every time it is offered. The learners in the courses do homework assignments, take quizzes, and peer assess the work of others in the class. All grading and assessment is handled automatically so that the process can scale to arbitrarily large enrollments. As an example, the April 2015 session of the R Programming course had nearly 45,000 learners enrolled. Each class is exactly 4 weeks long and every class runs every month.\n\n\nWe developed this sequence of courses in part to address the growing demand for data science training and education across the globe. Our background as biostatisticians was very closely aligned with the training needs of people interested in data science because, essentially, data science is what we do every single day. Indeed, one curriculum rule that we had was that we couldn’t include something if we didn’t in fact use it in our own work.\n\n\nThe sequence has a substantial amount of standard statistics content, such as probability and inference, linear models, and machine learning. It also has non-standard content, such as git, GitHub, R programming, Shiny, and Markdown. Together, the sequence covers the full spectrum of tools that we believe will be needed by the practicing data scientist.\n\n\nFor those who complete the nine courses, there is a capstone project at the end, that involves taking all of the skills in the course and developing a data product. For our first capstone project we partnered with SwiftKey, a predictive text analytics company, to develop a project where learners had to build a statistical model for predicting words in a sentence. This project involves taking unstructured, messy data, processing it into an analyzable form, developing a statistical model while making tradeoffs for efficiency and accuracy, and creating a Shiny app to show off their model to the public.\n\n\nDegree Alternatives\n\n\nThe Data Science Specialization is not a formal degree program offered by Johns Hopkins University—learners who complete the sequence do not get any Johns Hopkins University credit—and so one might wonder what the learners get out of the program (besides, of course, the knowledge itself). To begin with, the sequence is completely portfolio based, so learners complete projects that are immediately viewable by others. This allows others to evaluate a learner’s ability on the spot with real code or data analysis.\n\n\nAll of the lecture content is openly available and hosted on GitHub, so outsiders can view the content and see for themselves what is being taught. This give outsiders an opportunity to evaluate the program directly rather than have to rely on the sterling reputation of the institution teaching the courses.\n\n\nEach learner who completes a course using Coursera’s “Signature Track” (which currently costs $49 per course) can get a badge on their LinkedIn profile, which shows that they completed the course. This can often be as valuable as a degree or other certification as recruiters scouring LinkedIn for data scientist positions will be able to see our completers’ certifications in various data science courses.\n\n\nFinally, the scale and reach of our specialization immediately creates a large alumni social network that learners can take advantage of. As of March 2015, there were approximately 700,000 people who had taken at least one course in the specialization. These 700,000 people have a shared experience that, while not quite at the level of a college education, still is useful for forging connections between people, especially when people are searching around for jobs.\n\n\nEarly Numbers\n\n\nSo far, the sequence has been wildly successful. It averaged 182,507 enrollees a month for the first year in existence. The overall course completion rate was about 6% and the completion rate amongst those in the “Signature Track” (i.e. paid enrollees) was 67%. In October of 2014, barely 7 months since the start of the specialization, we had 663 learners enroll in the capstone project.\n\n\nSome Early Lessons\n\n\nFrom running the Data Science Specialization for over a year now, we have learned a number of lessons, some of which were unexpected. Here, I summarize the highlights of what we’ve learned.\n\n\nData Science as Art and Science. Ironically, although the word “Science” appears in the name “Data Science”, there’s actually quite a bit about the practice of data science that doesn’t really resemble science at all. Much of what statisticians do in the act of data analysis is intuitive and ad hoc, with each data analysis being viewed as a unique flower.\n\n\nWhen attempting to design data analysis assignments that could be graded at scale with tens of thousands of people, we discovered that designing the rubrics for grading these assignments was not trivial. The reason is because our understanding of what makes a “good” analysis different from a bad one is not well-articulated. We could not identify any community-wide understanding of what are the components of a good analysis. What are the “correct” methods to use in a given data analysis situation? What is definitely the “wrong” approach?\n\n\nAlthough each one of us had been doing data analysis for the better part of a decade, none of us could succinctly write down what the process was and how to recognize when it was being done wrong. To paraphrase Daryl Pregibon from his 1991 talk at the National Academies of Science, we had a process that we regularly espoused but barely understood.\n\n\nContent vs. Curation. Much of the content that we put online is available elsewhere. With YouTube, you can find high-quality videos on almost any topic, and our videos are not really that much better. Furthermore, the subject matter that we were teaching was in now way proprietary. The linear models that we teach are the same linear models taught everywhere else. So what exactly was the value we were providing?\n\n\nSearching on YouTube requires that you know what you are looking for. This is a problem for people who are just getting into an area. Effectively, what we provided was a curation of all the knowledge that’s out there on the topic of data science (we also added our own quirky spin). Curation is hard, because you need to make definitive choices between what is and is not a core element of a field. But curation is essential for learning a field for the uninitiated.\n\n\nSkill sets vs. Certification. Because we knew that we were not developing a true degree program, we knew we had to develop the program in a manner so that the learners could quickly see for themselves the value they were getting out of it. This lead us to taking a portfolio approach where learners produced things that could be viewed publicly.\n\n\nIn part because of the self-selection of the population seeking to learn data science skills, our learners were more interested in being able to demonstrate the skills taught in the course rather than an abstract (but official) certification as might be gotten in a degree program. This is not unlike going to a music conservatory, where the output is your ability to play an instrument rather than the piece of paper you receive upon graduation. We feel that giving people the ability to demonstrate skills and skill sets is perhaps more important than official degrees in some instances because it gives employers a concrete sense of what a person is capable of doing.\n\n\nConclusions\n\n\nAs of April 2015, we had a total of 1,158 learners complete the entire specialization, including the capstone project. Given these numbers and our rate of completion for the specialization as a whole, we believe we are on our way to achieving our goal of creating a highly scalable program for training people in data science skills. Of course, this program alone will not be sufficient for all of the data science training needs of society. But we believe that the approach that we’ve taken, using non-standard MOOC channels, focusing on skill sets instead of certification, and emphasizing our role in curation, is a rich opportunity for the field of statistics to explore in order to educate the masses about our important work.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-07-02-looks-like-this-r-thing-might-be-for-real/",
    "title": "Looks like this R thing might be for real",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-07-02",
    "categories": [],
    "contents": "\nNot sure how I missed this, but the Linux Foundation just announced the R Consortium for supporting the “world’s most popular language for analytics and data science and support the rapid growth of the R user community”. From the Linux Foundation:\n\nThe R language is used by statisticians, analysts and data scientists to unlock value from data. It is a free and open source programming language for statistical computing and provides an interactive environment for data analysis, modeling and visualization. The R Consortium will complement the work of the R Foundation, a nonprofit organization based in Austria that maintains the language. The R Consortium will focus on user outreach and other projects designed to assist the R user and developer communities.\nFounding companies and organizations of the R Consortium include The R Foundation, Platinum members Microsoft and RStudio; Gold member TIBCO Software Inc.; and Silver members Alteryx, Google, HP, Mango Solutions, Ketchum Trading and Oracle.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-07-01-how-airbnb-built-a-data-science-team/",
    "title": "How Airbnb built a data science team",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-07-01",
    "categories": [],
    "contents": "\nFrom Venturebeat:\n\nBack then we knew so little about the business that any insight was groundbreaking; data infrastructure was fast, stable, and real-time (I was querying our production MySQL database); the company was so small that everyone was in the loop about every decision; and the data team (me) was aligned around a singular set of metrics and methodologies.\nBut five years and 43,000 percent growth later, things have gotten a bit more complicated. I’m happy to say that we’re also more sophisticated in the way we leverage data, and there’s now a lot more of it. The trick has been to manage scale in a way that brings together the magic of those early days with the growing needs of the present — a challenge that I know we aren’t alone in facing.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-06-24-how-public-relations-and-the-media-are-distorting-science/",
    "title": "How public relations and the media are distorting science",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-06-24",
    "categories": [],
    "contents": "\nThroughout history, engineers, medical doctors and other applied scientists have helped convert  basic science discoveries into products, public goods and policy that have greatly improved our quality of life. With rare exceptions, it has taken years if not decades to establish these discoveries. And even the exceptions stand on the shoulders of incremental contributions. The researchers that produce this knowledge go through a slow and painstaking process to reach these achievements.\nIn contrast, most science related media reports that grab the public’s attention fall into three categories:\nThe exaggerated big discovery: Recent examples include the discovery of the bubonic plague in the NYC subway, liquid water in mars, and the infidelity gene.\nOver-promising:  These try to explain a complicated basic science finding and, in the case of biomedical research, then speculate without much explanation that the finding will “lead to a deeper understanding of diseases and new ways to treat or cure them”.\nScience is broken:  These tend to report an anecdote about an allegedly corrupt scientist, maybe cite the “Why Most Published Research Findings are False” paper, and then extrapolate speculatively.\nIn my estimation, despite the attention grabbing headlines, the great majority of the subject matter included in these reports will not have an impact on our lives and will not even make it into scientific textbooks. So does science still have anything to offer? Reports of the third category have even scientists particularly worried. I, however, remain optimistic. First, I do not see any empirical evidence showing that the negative effects of the lack of reproducibility are worse now than 50 years ago. Furthermore, although not widely reported in the lay press, I continue to see bodies of work built by several scientists over several years or decades with much promise of leading to tangible improvements to our quality of life.  Recent advances that I am excited about include insulators, PD-1 pathway inhibitors, clustered regularly interspaced short palindromic repeats, advances in solar energy technology, and prosthetic robotics.\nHowever, there is one general aspect of science that I do believe has become worse.  Specifically, it’s a shift in how much scientists jockey for media attention, even if it’s short-lived. Instead of striving for having a sustained impact on our field, which may take decades to achieve, an increasing number of scientists seem to be placing more value on appearing in the New York Times, giving a Ted Talk or having a blog or tweet go viral. As a consequence, too many of us end up working on superficial short term challenges that, with the help of a professionally crafted press release, may result in an attention grabbing media report. NB: I fully support science communication efforts, but not when the primary purpose is garnering attention, rather than educating.\nMy concern spills over to funding agencies and philanthropic organizations as well. Consider the following two options. Option 1: be the funding agency representative tasked with organizing a big science project with a well-oiled PR machine. Option 2: be the funding agency representative in charge of several small projects, one of which may with low, but non-negligible, probability result in a Nobel Prize 30 years down the road. In the current environment, I see a preference for option 1.\nI am also concerned about how this atmosphere may negatively affect societal improvements within science. Publicly shaming transgressors on Twitter or expressing one’s outrage on a blog post can garner many social media clicks. However, these may have a smaller positive impact than mundane activities such as serving on a committee that, after several months of meetings, implements incremental, yet positive, changes. Time and energy spent on trying to increase internet clicks is time and energy we don’t spend on the tedious administrative activities that are needed to actually affect change.\nBecause so many of the scientists that thrive in this atmosphere of short-lived media reports are disproportionately rewarded, I imagine investigators starting their careers feel some pressure to garner some media attention of their own. Furthermore, their view of how they are evaluated may be highly biased because evaluators that ignore media reports and focus more on the specifics of the scientific content, tend to be less visible. So if you want to spend your academic career slowly building a body of work with the hopes of being appreciated decades from now, you should not think that it is hopeless based on what is perhaps, a distorted view of how we are currently being evaluated.\nUpdate: changed topological insulators links to these two. Here is one more. Via David S.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-06-16-interview-at-leanpub/",
    "title": "Interview at Leanpub",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-06-16",
    "categories": [],
    "contents": "\nA few weeks ago I sat down with Len Epp over at Leanpub to talk about my recently published book R Programming for Data Science. So far, I’ve only published one book through Leanpub but I’m a huge fan. They’ve developed a system that is, in my opinion, perfect for academic publishing. The book’s written in Markdown and they compile it into PDF, ePub, and mobi formats automatically.\nThe full interview transcript is over at the Leanpub blog. If you want to listen to the audio of the interview, you can subscribe to the Leanpub podcast on iTunes.\nR Programming for Data Science is available at Leanpub for a suggested price of $15 (but you can get it for free if you want). R code files, datasets, and video lectures are available through the various add-on packages. Thanks to all of you who’ve already bought a copy!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-06-10-johns-hopkins-data-science-specialization-captsone-2-top-performers/",
    "title": "Johns Hopkins Data Science Specialization Captsone 2 Top Performers",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-06-10",
    "categories": [],
    "contents": "\nThe second capstone session of the Johns Hopkins Data Science Specialization concluded recently. This time, we had 1,040 learners sign up to participate in the session, which again featured a project developed in collaboration with the amazingly innovative folks at SwiftKey. \nWe’ve identified the learners listed below as the top performers in this capstone session. This is an incredibly talented group of people who have worked very hard throughout the entire nine-course specialization.  Please take some time to read their stories and look at their work. \nBen Apple\n\nBen Apple is a Data Scientist and Enterprise Architect with the Department of Defense.  Mr. Apple holds a MS in Information Assurance and is a PhD candidate in Information Sciences.\n****Why did you take the JHU Data Science Specialization?****\nAs a self trained data scientist I was looking for a program that would formalize my established skills while expanding my data science knowledge and tool box.\n****What are you most proud of doing as part of the JHU Data Science Specialization?****\nThe capstone project was the most demanding aspect of the program.  As such I most proud of the finale project.  The project stretched each of us beyond the standard course work of the program and was quite satisfying.\n****How are you planning on using your Data Science Specialization Certificate?****\nTo open doors so that I may further my research into the operational value of applying data science thought and practice to analytics of my domain.\nFinal Project: https://bengapple.shinyapps.io/coursera_nlp_capstone\nProject Slide Deck: http://rpubs.com/bengapple/71376\n \nIvan Corneillet\n\nA technologist, thinker, and tinkerer, Ivan facilitates the establishment of start-up companies by advising these companies about the hiring process, product development, and technology development, including big data, cloud computing, and cybersecurity. In his 17-year career, Ivan has held a wide range of engineering and management positions at various Silicon Valley companies. Ivan is a recent Wharton MBA graduate, and he previously earned his master’s degree in computer science from the Ensimag, and his master’s degree in electrical engineering from Université Joseph Fourier, both located in France.\n****Why did you take the JHU Data Science Specialization?****\nThere are three reasons why I decided to enroll in the JHU Data Science Specialization. First, fresh from college, my formal education was best suited for scaling up the Internet’s infrastructure. However, because every firm in every industry now creates products and services from analyses of data, I challenged myself to learn about Internet-scale datasets. Second, I am a big supporter of MOOCs. I do not believe that MOOCs should replace traditional education; however, I do believe that MOOCs and traditional education will eventually coexist in the same way that open-source and closed-source software does (read my blog post for more information on this topic: http://ivantur.es/16PHild). Third, the Johns Hopkins University brand certainly motivated me to choose their program. With a great name comes a great curriculum and fantastic professors, right?\nOnce I had completed the program, I was not disappointed at all. I had read a blog post that explained that the JHU Data Science Specialization was only a start to learning about data science. I certainly agree, but I would add that this program is a great start, because the curriculum emphasizes information that is crucial, while providing additional resources to those who wish to deepen their understanding of data science. My thanks to Professors Caffo, Leek, and Peng; the TAs, and Coursera for building and delivering this track!\n****What are you most proud of doing as part of the JHU Data Science Specialization?****\nThe capstone project made for a very rich and exhilarating learning experience, and was my favorite course in the specialization. Because I did not have prior knowledge in natural language processing (NLP), I had to conduct a fair amount of research. However, the program’s minimal-guidance approach mimicked a real-world environment, and gave me the opportunity to leverage my experience with developing code and designing products to get the most out of the skillset taught in the track. The result was that I created a data product that implemented state-of-the-art NLP algorithms using what I think are the best technologies (i.e., C++, JavaScript, R, Ruby, and SQL), given the choices that I had made. Bringing everything together is what made me the most proud. Additionally, my product capabilities are a far cry from IBM’s Watson, but while I am well versed in supercomputer hardware, this track helped me to gain a much deeper appreciation of Watson’s AI.\n****How are you planning on using your Data Science Specialization Certificate?****\nThanks to the broad skillset that the specialization covered, I feel confident wearing a data science hat. The concepts and tools covered in this program helped me to better understand the concerns that data scientists have and the challenges they face. From a business standpoint, I am also better equipped to identify the opportunities that lie ahead.\nFinal Project: https://paspeur.shinyapps.io/wordmaster-io/\nProject Slide Deck: http://rpubs.com/paspeur/wordmaster-io\n\nOscar de León\n\nOscar is an assistant researcher at a research institute in a developing country, graduated as a licentiate in biochemistry and microbiology in 2010 from the same university which hosts the institute. He has always loved technology, programming and statistics and has engaged in self learning of these subjects from an early age, finally using his abilities in the health-related research in which he has been involved since 2008. He is now working on the design, execution and analysis of various research projects, consulting for other researchers and students, and is looking forward to develop his academic career in biostatistics.\n****Why did you take the JHU Data Science Specialization?****\nI wanted to integrate my R experience into a more comprehensive data analysis workflow, which is exactly what this specialization offers. This was in line with the objectives of my position at the research institute in which I work, so I presented a study plan to my supervisor and she approved it. I also wanted to engage in an activity which enabled me to document my abilities in a verifiable way, and a Coursera Specialization seemed like a good option.\nAdditionally, I’ve followed the JHSPH group’s courses since the first offering of Mathematical Biostatistics Bootcamp in November 2012. They have proved the standards and quality of education at their institution, and it was not something to let go by.\n****What are you most proud of doing as part of the JHU Data Science Specialization?****\nI’m not one to usually interact with other students, and certainly didn’t do it during most of the specialization courses, but I decided to try out the fora on the Capstone project. It was wonderful; sharing ideas with, and receiving criticism form, my peers provided a very complete learning experience. After all, my contributions ended being appreciated by the community and a few posts stating it were very rewarding. This re-kindled my passion for teaching, and I’ll try to engage in it more from now on.\n****How are you planning on using your Data Science Specialization Certificate?****\nFirst, I’ll file it with HR at my workplace, since our research projects payed for the specialization \nI plan to use the certificate as a credential for data analysis with R when it is relevant. For example, I’ve been interested in offering an R workshop for life sciences students and researchers at my University, and this certificate (and the projects I prepared during the specialization) could help me show I have a working knowledge on the subject.\nFinal Project: https://odeleon.shinyapps.io/ngram/\nProject Slide Deck: http://rpubs.com/chemman/n-gram\n\nJeff Hedberg\n\nI am passionate about turning raw data into actionable insights that solve relevant business problems. I also greatly enjoy leading large, multi-functional projects with impact in areas pertaining to machine and/or sensor data.  I have a Mechanical Engineering Degree and an MBA, in addition to a wide range of Data Science (IT/Coding) skills.\n****Why did you take the JHU Data Science Specialization?****\nI was looking to gain additional exposure into Data Science as a current practitioner, and thought this would be a great program.\n****What are you most proud of doing as part of the JHU Data Science Specialization?****\nI am most proud of completing all courses with distinction (top of peers).  Also, I’m proud to have achieved full points on my Capstone project having no prior experience in Natural Language Processing.\n****How are you planning on using your Data Science Specialization Certificate?****\nI am going to add this to my Resume and LinkedIn Profile.  I will use it to solidify my credibility as a data science practitioner of value.\nFinal Project: https://hedbergjeffm.shinyapps.io/Next_Word_Prediction/\nProject Slide Deck: https://rpubs.com/jhedbergfd3s/74960\n\nHernán Martínez-Foffani\n\nI was born in Argentina but now I’m settled in Spain. I’ve been working in computer technology since the eighties, in digital networks, programming, consulting, project management. Now, as CTO in a software company, I lead a small team of programmers developing a supply chain management app.\n****Why did you take the JHU Data Science Specialization?****\nIn my opinion the curriculum is carefully designed with a nice balance between theory and practice. The JHU authoring and the teachers’ widely known prestige ensure the content quality. The ability to choose the learning pace, one per month in my case, fits everyone’s schedule.\n****What are you most proud of doing as part of the JHU Data Science Specialization?****\nThe capstone definitely. It resulted in a fresh and interesting challenge. I sweat a lot, learned much more and in the end had a lot of fun.\n****How are you planning on using your Data Science Specialization Certificate?****\nWhile for the time being I don’t have any specific plan for the certificate, it’s a beautiful reward for the effort done.\nFinal Project: https://herchu.shinyapps.io/shinytextpredict/\nProject Slide Deck: http://rpubs.com/herchu1/shinytextprediction\n\nFrancois Schonken\n \n\nI’m a 36 year old South African male born and raised. I recently (4 years now) immigrated to lovely Melbourne, Australia. I wrapped up a BSc (Hons) Computer Science with specialization in Computer Systems back in 2001. Next I co-found a small boutique Software Development house operating from South Africa. I wrapped my MBA, from Melbourne Business School, in 2013 and now I consult for my small boutique Software Development house and 2 (very) small internet start-ups.\n****Why did you take the JHU Data Science Specialization?****\nOne of the core subjects in my MBA was Data Analysis, basically an MBA take on undergrad Statistics with focus on application over theory (not that there was any shortage of theory). Waiting in a lobby room some 6 months later I was paging through the financial section of business focused weekly. I came across an article explaining how a Melbourne local applied a language called R to solve a grammatically and statistically challenging issue. The rest, as they say, is history.\n****What are you most proud of doing as part of the JHU Data Science Specialization?****\nI’m quite proud of both my Developing Data Products and Capstone projects, but for me these tangible outputs merely served as a vehicle to better understand a different way of thinking about data. I’ve spend most of my Software Development life dealing with one form or the other form of RDBS (Relational Database Management System). This, in my experience, leads to a very set oriented way of thinking about data.\nI’m most proud of developing a new tool in my “Skills Toolbox” that I consider highly complementary to both my Software and Business outlook on projects.\n****How are you planning on using your Data Science Specialization Certificate?****\nHonestly, I had not planned on using my Certificate in and of itself. The skills I’ve acquired has already helped shape my thinking in designing an in-house web based consulting collaboration platform.\nI do not foresee this being the last time I’ll be applying Data Science thinking moving forward on my journey.\nFinal Project: https://schonken.shinyapps.io/WordPredictor\nProject Slide Deck: http://rpubs.com/schonken/sentence-builder\n\nDavid J. Tagler\nDavid is passionate about solving the world’s most important and challenging problems. His expertise spans chemical/biomedical engineering, regenerative medicine, healthcare technology management, information technology/security, and data science/analysis. David earned his Ph.D. in Chemical Engineering from Northwestern University and B.S. in Chemical Engineering from the University of Notre Dame.\n****Why did you take the JHU Data Science Specialization?****\nI enrolled in this specialization in order to advance my statistics, programming, and data analysis skills. I was interested in taking a series of courses that covered the entire data science pipeline. I believe that these skills will be critical for success in the future.\n****What are you most proud of doing as part of the JHU Data Science Specialization?****\nI am most proud of the R programming and modeling skills that I developed throughout this specialization. Previously, I had no experience with R. Now, I can effectively manage complex data sets, perform statistical analyses, build prediction models, create publication-quality figures, and deploy web applications.\n****How are you planning on using your Data Science Specialization Certificate?****\nI look forward to utilizing these skills in future research projects. Furthermore, I plan to take additional courses in data science, machine learning, and bioinformatics.\nFinal Project: http://dt444.shinyapps.io/next-word-predict\nProject Slide Deck: http://rpubs.com/dt444/next-word-predict\n\nMelissa Tan\n \n\nI’m a financial journalist from Singapore. I did philosophy and computer science at the University of Chicago, and I’m keen on picking up more machine learning and data viz skills.\n****Why did you take the JHU Data Science Specialization?****\nI wanted to keep up with coding, while learning new tools and techniques for wrangling and analyzing data that I could potentially apply to my job. Plus, it sounded fun. \n****What are you most proud of doing as part of the JHU Data Science Specialization?****\nBuilding a word prediction app pretty much from scratch (with a truckload of forum reading). The capstone project seemed insurmountable initially and ate up all my weekends, but getting the app to work passably was worth it.\n****How are you planning on using your Data Science Specialization Certificate?****\nIt’ll go on my CV, but I think it’s more important to be able to actually do useful things. I’m keeping an eye out for more practical opportunities to apply and sharpen what I’ve learnt.\nFinal Project: https://melissatan.shinyapps.io/word_psychic/\nProject Slide Deck: https://rpubs.com/melissatan/capstone\n\nFelicia Yii\n\nFelicia likes to dream, think and do. With over 20 years in the IT industry, her current fascination is at the intersection of people, information and decision-making.  Ever inquisitive, she has acquired an expertise in subjects as diverse as coding to cookery to costume making to cosmetics chemistry. It’s not apparent that there is anything she can’t learn to do, apart from housework.  Felicia lives in Wellington, New Zealand with her husband, two children and two cats.\n****Why did you take the JHU Data Science Specialization?****\nWell, I love learning and the JHU Data Science Specialization appealed to my thirst for a new challenge. I’m really interested in how we can use data to help people make better decisions.  There’s so much data out there these days that it is easy to be overwhelmed by it all. Data visualisation was at the heart of my motivation when starting out. As I got into the nitty gritty of the course, I really began to see the power of making data accessible and appealing to the data-agnostic world. There’s so much potential for data science thinking in my professional work.\n****What are you most proud of doing as part of the JHU Data Science Specialization?****\nGetting through it for starters while also working and looking after two children. Seriously though, being able to say I know what ‘practical machine learning’ is all about.  Before I started the course, I had limited knowledge of statistics, let alone knowing how to apply them in a machine learning context.  I was thrilled to be able to use what I learned to test a cool game concept in my final project.\n****How are you planning on using your Data Science Specialization Certificate?****\nI want to use what I have learned in as many ways possible. Firstly, I see opportunities to apply my skills to my day-to-day work in information technology. Secondly, I would like to help organisations that don’t have the skills or expertise in-house to apply data science thinking to help their decision making and communication. Thirdly, it would be cool one day to have my own company consulting on data science. I’ve more work to do to get there though!\nFinal Project: https://micasagroup.shinyapps.io/nwpgame/\nProject Slide Deck: https://rpubs.com/MicasaGroup/74788\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-06-09-batch-effects-are-everywhere-deflategate-edition/",
    "title": "Batch effects are everywhere! Deflategate edition",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-06-09",
    "categories": [],
    "contents": "\nIn my opinion, batch effects are the biggest challenge faced by genomics research, especially in precision medicine. As we point out in this review, they are everywhere among high-throughput experiments. But batch effects are not specific to genomics technology. In fact, in this 1972 paper (paywalled), WJ Youden describes batch effects in the context of measurements made by physicists. Check out this plot of astronomical unit speed of light estimates with an estimate of spread confidence intervals (red and green are same lab).\n\n\n\n \n\n\nSometimes you find batch effects where you least expect them. For example, in the deflategate debate. Here is quote from the New England patriot’s deflategate rebuttal (written with help from Nobel Prize winner Roderick MacKinnon)\n\n\n\nin other words, the Colts balls were measured after the Patriots balls and had warmed up more. For the above reasons, the Wells Report conclusion that physical law cannot explain the pressures is incorrect.\n\n\n\nHere is another one:\n\n\n\nIn the pressure measurements physical conditions were not very well-defined and major uncertainties, such as which gauge was used in pre-game measurements, affect conclusions.\n\n\n\nSo NFL, please read our paper before you accuse a player of cheating.\n\n\nDisclaimer: I live in New England but I am Ravens fan.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-06-08-im-a-data-scientist-mind-if-i-do-surgery-on-your-heart/",
    "title": "I'm a data scientist - mind if I do surgery on your heart?",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-06-08",
    "categories": [],
    "contents": "\nThere has been a lot of recent interest from scientific journals and from other folks in creating checklists for data science and data analysis. The idea is that the checklist will help prevent results that won’t reproduce or replicate from the literature. One analogy that I’m frequently hearing is the analogy with checklists for surgeons that can help reduce patient mortality.\nThe one major difference between checklists for surgeons and checklists I’m seeing for research purposes is the difference in credentialing between people allowed to perform surgery and people allowed to perform complex data analysis. You would never let me do surgery on you. I have no medical training at all. But I’m frequently asked to review papers that include complicated and technical data analyses, but have no trained data analysts or statisticians. The most common approach is that a postdoc or graduate student in the group is assigned to do the analysis, even if they don’t have much formal training. Whenever this happens red flags are up all over the place. Just like I wouldn’t trust someone without years of training and a medical license to do surgery on me, I wouldn’t let someone without years of training and credentials in data analysis make major conclusions from complex data analysis.\nYou might argue that the consequences for surgery and for complex data analysis are on completely different scales. I’d agree with you, but not in the direction that you might think. I would argue that high pressure and complex data analysis can have much larger consequences than surgery. In surgery there is usually only one person that can be hurt. But if you do a bad data analysis, say claiming say that vaccines cause autism, that can have massive consequences for hundreds or even thousands of people. So complex data analysis, especially for important results, should be treated with at least as much care as surgery.\nThe reason why I don’t think checklists alone will solve the problem is that they are likely to be used by people without formal training. One obvious (and recent) example that I think makes this really clear is the HealthKit data we are about to start seeing. A ton of people signed up for studies on their iPhones and it has been all over the news. The checklist will (almost certainly) say to have a big sample size. HealthKit studies will certainly pass the checklist, but they are going to get Truman/Deweyed big time if they aren’t careful about biased sampling.\n\nIf I walked into an operating room and said I’m going to start dabbling in surgery I would be immediately thrown out. But people do that with statistics and data analysis all the time. What they really need is to require careful training and expertise in data analysis on each paper that analyzes data. Until we treat it as a first class component of the scientific process we’ll continue to see retractions, falsifications, and irreproducible results flourish.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-06-04-4063/",
    "title": "Interview with Class Central",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-06-04",
    "categories": [],
    "contents": "\nRecently I sat down with Class Central to do an interview about the Johns Hopkins Data Science Specialization. We talked about the motivation for designing the sequence and and the capstone project. With the demand for data science skills greater than ever, the importance of the specialization is only increasing.\nSee the full interview at the Class Central site. Below is short excerpt.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-06-01-interview-with-chris-wiggins-chief-data-scientist-at-the-new-york-times/",
    "title": "Interview with Chris Wiggins, chief data scientist at the New York Times",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-06-01",
    "categories": [],
    "contents": "\nEditor’s note: We are trying something a little new here and doing an interview with Google Hangouts on Air. The interview will be live at 11:30am EST. I have some questions lined up for Chris, but if you have others you’d like to ask, you can tweet them @simplystats and I’ll see if I can work them in. After the livestream we’ll leave the video on Youtube so you can check out the interview if you can’t watch the live stream. I’m embedding the Youtube video here but if you can’t see the live stream when it is running go check out the event page: https://plus.google.com/events/c7chrkg0ene47mikqrvevrg3a4o.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-05-28-science-is-a-calling-and-a-career-here-is-a-career-planning-guide-for-students-and-postdocs/",
    "title": "Science is a calling and a career, here is a career planning guide for students and postdocs",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-05-28",
    "categories": [],
    "contents": "\nEditor’s note: This post was inspired by a really awesome career planning guide that Ben Langmead Editor’s note: This post was inspired by a really awesome career planning guide that Ben Langmead which you should go check out right now. You can also find the slightly adapted Leek group career planning guide here.\nThe most common reason that people go into science is altruistic. They loved dinosaurs and spaceships when they were a kid and that never wore off. On some level this is one of the reasons I love this field so much, it is an area where if you can get past all the hard parts can really keep introducing wonder into what you work on every day.\nSometimes I feel like this altruism has negative consequences. For example, I think that there is less emphasis on the career planning and development side in the academic community. I don’t think this is malicious, but I do think that sometimes people think of the career part of science as unseemly. But if you have any job that you want people to pay you to do, then there will be parts of that job that will be career oriented. So if you want to be a professional scientist, being brilliant and good at science is not enough. You also need to pay attention to and plan carefully your career trajectory.\nA colleague of mine, Ben Langmead, created a really nice guide for his postdocs to thinking about and planning the career side of a postdoc which he has over on Github. I thought it was such a good idea that I immediately modified it and asked all of my graduate students and postdocs to fill it out. It is kind of long so there was no penalty if they didn’t finish it, but I think it is an incredibly useful tool for thinking about how to strategize a career in the sciences. I think that the more we are concrete about the career side of graduate school and postdocs, including being honest about all the realistic options available, the better prepared our students will be to succeed on the market.\nYou can find the Leek Group Guide to Career Planning here and make sure you also go check out Ben’s since it was his idea and his is great.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-05-20-is-it-species-or-is-it-batch-they-are-confounded-so-we-cant-know/",
    "title": "Is it species or is it batch? They are confounded, so we can't know",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-05-20",
    "categories": [],
    "contents": "\nIn a 2005 OMICS paper, an analysis of human and mouse gene expression microarray measurements from several tissues led the authors to conclude that “any tissue is more similar to any other human tissue examined than to its corresponding mouse tissue”. Note that this was a rather surprising result given how similar tissues are between species. For example, both mice and humans see with their eyes, breathe with their lungs, pump blood with their hearts, etc… Two follow-up papers (here and here) demonstrated that platform-specific technical variability was the cause of this apparent dissimilarity. The arrays used for the two species were different and thus measurement platform and species were completely confounded. In a 2010 paper, we confirmed that once this technical variability  was accounted for, the number of genes expressed in common  between the same tissue across the two species was much higher than the those expressed in common  between two species across the different tissues (see Figure 2 here).\nSo what is confounding and why is it a problem? This topic has been discussed broadly. We wrote a review some time ago. But based on recent discussions I’ve participated in, it seems that there is still some confusion. Here I explain, aided by some math, how confounding leads to problems in the context of estimating species effects in genomics. We will use\nXi to represent the gene expression measurements for human tissue i,\naX to represent the level of expression that is specific to humans and\nbX to represent the batch effect introduced by the use of the human microarray platform.\nTherefore Xi =aX + bX + ei, with ei the tissue i effect and other uninteresting sources of variability.\nSimilarly, we will use:\nYi to represent the measurements for mouse tissue i\naY  to represent the mouse specific level and\nbY the batch effect introduced by the use of the mouse microarray platform.\nTherefore Yi = aY+ bY + fi, with fi tissue i effect and other uninteresting sources of variability.\nIf we are interested in estimating a species effect that is general across tissues, then we are interested in the following quantity:\n\n aY - aX\n\nNaively, we would think that we can estimate this quantity using the observed differences between the species that cancel out the tissue effect. We observe a difference for each tissue: Y1  - X1 , Y2 - X2 , etc… The problem is that aX and bX are always together as are aY and bY. We say that the batch effect bX is confounded with the species effect aX. Therefore, on average, the observed differences include both the species and the batch effects. To estimate the difference above we would write a model like this:\n\nYi - Xi = (aY - aX) + (bY - bX) + other sources of variability\n\n\nand then estimate the unknown quantities of interest: (aY - aX) and (bY - bX) from the observed data Y1 - X1, Y2 - X2, etc… The problem is that, we can estimate the aggregate effect (aY - aX) + (bY - bX), but, mathematically, we can’t tease apart the two differences.  To see this note that if we are using least squares, the estimates (aY - aX) = 7,  (bY - bX)=3  will fit the data exactly as well as (aY - aX)=3,(bY - bX)=7 since\n\n\n{(Y-X) -(7+3))^2 = {(Y-X)- (3+7)}^2.\n\n\nIn fact, under these circumstances, there are an infinite number of solutions to the standard statistical estimation approaches. A simple analogy is to try to find a unique solution to the equations m+n = 0. If batch and species are not confounded then we are able to tease apart differences just as if we were given another equation: m+n=0; m-n=2. You can learn more about this in this linear models course.\n\n\nNote that the above derivation apply to each gene affected by the batch effect. In practice we commonly see hundreds of genes affected. As a consequence, when we compute distances between two samples from different species we may see large differences even where there is no species effect. This is because the bY - bX  differences for each gene are squared and added up.\n\n\nIn summary, if you completely confound your variable of interest, in this case species, with a batch effect, you will not be able to estimate the effect of either. In fact, in a 2010 Nature Genetics Review  about batch effects we warned about “cases in which batch effects are confounded with an outcome of interest and result in misleading biological or clinical conclusions”. We also warned that none of the existing solutions for batch effects (Combat, SVA, RUV, etc…) can save you from a situation with perfect confounding. Because we can’t always predict what will introduce unwanted variability, we recommend randomization as an experimental design approach.\n\n\nAlmost a decade later after the OMICS paper was published, the same surprising conclusion was reached in this PNAS paper:  “tissues appear more similar to one another within the same species than to the comparable organs of other species”. This time RNAseq was used for both species and therefore the different platform issue was not considered*. Therefore, the authors implicitly assumed that (bY - bX)=0. However, in a recent F1000 Research publication Gilad and Mizrahi-Man describe describe an exercise in forensic bioinformatics that led them to discover that mice and human samples were run in different lanes or different instruments. The confounding was near perfect (see Figure 1). As pointed out by these authors, with this experimental design we can’t  simply accept that (bY - bX)=0, which implies that we can’t estimate a species effect. Gilad and Mizrahi-Man then apply a linear model (ComBat) to account for the batch/species effect and find that samples cluster almost perfectly by tissue. However, Gilad and Mizrahi-Man correctly note that,  due to the confounding, if there is in fact a species effect, this approach will remove it along with the batch effect. Unfortunately, due to the experimental design it will be hard or impossible to determine if it’s batch or if it’s species. More data  and more analyses are needed.\n\nConfounded designs ruin experiments. Current batch effect removal methods will not save you. If you are designing a large genomics experiments, learn about randomization.\n\n * The fact that RNAseq was used does not necessarily mean there is no platform effect. The species have different genomes, with different sequences and thus can lead to different biases during experimental protocols.\n\n\nUpdate: Shin Lin has repeated a small version of the experiment described in the PNAS paper. The new experimental design does not confound lane/instrument with species. The new data confirms their original results pointing to the fact that lane/instrument do not explain the clustering by species. You can see his response in the comments here.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-05-18-residual-expertise/",
    "title": "Residual expertise - or why scientists are amateurs at most of science",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-05-18",
    "categories": [],
    "contents": "\nEditor’s note: I have been unsuccessfully attempting to finish a book I started 3 years ago about how and why everyone should get pumped about reading and understanding scientific papers. I’ve adapted part of one of the chapters into this blogpost. It is pretty raw but hopefully gets the idea across. \nAn episode of_ The Daily Show with Jon Stewart_ featured physicist Lisa Randall, an incredible physicist and noted scientific communicator, as the invited guest.\n\n\n<\/p> \n\n<p style=\"text-align: left; background-color: #ffffff; padding: 4px; margin-top: 4px; margin-bottom: 0px; font-family: Arial, Helvetica, sans-serif; font-size: 12px;\">\n  <b><a href=\"http://thedailyshow.cc.com/\">The Daily Show<\/a><\/b><br /> Get More: <a href=\"http://thedailyshow.cc.com/full-episodes/\">Daily Show Full Episodes<\/a>,<a href=\"http://www.facebook.com/thedailyshow\">The Daily Show on Facebook<\/a>,<a href=\"http://thedailyshow.cc.com/videos\">Daily Show Video Archive<\/a>\n<\/p>\n\n\nNear the end of the interview, Stewart asked Randall why, with all the scientific progress we have made, that we have been unable to move away from fossil fuel-based engines. The question led to the exchange:\n\nRandall: “So this is part of the problem, because I’m a scientist doesn’t mean I know the answer to that question.”\n**\n\n\n** Stewart: ”Oh is that true? Here’s the thing, here’s what’s part of the answer. You could say anything and I would have no idea what you are talking about.”\n\nProfessor Randall is a world leading physicist, the first woman to achieve tenure in physics at Princeton, Harvard, and MIT, and a member of the National Academy of Sciences.2 But when it comes to the science of fossil fuels, she is just an amateur. Her response to this question is just perfect - it shows that even brilliant scientists can just be interested amateurs on topics outside of their expertise. Despite Professor Randall’s over-the-top qualifications, she is an amateur on a whole range of scientific topics from medicine, to computer science, to nuclear engineering. Being an amateur isn’t a bad thing, and recognizing where you are an amateur may be the truest indicator of genius. That doesn’t mean Professor Randall can’t know a little bit about fossil fuels or be curious about why we don’t all have nuclear-powered hovercrafts yet. It just means she isn’t the authority.\nStewart’s response is particularly telling and indicative of what a lot of people think about scientists. It takes years of experience to become an expert in a scientific field - some have suggested as many as 10,000 hours of dedicated time. Professor Randall is a scientist - so she must have more information about any scientific problem than an informed amateur like Jon Stewart. But of course this isn’t true, Jon Stewart (and you) could quickly learn as much about fossil fuels as a scientist if the scientist wasn’t already an expert in the area. Sure a background in physics would help, but there are a lot of moving parts in our dependence on fossil fuels, including social, political, economic problems in addition to the physics involved.\nThis is an example of “residual expertise” - when people without deep scientific training are willing to attribute expertise to scientists even if it is outside their primary area of focus. It is closely related to the logical fallacy behind the argument from authority:\n\nA is an authority on a particular topic\nA says something about that topic\nA is probably correct\n\nthe difference is that with residual expertise you assume that since A is an authority on a particular topic, if they say something about another, potentially related topic, they will probably be correct. This idea is critically important, it is how quacks make their living. The logical leap of faith from “that person is a doctor” to “that person is a doctor so of course they understand epidemiology, or vaccination, or risk communication” is exactly the leap empowered by the idea of residual expertise. It is also how you can line up scientific experts against any well established doctrine like evolution or climate change. Experts in the field will know all of the relevant information that supports key ideas in the field and what it would take to overturn those ideas. But experts outside of the field can be lined up and their residual expertise used to call into question even the most supported ideas.\nWhat does this have to do with you?\nMost people aren’t necessarily experts in scientific disciplines they care about. But becoming a successful amateur requires a much smaller time commitment than becoming an expert, but can still be incredibly satisfying, fun, and useful. This book is designed to help you become a fired-up amateur in the science of your choice. Think of it like a hobby, but one where you get to learn about some of the coolest new technologies and ideas coming out in the scientific literature. If you can ignore the way residual expertise makes you feel silly for reading scientific papers you don’t fully understand - you can still learn a ton and have a pretty fun time doing it.\n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-05-08-the-tyranny-of-the-idea-in-science/",
    "title": "The tyranny of the idea in science",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-05-08",
    "categories": [],
    "contents": "\nThere are a lot of analogies between startups and academic science labs. One thing that is definitely very different is the relative value of ideas in the startup world and in the academic world. For example, Paul Graham has said:\n\nActually, startup ideas are not million dollar ideas, and here’s an experiment you can try to prove it: just try to sell one. Nothing evolves faster than markets. The fact that there’s no market for startup ideas suggests there’s no demand. Which means, in the narrow sense of the word, that startup ideas are worthless.\n\nIn academics, almost the opposite is true. There is huge value to being first with an idea, even if you haven’t gotten all the details worked out or stable software in place. Here are a couple of extreme examples illustrated with Nobel prizes:\nHiggs Boson - Peter Higgs postulated the Boson in 1964, he won the Nobel Prize in 2013 for that prediction, in between tons of people did follow on work, someone convinced Europe to build one of the most expensive pieces of scientific equipment ever built and conservatively thousands of scientists and engineers had to do a ton of work to get the equipment to (a) work and (b) confirm the prediction.\nHuman genome - Watson and Crick postulated the structure of DNA in 1953, they won the Nobel Prize in  medicine in 1962 for this work. But the real value of the human genome was realized when the largest biological collaboration in history sequenced the human genome, along with all of the subsequent work in the genomics revolution.\nThese are two large scale examples where the academic scientific community (as represented by the Nobel committee, mostly because it is a concrete example) rewards the original idea and not the hard work to achieve that idea. I call this, “the tyranny of the idea.” I notice a similar issue on a much smaller scale, for example when people don’t recognize software as a primary product of science. I feel like these decisions devalue the real work it takes to make any scientific idea a reality. Sure the ideas are good, but it isn’t clear that some ideas wouldn’t be discovered by someone else - but surely we aren’t going to build another large hadron collider. I’d like to see the scales correct back the other way a little bit so we put at least as much emphasis on the science it takes to follow through on an idea as on discovering it in the first place.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:18:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-05-07-mendelian-randomization-inspires-a-randomized-trial-design-for-multiple-drugs-simultaneously/",
    "title": "Mendelian randomization inspires a randomized trial design for multiple drugs simultaneously",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-05-07",
    "categories": [],
    "contents": "\nJoe Pickrell has an interesting new paper out about Mendelian randomization. He discusses some of the interesting issues that come up with these studies and performs a mini-review of previously published studies using the technique.\nThe basic idea behind Mendelian Randomization is the following. In a simple, randomly mating population Mendel’s laws tell us that at any genomic locus (a measured spot in the genome) the allele (genetic material you got) you get is assigned at random. At the chromosome level this is very close to true due to properties of meiosis (here is an example of how this looks in very cartoonish form in yeast). A very famous example of this was an experiment performed by Leonid Kruglyak’s group where they took two strains of yeast and repeatedly mated them, then measured genetics and gene expression data. The experimental design looked like this:\n\n \nIf you look at the allele inherited from the two parental strains (BY, RM)  at two separate genes on different chromsomes in each of the 112 segregants (yeast offspring)  they do appear to be random and independent:\n\n \n \nSo this is a randomized trial in yeast where the yeast were each randomized to many many genetic “treatments” simultaneously. Now this isn’t strictly true, since genes on the same chromosomes near each other aren’t exactly random and in humans it is definitely not true since there is population structure, non-random mating and a host of other issues. But you can still do cool things to try to infer causality from the genetic “treatments” to downstream things like gene expression ( and even do a reasonable job in the model organism case).\nIn my mind this raises a potentially interesting study design for clinical trials. Suppose that there are 10 treatments for a disease that we know about. We design a study where each of the patients in the trial was randomized to receive treatment or placebo for each of the 10 treatments. So on average each person would get 5 treatments.  Then you could try to tease apart the effects using methods developed for the Mendelian randomization case. Of course, this is ignoring potential interactions, side effects of taking multiple drugs simultaneously, etc. But I’m seeing lots of interesting proposals for new trial designs (which may or may not work), so I thought I’d contribute my own interesting idea.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-05-01-rafas-citations-above-replacement-in-statistics-journals-is-crazy-high/",
    "title": "Rafa's citations above replacement in statistics journals is crazy high.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-05-01",
    "categories": [],
    "contents": "\nEditor’s note:  I thought it would be fun to do some bibliometrics on a Friday. This is super hacky and the CAR/Y stat should not be taken seriously. \nI downloaded data on the 400 most cited papers between 2000-2010 in some statistical journals from Web of Science. Here is a boxplot of the average number of citations per year (from publication date - 2015) to these papers in the journals Annals of Statistics, Biometrics, Biometrika, Biostatistics, JASA, Journal of Computational and Graphical Statistics, Journal of Machine Learning Research, and Journal of the Royal Statistical Society Series B.\n \n\n \nThere are several interesting things about this graph right away. One is that JASA has the highest median number of citations, but has fewer “big hits” (papers with 100+ citations/year) than Annals of Statistics, JMLR, or JRSS-B. Another thing is how much of a lottery developing statistical methods seems to be. Most papers, even among the 400 most cited, have around 3 citations/year on average. But a few lucky winners have 100+ citations per year. One interesting thing for me is the papers that get 10 or more citations per year but aren’t huge hits. I suspect these are the papers that solve one problem well but don’t solve the most general problem ever.\nSomething that jumps out from that plot is the outlier for the journal Biostatistics. One of their papers is cited 367.85 times per year. The next nearest competitor is 67.75 and it is 19 standard deviations above the mean! The paper in question is: “Exploration, normalization, and summaries of high density oligonucleotide array probe level data”, which is the paper that introduced RMA, one of the most popular methods for pre-processing microarrays ever created. It was written by Rafa and colleagues. It made me think of the statistic “wins above replacement” which quantifies how many extra wins a baseball team gets by playing a specific player in place of a league average replacement.\nWhat about a “citations /year above replacement” statistic where you calculate for each journal:\n\nMedian number of citations to a paper/year with Author X - Median number of citations/year to an average paper in that journal\n\nThen average this number across journals. This attempts to quantify how many extra citations/year a person’s papers generate compared to the “average” paper in that journal. For Rafa the numbers look like this:\nBiostatistics: Rafa = 15.475, Journal = 1.855, CAR/Y =  13.62\nJASA: Rafa = 74.5, Journal = 5.2, CAR/Y = 69.3\nBiometrics: Rafa = 4.33, Journal = 3.38, CAR/Y = 0.95\nSo Rafa’s citations above replacement is (13.62 + 69.3 + 0.95)/3 =  27.96! There are a couple of reasons why this isn’t a completely accurate picture. One is the low sample size, the second is the fact that I only took the 400 most cited papers in each journal. Rafa has a few papers that didn’t make the top 400 for journals like JASA - which would bring down his CAR/Y.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-04-30-figuring-out-learning-objectives-the-hard-way/",
    "title": "Figuring Out Learning Objectives the Hard Way",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-04-30",
    "categories": [],
    "contents": "\nWhen building the Genomic Data Science Specialization (which starts in June!) we had to figure out the learning objectives for each course. We initially set our ambitions high, but as you can see in this video below, Steven Salzberg brought us back to Earth.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-04-29-data-analysis-subcultures/",
    "title": "Data analysis subcultures",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-04-29",
    "categories": [],
    "contents": "\nRoger and I responded to the controversy around the journal that banned p-values today in Nature. A piece like this requires a lot of information packed into very little space but I thought one idea that deserved to be talked about more was the idea of data analysis subcultures. From the paper:\n\nData analysis is taught through an apprenticeship model, and different disciplines develop their own analysis subcultures. Decisions are based on cultural conventions in specific communities rather than on empirical evidence. For example, economists call data measured over time ‘panel data’, to which they frequently apply mixed-effects models. Biomedical scientists refer to the same type of data structure as ‘longitudinal data’, and often go at it with generalized estimating equations.\n\nI think this is one of the least appreciated components of modern data analysis. Data analysis is almost entirely taught through an apprenticeship culture with completely different behaviors taught in different disciplines. All of these disciplines agree about the mathematical optimality of specific methods under very specific conditions. That is why you see methods like randomized trials Roger and I responded to the controversy around the journal that banned p-values today [in Nature.](http://www.nature.com/news/statistics-p-values-are-just-the-tip-of-the-iceberg-1.17412) A piece like this requires a lot of information packed into very little space but I thought one idea that deserved to be talked about more was the idea of data analysis subcultures. From the paper: across multiple disciplines.\nBut any real data analysis is always a multi-step process involving data cleaning and tidying, exploratory analysis, model fitting and checking, summarization and communication. If you gave someone from economics, biostatistics, statistics, and applied math an identical data set they’d give you back very different reports on what they did, why they did it, and what it all meant. Here are a few examples I can think of off the top of my head:\nEconomics calls longitudinal data panel data and uses mostly linear mixed effects models, while generalized estimating equations are more common in biostatistics (this is the example from Roger/my paper).\nIn genome wide association studies the family wise error rate is the most common error rate to control. In gene expression studies people frequently use the false discovery rate.\nThis is changing a bit, but if you learned statistics at Duke you are probably a Bayesian and if you learned at Berkeley you are probably a frequentist.\nPsychology has a history of using parametric statistics, genomics is big into empirical Bayes, and you see a lot of Bayesian statistics in climate studies.\nYou see [Roger and I responded to the controversy around the journal that banned p-values today in Nature. A piece like this requires a lot of information packed into very little space but I thought one idea that deserved to be talked about more was the idea of data analysis subcultures. From the paper:\n\nData analysis is taught through an apprenticeship model, and different disciplines develop their own analysis subcultures. Decisions are based on cultural conventions in specific communities rather than on empirical evidence. For example, economists call data measured over time ‘panel data’, to which they frequently apply mixed-effects models. Biomedical scientists refer to the same type of data structure as ‘longitudinal data’, and often go at it with generalized estimating equations.\n\nI think this is one of the least appreciated components of modern data analysis. Data analysis is almost entirely taught through an apprenticeship culture with completely different behaviors taught in different disciplines. All of these disciplines agree about the mathematical optimality of specific methods under very specific conditions. That is why you see methods like randomized trials Roger and I responded to the controversy around the journal that banned p-values today [in Nature.](http://www.nature.com/news/statistics-p-values-are-just-the-tip-of-the-iceberg-1.17412) A piece like this requires a lot of information packed into very little space but I thought one idea that deserved to be talked about more was the idea of data analysis subcultures. From the paper: across multiple disciplines.\nBut any real data analysis is always a multi-step process involving data cleaning and tidying, exploratory analysis, model fitting and checking, summarization and communication. If you gave someone from economics, biostatistics, statistics, and applied math an identical data set they’d give you back very different reports on what they did, why they did it, and what it all meant. Here are a few examples I can think of off the top of my head:\nEconomics calls longitudinal data panel data and uses mostly linear mixed effects models, while generalized estimating equations are more common in biostatistics (this is the example from Roger/my paper).\nIn genome wide association studies the family wise error rate is the most common error rate to control. In gene expression studies people frequently use the false discovery rate.\nThis is changing a bit, but if you learned statistics at Duke you are probably a Bayesian and if you learned at Berkeley you are probably a frequentist.\nPsychology has a history of using parametric statistics, genomics is big into empirical Bayes, and you see a lot of Bayesian statistics in climate studies.\nYou see](http://en.wikipedia.org/wiki/White_test) used a lot in econometrics, but that is hardly ever done through formal hypothesis testing in biostatistics.\nTraining sets and test sets are used in machine learning for prediction, but rarely used for inference.\nThis is just a partial list I thought of off the top of my head, there are a ton more. These decisions matter a lot in a data analysis.  The problem is that the behavioral component of a data analysis is incredibly strong, no matter how much we’d like to think of the process as mathematico-theoretical. Until we acknowledge that the most common reason a method is chosen is because, “I saw it in a widely-cited paper in journal XX from my field” it is likely that little progress will be made on resolving the statistical problems in science.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-04-13-genomics-case-studies-online-courses-start-in-two-weeks-427/",
    "title": "Genomics Case Studies Online Courses Start in Two Weeks (4/27)",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-04-13",
    "categories": [],
    "contents": "\nThe last month of the HarvardX Data Analysis for Genomics series start on 4/27. We will cover case studies on RNAseq, Variant calling, ChipSeq and DNA methylation. Faculty includes Shirley Liu, Mike Love, Oliver Hoffman and the HSPH Bioinformatics Core. Although taking the previous courses on the series will help, the four case study courses were developed as stand alone and you can obtain a certificate for each one without taking any other course.\nEach course is presented over two weeks but will remain open until June 13 to give students an opportunity to take them all if they wish. For more information follow the links listed below.\nRNA-seq data analysis will be lead by Mike Love\nVariant Discovery and Genotyping will be taught by Shannan Ho Sui, Oliver Hofmann, Radhika Khetani and Meeta Mistry (from the The HSPH Bioinformatics Core)\nChIP-seq data analysis will be lead by Shirley Liu\nDNA methylation data analysis will be lead by Rafael Irizarry\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-04-13-why-is-there-so-much-university-administration-we-kind-of-asked-for-it/",
    "title": "Why is there so much university administration? We kind of asked for it.",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-04-13",
    "categories": [],
    "contents": "\nThe latest commentary on the rising cost of college tuition is by Paul F. Campos and is titled The Real Reason College Tuition Costs So Much. There has been much debate about this article and whether Campos is right or wrong…and I don’t plan to add to that. However, I wanted to pick up on a major point of the article that I felt got left hanging out there: The rising levels of administrative personnel at universities.\nCampos argues that the reason college tuition is on the rise is not that colleges get less and less money from the government (mostly state government for state schools), but rather that there is an increasing number of administrators at universities that need to be paid in dollars and cents. He cites a study that shows that for the California State University system, in a 34 year period, the number of of faculty rose by about 3% whereas the number of administrators rose by 221%.\nMy initial thinking when I saw the 221% number was “only that much?” I’ve been a faculty member at Johns Hopkins now for about 10 years, and just in that short period I’ve seen the amount of administrative work I need to do go up what feels like at least 221%. Partially, of course, that is a result of climbing up the ranks. As you get more qualified to do administrative work, you get asked to do it! But even adjusting for that, there are quite a few things that faculty need to do now that they weren’t required to do before.  Frankly, I’m grateful for the few administrators that we do have around here to help me out with various things.\nCampos seems to imply (but doesn’t come out and say) that the bulk of administrators are not necessary. And that if we were to cut these people from the payrolls, that we could reduce tuition down to what it was in the old days. Or at least, it would be cheaper. This argument reminds me about debates over the federal budget: Everyone thinks the budget is too big, but no one wants to suggest something to cut.\nMy point here is that the reason there are so many administrators is that there’s actually quite a bit of administration to do. And the amount of administration that needs to be done has increased over the past 30 years.\nJust for fun, I decided to go to the Johns Hopkins University Administration web site to see who all these administrators were.  This site shows the President’s Cabinet and the Deans of the individual schools, which isn’t everybody, but it represents a large chunk. I don’t know all of these people, but I have met and worked with a few of them.\nFor the moment I’m going to skip over individual people because, as much as you might think they are overpaid, no individual’s salary is large enough to move the needle on college tuition. So I’ll stick with people who actually represent large offices with staff. Here’s a sample.\nUniversity President. Call me crazy, but I think the university needs a President. In the U.S. the university President tends to focus on outward facing activities like raising money from various sources, liasoning with the government(s), and pushing university initiatives around the world. This is not something I want to do (but I think it’s necessary), I’d rather have the President take care of it for me.\nUniversity Provost. At most universities in the U.S. the Provost is the “senior academic officer”, which means that he/she runs the university. This is a big job, especially at big universities, and require coordinating across a variety of constituencies. Also, at JHU, the Provost’s office deals with a number of compliance related issues like Title IX, accreditation, Americans with Disabilities Act, and many others. I suppose we could save some money by violating federal law, but that seems short-sighted. The people in this office do tough work involving a ton of paper. One example involves online education. Most states in the U.S. say that if you’re going to run an education program in their state, it needs to be approved by some regulatory body. Some states have essentially a reciprocal agreement, so if it’s okay in your state, then it’s okay in their state. But many states require an entire approval process for a program to run in that state. And by “a program” I mean something like an M.S. in Mathematics. If you want to run an M.S. in English that’s another approval, etc. So someone has to go to all the 50 states and D.C. and get approval for every online program that JHU runs in order to enroll students into that program from that state. I think Arkansas actually requires that someone come to Arkansas and testify in person about a program asking for approval.\nI support online education programs, and I’m glad the Provost’s office is getting all those approvals for us.\n\nCorporate Security. This may be a difficult one for some people to understand, but bear in mind that much of Johns Hopkins is located in East Baltimore. If you’ve ever seen the TV show The Wire, then you know why we need corporate security.\nFacilities and Real Estate. Johns Hopkins owns and deals with a lot of real estate; it’s a big organization. Who is supposed to take care of all that? For example, we just installed a brand new supercomputer jointly with the University of Maryland, called MARCC. I’m really excited to use this supercomputer for research, but systems like this require a bit of space. A lot of space actually. So we needed to get some land to put it on. If you’ve ever bought a house, you know how much paperwork is involved.\nDevelopment and Alumni Relations. I have a new appreciation for this office now that I co-direct a program that has enrolled over 1.5 million people in just over a year. It’s critically important that we keep track of our students for many reasons: tracking student careers and success, tapping them to mentor current students, developing relationships with organizations that they’re connected to are just a few.\nGeneral Counsel. I’m not he lawbreaking type, so I need lawyers to help me out.\nEnterprise Development. This office involves, among other things, technology transfer, which I have recently been involved with quite a bit for my role in the Data Science Specialization offered through Coursera. This is just to say that I personally benefit from this office. I’ve heard people say that universities shouldn’t be involved in tech transfer, but Bayh-Dole is what it is and I think Johns Hopkins should play by the same rules as everyone else. I’m not interested in filing patents, trademarks, and copyrights, so it’s good to have people doing that for me.\nOkay, that’s just a few offices, but you get the point. These administrators seem to be doing a real job (imagine that!) and actually helping out the university. Many of these people are actually helping me out. Some of these jobs are essentially required by the existence of federal laws, and so we need people like this.\nSo, just to recap, I think there are in fact more administrators in universities than there used to be. Is this causing an increase in tuition? It’s possible, but it’s probably not the only cause. If you believe the CSU study, there was about a 3.5% annual increase in the number of administrators each year from 1975 to 2008. College tuition during that time period went up around 4% per year (inflation adjusted). But even so, much of this administration needs to be done (because faculty don’t want to do it), so this is a difficult path to go down if you’re looking for ways to lower tuition.\nEven if we’ve found the smoking gun, the question is what do we do about it?\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-04-09-a-blessing-of-dimensionality-often-observed-in-high-dimensional-data-sets/",
    "title": "A blessing of dimensionality often observed in high-dimensional data sets",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-04-09",
    "categories": [],
    "contents": "\n have one observation per row and one variable per column.  Using this definition, big data sets can be either:\nWide - a wide data set has a large number of measurements per observation, but fewer observations. This type of data set is typical in neuroimaging, genomics, and other biomedical applications.\nTall - a tall data set has a large number of observations, but fewer measurements. This is the typical setting in a large clinical trial or in a basic social network analysis.\nThe curse of dimensionality tells us that estimating some quantities gets harder as the number of dimensions of a data set increases - as the data gets taller or wider. An example of this was nicely illustrated by my student Prasad (although it looks like his quota may be up on Rstudio).\nFor wide data sets there is also a blessing of dimensionality. The basic reason for the blessing of dimensionality is that:\n\nNo matter how many new measurements you take on a small set of observations, the number of observations and all of their characteristics are fixed.\n\nAs an example, suppose that we make measurements on 10 people. We start out by making one measurement (blood pressure), then another (height), then another (hair color) and we keep going and going until we have one million measurements on those same 10 people. The blessing occurs because the measurements on those 10 people will all be related to each other. If 5 of the people are women and 5 or men, then any measurement that has a relationship with sex will be highly correlated with any other measurement that has a relationship with sex. So by knowing one small bit of information, you can learn a lot about many of the different measurements.\nThis blessing of dimensionality is the key idea behind many of the statistical approaches to wide data sets whether it is stated explicitly or not. I thought I’d make a very short list of some of these ideas:\n1. Idea: De-convolving mixed observations from high-dimensional data. \nHow the blessing plays a role: The measurements for each observation are assumed to be a mixture of values measured from different observation types. The proportion of each observation type is assumed to be fixed across measurements, so you can take advantage of the multiple measurements to estimate the mixing percentage and perform the deconvolution. (Wenyi Wang came and gave an excellent seminar on this idea at JHU a couple of days ago, which inspired this post).\n2. Idea: The two groups model for false discovery rates.\nHow the blessing plays a role:  The models assume that a hypothesis test is performed for each observation and that the probability any observation is drawn from the null, the null distribution, and the alternative distributions are common across observations. If the null is assumed known, then it is possible to use the known null distribution to estimate the common probability that an observation is drawn from the null.\n \n3. Idea: Empirical Bayes variance shrinkage for linear models\nHow the blessing plays a role:  A linear model is fit for each observation and the means and variances of the log ratios calculated from the model are assumed to follow a common distribution across observations. The method estimates the hyper-parameters of these common distributions and uses them to adjust any individual measurement’s estimates.\n \n4. Idea: Surrogate variable analysis\nHow the blessing plays a role:  Each observation is assumed to be influenced by a single variable of interest (a primary variable) and multiple unmeasured confounders. Since the observations are fixed, the values of the unmeasured confounders are the same for each measurement and a supervised PCA can be used to estimate surrogates for the confounders. (see my JHU job talk for more on the blessing)\n \nThe blessing of dimensionality I’m describing here is related to the idea that Andrew Gelman refers to in this 2004 post.  Basically, since increasingly large number of measurements are made on the same observations there is an inherent structure to those observations. If you take advantage of that structure, then as the dimensionality of your problem increases you actually get better estimates of the structure in your high-dimensional data - a nice blessing!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-04-09-how-to-get-ahead-in-academia/",
    "title": "How to Get Ahead in Academia",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-04-09",
    "categories": [],
    "contents": "\nThis video on how to make it in academia was produced over 10 years ago by Steven Goodman for the ENAR Junior Researchers Workshop. Now the whole world can benefit from its wisdom.\nThe movie features current and former JHU Biostatistics faculty, including Francesca Dominici, Giovanni Parmigiani, Scott Zeger, and Tom Louis. You don’t want to miss Scott Zeger’s secret formula for getting promoted!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-04-02-why-you-need-to-study-statistics/",
    "title": "Why You Need to Study Statistics",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-04-02",
    "categories": [],
    "contents": "\nThe American Statistical Association is continuing its campaign to get you to study statistics, if you haven’t already. I have to agree with them that being a statistician is a pretty good job. Their latest video highlights a wide range of statisticians working in industry, government, and academia. You can check it out here:\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-03-26-teaser-trailer-for-the-genomic-data-science-specialization-on-coursera/",
    "title": "Teaser trailer for the Genomic Data Science Specialization on Coursera",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-03-26",
    "categories": [],
    "contents": "\n \nWe have been hard at work in the studio putting together our next specialization to launch on Coursera. It will be called the “Genomic Data Science Specialization” and includes a spectacular line up of instructors: Steven Salzberg, Ela Pertea, James Taylor, Liliana Florea, Kasper Hansen, and me. The specialization will cover command line tools, statistics, Galaxy, Bioconductor, and Python. There will be a capstone course at the end of the sequence featuring an in-depth genomic analysis. If you are a grad student, postdoc, or principal investigator in a group that does genomics this specialization is for you. If you are a person looking to transition into one of the hottest areas of research with the new precision medicine initiative this is for you. Get pumped and share the teaser-trailer with your friends!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-03-24-introduction-to-bioconductor-harvardx-mooc-starts-this-monday-march-30/",
    "title": "Introduction to Bioconductor HarvardX MOOC starts this Monday March 30",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-03-24",
    "categories": [],
    "contents": "\nBioconductor is one of the most widely used open source toolkits for biological high-throughput data. In this four week course, co-taught with Vince Carey and Mike Love, we will introduce you to Bioconductor’s general infrastructure and then focus on two specific technologies: next generation sequencing and microarrays. The lectures and assessments will be annotated in case you want to focus only on one of these two technologies. Although if you plan to be a bioinformatician we recommend you learn both.\nTopics covered include:\nA short introduction to molecular biology and measurement technology\nAn overview on how to leverage the platform and genome annotation packages and experimental archives\nGenomicsRanges: the infrastructure for storing, manipulating and analyzing next generation sequencing data\nParallel computing and cloud concepts\nNormalization, preprocessing and bias correction.\nStatistical inference in practice: including hierarchical models and gene set enrichment analysis\nBuilding statistical analysis pipelines of genome-scale assays including the creation of reproducible reports\nThroughout the class we will be using data examples from both next generation sequencing and microarray experiments.\nWe will assume basic knowledge of Statistics and R.\nFor more information visit the course website.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-03-19-a-surprisingly-tricky-issue-when-using-genomic-signatures-for-personalized-medicine/",
    "title": "A surprisingly tricky issue when using genomic signatures for personalized medicine",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-03-19",
    "categories": [],
    "contents": "\nMy student Prasad Patil has a really nice paper that just came out in Bioinformatics (preprint in case paywalled). The paper is about a surprisingly tricky normalization issue with genomic signatures. Genomic signatures are basically statistical/machine learning functions applied to the measurements for a set of genes to predict how long patients will survive, or how they will respond to therapy. The issue is that usually when building and applying these signatures, people normalize across samples in the training and testing set.\nAn example of this normalization is to mean-center the measurements for each gene in the testing/application stage, then apply the prediction rule. The problem is that if you use a different set of samples when calculating the mean you can get a totally different prediction function. The basic problem is illustrated in this graphic.\n \n\n \nThis seems like a pretty esoteric statistical issue, but it turns out that this one simple normalization problem can dramatically change the results of the predictions. In particular, we show that the predictions for the same patient, with the exact same data, can change dramatically if you just change the subpopulations of patients within the testing set. In this plot, Prasad made predictions for the exact same set of patients two times when the patient population varied in ER status composition. As many as 30% of the predictions were different for the same patient with the same data if you just varied who they were being predicted with.\n\n \nThis paper highlights how tricky statistical issues can slow down the process of translating ostensibly really useful genomic signatures into clinical practice and lends even more weight to the idea that precision medicine is a statistical field.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-03-18-a-simple-and-fair-way-all-statistics-journals-could-drive-up-their-impact-factor/",
    "title": "A simple (and fair) way all statistics journals could drive up their impact factor.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-03-18",
    "categories": [],
    "contents": "\nHypothesis:\n\nIf every method in every stats journal was implemented in a corresponding R package (easy), was required to have a  companion document that was a tutorial on how to use the software (easy), included a reference to how to cite the paper if you used the software (easy) and the paper/tutorial was posted to the relevant message boards for the communities of interest (easy) that journal would see a dramatic bump in its impact factor.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-03-17-data-science-done-well-looks-easy-and-that-is-a-big-problem-for-data-scientists/",
    "title": "Data science done well looks easy - and that is a big problem for data scientists",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-03-17",
    "categories": [],
    "contents": "\nData science has a ton of different definitions. For the purposes of this post I’m going to use the definition of data science we used when creating our Data Science program online. Data science is:\n\nData science is the process of formulating a quantitative question that can be answered with data, collecting and cleaning the data, analyzing the data, and communicating the answer to the question to a relevant audience.\n\nIn general the data science process is iterative and the different components blend together a little bit. But for simplicity lets discretize the tasks into the following 7 steps:\nDefine the question of interest\nGet the data\nClean the data\nExplore the data\nFit statistical models\nCommunicate the results\nMake your analysis reproducible\nA good data science project answers a real scientific or business analytics question. In almost all of these experiments the vast majority of the analyst’s time is spent on getting and cleaning the data (steps 2-3) and communication and reproducibility (6-7). In most cases, if the data scientist has done her job right the statistical models don’t need to be incredibly complicated to identify the important relationships the project is trying to find. In fact, if a complicated statistical model seems necessary, it often means that you don’t have the right data to answer the question you really want to answer. One option is to spend a huge amount of time trying to tune a statistical model to try to answer the question but serious data scientist’s usually instead try to go back and get the right data.\nThe result of this process is that most well executed and successful data science projects don’t (a) use super complicated tools or (b) fit super complicated statistical models. The characteristics of the most successful data science projects I’ve evaluated or been a part of are: (a) a laser focus on solving the scientific problem, (b) careful and thoughtful consideration of whether the data is the right data and whether there are any lurking confounders or biases and (c) relatively simple statistical models applied and interpreted skeptically.\nIt turns out doing those three things is actually surprisingly hard and very, very time consuming. It is my experience that data science projects take a solid 2-3 times as long to complete as a project in theoretical statistics. The reason is that inevitably the data are a mess and you have to clean them up, then you find out the data aren’t quite what you wanted to answer the question, so you go find a new data set and clean it up, etc. After a ton of work like that, you have a nice set of data to which you fit simple statistical models and then it looks super easy to someone who either doesn’t know about the data collection and cleaning process or doesn’t care.\nThis poses a major public relations problem for serious data scientists. When you show someone a good data science project they almost invariably think “oh that is easy” or “that is just a trivial statistical/machine learning model” and don’t see all of the work that goes into solving the real problems in data science. A concrete example of this is in academic statistics. It is customary for people to show theorems in their talks and maybe even some of the proof. This gives people working on theoretical projects an opportunity to “show their stuff” and demonstrate how good they are. The equivalent for a data scientist would be showing how they found and cleaned multiple data sets, merged them together, checked for biases, and arrived at a simplified data set. Showing the “proof” would be equivalent to showing how they matched IDs. These things often don’t look nearly as impressive in talks, particularly if the audience doesn’t have experience with how incredibly delicate real data analysis is. I imagine versions of this problem play out in industry as well (candidate X did a good analysis but it wasn’t anything special, candidate Y used Hadoop to do BIG DATA!).\nThe really tricky twist is that bad data science looks easy too. You can scrape a data set off the web and slap a machine learning algorithm on it no problem. So how do you judge whether a data science project is really “hard” and whether the data scientist is an expert? Just like with anything, there is no easy shortcut to evaluating data science projects. You have to ask questions about the details of how the data were collected, what kind of biases might exist, why they picked one data set over another, etc.  In the meantime, don’t be fooled by what looks like simple data science - it can often be pretty effective.\n \nEditor’s note: If you like this post, you might like my pay-what-you-want book Elements of Data Analytic Style: https://leanpub.com/datastyle\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-03-14-pi-day-special-how-to-use-bioconductor-to-find-empirical-evidence-in-support-of-pi-being-a-normal-number/",
    "title": "pi day special: How to use Bioconductor to find empirical evidence in support of pi being a normal number",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-03-14",
    "categories": [],
    "contents": "\nEditor’s note: Today 3/14/15 at some point between 9:26:53 and 9:26:54 it was the most π day of them all. Below is a repost from last year.\nHappy π day everybody!\nI wanted to write some simple code (included below) to the test parallelization capabilities of my new cluster. So, in honor of π day, I decided to check for evidence that π is a normal number. A normal number is a real number whose infinite sequence of digits has the property that picking any given random m digit pattern is 10−m. For example, using the Poisson approximation, we can predict that the pattern “123456789” should show up between 0 and 3 times in the first billion digits of π (it actually shows up twice, starting at the 523,551,502-th and 773,349,079-th decimal places).\nTo test our hypothesis, let Y1, …, Y100 be the number of “00”, “01”, …,“99” in the first billion digits of π. If π is in fact normal then the Ys should be approximately IID binomials with N=1 billon and p=0.01. In the qq-plot below I show Z-scores (Y - 10,000,000) / √9,900,000) which appear to follow a normal distribution as predicted by our hypothesis. Further evidence for π being normal is provided by repeating this experiment for 3,4,5,6, and 7 digit patterns (for 5,6 and 7 I sampled 10,000 patterns). Note that we can perform a chi-square test for the uniform distribution as well. For patterns of size 1,2,3 the p-values were 0.84, 0.89, 0.92, and 0.99.\n\nAnother test we can perform is to divide the 1 billion digits into 100,000 non-overlapping segments of length 10,000. The vector of counts for any given pattern should also be binomial. Below I also include these qq-plots.\n\nThese observed counts should also be independent, and to explore this we can look at autocorrelation plots:\n\nTo do this in about an hour and with just a few lines of code (included below), I used the Bioconductor Biostrings package to match strings and the foreach function to parallelize.\n\n\nlibrary(Biostrings)\nlibrary(doParallel)\n\nregisterDoParallel(cores = 48)\n\nx <- scan(\"pi-billion.txt\", what=\"c\")\nx <- substr(x, 3, nchar(x)) ##remove 3.\n\nx <- BString(x)\nn <- length(x)\np <- 1/(10^d)\n\npar(mfrow=c(2,3))\nfor(d in 2:4){\n  if(d<5){\n    patterns <- sprintf(paste0(\"%0\", d, \"d\"), seq(0, 10^d - 1))\n    } \n  else{\n    patterns <- sprintf(paste0(\"%0\", d, \"d\"), sample(10^d, 10^4) - 1)\n  }\n  \n  res <- foreach(pat=patterns, .combine=c) %dopar% countPattern(pat, x)\n  \n  z <- (res - n*p) / sqrt(n*p*(1-p))\n  \n  qqnorm(z, xlab=\"Theoretical quantiles\", ylab=\"Observed z-scores\",\n         main=paste(d, \"digits\"))\n  abline(0,1)\n  if(d<5) print(1-pchisq(sum((res - n*p)^2 / (n*p)), length(res) - 1))\n}\n\n## Now count in segments\nd <- 1\nm <- 10^5\npatterns <-sprintf(paste0(\"%0\", d, \"d\"), seq(0, 10^d - 1))\n\nres <- foreach(pat=patterns, .combine=cbind) %dopar% { \n  tmp <- start(matchPattern(pat, x))\n  tmp2 <- floor((tmp-1)/m)\n  return(tabulate(tmp2+1, nbins=n/m))\n}\n\n## qq-plots\npar(mfrow=c(2,5))\np <- 1/(10^d)\n\nfor(i in 1:ncol(res)){\n  z <- (res[,i] - m*p) / sqrt(m*p*(1-p))\n  qqnorm(z, xlab=\"Theoretical quantiles\", ylab=\"Observed z-scores\", main=paste(i-1))\n  abline(0,1)\n}\n\n## ACF plots\npar(mfrow=c(2,5))\nfor(i in 1:ncol(res)) \n  acf(res[,i])\n\n\nNB: A normal number has the above stated property in any base. The examples above a for base 10.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-12-27T15:39:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-03-13-de-weaponizing-reproducibility/",
    "title": "De-weaponizing reproducibility",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-03-13",
    "categories": [],
    "contents": "\n\nA couple of weeks ago Roger and I went to a conference on statistical reproducibility held at the National Academy of Sciences. The discussion was pretty wide ranging and I love that the thinking about reproducibility is coming back to statistics. There was pretty widespread support for the idea that prevention is the right way to approach reproducibility.\n\n\n\n\n\nIt turns out I was the last speaker of the whole conference. This is an unenviable position to be in with so many bright folks speaking first as they covered a huge amount of what I wanted to say. My talk focused on three key points:\n\n\n\n\nThe tools for reproducibility already exist, the barrier isn’t tools\nWe need to de-weaponize reproducibility\nPrevention is the right approach to reproducibility\n \nIn terms of the first point, tools like iPython, knitr, and Galaxy can be used to all but the absolutely largest analysis reproducible right now.  Our group does this all the time with our papers and so do many others. The problem isn’t a lack of tools.\nSpeaking to point two, I think many people would agree that part of the issue is culture change. One issue that is increasingly concerning to me is the “weaponization” of reproducibility.  I have been noticing is that some of us (like me, my students, other folks at JHU, and lots of particularly junior computational people elsewhere) are trying really hard to be reproducible. Most of the time this results in really positive reactions from the community. But when a co-author of mine and I wrote that paper about the science-wise false discovery rate, one of the discussants used our code (great), improved on it (great), identified a bug (great), and then did his level best to humiliate us both in front of the editor and the general public because of that bug (not so great).\n\n\n\n\nI have seen this happen several times. Most of the time if a paper is reproducible the authors get a pat on the back and their code is either ignored, or used in a positive way. But for high-profile and important problems, people  largely use eproducibility to:\n\n\n\n\n Impose regulatory hurdles in the short term while people transition to reproducibility. One clear example of this is the Secret Science Reform Act which is a bill that imposes strict reproducibility conditions on all science before it can be used as evidence for regulation.\nHumiliate people who aren’t good coders or who make mistakes in their code. This is what happened in my paper when I produced reproducible code for my analysis, but has also happened to other people.\nTake advantage of people’s code to plagiarize/straight up steal work. I have stories about this I’d rather not put on the internet\n \nOf the three, I feel like (1) and (2) are the most common. Plagiarism and scooping by theft I think are actually relatively rare based on my own anecdotal experience. But I think that the “weaponization” of reproducibility to block regulation or to humiliate folks who are new to computational sciences is more common than I’d like it to be. Until reproducibility is the standard for everyone - which I think is possible now and will happen as the culture changes -  the people who are the early adopters are at risk of being bludgeoned with their own reproducibility. As a community, if we want widespread reproducibility adoption we have to be ferocious about not allowing this to happen.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-03-03-the-elements-of-data-analytic-style-so-much-for-a-soft-launch/",
    "title": "The elements of data analytic style - so much for a soft launch",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-03-03",
    "categories": [],
    "contents": "\nEditor’s note: I wrote a book called Elements of Data Analytic Style. Buy it on Leanpub or Amazon! If you buy it on Leanpub, you get all updates (there are likely to be some) for free and you can pay what you want (including zero) but the author would be appreciative if you’d throw a little scratch his way. \nSo uh, I was going to soft launch my new book The Elements of Data Analytic Style yesterday. I figured I’d just quietly email my Coursera courses to let them know I created a new reference. It turns out that that wasn’t very quiet. First this happened:\n\n\n@jtleek @albertocairo @simplystats Instabuy. And apparently not just for me: it looks like you just Slashdotted @leanpub’s website.\n\n\n— Andrew Janke (@AndrewJanke) March 2, 2015\n\n\n \nand sure enough the website was down:\n \n\n \n \nthen overnight it did something like 6,000+ units:\n \n\n \n \nSo lesson learned, there is no soft open with Coursera. Here is the post I was going to write though:\n \n### Post I was gonna write\nI have been doing data analysis for something like 10 years now (gulp!) and teaching data analysis in person for 6+ years. One of the things we do in my data analysis class at Hopkins is to perform a complete data analysis (from raw data to written report) every couple of weeks. Then I grade each assignment for everything from data cleaning to the written report and reproducibility. I’ve noticed over the course of teaching this class (and classes online) that there are many common elements of data analytic style that I don’t often see in textbooks, or when I do, I see them spread across multiple books.\nI’ve posted on some of these issues in some open source guides I’ve posted to Github like:\n10 things statistics taught us about big data analysis\nThe Leek Group Guide to R packages\nHow to share data with a statistician\nBut I decided that it might be useful to have a more complete guide to the “art” part of data analysis. One goal is to summarize in a succinct way the most common difficulties encountered by practicing data analysts. It may be a useful guide for peer reviewers who could refer to section numbers when evaluating manuscripts, for instructors who have to grade data analyses, as a supplementary text for a data analysis class, or just as a useful reference. It is modeled loosely in format and aim on the Elements of Style by William Strunk. Just as with the EoS, both the checklist and my book cover a small fraction of the field of data analysis, but my experience is that once these elements are mastered, data analysts benefit most from hands on experience in their own discipline of application, and that many principles may be non-transferable beyond the basics. But just as with writing, new analysts would do better to follow the rules until they know them well enough to violate them.\nBuy EDAS on Leanpub\nBuy EDAS on Amazon\nThe book includes a basic checklist that may be useful as a guide for beginning data analysts or as a rubric for evaluating data analyses. I’m reproducing it here so you can comment/hate/enjoy on it.\n \nThe data analysis checklist\nThis checklist provides a condensed look at the information in this book. It can be used as a guide during the process of a data analysis, as a rubric for grading data analysis projects, or as a way to evaluate the quality of a reported data analysis.\nI Answering the question\nDid you specify the type of data analytic question (e.g. exploration, assocation causality) before touching the data?\nDid you define the metric for success before beginning?\nDid you understand the context for the question and the scientific or business application?\nDid you record the experimental design?\nDid you consider whether the question could be answered with the available data?\nII Checking the data\nDid you plot univariate and multivariate summaries of the data?\nDid you check for outliers?\nDid you identify the missing data code?\nIII Tidying the data\nIs each variable one column?\nIs each observation one row?\nDo different data types appear in each table?\nDid you record the recipe for moving from raw to tidy data?\nDid you create a code book?\nDid you record all parameters, units, and functions applied to the data?\nIV Exploratory analysis\nDid you identify missing values?\nDid you make univariate plots (histograms, density plots, boxplots)?\nDid you consider correlations between variables (scatterplots)?\nDid you check the units of all data points to make sure they are in the right range?\nDid you try to identify any errors or miscoding of variables?\nDid you consider plotting on a log scale?\nWould a scatterplot be more informative?\nV Inference\nDid you identify what large population you are trying to describe?\nDid you clearly identify the quantities of interest in your model?\nDid you consider potential confounders?\nDid you identify and model potential sources of correlation such as measurements over time or space?\nDid you calculate a measure of uncertainty for each estimate on the scientific scale?\nVI Prediction\nDid you identify in advance your error measure?\nDid you immediately split your data into training and validation?\nDid you use cross validation, resampling, or bootstrapping only on the training data?\nDid you create features using only the training data?\nDid you estimate parameters only on the training data?\nDid you fix all features, parameters, and models before applying to the validation data?\nDid you apply only one final model to the validation data and report the error rate?\nVII Causality\nDid you identify whether your study was randomized?\nDid you identify potential reasons that causality may not be appropriate such as confounders, missing data, non-ignorable dropout, or unblinded experiments?\nIf not, did you avoid using language that would imply cause and effect?\nVIII Written analyses\nDid you describe the question of interest?\nDid you describe the data set, experimental design, and question you are answering?\nDid you specify the type of data analytic question you are answering?\nDid you specify in clear notation the exact model you are fitting?\nDid you explain on the scale of interest what each estimate and measure of uncertainty means?\nDid you report a measure of uncertainty for each estimate on the scientific scale?\nIX Figures\nDoes each figure communicate an important piece of information or address a question of interest?\nDo all your figures include plain language axis labels?\nIs the font size large enough to read?\nDoes every figure have a detailed caption that explains all axes, legends, and trends in the figure?\nX Presentations\nDid you lead with a brief, understandable to everyone statement of your problem?\nDid you explain the data, measurement technology, and experimental design before you explained your model?\nDid you explain the features you will use to model data before you explain the model?\nDid you make sure all legends and axes were legible from the back of the room?\nXI Reproducibility\nDid you avoid doing calculations manually?\nDid you create a script that reproduces all your analyses?\nDid you save the raw and processed versions of your data?\nDid you record all versions of the software you used to process the data?\nDid you try to have someone else run your analysis code to confirm they got the same answers?\nXI R packages\nDid you make your package name “Googleable”\nDid you write unit tests for your functions?\nDid you write help files for all functions?\nDid you write a vignette?\nDid you try to reduce dependencies to actively maintained packages?\nHave you eliminated all errors and warnings from R CMD CHECK?\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-03-02-advanced-statistics-for-the-life-sciences-mooc-launches-today/",
    "title": "Advanced Statistics for the Life Sciences MOOC Launches Today",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-03-02",
    "categories": [],
    "contents": "\nIn In we will teach statistical techniques that are commonly used in the analysis of high-throughput data and their corresponding R implementations. In Week 1 we will explain inference in the context of high-throughput data and introduce the concept of error controlling procedures. We will describe the strengths and weakness of the Bonferroni correction, FDR and q-values. We will show how to implement these in cases in which  thousands of tests are conducted, as is typically done with genomics data. In Week 2 we will introduce the concept of mathematical distance and how it is used in exploratory data analysis, clustering, and machine learning. We will describe how techniques such as principal component analysis (PCA) and the singular value decomposition (SVD) can be used for dimension reduction in high dimensional data. During week 3 we will describe confounding, latent variables and factor analysis in the context of high dimensional data and how this relates to batch effects. We will show how to implement methods such as SVA to perform inference on data affected by batch effects. Finally, during week 4 we will show how statistical modeling, and empirical Bayes modeling in particular, are powerful techniques that greatly improve precision in high-throughput data. We will be using R code to explain concepts throughout the course. We will also be using exploratory data analysis and data visualization to motivate the techniques we teach during each week.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-02-18-navigating-big-data-careers-with-a-statistics-phd/",
    "title": "Navigating Big Data Careers with a Statistics PhD",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-02-18",
    "categories": [],
    "contents": "\n\nEditor’s note: This is a guest post by Sherri Rose. She is an Assistant Professor of Biostatistics in the Department of Health Care Policy at Harvard Medical School. Her work focuses on nonparametric estimation, causal inference, and machine learning in health settings. Dr. Rose received her BS in statistics from The George Washington University and her PhD in biostatistics from the University of California, Berkeley, where she coauthored a book on Targeted Learning. She tweets @sherrirose.\n\n\n\n\n\nA quick scan of the science and technology headlines often yields two words: big data. The amount of information we collect has continued to increase, and this data can be found in varied sectors, ranging from social media to genomics. Claims are made that big data will solve an array of problems, from understanding devastating diseases to predicting political outcomes. There is substantial “big data” hype in the press, as well as business and academic communities, but how do upcoming, current, and recent statistical science PhDs handle the array of training opportunities and career paths in this new era? Undergraduate interest in statistics degrees is exploding, bringing new talent to graduate programs and the post-PhD job pipeline.  Statistics training is diversifying, with students focusing on theory, methods, computation, and applications, or a blending of these areas. A few years ago, Rafa outlined the academic career options for statistics PhDs in two posts, which cover great background material I do not repeat here. The landscape for statistics PhD careers is also changing quickly, with a variety of companies attracting top statistics students in new roles.  As a new faculty member at the intersection of machine learning, causal inference, and health care policy, I’ve already found myself frequently giving career advice to trainees.  The choices have become much more nuanced than just academia vs. industry vs. government.\n\n\n\n\n\n\n\n\nSo, you find yourself inspired by big data problems and fascinated by statistics. While you are a student, figuring out what you enjoy working on is crucial. This exploration could involve engaging in internship opportunities or collaborating with multiple faculty on different types of projects. Both positive and negative experiences can help you identify your preferences.\n\n\n\n\n\n\n\n\nUndergraduates may wish to spend a couple months at a Summer Institute for Training in Biostatistics or National Science Foundation Research Experience for Undergraduates. There are also many MOOC options to get a taste of different areas ofstatistics. Selecting a graduate program for PhD study can be a difficult choice, especially when your interests within statistics have yet to be identified, as is often the case for undergraduates. However, if you know that you have interests in software and programming, it can be easy to sort which statistical science PhD programs have a curricular or research focus in this area by looking at department websites. Similarly, if you know you want to work in epidemiologic methods, genomics, or imaging, specific programs are going to jump right to the top as good fits. Getting advice from faculty in your department will be important. Competition for admissions into statistics and biostatistics PhD programs has continued to increase, and most faculty advise applying to as many relevant programs as is reasonable given the demands on your time and finances. If you end up sitting on multiple (funded) offers come April, talking to current students, student alums, and looking at alumni placement can be helpful. Don’t hesitate to contact these people, selectively. Most PhD programs genuinely do want you to end up in the place that is best for you, even if it is not with them.\n\n\n\n\n\n\n\n\nOnce you’re in a PhD program, internship opportunities for graduate students are listed each year by the American Statistical Association. Your home department may also have ties with local research organizations and companies with openings. Internships can help you identify future positions and the types of environments where you will flourish in your career. Lauren Kunz, a recent PhD graduate in biostatistics from Harvard University, is currently a Statistician at the National Heart, Lung, and Blood Institute (NHLBI) of the National Institutes of Health. Dr. Kunz said, “As a previous summer intern at the NHLBI, I was able to get a feel for the day to day life of a biostatistician at the NHLBI. I found the NHLBI Office of Biostatistical Research to be a collegial, welcoming environment, and I soon learned that NHLBI biostatisticians have the opportunity to work on a variety of projects, very often collaborating with scientists and clinicians. Due to the nature of these collaborations, the biostatisticians are frequently presented with scientifically interesting and important statistical problems. This work often motivates methodological research which in turn has immediate, practical applications. These factors matched well with my interest in collaborative research that is both methodological and applied.”\n\n\n\n\n\n\n\n\nIndustry is also enticing to statistics PhDs, particularly those with an applied or computational focus, like Stephanie Sapp and Alyssa Frazee. Dr. Sapp has a PhD in statistics from the University of California, Berkeley, and is currently a Quantitative Analyst at Google. She also completed an internship there the summer before she graduated. In commenting about her choice to join Google, Dr. Sapp said,  “I really enjoy both academic research and seeing my work used in practice.  Working at Google allows me to continue pursuing new and interesting research topics, as well as see my results drive more immediate impact.”  Dr. Frazee just finished her PhD in biostatistics at Johns Hopkins University and previously spent a summer exploring her interests in Hacker School.  While she applied to both academic and industry positions, receiving multiple offers, she ultimately chose to go into industry and work for Stripe: “I accepted a tech company’s offer for many reasons, one of them being that I really like programming and writing code. There are tons of opportunities to grow as a programmer/engineer at a tech company, but building an academic career on that foundation would be more of a challenge. I’m also excited about seeing my statistical work have more immediate impact. At smaller companies, much of the work done there has visible/tangible bearing on the product. Academic research in statistics is operating a lot closer to the boundaries of what we know and discovering a lot of cool stuff, which means researchers get to try out original ideas more often, but the impact is less immediately tangible. A new method or estimator has to go through a lengthy peer review/publication process and be integrated into the community’s body of knowledge, which could take several years, before its impact can be fully observed.”  One of Dr. Frazee, Dr. Sapp, and Dr. Kunz’s considerations in choosing a job reflects many of those in the early career statistics community: having an impact.\n\n\n\n\n\n\n\n\nInterest in both developing methods and translating statistical advances into practice is a common theme in the big data statistics world, but not one that always leads to an industry or government career. There are also academic opportunities in statistics, biostatistics, and interdisciplinary departments like my own where your work can have an impact on current science.  The Department of Health Care Policy (HCP) at Harvard Medical School has 5 tenure-track/tenured statistics faculty members, including myself, among a total of about 20 core faculty members. The statistics faculty work on a range of theoretical and methodological problems while collaborating with HCP faculty (health economists, clinician researchers, and sociologists) and leading our own substantive projects in health care policy (e.g., Mass-DAC). I find it to be a unique and exciting combination of roles, and love that the science truly informs my statistical research, giving it broader impact. Since joining the department a year and a half ago, I’ve worked in many new areas, such as plan payment risk adjustment methodology. I have also applied some of my previous work in machine learning to predicting adverse health outcomes in large datasets. Here, I immediately saw a need for new avenues of statistical research to make the optimal approach based on statistical theory align with an optimal approach in practice. My current research portfolio is diverse; example projects include the development of a double robust estimator for the study of chronic disease, leading an evaluation of a new state-wide health plan initiative, and collaborating with department colleagues on statistical issues in all-payer claims databases, physician prescribing intensification behavior, and predicting readmissions. The larger statistics community at Harvard also affords many opportunities to interact with statistics faculty across the campus, and university-wide junior faculty events have connected me with professors in computer science and engineering. I feel an immense sense of research freedom to pursue my interests at HCP, which was a top priority when I was comparing job offers.\n\n\n\n\n\n\n\n\nHadley Wickam, of ggplot2 and Advanced R fame, took on a new role as Chief Scientist at RStudio in 2013. Freedom was also a key component in his choice to move sectors: “For me, the driving motivation is freedom: I know what I want to work on, I just need the freedom (and support) to work on it. It’s pretty unusual to find an industry job that has more freedom than academia, but I’ve been noticeably more productive at RStudio because I don’t have any meetings, and I can spend large chunks of time devoted to thinking about hard problems. It’s not possible for everyone to get that sort of job, but everyone should be thinking about how they can negotiate the freedom to do what makes them happy. I really like the thesis of Cal Newport’s book So Good They Can’t Ignore You - the better you are at your job, the greater your ability to negotiate for what you want.”\n\n\n\n\n\n\n\n\nThere continues to be a strong emphasis in the work force on the vaguely defined field of “data science,” which incorporates the collection, storage, analysis, and interpretation of big data.  Statisticians not only work in and lead teams with other scientists (e.g., clinicians, biologists, computer scientists) to attack big data challenges, but with each other. Your time as a statistics trainee is an amazing opportunity to explore your strengths and preferences, and which sectors and jobs appeal to you. Do your due diligence to figure out which employers are interested in and supportive of the type of career you want to create for yourself. Think about how you want to spend your time, and remember that you’re the only person who has to live your life once you get that job. Other people’s opinions are great, but your values and instincts matter too. Your definition of “best” doesn’t have to match someone else’s. Ask questions! Try new things! The potential for breakthroughs with novel flexible methods is strong. Statistical science training has progressed to the point where trainees are armed with thorough knowledge in design, methodology, theory, and, increasingly, data collection, applications, and computation.  Statisticians working in data science are poised to continue making important contributions in all sectors for years to come. Now, you just need to decide where you fit.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:17:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-02-13-introduction-to-linear-models-and-matrix-algebra-mooc-starts-this-monday-feb-16/",
    "title": "Introduction to Linear Models and Matrix Algebra MOOC starts this Monday Feb 16",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-02-13",
    "categories": [],
    "contents": "\nMatrix algebra is the language of modern data analysis. We use it to develop and describe statistical and machine learning methods, and to code efficiently in languages such as R, matlab and python. Concepts such as principal component analysis (PCA) are best described with matrix algebra. It is particularly useful to describe linear models.\nLinear models are everywhere in data analysis. ANOVA, linear regression, limma, edgeR, DEseq, most smoothing techniques, and batch correction methods such as SVA and Combat are based on linear models. In this two week MOOC we well describe the basics of matrix algebra, demonstrate how linear models are used in the life sciences and show how to implement these efficiently in R.\nUpdate: Here is the link to the class\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-02-12-is-reproducibility-as-effective-as-disclosure-lets-hope-not/",
    "title": "Is Reproducibility as Effective as Disclosure? Let's Hope Not.",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-02-12",
    "categories": [],
    "contents": "\nJeff and I just this week published a commentary in the Proceedings of the National Academy of Sciences on our latest thinking on reproducible research and its ability to solve the reproducibility/replication “crisis” in science (there’s a version on arXiv too). In a nutshell, we believe reproducibility (making data and code available so that others can recompute your results) is an essential part of science, but it is not going to end the crisis of confidence in science. In fact, I don’t think it’ll even make a dent. The problem is that reproducibility, as a tool for preventing poor research, comes in at the wrong stage of the research process (the end). While requiring reproducibility may deter people from committing outright fraud (a small group), it won’t stop people who just don’t know what they’re doing with respect to data analysis (a much larger group).\nIn an eerie coincidence, Jesse Eisinger of the investigative journalism non-profit ProPublica, has just published a piece on the New York Times Dealbook site discussing how requiring disclosure rules in the financial industry has produced meager results. He writes\n\n\nOver the last century, disclosure and transparency have become our regulatory crutch, the answer to every vexing problem. We require corporations and government to release reams of information on food, medicine, household products, consumer financial tools, campaign finance and crime statistics. We have a booming “report card” industry for a range of services, including hospitals, public schools and restaurants.\n\n\n\nThe rationale for all this disclosure is that\n\n\n\nsomeone, somewhere reads the fine print in these contracts and keeps corporations honest. It turns out what we laymen intuit is true: No one reads them, according to research by a New York University law professor, Florencia Marotta-Wurgler.\n\n\n\nBut disclosure is nevertheless popular because how could you be against it?\n\n\n\nThe disclosure bonanza is easy to explain. Nobody is against it. It’s politically expedient. Companies prefer such rules, especially in lieu of actual regulations that would curtail bad products or behavior. The opacity lobby — the remora fish class of lawyers, lobbyists and consultants in New York and Washington — knows that disclosure requirements are no bar to dodgy practices. You just have to explain what you’re doing in sufficiently incomprehensible language, a task that earns those lawyers a hefty fee.\n\n\n\nIn the now infamous Duke Saga, Keith Baggerly was able to reproduce the work of Potti et al. after roughly 2,000 hours of work because the data were publicly available (although the code was not). It’s not clear how much time would have been saved if the code had been available, but it seems reasonable to assume that it would have taken some amount of time to understand the analysis, if not reproduce it. Once the errors in Potti’s work were discovered, it took 5 years for the original Nature Medicine paper to be retracted.\n\n\nAlthough you could argue that the process worked in some sense, it came at tremendous cost of time and money. Wouldn’t it have been better if the analysis had been done right in the first place?\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-02-09-the-trouble-with-evaluating-anything/",
    "title": "The trouble with evaluating anything",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-02-09",
    "categories": [],
    "contents": "\nIt is very hard to evaluate people’s productivity or work in any meaningful way. This problem is the source of:\nConsternation about peer review\nThe reason why post publication peer review doesn’t work\nConsternation about faculty evaluation\nMajor problems at companies like Yahoo and Microsoft.\nRoger and I were just talking about this problem in the context of evaluating the impact of software as a faculty member and Roger suggested the problem is that:\n\nEvaluating people requires real work and so people are always looking for shortcuts\n\nTo evaluate a person’s work or their productivity requires three things:\nTo be an expert in what they do\nTo have absolutely no reason to care whether they succeed or not\nTo have time available to evaluate them\nThese three fundamental things are at the heart of why it is so hard to get good evaluations of people and why peer review and other systems are under such fire. The main source of the problem is the conflict between 1 and 2. The group of people in any organization or on any scale that is truly world class at any given topic from software engineering to history is small. It has to be by definition. This group of people inevitably has some reason to care about the success of the other people in that same group. Either they work with the other world class people and want them to succeed or they  either intentionally or unintentionally are competing with them.\nThe conflict between being and expert and having no say wouldn’t be such a problem if it wasn’t for issue number 3: the time to evaluate people. To truly get good evaluations what you need is for someone who isn’t an expert in a field and so has no stake to take the time to become an expert and then evaluate the person/software. But this requires a huge amount of effort on the part of a reviewer who has to become expert in a new field. Given that reviewing is often considered the least important task in people’s workflow, evidenced by the value we put on people acting as peer reviewers for journals, or the value people get for doing a good job in people’s evaluation for promotion in companies, it is no wonder people don’t take the time to become experts.\nI actually think that tenure review committees at forward thinking places may be the best at this (Rafa said the same thing about NIH study section). They at least attempt to get outside reviews from people who are unbiased about the work that a faculty member is doing before they are promoted. This system, of course, has large and well-document problems, but I think it is better than having a person’s direct supervisor - who clearly has a stake - being the only person evaluating them.It is also better than only using the quantifiable metrics like number of papers and impact factor of the corresponding journals. I also think that most senior faculty who evaluate people take the job very seriously despite the only incentive being good citizenship.\nSince real evaluation requires hard work and expertise, most of the time people are looking for a short cut. These short cuts typically take the form of quantifiable metrics. In the academic world these shortcuts are things like:\nNumber of papers\nCitations to academic papers\nThe impact factor of a journal\nDownloads to a person’s software\nI think all of these things are associated with quality but none define quality. You could try to model the relationship, but it is very hard to come up with a universal definition for the outcome you are trying to model. In academics, some people have suggested that open review or post-publication review solves the problem. But this is only true for a very small subset of cases that violate rule number 2. The only papers that get serious post-publication review are where people have an incentive for the paper to go one way or the other. This means that papers in Science will be post-pub reviewed much much more often than equally important papers in discipline specific journals - just because people care more about Science. This will leave the vast majority of papers unreviewed - as evidenced by the relatively modest number of papers reviewed by PubPeer or Pubmed Commons.\nI’m beginning to think that the only way to do evaluation well is to hire people whose only job is to evaluate something well. In other words, peer reviewers who are paid to review papers full time and are only measured by how often those papers are retracted or proved false. Or tenure reviewers who are paid exclusively to evaluate tenure cases and are measured by how well the post-tenure process goes for the people they evaluate and whether there is any measurable bias in their reviews.\nThe trouble with evaluating anything is that it is hard work and right now we aren’t paying anyone to do it.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-02-05-early-data-on-knowledge-units-atoms-of-statistical-education/",
    "title": "Early data on knowledge units - atoms of statistical education",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-02-05",
    "categories": [],
    "contents": "\nYesterday I posted about atomizing statistical education into knowledge units. You can try out the first knowledge unit here: https://jtleek.typeform.com/to/jMPZQe. The early data is in and it is consistent with many of our hypotheses about the future of online education.\nNamely:\nCompletion rates are high when segments are shorter\nYou can learn something about statistics in a short amount of time (2 minutes to complete, many people got all questions right)\nPeople will consume educational material on tablets/smartphones more and more.\n\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-02-05-johns-hopkins-data-science-specialization-top-performers/",
    "title": "Johns Hopkins Data Science Specialization Top Performers",
    "description": {},
    "author": [
      {
        "name": "Ira Gooding",
        "url": {}
      }
    ],
    "date": "2015-02-05",
    "categories": [],
    "contents": "\nEditor’s note: The Johns Hopkins Data Science Specialization is the largest data science program in the world.  Brian, Roger, and myself  conceived the program at the beginning of January 2014 , then built, recorded, and launched the classes starting in April 2014 with the help of Ira.  Since April 2014 we have enrolled 1.76 million student and awarded 71,589 Signature Track verified certificates. The first capstone class ran in October - just 7 months after the first classes launched and 4 months after all classes were running. Despite this incredibly short time frame 917 students finished all 9 classes and enrolled in the Capstone Course. 478 successfully completed the course.\nWhen we first announced the the Data Science Specialization, we said that the top performers would be profiled here on Simply Statistics. Well, that time has come, and we’ve got a very impressive group of participants that we want to highlight. These folks have successfully completed all nine MOOCs in the specialization and earned top marks in our first capstone session with SwiftKey. We had the pleasure of meeting some of them last week in a video conference, and we were struck by their insights and expertise. Check them out below.\nSasa Bogdanovic\n\n \n \n \n \nSasa Bogdanovic is passionate about everything data. For the last 6 years, he’s been working in the iGaming industry, providing data products (integrations, data warehouse architectures and models, business intelligence tools, analyst reports and visualizations) for clients, helping them make better, data-driven, business decisions.\nWhy did you take the JHU Data Science Specialization?\nAlthough I’ve been working with data for many years, I wanted to take a different perspective and learn more about data science concepts and get insights into the whole pipeline from acquiring data to developing final data products. I also wanted to learn more about statistical models and machine learning.\nWhat are you most proud of doing as part of the JHU Data Science Specialization?\nI am very happy to have discovered the data science field. It is a whole new world that I find fascinating and inspiring to explore. I am looking forward to my new career in data science. This will allow me to combine all my previous knowledge and experience with my new insights and methods. I am very proud of every single quiz, assignment and project. For sure, the capstone project was a culmination, and I am very proud and happy to have succeeded to make a solid data product and to be a one of the top performers in the group. For this I am very grateful to the instructors, community TAs, all other peers for their contributions in the forums, and Coursera for putting it all together and making it possible.\nHow are you planning on using your Data Science Specialization Certificate?\nI have already put the certificate in motion. My company is preparing new projects, and I expect the certificate to add weight to our proposals.\nAlejandro Morales Gallardo\n \n\n \n \n \n \nI’m a trained physicist with strong coding skills. I have a passion for dissecting datasets to find the hidden stories in data and produce insights through creative visualizations. A hackathon and open-data aficionado, I have an interest in using data (and science) to improve our lives.\nWhy did you take the JHU Data Science Specialization?\nI wanted to close a gap in my skills and transition into to becoming a full blown Data Scientist by learning key concepts and practices in the field. Learning R, an industry relevant language, while creating a portfolio to showcase my abilities in the entire data science pipeline seemed very attractive.\nWhat are you most proud of doing as part of the JHU Data Science Specialization?\nI’m most proud of the Predictive Text App I developed. With the Capstone Project, it was extremely rewarding to be able to tackle a brand new data type and learn about text mining and natural language processing while building a fun and attractive data product. I was particularly proud that the accuracy of my app was not that far off from SwiftKey smartphone app. I’m also proud of being a top performer!\nHow are you planning on using your Data Science Specialization Certificate?\nI want to apply my new set of skills to develop other products, analyze new datasets and keep growing my portfolio. It is also helpful to have Verified Certificates to show prospective employers.\nNitin Gupta\n \n\n \n \n \n \nNitin is an independent trader and quant strategist with over 13 years of multi-faceted experience in the investment management industry. In the past he worked for a leading investment management firm where he built automated trading and risk management systems and gained complete life-cycle expertise in creating systematic investment products. He has a background in computer science with a strong interest in machine learning and its applications in quantitative modeling.\nWhy did you take the JHU Data Science Specialization?\nI was fortunate to have done the first Machine Learning course taught by Prof. Andrew Ng at the launch of Coursera in 2012, which really piqued my interest in the topic. The next course I did on Coursera was Prof. Roger Peng’s Computing For Data Analysis which introduced me to R. I realized that R was ideally suited for the quantitative modeling work I was doing. When I learned about the range of topics that the JHU DSS would cover - from the best practices in tidying and transforming data to modeling, analysis and visualization - I did not hesitate to sign up. Learning how to do all of this in an ecosystem built around R has been a huge plus.\nWhat are you most proud of doing as part of the JHU Data Science Specialization?\nI am quite pleased with the web apps I built which utilize the concepts learned during the track. One of my apps visualizes and compares historical stock performance with other stocks and market benchmarks after querying the data directly from web resources. Another one showcases a predictive typing engine that dynamically predicts the next few words to use and append, as the user types a sentence. The process of building these apps provided a fantastic learning experience. Also, for the first time I built something that even my near and dear ones could use and appreciate, which is terrific.\nHow are you planning on using your Data Science Specialization Certificate?\nThe broad skill set developed through this specialization could be applied across multiple domains. My current focus is on building robust quantitative models for systematic trading strategies that could learn and adapt to changing market environments. This would involve the application of machine learning techniques among other skills learned during the specialization. Using R and Shiny to interactively analyze the results would be tremendously useful.\nMarc Kreyer\n \n\n \n \n \n \nMarc Kreyer is an expert business analyst and software engineer with extensive experience in financial services in Austria and Liechtenstein. He successfully finishes complex projects by not only using broad IT knowledge but also outstanding comprehension of business needs. Marc loves combining his programming and database skills with his affinity for mathematics to transform data into insight.\nWhy did you take the JHU Data Science Specialization?\nThere are many data science MOOCs, but usually they are independent 4-6 week courses. The JHU Data Science Specialization was the first offering of a series of courses that build upon each other.\nWhat are you most proud of doing as part of the JHU Data Science Specialization?\nCreating a working text prediction app without any prior NLP knowledge and only minimal assistance from instructors.\nHow are you planning on using your Data Science Specialization Certificate?\nKnowledge and experience are the most valuable things gained from the Data Science Specialization. As they can’t be easily shown to future employers, the certificate can be a good indicator for them. Unfortunately there is neither an issue data nor a verification link on the certificate, therefore it will be interesting to see how valuable it really will be.\nHsing Liu\n \n\n\n\nI studied in the U.S. for a number of years, and received my M.S. in mathematics from NYU before returning to my home country, Taiwan. I’m most interested in how people think and learn, and education in general. This year I’m starting a new career as an iOS app engineer.\nWhy did you take the JHU Data Science Specialization?\nIn my brief past job as an instructional designer, I read a lot about the new wave of online education, and was especially intrigued by how Khan Academy’s data science division is using data to help students learn. It occurred to me that to leverage my math background and make a bigger impact in education (or otherwise), data science could be an exciting direction to take.\nWhat are you most proud of doing as part of the JHU Data Science Specialization?\nIt may sound boring, but I’m proud of having done my best for each course in the track, going beyond the bare requirements when I’m able. The parts of the Specialization fit into a coherent picture of the discipline, and I’m glad to have put in the effort to connect the dots and gained a new perspective.\nHow are you planning on using your Data Science Specialization Certificate?\nI’m listing the certificate on my resume and LinkedIn, and I expect to be applying what I’ve learned once my company’s e-commence app launch.\nYichen Liu\n \nYichen Liu is a business analyst at Toyota Western Australia where he is responsible for business intelligence development, data analytics and business improvement. His prior experience includes working as a sessional lecturer and tutor at Curtin University in finance and econometrics units.\nWhy did you take the JHU Data Science Specialization?\nRecognising the trend that the world is more data-driven than before, I felt it was necessary to gain further understanding in data analysis to tackle both current and future challenges at work.\nWhat are you most proud of doing as part of the JHU Data Science Specialization?\nThe most proud thing as part of the program is that I have gained some basic knowledge in a totally new area, natural language processing. Though its connection with my current working area is limited, I see the future of data analysis to be more unstructured-data-drive and am willing to develop more knowledge in this area.\nHow are you planning on using your Data Science Specialization Certificate?\nI see the certificate as a stepping stone into the data science world, and would like to conduct more advanced studies in data science especially for unstructured data analysis.\nJohann Posch\n\n\n\nAfter graduating form Vienna University of Technology with a specialization in Artificial Intelligence I joined Microsoft. There I worked as a developer on various products but the majority of the time as a Windows OS developer. After venturing into start-ups for a few years I joined GE Research to work on the Predix Big Data Platform and recently I joined on the Industrial Data Science team.\nWhy did you take the JHU Data Science Specialization?\nEver since I wrote my masters thesis in Neural Networks I have been intrigued with machine learning. I see data science as a field where great advances will happen over the next decade and as an opportunity to positively impact millions of lives. I like how JHU structured the course series.\nWhat are you most proud of doing as part of the JHU Data Science Specialization?\nBeing able to complete the JHU Data Science Specialization in 6 months and to get an distinction on every one of the courses was a great success. However, the best moment was probably the way my capstone project (next word prediction) turned out. The model could be trained in incremental steps and how it was able to provide meaningful options in real time.\nHow are you planning on using your Data Science Specialization Certificate?\nThe course covered the concepts and tools needed to successfully address data science problems. It gave me the confidence and knowledge to apply for data science position. I am now working in the field at GE Research. I am grateful to all who made this Specialization happen!\nJason Wilkinson\n \n\n \n \n \n \nJason Wilkinson is a trader of commodity futures and other financial securities at a small proprietary trading firm in New York City. He and his wife, Katie, and dog, Charlie, can frequently be seen at the Jersey shore. And no, it’s nothing like the tv show, aside from the fist pumping.\nWhy did you take the JHU Data Science Specialization?\nThe JHU Data Science Specialization helped me to prepare as I begin working on a Masters of Computer Science specializing in Machine Learning at Georgia Tech and also in researching algorithmic trading ideas. I also hope to find ways of using what I’ve learned in philanthropic endeavors, applying data science for social good.\nWhat are you most proud of doing as part of the JHU Data Science Specialization?\nI’m most proud of going from knowing zero R code to being able to apply it in the capstone and other projects in such a short amount of time.\nHow are you planning on using your Data Science Specialization Certificate?\nThe knowledge gained in pursuing the specialization certificate alone was worth the time put into it. A certificate is just a piece of paper. It’s what you can do with the knowledge gained that counts.\nUli Zellbeck\n \n\n\n\n \nI studied economics in Berlin with focus on econometrics and business informatics. I am currently working as a Business Intelligence / Data Warehouse Developer in an e-commerce company. I am interested in recommender systems and machine learning.\nWhy did you take the JHU Data Science Specialization?\nI wanted to learn about Data Science because it provides a different approach on solving business problems with data. I chose the JHU Data Science Specialization on Coursera because it promised a wide range of topics and I like the idea of online courses. Also, I had experience with R and I wanted to deepen my knowledge with this tool.\nWhat are you most proud of doing as part of the JHU Data Science Specialization?\nThere are two things. I successfully took all nine courses in 4 months and the capstone project was really hard work.\nHow are you planning on using your Data Science Specialization Certificate?\nI might get the chance to develop a Data Science department at my company. I like to use the certificate as basis to get a deeper knowledge in the many parts of Data Science.\nFred Zheng Zhenhao\n \n\n \n \n \n \nBy the time I enrolled in the JHU data science specialization, I was an undergraduate student in The Hong Kong Polytechnic university. Before that, I read some data mining books, feel excited about the content, but I never get to implement any of the algorithms because I barely have any programming skill. After taking this series of courses, now I am able to analyze the web content which is related to my research using R.\nWhy did you take the JHU Data Science Specialization?\nI took this series of courses as a challenge to me. I would like to see whether my interest can support me through 9 courses and 1 capstone project. And I do want to learn more in this field. This specialization is different from other data mining or machine learning class in that it covers the entire process including the Git, R, R-Markdown, shiny etc, and I think these are necessary skills too.\nWhat are you most proud of doing as part of the JHU Data Science Specialization?\nGetting my word prediction app to respond in 0.05 seconds is already exiting, and one of the reviewer says “congratulations your engine came up with the most correct prediction among those I reviewed: 3 out of 5, including one that stumped every one else :”child might stick her finger or a foreign object into an electrical (outlet)“. I guess that’s the part I am most proud of.\nHow are you planning on using your Data Science Specialization Certificate?\nIt definitely goes in my CV for future job hunting.\n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-02-04-knowledge-units-the-atoms-of-statistical-education/",
    "title": "Knowledge units - the atoms of statistical education",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-02-04",
    "categories": [],
    "contents": "\nEditor’s note: This idea is Brian’s idea and based on conversations with him and Roger, but I just executed it.\nThe length of academic courses has traditionally ranged between a few days for a short course to a few months for a semester-long course.  Lectures are typically either 30 minutes or one hour. Term and lecture lengths have been dictated by tradition and the relative inconvenience of coordinating schedules of the instructors and students for shorter periods of time. As classes have moved online the barrier of inconvenience to varying the length of an academic course has been removed. Despite this flexibilty, most academic online courses adhere to the traditional semester-long format. For example, the first massive online open courses were simply semester-long courses directly recorded and offered online.\nData collected from massive online open courses suggest that Editor’s note: This idea is [Brian’s idea](http://www.bcaffo.com/) and based on conversations with him and Roger, but I just executed it. and the Editor’s note: This idea is [Brian’s idea](http://www.bcaffo.com/) and based on conversations with him and Roger, but I just executed it. leads to higher student retention. These results line up with data on other online activities such as Youtube video watching or form completion, which also show that shorter activities lead to higher completion rates.\nWe have  some of the earliest and most highly subscribed massive online open courses through the Coursera platform: Data Analysis, Computing for Data Analysis, and Mathematical Biostatistics Bootcamp. Our original courses were translated from courses we offered locally and were therefore closer to semester long with longer lectures ranging from 15-30 minutes. Based on feedback from our students and the data we observed about completion rates, we made the decision to break our courses down into smaller, one-month courses with no more than two hours of lecture material per week. Since then, we have enrolled more than a million students in our MOOCs.\nThe data suggest that the shorter you can make an academic unit online, the higher the completion percentage. The question then becomes “How short can you make an online course?” To answer this question requires a definition of a course. For our purposes we will define a course as an educational unit consisting of the following three components:\n**** ****\n****Knowledge delivery** -** the distribution of educational material through lectures, audiovisual materials, and course notes.\nKnowledge evaluation - the evaluation of how much of the knowledge delivered to a student is retained.\nKnowledge certification - an independent claim or representation that a student has learned some set of knowledge.\n \nA typical university class delivers 36 hours = 12 weeks x 3 hours/week of content knowledge, evaluates that knowledge based on the order of 10 homework assignments and 2 tests, and results in a certification equivalent to 3 university credits.With this definition, what is the smallest possible unit that satisfies all three definitions of a course? We will call this smallest possible unit one knowledge unit. The smallest knowledge unit that satisfies all three definitions is a course that:\n****Delivers a single unit of content** -** We will define a single unit of content as a text, image, or video describing a single concept.\nEvaluates that single unit of content -  The smallest unit of evaluation possible is a single question to evaluate a student’s knowledge.\nCertifies knowlege - Provides the student with a statement of successful evaluation of the knowledge in the knowledge unit.\nAn example of a knowledge unit appears here: https://jtleek.typeform.com/to/jMPZQe. The knowledge unit consists of a short (less than 2 minute) video and 3 quiz questions. When completed, the unit sends the completer an email verifying that the quiz has been completed. Just as an atom is the smallest unit of mass that defines a chemical element, the knowledge unit is the smallest unit of education that defines a course.\nShrinking the units down to this scale opens up some ideas about how you can connect them together into courses and credentials. I’ll leave that for a future post.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-01-30-precision-medicine-will-never-be-very-precise-but-it-may-be-good-for-public-health/",
    "title": "Precision medicine may never be very precise - but it may be good for public health",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-01-30",
    "categories": [],
    "contents": "\nEditor’s note: This post was originally titled: Personalized medicine is primarily a population health intervention. It has been updated with the graph of odds ratios/betas from GWAS studies.\nThere has been a lot of discussion of personalized medicine, individualized health, and precision medicine in the news and in the medical research community and President Obama just announced a brand new initiative in precision medicine . Despite this recent attention, it is clear that healthcare has always been personalized to some extent. For example, men are rarely pregnant and heart attacks occur more often among older patients. In these cases, easily collected variables such as sex and age, can be used to predict health outcomes and therefore used to “personalize” healthcare for those individuals.\nSo why the recent excitement around personalized medicine? The reason is that it is increasingly cheap and easy to collect more precise measurements about patients that might be able to predict their health outcomes. An example that has recently been in the news is the measurement of mutations in the BRCA genes. Angelina Jolie made the decision to undergo a prophylactic double mastectomy based on her family history of breast cancer and measurements of mutations in her BRCA genes. Based on these measurements, previous studies had suggested she might have a lifetime risk as high as 80% of developing breast cancer.\nThis kind of scenario will become increasingly common as newer and more accurate genomic screening and predictive tests are used in medical practice. When I read these stories there are two points I think of that sometimes get obscured by the obviously fraught emotional, physical, and economic considerations involved with making decisions on the basis of new measurement technologies:\nIn individualized health/personalized medicine the “treatment” is information about risk. In some cases treatment will be personalized based on assays. But in many other cases, we still do not (and likely will not) have perfect predictors of therapeutic response. In those cases, the healthcare will be “personalized” in the sense that the patient will get more precise estimates of their likelihood of survival, recurrence etc. This means that patients and physicians will increasingly need to think about/make decisions with/act on information about risks. But communicating and acting on risk is a notoriously challenging problem; personalized medicine will dramatically raise the importance of understanding uncertainty.\nIndividualized health/personalized medicine is a population-level treatment. Assuming that the 80% lifetime risk estimate was correct for Angelina Jolie, it still means there is a 1 in 5 chance she was never going to develop breast cancer. If that had been her case, then the surgery was unnecessary. So while her decision was based on personal information, there is still uncertainty in that decision for her. So the “personal” decision may not always be the “best” decision for any specific individual. It may however, be the best thing to do for everyone in a population with the same characteristics.\nThe first point bears serious consideration in light of President Obama’s new proposal. We have already collected a massive amount of genetic data about a large number of common diseases. In almost all cases, the amount of predictive information that we can glean from genetic studies is modest. One paper pointed this issue out in a rather snarky way by comparing two approaches to predicting people’s heights: (1) averaging their parents heights - an approach from the Victorian era and (2) combing the latest information on the best genetic markers at the time. It turns out, all the genetic information we gathered isn’t as good as averaging parents heights. Another way to see this is to download data on all genetic variants associated with disease from the GWAS catalog that have a P-value less than 1 x 10e-8. If you do that and look at the distribution of effect sizes, you see that 95% have an odds ratio or beta coefficient less than about 4. Here is a histogram of the effect sizes:\n \n\n \n \nThis means that nearly all identified genetic effects are small. The ones that are really large (effect size greater than 100) are not for common disease outcomes, they are for Birdshot chorioretinopathy and hippocampal volume. You can really see this if you look at the bulk of the distribution of effect sizes, which are mostly less than 2 by zooming the plot on the x-axis:\n \n\n \n \nThese effect sizes translate into very limited predictive capacity for most identified genetic biomarkers.  The implication is that personalized medicine, at least for common diseases, is highly likely to be inaccurate for any individual person. But if we can take advantage of the population-level improvements in health from precision medicine by increasing risk literacy, improving our use of uncertain markers, and understanding that precision medicine isn’t precise for any one person, it could be a really big deal.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-01-26-reproducible-research-course-companion/",
    "title": "Reproducible Research Course Companion",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2015-01-26",
    "categories": [],
    "contents": "\nI’m happy to announce that you can now get a copy of the Reproducible Research Course Companion from the Apple iBookstore. The purpose of this e-book is pretty simple. The book provides all of the key video lectures from my Reproducible Research course offered on Coursera, in a simple offline e-book format. The book can be viewed on a Mac, iPad, or iPad mini.\nIf you’re interested in taking my Reproducible Research course on Coursera and would like a flavor of what the course will be like, then you can view the lectures through the book (the free sample contains three lectures). On the other hand, if you already took the course and would like access to the lecture material afterwards, then this might be a useful add-on. If you care currently enrolled in the course, then this could be a handy way for you to take the lectures on the road with you.\nPlease note that all of the lectures are still available for free on YouTube via my YouTube channel. Also, the book provides content only. If you wish to actually complete the course, you must take it through the Coursera web site.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-01-21-data-as-an-antidote/",
    "title": "Data as an antidote to aggressive overconfidence",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-01-21",
    "categories": [],
    "contents": "\nA recent NY Times op-ed reminded us of the many biases faced by women at work. A A recent [NY Times op-ed](http://www.nytimes.com/2014/12/07/opinion/sunday/adam-grant-and-sheryl-sandberg-on-discrimination-at-work.html?_r=0) reminded us of the many biases faced by women at work. A   gave specific recommendations for how to conduct ourselves in meetings_. _In general, I found these very insightful, but don’t necessarily agree with the recommendations that women should “Practice Assertive Body Language”.  Instead, we should make an effort to judge ideas by their content and not be impressed by body language. More generally, it is a problem that many of the characteristics that help advance careers contribute nothing to intellectual output. One of these is what I call aggressive overconfidence.\nHere is an example (based on a true story). A data scientist finds a major flaw with the data analysis performed by a prominent data-producing scientist’s lab. Both are part of a large collaborative project. A meeting is held among the project leaders to discuss the disagreement. The data producer is very self-confident in defending his approach. The data scientist, who in not nearly as aggressive, is interrupted so much that she barely gets her point across. The project leaders decide that this seems to be simply a difference of opinion and, for all practical purposes, ignore the data scientist. I imagine this story sounds familiar to many. While in many situations this story ends here, when the results are data driven we can actually fact check opinions that are pronounced as fact. In this example, the data is public and anybody with the right expertise can download the data and corroborate the flaw in the analysis. This is typically quite tedious, but it can be done. Because the key flaws are rather complex, the project leaders, lacking expertise in data analysis, can’t make this determination. But eventually, a chorus of fellow data analysts will be too loud to ignore.\nThat aggressive overconfidence is generally rewarded in academia is a problem. And if this trait is highly correlated with being male, then a manifestation of this is a worsened gender gap. My experience (including reading internet discussions among scientists on controversial topics) has convinced me that this trait is in fact correlated with gender. But the solution is not to help women become more aggressively overconfident. Instead we should continue to strive to judge work based on content rather than style. I am optimistic that more and more, data, rather than who sounds more sure of themselves, will help us decide who wins a debate.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-01-20-gorging-ourselves-on-free-health-care-harvards-dilemma/",
    "title": "Gorging ourselves on \"free\" health care: Harvard's dilemma\n",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-01-20",
    "categories": [],
    "contents": "\n_Editor’s note: This is a guest post by Laura Hatfield. Laura is an Assistant Professor of Health Care Policy at Harvard Medical School, with a specialty in Biostatistics. Her work focuses on understanding trade-offs and relationships among health outcomes. Dr. Hatfield received her BS in genetics from Iowa State University and her PhD in biostatistics from the University of Minnesota. She tweets [@bioannie](https://twitter.com/bioannie)_\nI didn’t imagine when I joined Harvard’s Department of Health Care Policy that the New York Times would be writing about my benefits package. Then a vocal and aggrieved group of faculty rebelled against health benefits changes for 2015, and commentators responded by gleefully skewering entitled-sounding Harvard professors. But I’m a statistician, so I want to talk data.\nHealth care spending is tremendously right-skewed. The figure below shows the annual spending distribution among people with any spending (~80% of the total population) in two data sources on people covered by employer-sponsored insurance, such as the Harvard faculty. Notice that the y axis is on the log scale. More than half of people spend $1000 or less, but a few very unfortunate folks top out near half a million.\n\nSource: Measuring health care costs of individuals with employer-sponsored health insurance in the US: A comparison of survey and claims data. A. Aizcorbe, E. Liebman, S. Pack, D.M. Cutler, M.E. Chernew, A.B. Rosen. BEA working paper. WP2010-06. June 2010.\nIf instead of contributing to my premiums, Harvard instead gave me the $1000/month premium contribution in the form of wages, I would be on the hook for my own health care expenses. If I stay healthy, I pocket the money, minus income taxes. If I get sick, I have the extra money available to cover the expenses…provided I’m not one of the unlucky 10% of people spending more than $12,000/year. In that case, the additional wages would be insufficient to cover my health care expenses. This “every woman for herself” system lacks the key benefit of insurance: risk pooling. The sickest among us would be bankrupted by health costs. Another good reason for an employer to give me benefits is that I do not pay taxes on this part of my compensation (more on that later).\nAt the opposite end of the spectrum is the Harvard faculty health insurance plan. Last year, the university paid ~$1030/month toward my premium and I put in ~$425 (tax-free). In exchange for this ~$17,000 of premiums, my family got first-dollar insurance coverage with very low co-pays. Faculty contributions to our collective expenses health care were distributed fairly evenly among all of us, with only minimal cost sharing to reflect how much care each person consumed. The sickest among us were in no financial peril. My family didn’t use much care and thus didn’t get our (or Harvard’s) money’s worth for all that coverage, but I’m ok with it. I still prefer risk pooling.\nHere’s the problem: moral hazard. It’s a word I learned when I started hanging out with health economists. It describes the tendency of people to over-consume goods that feel free, such as health care paid through premiums or desserts at an all-you-can-eat buffet. Just look at this array—how much cake do *you* want to eat for $9.99?\n \n\nSource: https://www.flickr.com/photos/jimmonk/5687939526/in/photostream/\nOne way to mitigate moral hazard is to expose people to more of their cost of care at the point of service instead of through premiums. You might think twice about that fifth tiny cake if you were paying per morsel. This is what the new Harvard faculty plans do: our premiums actually go down, but now we face a modest deductible, $250 per person or $750 max for a family. This is meant to encourage faculty to use their health care more efficiently, but it still affords good protection against catastrophic costs. The out-of-pocket max remains low at $1500 per individual or $4500 per family, with recent announcements to further protect individuals who pay more than 3% of salary in out-of-pocket health costs through a reimbursement program.\nThe allocation of individuals’ contributions between premiums and point-of-service costs is partly a question of how we cross-subsidize each other. If Harvard’s total contribution remains the same and health care costs do not grow faster than wages (ha!), then increased cost sharing decreases the amount by which people who use less care subsidize those who use more. How you feel about the “right” level of cost sharing may depend on whether you’re paying or receiving a subsidy from your fellow employees. And maybe your political leanings.\nWhat about the argument that it is better for an employer to “pay” workers by health insurance premium contributions rather than wages because of the tax benefits? While we might prefer to get our compensation in the form of tax-free health benefits vs taxed wages, the university, like all employers, is looking ahead to the Cadillac tax provision of the ACA. So they have to do some re-balancing of our overall compensation. If Harvard reduces its health insurance contributions to avoid the tax, we might reasonably expect to make up that difference in higher wages. The empirical evidence is complicated and suggests that employers may not immediately return savings on health benefits dollar-for-dollar in the form of wages.\nAs far as I can tell, Harvard is contributing roughly the same amount as last year toward my health benefits, but exact numbers are difficult to find. I switched plan types, so I can’t find and directly compare Harvard’s contributions in the same plan type this year and last. Peter Ubel argues that if the faculty *had* seen these figures, we might not have revolted. The actuarial value of our plans remains very high (91%, just a bit better than the expensive Platinum plans on the exchanges) and Harvard’s spending on health care has grown from 8% to 12% of the university’s budget over the past few years. Would these data have been sufficient to quell the insurrection? Good question.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-01-16-if-you-were-going-to-write-a-paper-about-the-false-discovery-rate-you-should-have-done-it-in-2002/",
    "title": "If you were going to write a paper about the false discovery rate you should have done it in 2002",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-01-16",
    "categories": [],
    "contents": "\nPeople often talk about academic superstars as people who have written highly cited papers. Some of that has to do with people’s genius, or ability, or whatever. But one factor that I think sometimes gets lost is luck and timing. So I wrote a little script to get the first 30 papers that appear when you search Google Scholar for the terms:\nempirical processes\nproportional hazards model\ngeneralized linear model\nsemiparametric\ngeneralized estimating equation\nfalse discovery rate\nmicroarray statistics\nlasso shrinkage\nrna-seq statistics\nGoogle Scholar sorts by relevance, but that relevance is driven to a large degree by citations. For example, if you look at the first 10 papers you get for searching for false discovery rate you get.\nControlling the false discovery rate: a practical and powerful approach to multiple testing\nThresholding of statistical maps in functional neuroimaging using the false discovery rate\nThe control of the false discovery rate in multiple testing under dependency\nControlling the false discovery rate in behavior genetics research\nIdentifying differentially expressed genes using false discovery rate controlling procedures\nThe positive false discovery rate: A Bayesian interpretation and the q-value\nOn the adaptive control of the false discovery rate in multiple testing with independent statistics\nImplementing false discovery rate control: increasing your power\nOperating characteristics and extensions of the false discovery rate procedure\nAdaptive linear step-up procedures that control the false discovery rate\nPeople who work in this area will recognize that many of these papers are the most important/most cited in the field.\nNow we can make a plot that shows for each term when these 30 highest ranked papers appear. There are some missing values, because of the way the data are scraped, but this plot gives you some idea of when the most cited papers on these topics were published:\n \n\nYou can see from the plot that the median publication year of the top 30 hits for “empirical processes” was 1990 and for “RNA-seq statistics” was 2010. The medians for the other topics were:\nEmp. Proc. 1990.241\nProp. Haz. 1990.929\nGLM 1994.433\nSemi-param. 1994.433\nGEE 2000.379\nFDR 2002.760\nmicroarray 2003.600\nlasso 2004.900\nrna-seq 2010.765\nI think this pretty much matches up with the intuition most people have about the relative timing of fields, with a few exceptions (GEE in particular seems a bit late). There are a bunch of reasons this analysis isn’t perfect, but it does suggest that luck and timing in choosing a problem can play a major role in the “success” of academic work as measured by citations.  It also suggests another reason for success in science than individual brilliance. Given the potentially negative consequences the expectation of brilliance has on certain subgroups, it is important to recognize the importance of timing and luck. The median most cited “false discovery rate” paper was 2002, but almost none of the 30 top hits were published after about 2008.\nThe code for my analysis is here. It is super hacky so have mercy.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-01-15-how-to-find-the-science-paper-behind-a-headline-when-the-link-is-missing/",
    "title": "How to find the science paper behind a headline when the link is missing",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-01-15",
    "categories": [],
    "contents": "\nI just saw a pretty wild statistic on Twitter that less than 60% of university news releases link to the papers they are describing.\n \n\n\nAmazingly, less than 60% of university news releases link to the papers they’re describing http://t.co/daN11xYvKs pic.twitter.com/QtneZUAeFD\n\n\n— Justin Wolfers (@JustinWolfers) January 15, 2015\n\n\nBefore you believe anything your read about science in the news, you need to go and find the original article.  When the article isn’t linked in the press release, sometimes you need to do a bit of sleuthing.  Here is an example of how I do it for a news article. In general the press-release approach is very similar, but you skip the first step I describe below.\nHere is the news article (link):\n \nNews Article \n \nStep 1: Look for a link to the article\nUsually it will be linked near the top or the bottom of the article. In this case, the article links to the press release about the paper. This is not the original research article. If you don’t get to a scientific journal you aren’t finished. In this case, the press release actually gives the full title of the article, but that will happen less than 60% of the time according to the statistic above.\n \nStep 2: Look for names of the authors, scientific key words and journal name if available\nYou are going to use these terms to search in a minute. In this case the only two things we have are the journal name:\nJournal \nAnd some key words:\n \nKeywords \nStep 3 Use Google Scholar\nYou could just google those words and sometimes you get the real paper, but often you just end up back at the press release/news article. So instead the best way to find the article is to go to Google Scholar , click on the menu item on the top left, then click on “Advanced Search”.\n \n \n \nGoogle ScholarFill in information while you can. Fill in the same year as the press release, information about the journal, university and key words.\n \nAdvanced Search \nStep 4 Victory\nOften this will come up with the article you are looking for:\nScientific Article \nUnfortunately, the article may be paywalled, so if you don’t work at a university or institute with a subscription, you can always tweet the article name with the hashtag and your contact info. Then you just have to hope that someone will send it to you (they often do).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-01-12-statistics-and-r-for-the-life-sciences-new-harvardx-course-starts-january-19/",
    "title": "Statistics and R for the Life Sciences: New HarvardX course starts January 19",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2015-01-12",
    "categories": [],
    "contents": "\nThe first course of our Biomedical Data Science online curriculum\nstarts next week. You can sign up here. Instead of relying on\nmathematical formulas to teach statistical concepts, students can\nprogram along as we show computer code for simulations that illustrate\nthe main ideas of exploratory data analysis and statistical inference\n(p-values, confidence intervals and power calculations for example).\nBy doing this, students will learn Statistics and R simultaneously and\nwill not be bogged down by having to memorize formulas. We have three types of learning modules: lectures (see picture below), screencasts and assessments. After each\nvideo students will have the opportunity to assess their understanding\nthrough homeworks involving coding in R. A big improvement over the\nfirst version is that we have added dozens of assessment.\nNote that this course is the first in an eight part series on Data Analysis for Genomics. Updates will be provided via twitter [@rafalab](https://twitter.com/rafalab).\n \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-01-07-beast-mode-parenting-as-shown-by-my-fitbit-data/",
    "title": "Beast mode parenting as shown by my Fitbit data",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-01-07",
    "categories": [],
    "contents": "\nThis weekend was one of those hardcore parenting weekends that any parent of little kids will understand. We were up and actively taking care of kids for a huge fraction of the weekend. (Un)fortunately I was wearing my Fitbit, so I can quantify exactly how little we were sleeping over the weekend.\nHere is Saturday:\n\n \n \nThere you can see that I rocked about midnight-4am without running around chasing a kid or bouncing one to sleep. But Sunday was the real winner:\n \n\nCheck that out. I was totally asleep from like 4am-6am there. Nice.\nStay tuned for much more from my Fitbit data over the next few weeks.\n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-01-04-sunday-datastatistics-link-roundup-1415/",
    "title": "Sunday data/statistics link roundup (1/4/15)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2015-01-04",
    "categories": [],
    "contents": "\nI am digging this visualization of your life in weeks. I might have to go so far as to actually make one for myself.\nI’m very excited about the new podcast TalkingMachines and what an awesome name! I wish someone would do that same thing for applied statistics (Roger?)\nI love that they call Ben Goldacre the anti-Dr. Oz in this piece, especially given how often Dr. Oz is telling the truth.\nIf you haven’t read it yet, this piece in the Economist on statisticians during the war is really good.\nThe arXiv celebrated it’s 1M paper upload. It costs less to run than the top 2 executives at PLoS make. It is too darn expensive to publish open access right now.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-31-ugh-so-close-to-one-million-page-views-for-2014/",
    "title": "Ugh ... so close to one million page views for 2014",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-12-31",
    "categories": [],
    "contents": "\nIn my last Sunday Links roundup I mentioned we were going to be really close to 1 million page views this year. Chris V. tried to rally the troops:\n \n\n\nLets get them over the hump // “@simplystats: Sunday data/statistics link roundup (12/21/14) http://t.co/X1WDF9zZc1 #simplystats1e6”\n\n\n— Chris Volinsky (@statpumpkin) December 22, 2014\n\n\n \nbut alas we are probably not going to make it (unless by some miracle one of our posts goes viral in the next 12 hours):\n\n \nStay tuned for a bunch of cool new stuff from Simply Stats in 2015, including a new podcasting idea, more interviews, another unconference, and a new plotting theme!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-22-meetings-2/",
    "title": "On how meetings and conference calls are disruptive to a data scientist",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-12-22",
    "categories": [],
    "contents": "\nEditor’s note: The week of Xmas eve is usually my most productive of the year. This is because there is reduced emails and 0 meetings (I do take a break, but after this great week for work). Here is a repost of one of our first entries explaining how meetings and conference calls are particularly disruptive in data science. \nIn this TED talk Jason Fried explains why work doesn’t happen at work. He describes the evils of meetings. Meetings are particularly disruptive for applied statisticians, especially for those of us that hack data files, explore data for systematic errors, get inspiration from visual inspection, and thoroughly test our code. Why? Before I become productive I go through a ramp-up/boot-up stage. Scripts need to be found, data loaded into memory, and most importantly, my brains needs to re-familiarize itself with the data and the essence of the problem at hand. I need a similar ramp up for writing as well. It usually takes me between 15 to 60 minutes before I am in full-productivity mode. But once I am in “the zone”, I become very focused and I can stay in this mode for hours. There is nothing worse than interrupting this state of mind to go to a meeting. I lose much more than the hour I spend at the meeting. A short way to explain this is that having 10 separate hours to work is basically nothing, while having 10 hours in the zone is when I get stuff done.\nOf course not all meetings are a waste of time. Academic leaders and administrators need to consult and get advice before making important decisions. I find lab meetings very stimulating and, generally, productive: we unstick the stuck and realign the derailed. But before you go and set up a standing meeting consider this calculation: a weekly one hour meeting with 20 people translates into 1 hour x 20 people x 52 weeks/year = 1040 person hours of potentially lost production per year. Assuming 40 hour weeks, that translates into six months. How many grants, papers, and lectures can we produce in six months? And this does not take into account the non-linear effect described above. Jason Fried suggest you cancel your next meeting, notice that nothing bad happens and enjoy the extra hour of work.\nI know many others that are like me in this regard and for you I have these recommendations: 1- avoid unnecessary meetings, especially if you are already in full-productivity mode. Don’t be afraid to use this as an excuse to cancel.  If you are in a soft $ institution, remember who pays your salary.  2- Try to bunch all the necessary meetings all together into one day. 3- Separate at least one day a week to stay home and work for 10 hours straight. Jason Fried also recommends that every work place declare a day in which no one talks. No meetings, no chit-chat, no friendly banter, etc… No talk Thursdays anyone?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:16:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-21-sunday-datastatistics-link-roundup-122114/",
    "title": "Sunday data/statistics link roundup (12/21/14)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-12-21",
    "categories": [],
    "contents": "\nJames Stewart, author of the most popular Calculus textbook in the world, passed away. In case you wonder if there is any money in textbooks, he had a $32 million house in Toronto. Maybe I should get out of MOOCs and into textbooks.\nThis post on medium about a new test for causality is making the rounds.  The authors of the original paper make clear their assumptions make the results basically unrealistic for any real analysis for example:“We simplify the causal discovery problem by assuming no confounding, selection bias and feedback.” The medium article is too bold and as I replied to an economist who tweeted there was a new test that could distinguish causality: “Nope”.\nI’m excited that the Rafa + the ASA have started a section on Genomics and Genetics. It is nice to have a place to belong within our community. I hope it can be a place where folks who aren’t into the hype (a lot of those in genomics), but really care about applications, can meet each other and work together.\nGreat essay by Hanna W. about data, machine learning and fairness. I love this quote: “in order to responsibly articulate and address issues relating to bias, fairness, and inclusion, we need to stop thinking of big data sets as being homogeneous, and instead shift our focus to the many diverse data sets nested within these larger collections.” (via Hilary M.)\nOver at Flowing Data they ran down the best data visualizations of the year.\nThis rant from Dirk E. perfectly encapsulates every annoying thing about the Julia versus R comparisons I see regularly.\nWe are tantalizingly close to 1 million page views for the year for Simply Stats. Help get us over the edge, share your favorite simply stats article with all your friends using the hashtag #simplystats1e6\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-19-interview-with-emily-oster/",
    "title": "Interview with Emily Oster",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-12-19",
    "categories": [],
    "contents": "\n\n\n<div dir=\"ltr\">\n  <div>\n    <a href=\"http://simplystatistics.org/wp-content/uploads/2014/12/Emily_Oster_Photo.jpg\"><img class=\"aligncenter wp-image-3714 \" src=\"http://simplystatistics.org/wp-content/uploads/2014/12/Emily_Oster_Photo-198x300.jpg\" alt=\"Emily Oster\" width=\"121\" height=\"184\" /><\/a>\n  <\/div>\n  \n  <div>\n  <\/div>\n  \n  <div>\n  <\/div>\n  \n  <div>\n    <em><a href=\"http://en.wikipedia.org/wiki/Emily_Oster\">Emily Oster<\/a> is an Associate Professor of Economics at Brown University. She is a frequent and highly respected <a href=\"http://fivethirtyeight.com/contributors/emily-oster/\">contributor to 538 <\/a>where she brings clarity to areas of interest to parents, pregnant woman, and the general public where empirical research is conflicting or difficult to interpret. She is also the author of the popular new book about pregnancy:<a href=\"http://www.amazon.com/Expecting-Better-Conventional-Pregnancy-Wrong/dp/0143125702\"> Expecting Better: Why the Conventional Pregnancy Wisdom Is Wrong--and What You Really Need to Know<\/a><b>. <\/b>We interviewed Emily as part of our <a href=\"http://simplystatistics.org/interviews/\">ongoing interview series<\/a> with exciting empirical data scientists. <\/em>\n  <\/div>\n  \n  <div>\n    <em> <\/em>\n  <\/div>\n  \n  <div>\n  <\/div>\n  \n  <div>\n    <b>SS: Do you consider yourself an economist, econometrician, statistician, data scientist or something else?<\/b>\n  <\/div>\n  \n  <div>\n  <\/div>\n<\/div>\n\n\n\n\n<div dir=\"ltr\">\n  <div>\n    EO: I consider myself an empirical economist. I think my econometrics colleagues would have a hearty laugh at the idea that I'm an econometrician! The questions I'm most interested in tend to have a very heavy empirical component - I really want to understand what we can learn from data. In this sense, there is a lot of overlap with statistics. But at the end of the day, the motivating questions and the theories of behavior I want to test come straight out of economics.\n  <\/div>\n<\/div>\n\n\n\n\n<div dir=\"ltr\">\n  <div>\n    <div>\n    <\/div>\n    \n    <div>\n      <b>SS: You are a frequent contributor to 538. Many of your pieces are attempts to demystify often conflicting sets of empirical research (about concussions and suicide, or the dangers of water flouridation). What would you say are the issues that make empirical research about these topics most difficult?<\/b>\n    <\/div>\n    \n    <div>\n      <b> <\/b>\n    <\/div>\n  <\/div>\n<\/div>\n\n\n\n\n<div dir=\"ltr\">\n  <div>\n    <div>\n      EO: In nearly all the cases, I'd summarize the problem as : \"The data isn't good enough.\" Sometimes this is because we only see observational data, not anything randomized. A large share of studies using observational data that I discuss have serious problems with either omitted variables or reverse causality (or both).  This means that the results are suggestive, but really not conclusive.  A second issue is even when we do have some randomized data, it's usually on a particular population, or a small group, or in the wrong time period. In the flouride case, the studies which come closest to being \"randomized\" are from 50 years ago. How do we know they still apply now?  This makes even these studies challenging to interpret.\n    <\/div>\n  <\/div>\n<\/div>\n\n\n\n\n<div dir=\"ltr\">\n  <div>\n    <div>\n    <\/div>\n    \n    <div>\n      <b>SS: Your recent book \"Expecting Better: Why the Conventional Pregnancy Wisdom Is Wrong--and What You Really Need to Know\" takes a similar approach to pregnancy. Why do you think there are so many conflicting studies about pregnancy? Is it because it is so hard to perform randomized studies?<\/b>\n    <\/div>\n    \n    <div>\n      <b> <\/b>\n    <\/div>\n  <\/div>\n<\/div>\n\n\n\n\n<div dir=\"ltr\">\n  <div>\n    <div>\n      EO: I think the inability to run randomized studies is a big part of this, yes. One area of pregnancy where the data is actually quite good is labor and delivery. If you want to know the benefits and consequences of pain medication in labor, for example, it is possible to point you to some reasonably sized randomized trials. For various reasons, there has been more willingness to run randomized studies in this area. When pregnant women want answers to less medical questions (like, \"Can I have a cup of coffee?\") there is typically no randomized data to rely on. Because the possible benefits of drinking coffee while pregnant are pretty much nil, it is difficult to conceptualize a randomized study of this type of thing.\n    <\/div>\n    \n    <div>\n    <\/div>\n    \n    <div>\n      Another big issue I found in writing the book was that even in cases where the data was quite good, data often diverges from practice. This was eye-opening for me and convinced me that in pregnancy (and probably in other areas of health) people really do need to be their own advocates and know the data for themselves.\n    <\/div>\n  <\/div>\n<\/div>\n\n\n\n\n<div dir=\"ltr\">\n  <div>\n    <div>\n    <\/div>\n    \n    <div>\n      <b>SS: Have you been surprised about the backlash to your book for your discussion of the zero-alcohol policy during pregnancy? <\/b>\n    <\/div>\n    \n    <div>\n      <b> <\/b>\n    <\/div>\n  <\/div>\n<\/div>\n\n\n\n\n<div dir=\"ltr\">\n  <div>\n    <div>\n      EO: A little bit, yes. This backlash has died down a lot as pregnant women actually read the book and use it. As it turns out, the discussion of alcohol makes up a tiny fraction of the book and most pregnant women are more interested in the rest of it!  But certainly when the book came out this got a lot of focus. I suspected it would be somewhat controversial, although the truth is that every OB I actually talked to told me they thought it was fine. So I was surprised that the reaction was as sharp as it was.  I think in the end a number of people felt that even if the data were supportive of this view, it was important not to say it because of the concern that some women would over-react. I am not convinced by this argument.\n    <\/div>\n  <\/div>\n<\/div>\n\n\n\n\n<div dir=\"ltr\">\n  <div>\n    <div>\n    <\/div>\n    \n    <div>\n      <b>SS: What are the three most important statistical concepts for new mothers to know? <\/b>\n    <\/div>\n    \n    <div>\n      <b> <\/b>\n    <\/div>\n  <\/div>\n<\/div>\n\n\n\n\n<div dir=\"ltr\">\n  <div>\n    <div>\n      EO: I really only have two!\n    <\/div>\n    \n    <div>\n    <\/div>\n    \n    <div>\n      I think the biggest thing is to understand the difference between randomized and non-randomized data and to have some sense of the pittfalls of non-randomized data. I reviewed studies of alcohol where the drinkers were twice as likely as non-drinkers to use cocaine. I think people (pregnant or not) should be able to understand why one is going to struggle to draw conclusions about alcohol from these data.\n    <\/div>\n    \n    <div>\n    <\/div>\n    \n    <div>\n      A second issue is the concept of probability. It is easy to say, \"There is a 10% chance of the following\" but do we really understand that? If someone quotes you a 1 in 100 risk from a procedure, it is important to understand the difference between 1 in 100 and 1 in 400.  For most of us, those seem basically the same - they are both small. But they are not, and people need to think of ways to structure decision-making that acknowledge these differences.\n    <\/div>\n  <\/div>\n<\/div>\n\n\n\n\n<div dir=\"ltr\">\n  <div>\n    <div>\n    <\/div>\n    \n    <div>\n      <b>SS: What computer programming language is most commonly taught for data analysis in economics? <\/b>\n    <\/div>\n    \n    <div>\n      <b> <\/b>\n    <\/div>\n  <\/div>\n<\/div>\n\n\n\n\n<div dir=\"ltr\">\n  <div>\n    <div>\n      EO: So, I think the majority of empirical economists use Stata. I have been seeing more R, as well as a variety of other things, but more commonly among people who do heavier computational fields.\n    <\/div>\n  <\/div>\n<\/div>\n\n\n\n\n<div dir=\"ltr\">\n  <div>\n    <div>\n    <\/div>\n    \n    <div>\n      <b>SS: Do you have any advice for young economists/statisticians who are interested in empirical research? <\/b>\n    <\/div>\n  <\/div>\n<\/div>\n\n\n\n\n<div dir=\"ltr\">\n  <div>\n  <\/div>\n  \n  <div>\n    EO:\n  <\/div>\n  \n  <div>\n    1. Work on topics that interest you. As an academic you will ultimately have to motivate yourself to work. If you aren't interested in your topic (at least initially!), you'll never succeed.\n  <\/div>\n  \n  <div>\n    2. One project which is 100% done is way better than five projects at 80%. You need to actually finish things, something which many of us struggle with.\n  <\/div>\n  \n  <div>\n    3. Presentation matters. Yes, the substance is the most important thing, but don't discount the importance of conveying your ideas well.\n  <\/div>\n<\/div>\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-18-repost-statistical-illiteracy-may-lead-to-parents-panicking-about-autism/",
    "title": "Repost: Statistical illiteracy may lead to parents panicking about Autism",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-12-18",
    "categories": [],
    "contents": "\nEditor’s Note: This is a repost of a previous post on our blog from 2012. The repost is inspired by similar issues with statistical illiteracy that are coming up in allergy screening and pregnancy screening. \nI just was doing my morning reading of a few news sources and stumbled across this Huffington Post article talking about research correlating babies cries to autism. It suggests that the sound of a babies cries may predict their future risk for autism. As the parent of a young son, this obviously caught my attention in a very lizard-brain, caveman sort of way. I couldn’t find a link to the research paper in the article so I did some searching and found out this result is also being covered by Time, Science Daily, Medical Daily, and a bunch of other news outlets.\nNow thoroughly freaked, I looked online and found the pdf of the original research article. I started looking at the statistics and took a deep breath. Based on the analysis they present in the article there is absolutely no statistical evidence that a babies’ cries can predict autism. Here are the flaws with the study:\nSmall sample size. The authors only recruited 21 at risk infants and 18 healthy infants. Then, because of data processing issues, only ended up analyzing 7 high autistic risk versus 5 low autistic-risk in one analysis and 10 versus 6 in another. That is no where near a representative sample and barely qualifies as a pilot study.\nMajor and unavoidable confounding. The way the authors determined high autistic risk versus low risk was based on whether an older sibling had autism. Leaving aside the quality of this metric for measuring risk of autism, there is a major confounding factor: the families of the high risk children all had an older sibling with autism and the families of the low risk children did not! It would not be surprising at all if children with one autistic older sibling might get a different kind of attention and hence cry differently regardless of their potential future risk of autism.\nNo correction for multiple testing. This is one of the oldest problems in statistical analysis. It is also one that is a consistent culprit of false positives in epidemiology studies. XKCD even did a cartoon about it! They tested 9 variables measuring the way babies cry and tested each one with a statistical hypothesis test. They did not correct for multiple testing. So I gathered resulting p-values and did the correction for them. It turns out that after adjusting for multiple comparisons, nothing is significant at the usual P < 0.05 level, which would probably have prevented publication.\nTaken together, these problems mean that the statistical analysis of these data do not show any connection between crying and autism.\nThe problem here exists on two levels. First, there was a failing in the statistical evaluation of this manuscript at the peer review level. Most statistical referees would have spotted these flaws and pointed them out for such a highly controversial paper. A second problem is that news agencies report on this result and despite paying lip-service to potential limitations, are not statistically literate enough to point out the major flaws in the analysis that reduce the probability of a true positive. Should journalists have some minimal in statistics that allows them to determine whether a result is likely to be a false positive to save us parents a lot of panic?\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-17-a-non-comprehensive-list-of-awesome-things-other-people-did-in-2014/",
    "title": "A non-comprehensive list of awesome things other people did in 2014",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-12-17",
    "categories": [],
    "contents": "\nEditor’s Note: Last year Editor’s Note: Last year _ off the top of my head of awesome things other people did. I loved doing it so much that I’m doing it again for 2014. Like last year, I have surely missed awesome things people have done. If you know of some, you should make your own list or add it to the comments! The rules remain the same. I have avoided talking about stuff I worked on or that people here at Hopkins are doing because this post is supposed to be about other people’s awesome stuff. I wrote this post because a blog often feels like a place to complain, but we started Simply Stats as a place to be pumped up about the stuff people were doing with data. Update: I missed pipes in R, now added!\n \nI’m copying everything about Jenny Bryan’s amazing Stat 545 class in my data analysis classes. It is one of my absolute favorite open online set of notes on data analysis.\nBen Baumer, Mine Cetinkaya-Rundel, Andrew Bray, Linda Loi, Nicholas J. Horton wrote this awesome paper on integrating R markdown into the curriculum. I love the stuff that Mine and Nick are doing to push data analysis into undergrad stats curricula.\nSpeaking of those folks, the undergrad guidelines for stats programs put out by the ASA do an impressive job of balancing the advantages of statistics and the excitement of modern data analysis.\nSomebody tell Hector Corrada Bravo to stop writing so many awesome papers. He is making us all look bad. His epiviz paper is great and you should go start using the Bioconductor package if you do genomics.\nHilary Mason founded fast forward labs. I love the business model of translating cutting edge academic (and otherwise) knowledge to practice. I am really pulling for this model to work.\nAs far as I can tell 2014 was the year that causal inference become the new hotness. One example of that is this awesome paper from the Google folks on trying to infer causality from related time series. The R package has some cool features too. I definitely am excited to see all the new innovation in this area.\nHadley was Hadley.\nRafa and Mike taught an awesome class on data analysis for genomics. They also created a book on Github that I think is one of the best introductions to the statistics of genomics that exists so far.\nHilary Parker [Editor’s Note: Last year Editor’s Note: Last year _ off the top of my head of awesome things other people did. I loved doing it so much that I’m doing it again for 2014. Like last year, I have surely missed awesome things people have done. If you know of some, you should make your own list or add it to the comments! The rules remain the same. I have avoided talking about stuff I worked on or that people here at Hopkins are doing because this post is supposed to be about other people’s awesome stuff. I wrote this post because a blog often feels like a place to complain, but we started Simply Stats as a place to be pumped up about the stuff people were doing with data. Update: I missed pipes in R, now added!\n \nI’m copying everything about Jenny Bryan’s amazing Stat 545 class in my data analysis classes. It is one of my absolute favorite open online set of notes on data analysis.\nBen Baumer, Mine Cetinkaya-Rundel, Andrew Bray, Linda Loi, Nicholas J. Horton wrote this awesome paper on integrating R markdown into the curriculum. I love the stuff that Mine and Nick are doing to push data analysis into undergrad stats curricula.\nSpeaking of those folks, the undergrad guidelines for stats programs put out by the ASA do an impressive job of balancing the advantages of statistics and the excitement of modern data analysis.\nSomebody tell Hector Corrada Bravo to stop writing so many awesome papers. He is making us all look bad. His epiviz paper is great and you should go start using the Bioconductor package if you do genomics.\nHilary Mason founded fast forward labs. I love the business model of translating cutting edge academic (and otherwise) knowledge to practice. I am really pulling for this model to work.\nAs far as I can tell 2014 was the year that causal inference become the new hotness. One example of that is this awesome paper from the Google folks on trying to infer causality from related time series. The R package has some cool features too. I definitely am excited to see all the new innovation in this area.\nHadley was Hadley.\nRafa and Mike taught an awesome class on data analysis for genomics. They also created a book on Github that I think is one of the best introductions to the statistics of genomics that exists so far.\nHilary Parker](http://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/) that took the twitterverse by storm. It is perfectly written for people who are just at the point of being able to create their own R package. I think it probably generated 100+ R packages just by being so easy to follow.\nOh you’re not reading StatsChat yet? For real?\nFiveThirtyEight launched. Despite some early bumps they have done some really cool stuff. Loved the recent piece on the beer mile and I read every piece that Emily Oster writes. She does an amazing job of explaining pretty complicated statistical topics to a really broad audience.\nDavid Robinson’s broom package is one of my absolute favorite R packages that was built this year. One of the most annoying things about R is the variety of outputs different models give and this tidy version makes it really easy to do lots of neat stuff.\nChung and Storey introduced the jackstraw which is both a very clever idea and the perfect name for a method that can be used to identify variables associated with principal components in a statistically rigorous way.\nI rarely dig excel-type replacements, but the simplicity of charted.co makes me love it. It does one thing and one thing really well.\nThe hipsteR package for teaching old R dogs new tricks is one of the many cool things Karl Broman did this year. I read all of his tutorials and never cease to learn stuff. In related news if I was 1/10th as organized as that dude I’d actually you know, get stuff done.\nWhether I agree with them or not that they should be allowed to do unregulated human subjects research, statistics at tech companies, and in particular randomized experiments have never been hotter. The boldest of the bunch is OKCupid who writes blog posts with titles like, “We experiment on human beings!”\nIn related news, I love the PlanOut project by the folks over at Facebook, so cool to see an open source approach to experimentation at web scale.\nNo wonder Mike Jordan (no not that Mike Jordan) is such a superstar. His reddit AMA raised my respect for him from already super high levels. First, its awesome that he did it, and second it is amazing how well he articulates the relationship between CS and Stats.\nI’m trying to figure out a way to get Matthew Stephens to write more blog posts. He teased us with the Dynamic Statistical Comparisons post and then left us hanging. The people demand more Matthew.\nDi Cook also started a new blog in 2014. She was also part of this cool exploratory data analysis event for the UN. They have a monster program going over there at Iowa State, producing some amazing research and a bunch of students that are recognizable by one name (Yihui, Hadley, etc.).\nLove this paper on sure screening of graphical models out of Daniela Witten’s group at UW. It is so cool when a simple idea ends up being really well justified theoretically, it makes the world feel right.\nI’m sure this actually happened before 2014, but the Bioconductor folks are still the best open source data science project that exists in my opinion. My favorite development I started using in 2014 is the git-subversion bridge that lets me update my Bioc packages with pull requests.\nrOpenSci ran an awesome hackathon. The lineup of people they invited was great and I loved the commitment to a diverse group of junior R programmers. I really, really hope they run it again.\nDirk Eddelbuettel and Carl Boettiger continue to make bigtime contributions to R. This time it is Rocker, with Docker containers for R. I think this could be a reproducibility/teaching gamechanger.\nRegina Nuzzo brought the p-value debate to the masses. She is also incredible at communicating pretty complicated statistical ideas to a broad audience and I’m looking forward to more stats pieces by her in the top journals.\nBarbara Engelhardt keeps rocking out great papers. But she is also one of the best AE’s I have ever had handle a paper for me at PeerJ. Super efficient, super fair, and super demanding. People don’t get enough credit for being amazing in the peer review process and she deserves it.\nBen Goldacre and Hans Rosling continue to be two of the best advocates for statistics and the statistical discipline - I’m not sure either claims the title of statistician but they do a great job anyway. This piece about Professor Rosling in Science gives some idea about the impact a statistician can have on the most current problems in public health. Meanwhile, I think Dr. Goldacre does a great job of explaining how personalized medicine is an information science in this piece on statins in the BMJ.\nMichael Lopez’s series of posts on graduate school in statistics should be 100% required reading for anyone considering graduate school in statistics. He really nails it.\n Trey Causey has an equally awesome Getting Started in Data Science post that I read about 10 times.\nDrop everything and go read all of Philip Guo’s posts. Especially this one about industry versus academia or this one on the practical reason to do a PhD.\nThe top new Twitter feed of 2014 has to be [@ResearchMark](https://twitter.com/ResearchMark) (incidentally I’m still mourning the disappearance of [@STATSHULK](https://twitter.com/STATSHULK)).\nStephanie Hicks’ blog combines recipes for delicious treats and statistics, also I thought she had a great summary of the Women in Stats (#WiS2014) conference.\nEmma Pierson is a Rhodes Scholar who wrote for 538, 23andMe, and a bunch of other major outlets as an undergrad. Her blog, obsessionwithregression.blogspot.com is another must read. Here is an example of her awesome work on how different communities ignored each other on Twitter during the Ferguson protests.\nThe Rstudio crowd continues to be on fire. I think they are a huge part of the reason that R is gaining momentum. It wouldn’t be possible to list all their contributions (or it would be an Rstudio exclusive list) but I really like Packrat and R markdown v2.\nAnother huge reason for the movement with R has been the outreach and development efforts of the Revolution Analytics folks. The Revolutions blog has been a must read this year.\nJulian Wolfson and Joe Koopmeiners at University of Minnesota are straight up gamers. They live streamed their recruiting event this year. One way I judge good ideas is by how mad I am I didn’t think of it and this one had me seeing bright red.\nThis is just an awesome paper comparing lots of machine learning algorithms on lots of data sets. Random forests wins and this is a nice update of one of my favorite papers of all time: Classifier technology and the illusion of progress.\nPipes in R! This stuff is for real. The piping functionality created by Stefan Milton and Hadley is one of the few inventions over the last several years that immediately changed whole workflows for me.\n \nI’ll let [@ResearchMark](https://twitter.com/ResearchMark) take us out:\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-14-sunday-datastatistics-link-roundup-121414/",
    "title": "Sunday data/statistics link roundup (12/14/14)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-12-14",
    "categories": [],
    "contents": "\n1. suggests that economists are impartial when it comes to their liberal/conservative views. That being said, I’m not sure the regression line says what they think it does, particularly if you pay attention to the variance around the line (via Rafa).\nI am digging the simplicity of charted.co from the folks at Medium. But I worry about spurious correlations everywhere. I guess I should just let that ship sail.\nFiveThirtyEight does a run down of the beer mile. If they set up a data crunchers beer mile, we are in.\nI love it when Thomas Lumley interviews himself about silly research studies and particularly their associated press releases. I can actually hear his voice in my head when I read them. This time the lipstick/IQ silliness gets Lumleyed.\nJordan was better than Kobe. Surprise. Plus Rafa always takes the Kobe bait.\nMatlab/Python/R translation cheat sheet (via Stephanie H.).\nIf I’ve said it once, I’ve said it a million times, statistical thinking is now as important as reading and writing. The latest example is parents not understanding the difference between sensitivity and the predictive value of a positive may be leading to unnecessary abortions (via Dan M./Rafa).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-12-kobe-data-says-stop-blaming-your-teammates/",
    "title": "Kobe, data says stop blaming your teammates",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-12-12",
    "categories": [],
    "contents": "\nThis year, Kobe leads the league in missed shots (by a lot), has an abysmal FG% of 39 and his team plays better when he is on the bench. Yet he This year, Kobe leads the league in missed shots ([by a lot](http://ftw.usatoday.com/2014/11/kobe-bryant-lakers-shot-stats)), has an abysmal FG% of 39 and his team plays better [when he is on the bench](http://bleacherreport.com/articles/2292515-how-much-blame-does-kobe-bryant-deserve-for-los-angeles-lakers-pathetic-start). Yet he for the Lakers’ 6-16 record. Below is a plot showing that 2014 is not the first time the Lakers are mediocre during Kobe’s tenure. It shows the percentage points above .500 per season with the Shaq and twin towers eras highlighted. I include the same plot for Lebron as a control.\n\nSo stop blaming your teammates!\nAnd here is my hastily written code (don’t judge me!).\n \n \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-08-geneticamente-no-hay-tal-cosa-como-la-raza-puertorriquena/",
    "title": "Genéticamente, no hay tal cosa como la raza puertorriqueña",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-12-08",
    "categories": [],
    "contents": "\nEditor’s note: Last week the Latin American media picked up a blog post with the eye-catching title “The perfect human is Puerto Rican”. More attention appears to have been given to the title than the post itself. The coverage and comments on social media have demonstrated the need for scientific education on the topic of genetics and race. Here I will try to explain, in layman’s terms, why the interpretations I read in the main Puerto Rican paper is scientifically incorrect and somewhat concerning. The post is in Spanish.\nEn un artículo reciente titulado “Ser humano perfecto sería puertorriqueño”, El Nuevo Día resumió una entrada en el blog (erróneamente llamado un estudio) del matemático Lior Pachter. El autor del blog, intentando ridiculizar comentarios racistas que escuchó decir a James Watson, describe un experimento mental en el cual encuentra que el humano “perfecto” (las comilla son importantes), de existir, pertenecería a un grupo genéticamente mezclado. De las personas estudiadas,  la más genéticamente cercana a su humano “perfecto” resultó ser una mujer puertorriqueña. La motivación de este ejercicio era ridiculizar la idea de que una raza puede ser superior a otra. El Nuevo Día parece no captar este punto y nos dice que “el experto concluyó que en todo caso no es de sorprenderse que la persona más cercana a tal perfección sería una puertorriqueña, debido a la combinación de buenos genes que tiene la raza puertorriqueña.” Aquí describo por qué esta interpretación es científicamente errada.\n¿Qué es el genoma?\nEl genoma humano codifica (en moléculas de ADN) la información genética necesaria para nuestro desarrollo biológico. Podemos pensar en el genoma como dos series de 3,000,000,000 letras (A, T, C o G) concatenadas. Una la recibimos de nuestro padre y la otra de nuestra madre. Distintos pedazos (los genes) codifican proteínas necesarias para las miles de funciones que cumplen nuestras células y que conllevan a algunas de nuestras características físicas. Con unas pocas excepciones, todas las células en nuestro cuerpo contienen una copia exacta de estas dos series de letras. El esperma y el huevo tienen sólo una serie de letras, una mezcla de las otras dos. Cuando se unen el esperma y el huevo, la nueva célula, el cigoto, une las dos series y es así que heredamos características de cada progenitor.\n¿Qué es la variación genética?\nSi todos venimos del primer humano,¿cómo entonces es que somos diferentes? Aunque es muy raro, estas letras a veces mutan aleatoriamente. Por ejemplo, una C puede cambiar a una T. A través de cientos de miles de años suficientes mutaciones han ocurrido para crear variación entre los humanos. La teoría de selección natural nos dice que si esta mutación confiere una ventaja para la supervivencia, el que la posee tiene más probabilidad de pasarla a sus descendientes. Por ejemplo, en Europa la piel clara es más ventajosa, por su habilidad de absorber vitamina D cuando hay poco sol, que en África Occidental donde la melanina en la piel oscura protege del sol intenso. Se estima que las diferencias entre los humanos se pueden encontrar en por lo menos 10 millones de las 3 mil millones de letras (noten que es menos de 1%).\nGenéticamente, ¿qué es una “raza” ?\nEsta es un pregunta controversial. Lo que no es controversial es que si comparamos la serie de letras de los europeos del norte con los africanos occidentales o con los indígenas de las Américas, encontramos pedazos del código que son únicos a cada región. Si estudiamos las partes del código que cambian entre humanos, fácilmente podemos distinguir los tres grupos. Esto no nos debe sorprender dado que, por ejemplo, la diferencia en el color de ojos y la pigmentación de la piel se codifica con distintas letras en los genes asociados con estas características. En este sentido podríamos crear una definición genética de “raza” basada en las letras que distinguen a estos grupos. Ahora bien, ¿podemos hacer lo mismo para distinguir un puertorriqueño de un dominicano? ¿Podemos crear una definición genética que incluye a Carlos Delgado y a Mónica Puig, pero no a Robinson Canó y Juan Luis Guerra? La literatura científica nos dice que no.\nPCA for genotypesEn una serie de artículos , el genético Carlos Bustamante y sus colegas han estudiado los genomas de personas de varios grupos étnicos. Ellos definen una distancia genética que resumen con dos dimensiones en la gráfica arriba. Cada punto es una persona y el color presenta a su grupo. Noten los tres extremos de la gráfica con muchos puntos del mismo color amontonados. Estos son los europeos blancos (puntos rojo), africanos occidentales (verde) e indígenas americanos (azul). Los puntos más regados en el medio son las poblaciones mezcladas. Entre los europeos y los indígenas vemos a los mexicanos y entre los europeos y africanos a los afroamericanos. Los puertorriqueños son los puntos anaranjados. He resaltado con números a tres de ellos. El 1 está cerca del supuesto humano “perfecto”. El 2 es indistinguible de un europeo y el 3 es indistinguible de un afroamericano. Los demás cubrimos un espectro amplio. También resalto con el número 4 a un dominicano que está tan cerca a la “perfección” como la puertorriqueña. La observación principal es que hay mucha variación genética entre los puertorriqueños. En los que Bustamante estudió, la ascendencia africana varía de 5-60%, la europea de 35-95% y la taína de 0-20%. ¿Cómo entonces podemos hablar de una “raza” puertorriqueña cuando nuestros genomas abarcan un espacio tan grande que puede incluir, entre otros, europeos, afroamericanos y dominicanos  ?\n¿Qué son los genes “buenos”?\nAlgunas mutaciones son letales. Otras resultan en cambios a proteínas que causan enfermedades como la fibrosis quística y requieren que ambos padres tengan la mutación. Por lo tanto la mezcla de genomas diferentes disminuye las probabilidades de estas enfermedades. Recientemente una serie de estudios ha encontrado ventajas de algunas combinaciones de letras relacionadas a enfermedades comunes como la hipertensión. Una mezcla genética que evita tener dos copias de estos genes con más riesgo puede ser ventajosa. Pero las supuestas ventajas son pequeñísimas y específicas a enfermedades, no a otras características que asociamos con la “perfección”. El concepto de “genes buenos” es un vestigio de la eugenesia.\nA pesar de nuestros problemas sociales y económicos actuales, Puerto Rico tiene mucho de lo cual estar orgulloso. En particular, producimos buenísimos ingenieros, atletas y músicos. Atribuir su éxito a “genes buenos” de nuestra “raza” no sólo es un disparate científico, sino una falta de respeto a estos individuos que a través del trabajo duro, la disciplina y el esmero han logrado lo que han logrado. Si quieren saber si Puerto Rico tuvo algo que ver con el éxito de estos individuos, pregúntenle a un historiador, un antropólogo o un sociólogo y no a un genetista. Ahora, si quieren aprender del potencial de estudiar genomas para mejorar tratamientos médicos y la importancia de estudiar una diversidad de individuos, un genetista tendrá mucho que compartir.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-07-sunday-datastatistics-link-roundup-12714/",
    "title": "Sunday data/statistics link roundup (12/7/14)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-12-07",
    "categories": [],
    "contents": "\nA randomized controlled trial shows that using conversation to detect suspicious behavior is much more effective then just monitoring body language (via Ann L. on Twitter). This comes as a crushing blow to those of us who enjoyed the now-cancelled Lie to Me and assumed it was all real.\nCheck out this awesome real-time visualization of different types of network attacks. Rafa says if you watch long enough you will almost certainly observe a “storm” of attacks. A cool student project would be modeling the distribution of these attacks if you could collect the data (via David S.).\nConsider this: Did Big Data Kill the Statistician? I understand the sentiment, that statistical thinking and applied statistics has been around a long time and has produced some good ideas. On the other hand, there is definitely a large group of statisticians who aren’t willing to expand their thinking beyond a really narrow set of ideas (via Rafa)\nGangnam Style viewership creates integers too big for Youtube (via Rafa)\nA couple of interviews worth reading, [ 1. A randomized controlled trial shows that using conversation to detect suspicious behavior is much more effective then just monitoring body language (via Ann L. on Twitter). This comes as a crushing blow to those of us who enjoyed the now-cancelled Lie to Me and assumed it was all real.\nCheck out this awesome real-time visualization of different types of network attacks. Rafa says if you watch long enough you will almost certainly observe a “storm” of attacks. A cool student project would be modeling the distribution of these attacks if you could collect the data (via David S.).\nConsider this: Did Big Data Kill the Statistician? I understand the sentiment, that statistical thinking and applied statistics has been around a long time and has produced some good ideas. On the other hand, there is definitely a large group of statisticians who aren’t willing to expand their thinking beyond a really narrow set of ideas (via Rafa)\nGangnam Style viewership creates integers too big for Youtube (via Rafa)\nA couple of interviews worth reading,](http://simplystatistics.org/2014/12/05/interview-with-cole-trapnell-of-uw-genome-sciences/) and SAMSI’s with Jyotishka Data (via Jamie N.)\n A piece on the secrets we don’t know we are giving away through giving our data to [companies/the government/the internet].\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-05-interview-with-cole-trapnell-of-uw-genome-sciences/",
    "title": "Interview with Cole Trapnell of UW Genome Sciences",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-12-05",
    "categories": [],
    "contents": "\n\n\n<div class=\"pf\">\n  <div class=\"nXx3q\">\n    <div class=\"cA\">\n      <div class=\"cl ac\">\n        <div class=\"yDSKFc viy5Tb\">\n          <div class=\"rt\">\n            <div class=\"DsPmj\">\n              <div class=\"scroll-list-section-body scroll-list-section-body-0\">\n                <div class=\"scroll-list-item top-level-item scroll-list-item-open scroll-list-item-highlighted\" tabindex=\"0\" data-item-id=\"Bs#gmail:thread-f:1463549268702220125\" data-item-id-qs=\"qsBs-gmail-thread-f-1463549268702220125-0\">\n                  <div class=\"ah V T qX V-M\">\n                    <div class=\"af qX af-M\">\n                      <div class=\"fB qX\">\n                        <div class=\"ag qX\" tabindex=\"0\" data-msg-id=\"Bs#msg-f:1463577765776057801\" data-msg-id-qs=\"qsBs-msg-f-1463577765776057801\">\n                          <div class=\"nI qX\">\n                            <div class=\"gm qX\">\n                              <div class=\"bK xJNT8d\">\n                                <div>\n                                  <div class=\"nD\">\n                                    <blockquote>\n                                      <div dir=\"ltr\">\n                                        <div>\n                                          <a href=\"http://simplystatistics.org/wp-content/uploads/2014/12/cole_cropped.jpg\"><img class=\"aligncenter wp-image-3624\" src=\"http://simplystatistics.org/wp-content/uploads/2014/12/cole_cropped-278x300.jpg\" alt=\"cole_cropped\" width=\"186\" height=\"200\" /><\/a>\n                                        <\/div>\n                                      <\/div>\n                                    <\/blockquote>\n                                    \n                                    <div dir=\"ltr\">\n                                    <\/div>\n                                    \n                                    <div dir=\"ltr\">\n                                      <div style=\"text-align: left;\">\n                                        <em><a href=\"http://cole-trapnell-lab.github.io/\">Cole Trapnell<\/a> is an Assistant Professor of Genome Sciences at the University of Washington. He is the developer of multiple incredibly widely used tools for genomics including Tophat, Cufflinks, and Monocle. His lab at UW studies cell differentiation, reprogramming, and other transitions between stable or metastable cellular states using a combination of computational and experimental techniques. We talked to Cole as part of our <a href=\"http://simplystatistics.org/interviews/\">ongoing interview series<\/a> with exciting junior data scientists. <\/em>\n                                      <\/div>\n                                      \n                                      <div style=\"text-align: left;\">\n                                      <\/div>\n                                      \n                                      <div style=\"text-align: left;\">\n                                      <\/div>\n                                      \n                                      <div style=\"text-align: left;\">\n                                        <strong>SS: Do you consider yourself a computer scientist, a statistician, a computational biologist, or something else?<\/strong>\n                                      <\/div>\n                                    <\/div>\n                                  <\/div>\n                                <\/div>\n                                \n                                <div>\n                                  <div class=\"F3hlO\">\n                                    <div>\n                                      <p>\n                                        CT: The questions that get me up and out of bed in the morning the fastest are biology questions. I work on cell differentiation - I want to know how to define the state of a cell and how to predict transitions between states. That said, my approach to these questions so far has been to use new technologies to look at previously hard to access aspects of gene regulation.  For example, I’ve used RNA-Seq to look beyond gene expression into finer layers of regulation like splicing. Analyzing sequencing experiments often involves some pretty non-trivial math, computer science, and statistics.  These data sets are huge, so you need fast algorithms to even look at them. They all involve transforming reads into a useful readout of biology, and the technical and biological variability in that transformation needs to be understood and controlled for, so you see cool mathematical and statistical problems all the time. So I guess you could say that I’m a biologist, both experimental and computational. I have to do some computer science and statistics in order to do biology.\n                                      <\/p>\n                                      \n                                      <div>\n                                      <\/div>\n                                    <\/div>\n                                  <\/div>\n                                <\/div>\n                                \n                                <div>\n                                  <div class=\"nD\">\n                                    <div>\n                                      <div>\n                                        <div>\n                                          <div>\n                                            <div>\n                                              <div dir=\"ltr\">\n                                                <div>\n                                                  <strong>SS: You got a Ph.D. in computer science but have spent the last several years in a wet lab learning to be a bench biologist - why did you make that choice?<\/strong>\n                                                <\/div>\n                                              <\/div>\n                                            <\/div>\n                                          <\/div>\n                                        <\/div>\n                                      <\/div>\n                                    <\/div>\n                                  <\/div>\n                                <\/div>\n                                \n                                <div>\n                                  <div class=\"F3hlO\">\n                                    <div>\n                                      <div>\n                                        <p>\n                                          CT: Three reasons, mainly:\n                                        <\/p>\n                                        \n                                        <p>\n                                          1) I thought learning to do bench work would make me a better overall scientist.  It has, in many ways, I think. It’s fundamentally changed the way I approach the questions I work on, but it’s also made me more effective in lots of tiny ways. I remember when I first got to John Rinn’s lab, we needed some way to track lots of libraries and other material.  I came up with some scheme where each library would get an 8-digit alphanumeric code generated by a hash function or something like that (we’d never have to worry about collisions!). My lab mate handed me a marker and said, “OK, write that on the side of these 12 micro centrifuge tubes”.  I threw out my scheme and came up with something like “JR_1”, “JR_2”, etc.  That’s a silly example, but I mention it because it reminds me of how completely clueless I was about where biological data really comes from.\n                                        <\/p>\n                                        \n                                        <p>\n                                          2) I wanted to establish an independent, long-term research program investigating differentiation, and I didn’t want to have to rely on collaborators to generate data. I knew at the end of grad school that I wanted to have my own wet lab, and I doubted that anyone would trust me with that kind of investment without doing some formal training. Despite the now-common recognition by experimental biologists that analysis is incredibly important, there’s still a perception out there that computational biologists aren’t “real biologists”, and that computational folks are useful tools, but not the drivers of the intellectual agenda. That's of course not true, but I didn’t want to fight the stigma.\n                                        <\/p>\n                                        \n                                        <p>\n                                          3) It sounded fun. I had one or two friends who had followed the \"dry to wet” training trajectory, and they were having a blast.   Seeing a result live under the microscope is satisfying in a way that I’ve rarely experienced looking at a computer screen.\n                                        <\/p>\n                                        \n                                        <div>\n                                        <\/div>\n                                      <\/div>\n                                    <\/div>\n                                  <\/div>\n                                <\/div>\n                                \n                                <div>\n                                  <strong>SS: Do you plan to have both a wet lab and a dry lab when you start your new group? <\/strong>\n                                <\/div>\n                                \n                                <div>\n                                  <div class=\"F3hlO\">\n                                    <div>\n                                      <div>\n                                        <div>\n                                          <p>\n                                            CT: Yes. I’m going to be starting my lab at the University of Washington in the department of Genome Sciences this summer, and it’s going to be a roughly 50/50 operation, I hope. Many of the labs there are set up that way, and there’s a real culture of valuing both sides. As a postdoc, I’ve been extremely fortunate to collaborate with grad students and postdocs who were trained as cell or molecular biologists but wanted to learn sequencing analysis. We’d train each other, often at great cost in terms of time spent solving “somebody else’s problem”.  I’m going to do my best to create an environment like that, the way John did for me and my lab mates.\n                                          <\/p>\n                                          \n                                          <div>\n                                          <\/div>\n                                          \n                                          <div>\n                                            <strong>SS: You are frequently on the forefront of new genomic technologies. As data sets get larger and more complicated how do we ensure reproducibility and replicability of computational results? <\/strong>\n                                          <\/div>\n                                        <\/div>\n                                      <\/div>\n                                    <\/div>\n                                  <\/div>\n                                <\/div>\n                                \n                                <div>\n                                  <div class=\"F3hlO\">\n                                    <div>\n                                      <div>\n                                        <div>\n                                          <div>\n                                            <div>\n                                              <p>\n                                                CT: That’s a good question, and I don’t really have a good answer. You’ve talked a lot on this blog about the importance of making science more reproducible and how journals could change to make it so. I agree wholeheartedly with a lot of what you’ve said. I like the idea of \"papers as packages”, but I don’t see it happening soon, because it’s a huge amount of extra work and there’s not a big incentive to do so.  Doing so might make it easier to be attacked, so there could even a disincentive! Scientists do well when the publish papers and those papers are cited widely. We have lots of ways to quantify “impact” - h-index, total citation count, how many times your paper is shared via twitter on a given day, etc.  (Say what you want about whether these are meaningful measures).\n                                              <\/p>\n                                              \n                                              <p>\n                                                We don’t have a good way to track who’s right and who’s wrong, or whose results are reproducible and whose aren’t, short of full blown paper retraction.  Most papers aren’t even checked in a serious way. Worse, the papers that are checked are the ones that a lot of people see - few people spend precious time following up on tangential observations in low circulation journals.  So there’s actually an incentive to publish “controversial\" results in highly visible journals because at least you’re getting attention.\n                                              <\/p>\n                                              \n                                              <p>\n                                                Maybe we need a Yelp for papers and data sets?  One where in order to dispute the reproducibility of the analysis, you’d have to provide the code *you* ran to generate a contradictory result?  There needs to be a genuine and tangible *reward* (read: funding and career advancement) for putting up an analysis that others can dive into, verify, extend, and learn from.\n                                              <\/p>\n                                              \n                                              <p>\n                                                In any case, I think it’s worth noting that reproducibility is not a problem unique to computation - experimentalists have a hard time reproducing results they got last week, much less results that came from some other lab!  There’s all kinds of harmless reasons for that.  Experiments are hard.  Reagents come in bad lots. You had too much coffee that morning and can’t steady your pipet hand to save your life. But I worry a bit that we could spend a lot of effort making our analysis totally automated and perfectly reproducible and still be faced with the same problem.\n                                              <\/p>\n                                              \n                                              <div>\n                                              <\/div>\n                                              \n                                              <div>\n                                                <strong>SS: What are the interesting statistical challenges in single-cell RNA-sequencing? <\/strong>\n                                              <\/div>\n                                            <\/div>\n                                          <\/div>\n                                        <\/div>\n                                      <\/div>\n                                    <\/div>\n                                  <\/div>\n                                <\/div>\n                                \n                                <div>\n                                  <div class=\"F3hlO\">\n                                    <div>\n                                      <div>\n                                        <div>\n                                          <div>\n                                            <div>\n                                              <div>\n                                                <p>\n                                                  CT:\n                                                <\/p>\n                                                \n                                                <p>\n                                                  Oh man, there are many.  Here’s a few:\n                                                <\/p>\n                                                \n                                                <p>\n                                                  1) There some very interesting questions about variability in expression across cells, or within one cell across time. There’s clearly a lot of variability in the expression level of a given gene across cells.  But there’s really no way right now to take “replicate” measurements of a single cell.  What would that mean?  With current technology, to make an RNA-Seq library form a cell, you have to lyse it.  So that’s it for that cell.  Even if you had a non-invasive way to measure the whole transcriptome, the cell is a living machine that’s always changing in ways large and small, even in culture. Would you consider repeated measurements “replicates”.  Furthermore, how can you say that two different cells are “replicate” measurements of a  single, defined cell state?  Do such states even really exist?\n                                                <\/p>\n                                                \n                                                <p>\n                                                  For that matter, we don’t have a good way of assessing how much variability stems from technical sources as opposed to biological sources.  One common way of assessing technical variability is to spike some alien transcripts at known concentrations in to purified RNA before making the library, so you can see how variable your endpoint measurements are for those alien transcripts. But to do that for single-cell RNA-Seq, we’d have to actually spike transcripts *into* the nucleus of a cell before we lyse it and put it through the library prep process.  Just doping it into the lysate’s not good enough, because the lysis itself might (and likely does) destroy a substantial fraction of the endogenous RNA in the cell.  So there are some real barriers to overcome in order to get a handle on how much variability is really biological.\n                                                <\/p>\n                                                \n                                                <p>\n                                                  2) A second challenge is writing down what a biological process looks like at single cell resolution. I mean we want to write down a model that predicts the expression levels of each gene in a cell as it goes through some biological process. We want to be able to say this gene comes on first, then this one, then these genes, and so on. In genomics up until now, we’ve been in the situation where we are measuring many variables (P) from few measurements (N).  That is, N << P, typically, which has made this problem extremely difficult.  With single cell RNA-Seq, that may no longer be the case.  We can already easily capture hundreds of cells, and thousands of cells per capture is just around the corner, so soon, N will be close to P, and maybe someday greater.\n                                                <\/p>\n                                                \n                                                <p>\n                                                  Assume for the moment that we are capturing cells that are either resting at or transiting between well defined states. You can think of each cell as a point in a high-dimensional geometric space, where each gene is a different dimension.  We’d like to find those equilibrium states and figure out which genes are correlated with which other genes.  Even better, we’d like to study the transitions between states and identify the genes that drive them.  The curse of dimensionality is always going to be a problem (we’re not likely to capture millions or billions of cells anytime soon), but maybe we have enough data to make some progress. There’s interesting literature out there for tackling problems at this scale, but to my knowledge these methods haven’t yet been widely applied in biology.  I guess you can think of cell differentiation viewed at whole-transcriptome, single-cell resolution as one giant manifold learning problem.  Same goes for oncogenesis, tissue homeostasis, reprogramming, and on and on. It’s going to be very exciting to see the convergence of large scale statistical machine learning and cell biology.\n                                                <\/p>\n                                                \n                                                <p>\n                                                  <strong>SS: If you could do it again would you do computational training then wet lab training or the other way around? <\/strong>\n                                                <\/p>\n                                              <\/div>\n                                            <\/div>\n                                          <\/div>\n                                        <\/div>\n                                      <\/div>\n                                    <\/div>\n                                  <\/div>\n                                <\/div>\n                                \n                                <div>\n                                  <div class=\"F3hlO\">\n                                    <p>\n                                      CT: I’m happy with how I did things, but I’ve seen folks go the other direction very successfully.  My labmates Loyal Goff and Dave Hendrickson started out as molecular biologists, but they’re wizards at the command line now.\n                                    <\/p>\n                                    \n                                    <div>\n                                    <\/div>\n                                  <\/div>\n                                <\/div>\n                                \n                                <div>\n                                  <div class=\"nD\">\n                                    <div>\n                                      <div>\n                                        <div>\n                                          <div>\n                                            <div>\n                                              <div>\n                                                <div>\n                                                  <div>\n                                                    <div dir=\"ltr\">\n                                                      <div>\n                                                        <strong>SS: What is your programming language of choice? <\/strong>\n                                                      <\/div>\n                                                    <\/div>\n                                                  <\/div>\n                                                <\/div>\n                                              <\/div>\n                                            <\/div>\n                                          <\/div>\n                                        <\/div>\n                                      <\/div>\n                                    <\/div>\n                                  <\/div>\n                                <\/div>\n                                \n                                <div>\n                                  <div class=\"F3hlO\">\n                                    <div>\n                                      <div>\n                                        <div>\n                                          <div>\n                                            <div>\n                                              <p>\n                                                CT: Oh, I’d say I hate them all equally 😉\n                                              <\/p>\n                                              \n                                              <p>\n                                                Just kidding. I’ll always love C++. I work in R a lot these days, as my work has veered away from developing tools for other people towards analyzing data I’ve generated.  I still find lots of things about R to be very painful, but ggplot2, plyr, and a handful of other godsend packages make the juice worth the squeeze.\n                                              <\/p>\n                                            <\/div>\n                                          <\/div>\n                                        <\/div>\n                                      <\/div>\n                                    <\/div>\n                                  <\/div>\n                                <\/div>\n                              <\/div>\n                            <\/div>\n                          <\/div>\n                        <\/div>\n                      <\/div>\n                    <\/div>\n                  <\/div>\n                <\/div>\n              <\/div>\n            <\/div>\n          <\/div>\n        <\/div>\n      <\/div>\n    <\/div>\n  <\/div>\n<\/div>\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-04-repost-a-deterministic-statistical-machine/",
    "title": "Repost: A deterministic statistical machine",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-12-04",
    "categories": [],
    "contents": "\nEditor’s note: This is a repost of our previous post about deterministic statistical machines. It is inspired by the recent announcement that the Automatic Statistician received funding from Google. In 2012 we also applied to Google for a small research award to study this same problem, but didn’t get it. In the interest of extreme openness like Titus Brown or Ethan White, here is our application we submitted to Google. I showed this to a friend who told me the reason we didn’t get it is because our proposal was missing two words: “artificial”, “intelligence”. \nAs Roger pointed out the most recent batch of Y Combinator startups included a bunch of data-focused companies. One of these companies, StatWing, is a web-based tool for data analysis that looks like an improvement on SPSS with more plain text, more visualization, and a lot of the technical statistical details “under the hood”. I first read about StatWing on TechCrunch, where the title, “How Statwing Makes It Easier To Ask Questions About Data So You Don’t Have To Hire a Statistical Wizard”.\nStatWing looks super user-friendly and the idea of democratizing statistical analysis so more people can access these ideas is something that appeals to me. But, as one of the aforementioned statistical wizards, this had me freaked out for a minute. Once I looked at the software though, I realized it suffers from the same problem that most “user-friendly” statistical software suffers from. It makes it really easy to screw up a data analysis. It will tell you when something is significant and if you don’t like that it isn’t, you can keep slicing and dicing the data until it is. The key issue behind getting insight from data is knowing when you are fooling yourself with confounders, or small effect sizes, or overfitting. StatWing looks like an improvement on the UI experience of data analysis, but it won’t prevent false positives that plague science and cost business big $$.\nSo I started thinking about what kind of software would prevent these sort of problems while still being accessible to a big audience. My idea is a “deterministic statistical machine”. Here is how it works, you input a data set and then specify the question you are asking (is variable Y related to variable X? can i predict Z from W?) then, depending on your question, it uses a deterministic set of methods to analyze the data. Say regression for inference, linear discriminant analysis for prediction, etc. But the method is fixed and deterministic for each question. It also performs a pre-specified set of checks for outliers, confounders, missing data, maybe even data fudging. It generates a report with a markdown tool and then immediately publishes the result to figshare.\nThe advantage is that people can get their data-related questions answered using a standard tool. It does a lot of the “heavy lifting” in checking for potential problems and produces nice reports. But it is a deterministic algorithm for analysis so overfitting, fudging the analysis, etc. are harder. By publishing all reports to figshare, it makes it even harder to fudge the data. If you fiddle with the data to try to get a result you want, there will be a “multiple testing paper trail” following you around.\nThe DSM should be a web service that is easy to use. Anybody want to build it? Any suggestions for how to do it better?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-02-thinking-like-a-statistician-social-media-and-the-spiral-of-silence/",
    "title": "Thinking Like a Statistician: Social Media and the ‘Spiral of Silence’",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-12-02",
    "categories": [],
    "contents": "\nA few months ago the Pew Research Internet Project published a paper on social media and the ‘spiral of silence’. Their main finding is that people are less likely to discuss a controversial topic on social media than in person. Unlike others, I  did not find this result surprising, perhaps because I think like a statistician.\nShares or retweets of published opinions on controversial political topics - religion, abortion rights, gender inequality, immigration, income inequality, race relations, the role of government, foreign policy, education, climate change - are ubiquitous in social media. These are usually accompanied by passionate statements of strong support or outraged disagreement. Because these are posted by people we elect to follow, we generally agree with what we see on our feeds. Here is a statistical explanation for why many keep silent when they disagree.\nWe will summarize the political view of an individual as their opinions on the 10 topics listed above. For simplicity I will assume these opinions can be quantified with a left (liberal) to right (conservative) scale. Every individual can therefore be defined by a point in a 10 dimensional space. Once quantified in this way, we can define a political distance between any pair of individuals. In the American landscape there are two clear clusters which I will call the Fox News and MSNBC clusters. As seen in the illustration below, the cluster centers are very far from each other and individuals within the clusters are very close. Each cluster has a very low opinion of the other. A glance through a social media feed will quickly reveal individuals squarely inside one of these clusters. Members of the clusters fearlessly post their opinions on controversial topics as this behavior is rewarded by likes, retweets or supportive comments from others in their cluster. Based on the uniformity of opinion inferred from the comments, one would think that everybody is in one of these two groups. But this is obviously not the case.\n\nIn the illustration above I include an example of an individual (the green dot) that is outside the two clusters. Although not shown, there are many of these independent thinkers. In our example, this individual is very close to the MSNBC cluster, but not in it. The controversial topic posts in this person’s feed are mostly posted by those in the cluster of closest proximity, and the spiral of silence is due in part to the fact that independent thinkers are uniformly adverse to disagreeing publicly. For the mathematical explanation of why, we introduce the concept of a projection.\nIn mathematics, a projection can map a multidimensional point to a smaller, simpler, subset. In our illustration, the independent thinker is very close to the MSNBC cluster on all dimensions except one. To use education as an example, let’s say this person supports school choice. As seen in the illustration, in the projection to the education dimension, that mostly liberal person is squarely in the Fox News cluster. Now imagine that a friend shares an article on The Corporate Takeover of Public Education along with a passionate statement of approval. Independent thinkers have a feeling that by voicing their dissent, dozens, perhaps hundreds, of strangers on social media (friends of friends for example) will judge them solely on this projection. To make matters worse, public shaming of the independent thinker, for supposedly being a member of the Fox News cluster, will then be rewarded by increased social standing among the MSNBC cluster as evidenced by retweets, likes and supportive comments. In a worse case scenario for this person, and best case scenario for the critics, this public shaming goes viral. While the short term rewards for preaching to the echo chamber are clear, there are no apparent incentives for dissent.\nThe superficial and fast paced nature of social media is not amenable to nuances and subtleties. Disagreement with the groupthink on one specific topic can therefore get a person labeled as a “neoliberal corporate shill” by the MSNBC cluster or a “godless liberal” by the Fox News one. The irony is that in social media, those politically closest to you, will be the ones attaching the unwanted label.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-11-25-harvardx-biomedical-data-science-open-online-training-curriculum-launches-on-january-19/",
    "title": "HarvardX Biomedical Data Science Open Online Training Curriculum launches on January 19",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-11-25",
    "categories": [],
    "contents": "\nWe recently received We recently received  initiative to develop MOOCs for biomedical data science. Our first offering will be version 2 of my Data Analysis for Genomics course which will launch on January 19. In this version, the course will be turned into an 8 course series and you can get a certificate in each one of them. The motivation for doing this is to go more in-depth into the different topics and to provide different entry points for students with different levels of expertise. We provide four courses on concepts and skills and four case-study based course. We basically broke the original class into the following eight parts:\nStatistics and R for the Life Sciences\nIntroduction to Linear Models and Matrix Algebra\nAdvanced Statistics for the Life Sciences\nIntroduction to Bioconductor\nCase study: RNA-seq data analysis\nCase study: Variant Discovery and Genotyping\nCase study: ChIP-seq data analysis\nCase study: DNA methylation data analysis\nYou can follow the links to enroll. While not required, some familiarity with R and Rstudio will serve you well so consider taking Roger’s R course and Jeff’s Toolbox course before delving into this class.\nIn years 2 and 3 we plan to introduce several other courses covering topics such as python for data analysis, probability, software engineering, and data visualization which will be taught by a collaboration between the departments of Biostatistics, Statistics and Computer Science at Harvard.\nAnnouncements will be made here and on twitter: [@rafalab](https://twitter.com/rafalab)\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-11-12-data-science-students-predict-the-midterm-election-results/",
    "title": "Data Science Students Predict the Midterm Election Results",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-11-12",
    "categories": [],
    "contents": "\nAs explained in an earlier post, one of the homework assignments of my CS109 class was to predict the results of the midterm election. We created a competition in which 49 students entered. The most interesting challenge was to provide intervals for the republican - democrat difference in each of the 35 senate races. Anybody missing more than 2 was eliminated. The average size of the intervals was the tie breaker.\nThe main teaching objective here was to get students thinking about how to evaluate prediction strategies when chance is involved. To a naive observer, a biased strategy that favored democrats and correctly called, say, Virginia may look good in comparison to strategies that called it a toss-up.  However, a look at the other 34 states would reveal the weakness of this biased strategy. I wanted students to think of procedures that can help distinguish lucky guesses from strategies that universally perform well.\nOne of the concepts we discussed in class was the systematic bias of polls which we modeled as a random effect. One can’t infer this bias from polls until after the election passes. By studying previous elections students were able to estimate the SE of this random effect and incorporate it into the calculation of intervals. The realization of this random effect was very large in these elections (about +4 for the democrats) which clearly showed the importance of modeling this source of variability. Strategies that restricted standard error measures to sample estimates from this year’s polls did very poorly. The 90% credible intervals provided by 538, which I believe does incorporate this, missed 8 of the 35 races (23%). This suggests that they underestimated the variance.  Several of our students compared favorably to 538:\n\n\nname\n\n\navg bias\n\n\nMSE\n\n\navg interval size\n\n\nmissed\n\n\nManuel Andere\n\n\n-3.9\n\n\n6.9\n\n\n24.1\n\n\n3\n\n\nRichard Lopez\n\n\n-5.0\n\n\n7.4\n\n\n26.9\n\n\n3\n\n\nDaniel Sokol\n\n\n-4.5\n\n\n6.4\n\n\n24.1\n\n\n4\n\n\nIsabella Chiu\n\n\n-5.3\n\n\n9.6\n\n\n26.9\n\n\n6\n\n\nDenver Mosigisi Ogaro\n\n\n-3.2\n\n\n6.6\n\n\n18.9\n\n\n7\n\n\nYu Jiang\n\n\n-5.6\n\n\n9.6\n\n\n22.6\n\n\n7\n\n\nDavid Dowey\n\n\n-3.5\n\n\n6.2\n\n\n16.3\n\n\n8\n\n\nNate Silver\n\n\n-4.2\n\n\n6.6\n\n\n16.4\n\n\n8\n\n\nFilip Piasevoli\n\n\n-3.5\n\n\n7.4\n\n\n22.1\n\n\n8\n\n\nYapeng Lu\n\n\n-6.5\n\n\n8.2\n\n\n16.5\n\n\n10\n\n\nDavid Jacob Lieb\n\n\n-3.7\n\n\n7.2\n\n\n17.1\n\n\n10\n\n\nVincent Nguyen\n\n\n-3.8\n\n\n5.9\n\n\n11.1\n\n\n14\n\n\nIt is important to note that 538 would have probably increased their interval size had they actively participated in a competition requiring 95% of the intervals to cover. But all in all, students did very well. The majority correctly predicted the republican take over. The median mean square error across all 49 participantes was 8.2 which was not much worse that 538’s 6.6. Other example of strategies that I think helped some of these students perform well was the use of creative weighting schemes (based on previous elections) to average poll and the use of splines to estimate trends, which in this particular election were going in the republican’s favor.\nHere are some plots showing results from two of our top performers:\n \nI hope this exercise helped students realize that data science can be both fun and useful. I can’t wait to do this again in 2016.\n \n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-11-10-sunday-datastatistics-link-roundup-11914/",
    "title": "Sunday data/statistics link roundup (11/9/14)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-11-10",
    "categories": [],
    "contents": "\nSo I’m a day late, but you know, I got a new kid and stuff…\nThe New Yorker hating on MOOCs, they mention all the usual stuff. Including the really poorly designed San Jose State experiment. I think this deserves a longer post, but this is definitely a case where people are looking at MOOCs on the wrong part of the hype curve. MOOCs won’t solve all possible education problems, but they are hugely helpful to many people and writing them off is a little silly (via Rafa).\nMy colleague Dan S. is teaching a missing data workshop here at Hopkins next week (via Dan S.)\nA couple of cool Youtube videos explaining how the normal distribution sounds and the pareto principle with paperclips (via Presh T., pair with the 80/20 rule of statistical methods development)\nIf you aren’t following Research Wahlberg, you aren’t on academic twitter.\nI followed  #biodata14  closely. I think having a meeting on Biological Big Data is a great idea and many of the discussion leaders are people I admire a ton. I also am a big fan of Mike S. I have to say I was pretty bummed that more statisticians weren’t invited (we like to party too!).\nOur data science specialization generates almost 1,000 new R github repos a month! Roger and I are in a neck and neck race to be the person who has taught the most people statistics/data science in the history of the world.\nThe Rstudio guys have also put together what looks like a great course similar in spirit to our Data Science Specialization. The Rstudio folks have been *super* supportive of the DSS and we assume anything they make will be awesome.\nCongrats to Data Carpentry and [So I’m a day late, but you know, I got a new kid and stuff…\nThe New Yorker hating on MOOCs, they mention all the usual stuff. Including the really poorly designed San Jose State experiment. I think this deserves a longer post, but this is definitely a case where people are looking at MOOCs on the wrong part of the hype curve. MOOCs won’t solve all possible education problems, but they are hugely helpful to many people and writing them off is a little silly (via Rafa).\nMy colleague Dan S. is teaching a missing data workshop here at Hopkins next week (via Dan S.)\nA couple of cool Youtube videos explaining how the normal distribution sounds and the pareto principle with paperclips (via Presh T., pair with the 80/20 rule of statistical methods development)\nIf you aren’t following Research Wahlberg, you aren’t on academic twitter.\nI followed  #biodata14  closely. I think having a meeting on Biological Big Data is a great idea and many of the discussion leaders are people I admire a ton. I also am a big fan of Mike S. I have to say I was pretty bummed that more statisticians weren’t invited (we like to party too!).\nOur data science specialization generates almost 1,000 new R github repos a month! Roger and I are in a neck and neck race to be the person who has taught the most people statistics/data science in the history of the world.\nThe Rstudio guys have also put together what looks like a great course similar in spirit to our Data Science Specialization. The Rstudio folks have been *super* supportive of the DSS and we assume anything they make will be awesome.\nCongrats to Data Carpentry and](https://twitter.com/tracykteal) on their funding from the Moore Foundation!\n\n\nSup. Party’s over. Keep moving. pic.twitter.com/R8sTbKzpF8\n\n\n— Research Wahlberg (@ResearchMark) November 5, 2014\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-11-05-time-varying-causality-in-n1-experiments-with-applications-to-newborn-care/",
    "title": "Time varying causality in n=1 experiments with applications to newborn care",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-11-05",
    "categories": [],
    "contents": "\nWe just had our second son about a week ago and I’ve been hanging out at home with him and the rest of my family. It has reminded me of a few things from when we had our first son. First, newborns are tiny and super-duper adorable. Second, daylight savings time means gaining an extra hour of sleep for many people, but for people with young children it is more like (via Reddit):\n\n \nThird, taking care of a newborn is like performing a series of n=1 experiments where the causal structure of the problem changes every time you perform an experiment.\nSuppose, hypothetically, that your newborn has just had something to eat and it is 2am in the morning (again, just hypothetically). You are hoping he’ll go back down to sleep so you can catch some shut-eye yourself. But your baby just can’t sleep and seems uncomfortable. Here are a partial list of causes for this: (1) dirty diaper, (2) needs to burp, (3) still hungry, (4) not tired, (5) over tired, (6) has gas, (7) just chillin. So you start going down the list and trying to address each of the potential causes of late-night sleeplessness: (1) check diaper, (2) try burping, (3) feed him again, etc. etc. Then, miraculously, one works and the little guy falls asleep.\nIt is interesting how the natural human reaction  to this is to reorder the potential causes of sleeplessness and start with the thing that worked next time. Then often get frustrated when the same thing doesn’t work the next time. You can’t help it, you did an experiment, you have some data, you want to use it. But the reality is that the next time may have nothing to do with the first.\nI’m in the process of collecting some very poorly annotated data collected exclusively at night if anyone wants to write a dissertation on this problem.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-11-04-538-election-forecasts-made-simple/",
    "title": "538 election forecasts made simple",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-11-04",
    "categories": [],
    "contents": "\nNate Silver does a great job of explaining his forecast model to laypeople. However, as a statistician I’ve always wanted to know more details. After preparing a “predict the midterm elections“ homework for my data science class I have a better idea of what is going on.\nHere is my best attempt at explaining the ideas of 538 using formulas and data. And here is the R markdown.\n \n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-11-02-sunday-datastatistics-link-roundup-11214/",
    "title": "Sunday data/statistics link roundup (11/2/14)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-11-02",
    "categories": [],
    "contents": "\nBetter late than never! If you have something cool to share, please continue to email it to me with subject line “Sunday links”.\nA DrivenData is a Kaggle-like site but for social good. I like the principle of using data for societal benefit, since there are so many ways it seems to be used for nefarious purposes (via Rafa).\nThis article claiming academic science isn’t sexist has been widely panned Emily Willingham pretty much destroys it here (via Sherri R.). The thing that is interesting about this article is the way that it tries to use data to give the appearance of empiricism, while using language to try to skew the results. Is it just me or is this totally bizarre in light of the NYT also publishing this piece about academic sexual harassment at Yale?\nNoah Smith, an economist, tries to summarize the problem with “most research being wrong”. It is an interesting take, I wonder if he read Roger’s piece saying almost exactly the same thing  like the week before? He also mentions it is hard to quantify the rate of false discoveries in science, maybe he should read our paper?\nNature now requests that code sharing occur “where possible” (via Steven S.)\nGreat [Better late than never! If you have something cool to share, please continue to email it to me with subject line “Sunday links”.\nA DrivenData is a Kaggle-like site but for social good. I like the principle of using data for societal benefit, since there are so many ways it seems to be used for nefarious purposes (via Rafa).\nThis article claiming academic science isn’t sexist has been widely panned Emily Willingham pretty much destroys it here (via Sherri R.). The thing that is interesting about this article is the way that it tries to use data to give the appearance of empiricism, while using language to try to skew the results. Is it just me or is this totally bizarre in light of the NYT also publishing this piece about academic sexual harassment at Yale?\nNoah Smith, an economist, tries to summarize the problem with “most research being wrong”. It is an interesting take, I wonder if he read Roger’s piece saying almost exactly the same thing  like the week before? He also mentions it is hard to quantify the rate of false discoveries in science, maybe he should read our paper?\nNature now requests that code sharing occur “where possible” (via Steven S.)\nGreat](http://imgur.com/gallery/ZpgQz) cartoons, I particularly like the one about replication (via Steven S.).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-10-28-why-i-support-statisticians-and-their-resistance-to-hype/",
    "title": "Why I support statisticians and their resistance to hype",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-10-28",
    "categories": [],
    "contents": "\nDespite Statistics being the most mature data related discipline, statisticians have not fared well in terms of being selected for funding or leadership positions in the new initiatives brought about by the increasing interest in data. Just to give one example (Jeff and Terry Speed give many more) the White House Big Data Partners Workshop  had 19 members of which 0 were statisticians. The statistical community is clearly worried about this predicament and there is widespread consensus that we need to be better at marketing. Although I agree that only good can come from better communicating what we do, it is also important to continue doing one of the things we do best: resisting the hype and being realistic about data.\nThis week, after reading Mike Jordan’s reddit ask me anything, I was reminded of exactly how much I admire this quality in statisticians. From reading the interview one learns about instances where hype has led to confusion, how getting past this confusion helps us better understand and consequently appreciate the importance of his field. For the past 30 years, Mike Jordan has been one of the most prolific academics working in the areas that today are receiving increased attention_._ Yet, you won’t find a hyped-up press release coming out of his lab.  In fact when a journalist tried to hype up Jordan’s critique of hype, Jordan called out the author.\nAssessing the current situation with data initiatives it is hard not to conclude that hype is being rewarded. Many statisticians have come to the sad realization that by being cautious and skeptical, we may be losing out on funding possibilities and leadership roles. However, I remain very much upbeat about our discipline.  First, being skeptical and cautious has actually led to many important contributions. An important example is how randomized controlled experiments changed how medical procedures are evaluated. A more recent one is the concept of FDR, which helps control false discoveries in, for example,  high-throughput experiments. Second, many of us continue to work in the interface with real world applications placing us in a good position to make relevant contributions. Third, despite the failures alluded to above, we continue to successfully find ways to fund our work. Although resisting the hype has cost us in the short term, we will continue to produce methods that will be useful in the long term, as we have been doing for decades. Our methods will still be used when today’s hyped up press releases are long forgotten.\n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-10-26-return-of-the-sunday-links-102614/",
    "title": "Return of the sunday links! (10/26/14)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-10-26",
    "categories": [],
    "contents": "\nNew look for the blog and bringing back the links. If you have something that you’d like included in the Sunday links, email me and let me know. If you use the title of the message “Sunday Links” you’ll be more likely for me to find it when I search my gmail.\nThomas L. does a more technical post on semi-parametric efficiency, normally I’m a data n’ applications guy, but I love these in depth posts, especially when the papers remind me of all the things I studied at my alma mater.\nI am one of those people who only knows a tiny bit about Docker, but hears about it all the time. That being said, after I read about Rocker, I got pretty excited.\nHadley W.’s favorite tools, seems like that dude likes R Studio for some reason….(me too)\nA cool visualization of chess piece survival rates.\nA short movie by 538 about statistics and the battle between Deep Blue and Gary Kasparov. Where’s the popcorn?\nTwitter engineering released an R package for detecting outbreaks. I wonder how circular binary segmentation would do?\n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:15:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-10-24-an-interactive-visualization-to-teach-about-the-curse-of-dimensionality/",
    "title": "An interactive visualization to teach about the curse of dimensionality",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-10-24",
    "categories": [],
    "contents": "\nI recently was contacted for an interview about the curse of dimensionality. During the course of the conversation, I realized how hard it is to explain the curse to a general audience. One of the best descriptions I could come up with was trying to describe sampling from a unit line, square, cube, etc. and taking samples with side length fixed. You would capture fewer and fewer points. As I was saying this, I realized it is a pretty bad way to explain the curse of dimensionality in words. But there was potentially a cool data visualization that would illustrate the idea. I went to my student Prasad, our resident interactive viz design expert to see if he could build it for me. He came up with this cool Shiny app where you can simulate a number of points (n) and then fix a side length for 1-D, 2-D, 3-D, and 4-D and see how many points you capture in a cube of that length in that dimension. You can find the full app here or check it out on the blog here:\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-10-22-vote-on-simply-statistics-new-logo-design/",
    "title": "Vote on simply statistics new logo design",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-10-22",
    "categories": [],
    "contents": "\nAs you can tell, we have given the Simply Stats blog a little style update. It should be more readable on phones or tablets now. We are also about to get a new logo. We are down to the last couple of choices and can’t decide. Since we are statisticians, we thought we’d collect some data. Here is the link to the poll. Let us know\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-10-20-thinking-like-a-statistician-dont-judge-a-society-by-its-internet-comments/",
    "title": "Thinking like a statistician: don't judge a society by its internet comments",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-10-20",
    "categories": [],
    "contents": "\nIn a previous post I explained how thinking like a statistician can help you avoid  feeling sad after using Facebook. The basic point was that missing not at random (MNAR) data on your friends’ profiles (showing only the best parts of their life) can result in the biased view that your life is boring and uninspiring in comparison. A similar argument can be made to avoid  losing faith in humanity after reading internet comments or anonymous tweets, one of the most depressing activities that I have voluntarily engaged in.  If you want to see proof that racism, xenophobia, sexism and homophobia are still very much alive, read the unfiltered comments sections of articles related to race, immigration, gender or gay rights. However, as a statistician, I remain optimistic about our society after realizing how extremely biased these particular MNAR data can be.\nAssume we could summarize an individual’s “righteousness“ with a numerical index. I realize this is a gross oversimplification, but bear with me. Below is my view on the distribution of this index across all members of our society.\ninternet-comment-cartoonNote that the distribution is not bimodal. This means there is no gap between good and evil, instead we have a continuum. Although there is variability, and we do have some extreme outliers on both sides of the distribution, most of us are much closer to the median than we like to believe. The offending internet commentators represent a very small proportion (the “bad” tail shown in red). But in a large population, such as internet users, this extremely small proportion can be quite numerous and gives us a biased view.\nThere is one more level of variability here that introduces biases. Since internet comments can be anonymous, we get an unprecedentedly large glimpse into people’s opinions and thoughts. We assign a “righteousness” index to our thoughts and opinion and include it in the scatter plot shown in the figure above. Note that this index exhibits variability within individuals: even the best people have the occasional bad thought.  The points in red represent thoughts so awful that no one, not even the worst people, would ever express publicly. The red points give us an overly pessimistic estimate of the individuals that are posting these comments, which exacerbates our already pessimistic view due to a non-representative sample of individuals.\nI hope that thinking like a statistician will help the media and social networks put in statistical perspective the awful tweets or internet comments that represent the worst of the worst. These actually provide little to no information on humanity’s distribution of righteousness, that I think is moving consistently, albeit slowly, towards the good.\n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-10-17-bayes-rule-in-a-gif/",
    "title": "Bayes Rule in an animated gif",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-10-17",
    "categories": [],
    "contents": "\nSay Pr(A)=5% is the prevalence of a disease (% of red dots on top fig). Each individual is given a test with accuracy Pr(B|A)=Pr(no B| no A) = 90% .  The O in the middle turns into an X when the test fails. The rate of Xs is 1-Pr(B|A). We want to know the probability of having the disease if you tested positive: Pr(A|B). Many find it counterintuitive that this probability is much lower than 90%; this animated gif is meant to help.\n\nThe individual being tested is highlighted with a moving black circle. Pr(B) of these will test positive: we put these in the bottom left and the rest in the bottom right. The proportion of red points that end up in the bottom left is the proportion of red points Pr(A) with a positive test Pr(B|A), thus Pr(B|A) x Pr(A). Pr(A|B), or the proportion of reds in the bottom left, is therefore Pr(B|A) x Pr(A) divided by Pr(B):  Pr(A|B)=Pr(B|A) x Pr(A) / Pr(B)\nps - Is this a frequentist or Bayesian gif?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-10-16-creating-the-field-of-evidence-based-data-analysis-do-people-know-what-a-p-value-looks-like/",
    "title": "Creating the field of evidence based data analysis - do people know what a p-value looks like?",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-10-16",
    "categories": [],
    "contents": "\nIn the medical sciences, there is a discipline called “evidence based medicine”. The basic idea is to study the actual practice of medicine using experimental techniques. The reason is that while we may have good experimental evidence about specific medicines or practices, the global behavior and execution of medical practice may also matter. There have been some success stories from this approach and also backlash from physicians who don’t like to be told how to practice medicine. However, on the whole it is a valuable and interesting scientific exercise.\nRoger introduced the idea of evidence based data analysis in a previous post. The basic idea is to study the actual practice and behavior of data analysts to identify how analysts behave. There is a strong history of this type of research within the data visualization community starting with Bill Cleveland and extending forward to work by Diane Cook, , Jeffrey Heer, and others.\nToday we published a large-scale evidence based data analysis randomized trial. Two of the most common data analysis tasks (for better or worse) are exploratory analysis and the identification of statistically significant results. Di Cook’s group calls this idea “graphical inference” or “visual significance” and they have studied human’s ability to detect significance in the context of In the medical sciences, there is a discipline called “[evidence based medicine](http://en.wikipedia.org/wiki/Evidence-based_medicine)”. The basic idea is to study the actual practice of medicine using experimental techniques. The reason is that while we may have good experimental evidence about specific medicines or practices, the global behavior and execution of medical practice may also matter. There have been some success stories from this approach and also backlash from physicians who [don’t like to be told how to practice medicine.](http://onlinelibrary.wiley.com/doi/10.1111/j.1523-536X.1996.tb00491.x/abstract) However, on the whole it is a valuable and interesting scientific exercise. and how it associates with demographics and visual characteristics of the plot.\nWe performed a randomized study to determine if data analysts with basic training could identify statistically significant relationships. Or as the first author put it in a tweet:\n\n\nFirst paper just dropped!\nCan you tell the difference between these two plots?\nhttps://t.co/Lng0FWI0XY pic.twitter.com/zFCwwcxaAX\n\n\n— Aaron Fisher (@PrfFarnsworth) October 16, 2014\n\n\nWhat we found was that people were pretty bad at detecting statistically significant results, but that over multiple trials they could improve. This is a tentative first step toward understanding how the general practice of data analysis works. If you want to play around and see how good you are at seeing p-values we also built this interactive Shiny app. If you don’t see the app you can also go to the Shiny app page here.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-10-15-dear-laboratory-scientists-welcome-to-my-world/",
    "title": "Dear Laboratory Scientists: Welcome to My World",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-10-15",
    "categories": [],
    "contents": "\nConsider the following question: Is there a reproducibility/replication crisis in epidemiology?\nI think there are only two possible ways to answer that question:\nNo, there is no replication crisis in epidemiology because no one ever believes the result of an epidemiological study unless it has been replicated a minimum of 1,000 times in every possible population.\nYes, there is a replication crisis in epidemiology, and it started in 1854 when John Snow inferred, from observational data, that cholera was spread via contaminated water obtained from public pumps.\nIf you chose (2), then I don’t think you are allowed to call it a “crisis” because I think by definition, a crisis cannot last 160 years. In that case, it’s more of a chronic disease.\nI had an interesting conversation last week with a prominent environmental epidemiologist over the replication crisis that has been reported about extensively in the scientific and popular press. In his view, he felt this was less of an issue in epidemiology because epidemiologists never really had the luxury of people (or at least fellow scientists) believing their results because of their general inability to conduct controlled experiments.\nGiven the observational nature of most environmental epidemiological studies, it’s generally accepted in the community that no single study can be considered causal, and that many replications of a finding are need to establish a causal connection. Even the popular press knows now to include the phrase “correlation does not equal causation” when reporting on an observational study. The work of Sir Austin Bradford Hill essentially codifies the standard of evidence needed to draw causal conclusions from observational studies.\nSo if “correlation does not equal causation”, it begs the question, what does equal causation? Many would argue that a controlled experiment, whether it’s a randomized trial or a laboratory experiment, equals causation. But people who work in this area have long known that while controlled experiments do assign the treatment or exposure, there are still many other elements of the experiment that are _not _controlled.\nFor example, if subjects drop out of a randomized trial, you now essentially have an observational study (or at least a “broken” randomized trial). If you are conducting a laboratory experiment and all of the treatment samples are measured with one technology and all of the control samples are measured with a different technology (perhaps because of a lack of blinding), then you still have confounding.\nThe correct statement is not “correlation does not equal causation” but rather “no single study equals causation”, regardless of whether it was an observational study or a controlled experiment. Of course, a very tightly controlled and rigorously conducted controlled experiment will be more valuable than a similarly conducted observational study. But in general, all studies should simply be considered as further evidence for or against an hypothesis. We should not be lulled into thinking that any single study about an important question can truly be definitive.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-10-13-as-an-applied-statistician-i-find-the-frequentists-versus-bayesians-debate-completely-inconsequential/",
    "title": "I declare the Bayesian vs. Frequentist debate over for data scientists",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-10-13",
    "categories": [],
    "contents": "\nIn a recent New York Times article the “Frequentists versus Bayesians” debate was brought up once again. I agree with Roger:\n\n\nNYT wants to create a battle b/w Bayesians and Frequentists but it’s all crap. Statisticians develop techniques. http://t.co/736gbqZGuq\n\n\n— Roger D. Peng (@rdpeng) September 30, 2014\n\n\nBecause the real story (or non-story) is way too boring to sell newspapers, the author resorted to a sensationalist narrative that went something like this:  “Evil and/or stupid frequentists were ready to let a fisherman die; the persecuted Bayesian heroes saved him.” This piece adds to the growing number of writings blaming frequentist statistics for the so-called reproducibility crisis in science. If there is something Roger, Jeff and I agree on is that this debate is not constructive. As Rob Kass suggests it’s time to move on to pragmatism. Here I follow up Jeff’s recent post by sharing related thoughts brought about by two decades of practicing applied statistics and hope it helps put this unhelpful debate to rest.\nApplied statisticians help answer questions with data. How should I design a roulette so my casino makes $? Does this fertilizer increase crop yield? Does streptomycin cure pulmonary tuberculosis? Does smoking cause cancer? What movie would would this user enjoy? Which baseball player should the Red Sox give a contract to? Should this patient receive chemotherapy? Our involvement typically means analyzing data and designing experiments. To do this we use a variety of techniques that have been successfully applied in the past and that we have mathematically shown to have desirable properties. Some of these tools are frequentist, some of them are Bayesian, some could be argued to be both, and some don’t even use probability. The Casino will do just fine with frequentist statistics, while the baseball team might want to apply a Bayesian approach to avoid overpaying for players that have simply been lucky.\nIt is also important to remember that good applied statisticians also think. They don’t apply techniques blindly or religiously. If applied statisticians, regardless of their philosophical bent, are asked if the sun just exploded, they would not design an experiment as the one depicted in this popular XKCD cartoon.\n\nOnly someone that does not know how to think like a statistician would act like the frequentists in the cartoon. Unfortunately we do have such people analyzing data. But their choice of technique is not the problem, it’s their lack of critical thinking. However, even the most frequentist-appearing applied statistician understands Bayes rule and will adapt the Bayesian approach when appropriate. In the above XCKD example, any respectful applied statistician would not even bother examining the data (the dice roll), because they would assign a probability of 0 to the sun exploding (the empirical prior based on the fact that they are alive). However, superficial propositions arguing for wider adoption of Bayesian methods fail to realize that using these techniques in an actual data analysis project is very different from simply thinking like a Bayesian. To do this we have to represent our intuition or prior knowledge (or whatever you want to call it) with mathematical formulae. When theoretical Bayesians pick these priors, they mainly have mathematical/computational considerations in mind. In practice we can’t afford this luxury: a bad prior will render the analysis useless regardless of its convenient mathematically properties.\nDespite these challenges, applied statisticians regularly use Bayesian techniques successfully. In one of the fields I work in, Genomics, empirical Bayes techniques are widely used. In this popular application of empirical Bayes we use data from all genes to improve the precision of estimates obtained for specific genes. However, the most widely used output of the software implementation is not a posterior probability. Instead, an empirical Bayes technique is used to improve the estimate of the standard error used in a good ol’ fashioned t-test. This idea has changed the way thousands of Biologists search for differential expressed genes and is, in my opinion, one of the most important contributions of Statistics to Genomics. Is this approach frequentist? Bayesian? To this applied statistician it doesn’t really matter.\nFor those arguing that simply switching to a Bayesian philosophy will improve the current state of affairs, let’s consider the smoking and cancer example. Today there is wide agreement that smoking causes lung cancer. Without a clear deductive biochemical/physiological argument and without the possibility of a randomized trial, this connection was established with a series of observational studies. Most, if not all, of the associated data analyses were based on frequentist techniques. None of the reported confidence intervals on their own established the consensus. Instead, as usually happens in science, a long series of studies supporting this conclusion were needed. How exactly would this have been different with a strictly Bayesian approach? Would a single paper been enough? Would using priors helped given the “expert knowledge” at the time (see below)?\n\nAnd how would the Bayesian analysis performed by tabacco companies shape the debate? Ultimately, I think applied statisticians would have made an equally convincing case against smoking with Bayesian posteriors as opposed to frequentist confidence intervals. Going forward I hope applied statisticians continue to be free to use whatever techniques they see fit and that critical thinking about data continues to be what distinguishes us. Imposing Bayesian or frequentists philosophy on us would be a disaster.\n\n\n\n",
    "preview": "http://cdn.saveourbones.com/wp-content/uploads/smoking_doctor.jpg",
    "last_modified": "2021-11-11T14:14:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-10-09-data-science-cant-be-point-and-click/",
    "title": "Data science can't be point and click",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-10-09",
    "categories": [],
    "contents": "\nAs data becomes cheaper and cheaper there are more people that want to be able to analyze and interpret that data.  I see more and more that people are creating tools to accommodate folks who aren’t trained but who still want to look at data _right now. _While I admire the principle of this approach - we need to democratize access to data - I think it is the most dangerous way to solve the problem.\nThe reason is that, especially with big data, it is very easy to find things like this with point and click tools:\n\n\n\nUS spending on science, space, and technology correlates with Suicides by hanging, strangulation and suffocation (http://www.tylervigen.com/view_correlation?id=1597)\n\n\nThe danger with using point and click tools is that it is very hard to automate the identification of warning signs that seasoned analysts get when they have their hands in the data. These may be spurious correlation like the plot above or issues with data quality, or missing confounders, or implausible results. These things are much easier to spot when analysis is being done interactively. Point and click software is also getting better about reproducibility, but it still a major problem for many interfaces.\nDespite these issues, point and click software are still all the rage. I understand the sentiment, there is a bunch of data just laying there and there aren’t enough people to analyze it expertly. But you wouldn’t want me to operate on you using point and click surgery software. You’d want a surgeon who has practiced on real people and knows what to do when she has an artery in her hand. In the same way, I think point and click software allows untrained people to do awful things to big data.\nThe ways to solve this problem are:\nMore data analysis training\nEncouraging people to do their analysis interactively\nI have a few more tips which I have summarized in this talk on things statistics taught us about big data.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-10-08-the-leek-group-guide-to-genomics-papers/",
    "title": "The Leek group guide to genomics papers",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-10-08",
    "categories": [],
    "contents": "\nLeek group guide to genomics papers\nWhen I was a student, my advisor, John Storey, made a list of papers for me to read on nights and weekends. That list was incredibly helpful for a couple of reasons.\n\nIt got me caught up on the field of computational genomics\n\n\nIt was expertly curated, so it filtered a lot of papers I didn’t need to read\n\n\nIt gave me my first set of ideas to try to pursue as I was reading the papers\n\nI have often thought I should make a similar list for folks who may want to work wtih me (or who want to learn about statistial genomics). So this is my first attempt at that list. I’ve tried to separate the papers into categories and I’ve probably missed important papers. I’m happy to take suggestions for the list, but this is primarily designed for people in my group so I might be a little bit parsimonious.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-10-06-an-economic-model-for-peer-review/",
    "title": "An economic model for peer review",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-10-06",
    "categories": [],
    "contents": "\nI saw this tweet the other day:\n\n\nHas anyone applied game theory to the issue of anonymous peer review in academia?\n\n\n— Mick Watson (@BioMickWatson) October 2, 2014\n\n\nIt reminded me that a few years ago I had a paper that went through the peer review wringer. It drove me completely bananas. One thing that drove me so crazy about the process was how long the referees waited before reviewing and how terrible the reviews were after that long wait. So I started thinking about the “economics of peer review”. Basically, what is the incentive for scientists to contribute to the system.\nTo get a handle on this idea, I designed a “peer review game” where there are a fixed number of players N. The players play the game for a fixed period of time. During that time, they can submit papers or they can review papers. For each person, their final score at the end of the time is .\nBased on this model, under closed peer review, there is one Nash equilibrium under the strategy that no one reviews any papers. Basically, no one can hope to improve their score by reviewing, they can only hope to improve their score by submitting more papers (sound familiar?). Under open peer review, there are more potential equilibria, based on the relative amount of goodwill you earn from your fellow reviewers by submitting good reviews.\nWe then built a model system for testing out our theory. The system involved having groups of students play a “peer review game” where they submitted solutions to SAT problems like:\n\nEach solution was then randomly assigned to another player to review. Those players could (a) review it and reject it, (b) review it and accept it, or (c) not review it. The person with the most points at the end of the time (one hour) won.\nWe found some cool things:\nIn closed review, reviewing gave no benefit.\nIn open review, reviewing gave a small positive benefit.\nBoth systems gave comparable accuracy\nAll peer review increased the overall accuracy of responses\nThe paper is here and all of the data and code are here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-10-02-the-drake-index-for-academics/",
    "title": "The Drake index for academics",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-10-02",
    "categories": [],
    "contents": "\nI think academic indices are pretty silly; maybe we should introduce so many academic indices that people can’t even remember which one is which. There are pretty serious flaws with both citation indices and social media indices that I think render them pretty meaningless in a lot of ways.\nRegardless of these obvious flaws I want in the game. Instead of the K-index for academics I propose the Drake index. Drake has achieved both critical and popular success. His song “Honorable Mentions” from the ESPYs (especially the first verse) reminds me of the motivation of the K-index paper.\nTo quantify both the critical and popular success of a scientist, I propose the Drake Index (TM). The Drake Index is defined as follows\n\n(# Twitter Followers)/(Max Twitter Followers by a Person in your Field) + (#Citations)/(Max Citations by a Person in your Field)\n\nLet’s break the index down. There are two main components (Twitter followers and Citations) measuring popular and critical acclaim. But they are measured on different scales. So we attempt to normalize them to the maximum in their field so the indices will both be between 0 and 1. This means that your Drake index score is between 0 and 2. Let’s look at a few examples to see how the index works.\nDrake  = (16.9M followers)/(55.5 M followers for Justin Bieber) + (0 citations)/(134 Citations for Natalie Portman) = 0.30\nRafael Irizarry = (1.1K followers)/(17.6K followers for Simply Stats) + (38,194 citations)/(185,740 citations for Doug Altman) = 0.27\nRoger Peng - (4.5K followers)/(17.6K followers for Simply Stats) + (4,011 citations)/(185,740 citations for Doug Altman) = 0.27\nJeff Leek - (2.6K followers)/(17.6K followers for Simply + (2,348 citations)/(185,740 citations for Doug Altman) = 0.16\nIn the interest of this not being taken any seriously than an afternoon blogpost should be I won’t calculate any other people’s Drake index. But you can :-).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-09-30-you-think-p-values-are-bad-i-say-show-me-the-data/",
    "title": "You think P-values are bad? I say show me the data.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-09-30",
    "categories": [],
    "contents": "\n\n\n<div class=\"column\">\n  <p>\n    Both the scientific community and the popular press are freaking out about reproducibility right now. I think they have good reason to, because even the US Congress is now <a href=\"http://web.stanford.edu/~vcs/talks/Testimony-STODDEN.pdf\">investigating the transparency of science<\/a>. It has been driven by the very public reproducibility disasters in <a href=\"http://simplystatistics.org/2012/02/27/the-duke-saga-starter-set/\">genomics<\/a> and <a href=\"http://simplystatistics.org/2013/04/19/podcast-7-reinhart-rogoff-reproducibility/\">economics<\/a>.\n  <\/p>\n  \n  <p>\n    There are three major components to a reproducible and replicable study from a computational perspective: (1) the raw data from the experiment must be available, (2) the statistical code and documentation to reproduce the analysis must be available and (3) a correct data analysis must be performed.\n  <\/p>\n  \n  <p>\n    There have been successes and failures in releasing all the data, but <a href=\"http://blogs.plos.org/everyone/2014/02/24/plos-new-data-policy-public-access-data-2/\">PLoS' policy on data availability<\/a> and the <a href=\"http://www.alltrials.net/\">alltrials<\/a> initiative hold some hope. The most progress has been made on making code and documentation available. Galaxy, knitr, and iPython make it easier to distribute literate programs than it has ever been previously and people are actually using them!\n  <\/p>\n  \n  <p>\n    The trickiest part of reproducibility and replicability is ensuring that people perform a good data analysis. The first problem is that we actually don't know which statistical methods lead to higher reproducibility and replicability in users hands.  Articles like <a href=\"http://www.nytimes.com/2014/09/30/science/the-odds-continually-updated.html?_r=0\">the one that just came out in the NYT<\/a> suggest that using one type of method (Bayesian approaches) over another (p-values) will address the problem. But the real story is that those are still 100% philosophical arguments. We actually have very little good data on whether analysts will perform better analyses using one method or another.  <a href=\"http://simplystatistics.org/2014/02/14/on-the-scalability-of-statistical-procedures-why-the-p-value-bashers-just-dont-get-it/\">I agree with Roger<\/a> in his tweet storm (quick someone is wrong on the internet Roger, fix it!):\n  <\/p>\n  \n  <blockquote class=\"twitter-tweet\" width=\"550\">\n    <p>\n      5/If using Bayesian methods made you a better scientist, that would be great. But I need to see the evidence on that first.\n    <\/p>\n    \n    <p>\n      &mdash; Roger D. Peng (@rdpeng) <a href=\"https://twitter.com/rdpeng/status/516958707859857409\">September 30, 2014<\/a>\n    <\/p>\n  <\/blockquote>\n  \n  <p>\n  <\/p>\n  \n  <p>\n    This is even more of a problem because the data deluge demands that <a href=\"http://simplystatistics.org/2013/06/14/the-vast-majority-of-statistical-analysis-is-not-performed-by-statisticians/\">almost all data analysis be performed by people with basic to intermediate statistics training<\/a> at best. There is no way around this in the short term. There just aren't enough trained statisticians/data scientists to go around.  So we need to study statistics just like any other human behavior to figure out which methods work best in the hands of the people most likely to be using them.\n  <\/p>\n<\/div>\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-09-22-unbundling-the-educational-package/",
    "title": "Unbundling the educational package",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-09-22",
    "categories": [],
    "contents": "\nI just got back from the World Economic Forum’s summer meeting in Tianjin, China and there was much talk of disruption and innovation there. Basically, if you weren’t disrupting, you were furniture. Perhaps not surprisingly, one topic area that was universally considered ripe for disruption was Education.\nThere are many ideas bandied about with respect to “disrupting” education and some are interesting to consider. MOOCs were the darlings of…last year…but they’re old news now. Sam Lessin has a nice piece in the The Information (total paywall, sorry, but it’s worth it) about building a subscription model for universities. Aswath Damodaran has what I think is a nice framework for thinking about the “education business”.\nOne thing that I latched on to in Damodaran’s piece is the idea of education as a “bundled product”. Indeed, I think the key aspect of traditional on-site university education is the simultaneous offering of\nSubject matter content (i.e. course material)\nMentoring and guidance by faculty\nSocial and professional networking\nOther activities (sports, arts ensembles, etc.)\nMOOCs have attacked #1 for many subjects, typically large introductory courses. Endeavors like the Minerva project are attempting to provide lower-cost seminar-style courses (i.e. anti-MOOCs).\nI think the extent to which universities will truly be disrupted will hinge on how well we can unbundle the four (or maybe more?) elements described above and provide them separately but at roughly the same level of quality. Is it possible? I don’t know.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-09-15-applied-statisticians-people-want-to-learn-what-we-do-lets-teach-them/",
    "title": "Applied Statisticians: people want to learn what we do. Let's teach them.",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-09-15",
    "categories": [],
    "contents": "\nIn this recent opinion piece, Hadley Wickham explains how data science goes beyond Statistics and that data science is not promoted in academia. He defines data science as follows:\n\nI think there are three main steps in a data science project: you collect data (and questions), analyze it (using visualization and models), then communicate the results.\n\nand makes the important point that\n\nAny real data analysis involves data manipulation (sometimes called wrangling or munging), visualization and modelling.\n\nThe above describes what I have been doing since I became an academic applied statistician about 20 years ago. It describes what several of my colleagues do as well. For example, 15 years ago Karl Broman, in his excellent job talk, covered all the items in Hadley’s definition. The arc of the talk revolved around the scientific problem and not the statistical models. He spent a considerable amount of time describing how the data was acquired and how he used perl scripts to clean up microsatellites data.  More than half his slides contained visualizations, either illustrative cartoons or data plots. This research eventually led to his widely used “data product” R/qtl. Although not described in the talk, Karl used make to help make the results reproducible.\nSo why then does Hadley think that “Statistics research focuses on data collection and modeling, and there is little work on developing good questions, thinking about the shape of data, communicating results or building data products”?  I suspect one reason is that most applied work is published outside the flagship statistical journals. For example, Karl’s work was published in the American Journal of Human Genetics. A second reason may be that most of us academic applied statisticians don’t teach what we do. Despite writing a thesis that involved much data wrangling (reading music aiff files into Splus) and data visualization (including listening to fitted signals and residuals), the first few courses I taught as an assistant professor were almost solely on GLM theory.\nAbout five years ago I tried changing the Methods course for our PhD students from one focusing on the math behind statistical methods to a problem and data-driven course. This was not very successful as many of our students were interested in the mathematical aspects of statistics and did not like the open-ended assignments. Jeff Leek built on that class by incorporating question development, much more vague problem statements, data wrangling, and peer grading. He also found it challenging to teach the more messy parts of applied statistics. It often requires exploration and failure which can be frustrating for new students.\nThis story has a happy ending though. Last year Jeff created a data science Coursera course that enrolled over 180,000 students with 6,000+ completing. This year I am subbing for Joe Blitzstein (talk about filling in big shoes) in CS109: the Data Science undergraduate class Hanspeter Pfister and Joe created last year at Harvard. We have over 300 students registered, making it one of the largest classes on campus. I am not teaching them GLM theory.\nSo if you are an experienced applied statistician in academia, consider developing a data science class that teaches students what you do.\n \n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-09-09-a-non-comprehensive-list-of-awesome-female-data-people-on-twitter/",
    "title": "A non-comprehensive list of awesome female data people on Twitter",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-09-09",
    "categories": [],
    "contents": "\nI was just talking to a student who mentioned she didn’t know Jenny Bryan was on Twitter. She is and she is an awesome person to follow. I also realized that I hadn’t seen a good list of women on Twitter who do stats/data. So I thought I’d make one. This list is what I could make in 15 minutes based on my own feed and will, with 100% certainty, miss really people. Can you please add them in the comments and I’ll update the list?\n[@JennyBryan](https://twitter.com/JennyBryan) (Jenny Bryan) statistics professor at UBC, teaching a great intro to data science class right now.\n[@hspter](http://twitter.com/hspter) (Hilary Parker) data analyst at Etsy (former Hopkins grad student!) and co-creator (I think) of #rcatladies, also wrote this nice post on writing an R package from scratch\n[@acfrazee](https://twitter.com/acfrazee) (Alyssa Frazee) Ph.D. student at Hopkins, writes a great blog on data stuff, works on statistical genomics\n@emsweene57 (Elizabeth Sweeney) - Hopkins Ph.D. student, developer of methods for neuroimaging.\n[@hmason](https://twitter.com/hmason) (Hilary Mason) - currently running one of my favorite startups Fast Forward Labs, but basically needs no introduction, one of the biggest names in data science right now.\n[@sherrirose](https://twitter.com/sherrirose) (Sherri Rose) - former Hopkins postdoc, now at Harvard. Literally wrote the book on targeted learning.\n[@eloyan_ani](https://twitter.com/eloyan_ani) (Ani Eloyan) - Hopkins Biostat faculty, working on neuroimaging and EMRs. Lead the team that won the ADHD-200 competition.\n[@mrogati](https://twitter.com/mrogati) (Monica Rogati) - Former Linkedin data scientist, now running the data team at Jawbone.\n[@annmariastat](https://twitter.com/annmariastat) (AnnMaria De Mars) - runs the Julia group, also world class judoka, writes one of my favorite stats/education blogs. \n[@kara_woo](https://twitter.com/kara_woo) (Kara Woo) - Works at the National Center for Ecological Analysis and Synthesis and maintains their projections blog\n@jhubiostat (Betsy Ogburn) - Hopkins biostat faculty, not technically her account. But she is the reason this is the funniest/best academic department twitter account out there.\n@lovestats  (Annie Pettit) - Does surveys and data quality/MRX work. If you are into MRX, check out her blog.\n@ProfEmilyOster (Emily Oster) - Econ professor at U Chicago. Has been my favorite writer for FiveThirtyEight since their relaunch.\n@monachalabi (Mona Chalabi) - writer for FiveThirtyEight, I like her “Am I normal” series of posts.\n@lisaczhang  (Lisa Zhang)- cofounder of Polychart.\n[@notawful](https://twitter.com/notawful) (Jessica Hartnett) - professor at Gannon University, writes a great blog on teaching statistics.\n@AliciaOshlack (Alicia Oshlack) - researcher at Murdoch Children’s research institute, one of the real superstars in computational genomics.\n[@AmeliaMN](https://twitter.com/AmeliaMN) (Amelia McNamara) - graduate student at UCLA, works on the Mobilize project and other awesome data education initiatives in LA school system.\n [@leighadlr](https://twitter.com/leighadlr) (LEIGH ARINO DE LA RUBIA) Editor in chief of DataScience.LA\n[@inesgn](https://twitter.com/inesgn) (Ines Germendia) - data scientist working on official statistics at Basque Statistics - Eustat\n[@sgrifter](https://twitter.com/sgrifter)  (Sandy Griffith) - Biostat Ph.D., fellow #rcatladies creator, professor at the Cleveland Clinic in quantitative medicine\n[@ladamic](https://twitter.com/ladamic) (Lada Adamic) - professor at Michigan, teacher of really highly regarded social network analysis class on Coursera, now at Facebook (I think)\n[@stephaniehicks](https://twitter.com/stephaniehicks) - (Stephanie Hicks) postdoc in compbio at Harvard, lead teaching assistant for Data Science course at Harvard.\n[@ansate](https://twitter.com/ansate) - (Melissa Santos) manager of Hadoop infrastructure at Etsy, maintainer of the women in data list below.\n<@lauramclay> (Laura McClay) - professor of operations research at UW Madison, writes a blog with an amazing name: Punk Rock Operations Research.\n[@bioannie](https://twitter.com/bioannie) (Laura Hatfield) - professor at Harvard, also has one of the best data titles I’ve ever heard: Princess of Bayesia\n[@kaythaney](https://twitter.com/kaythaney) (Kaitlin Thaney)  - director of the Mozilla Science Lab, also works with Data Kind UK.\n<@laurieskelly>  (Laurie Skelly)- Data scientist at Data Scope analytics\n[@bo_p](https://twitter.com/bo_p) (Bo Peng) - Data scientist at Data Scope analytics\n[@siminaboca](https://twitter.com/siminaboca) (Simina Boca) - former Hopkins Ph.D. student, now assistant professor at Georgetown in Biomedical informatics.\n[@HelenPowell01](https://twitter.com/HelenPowell01) (Helen Powell) - postdoc in Biostatistics at Hopkins, works on statistics for relationship between air pollution and health.\n[@victoriastodden](https://twitter.com/victoriastodden) (Victoria Stodden) - one of the leaders in the legal and sociological aspects of reproducible research.\n[@hannawallach](https://twitter.com/hannawallach) (Hanna Wallach) - CS professor and researcher at Microsoft Research NY.\n[@kralljr](https://twitter.com/kralljr) (Jenna Krall) - postdoctoral fellow in environmental statistics at Emory (Hopkins grad!)\n[@LssLi](https://twitter.com/lssli) (Shanshan Li) - professor of Biostatistics at IUPI, works on neuroimaging, aging and epidemiology (Hopkins grad!)\n[@aheineike](https://twitter.com/aheineike) (Amy Heineike) - director of mathematics at Quid, also excellent interviewee.\n[@mathbabedotorg](https://twitter.com/mathbabedotorg) (Cathy O’Neil) program director of the Lede Program at Columbia’s J School, writes a very popular data science blog.\n[@ameiliashowalter](https://twitter.com/ameliashowalter) (Amelia Showalter)  Former director of digital analytics for Obama2012. Data consultant.\n[@minebocek](https://twitter.com/minebocek) (Mine Cetinkaya Rundel) Professor at Duke, teaches the great statistics MOOC from them based on OpenIntro.\n[@YennyWebbV](https://twitter.com/@YennyWebbV) (Yenny Webb Vargas) Ph.D. student in Biostatistics at Johns Hopkins, one of the founders of Bmore Biostats and a blogger\n[@OMGannaks](https://twitter.com/@OMGannaks) (Anna Smith) - former data scientist at Bitly, now analytics engineer at rentherunway.\n[@kristin_linn](https://twitter.com/@kristin_linn) (Kristin Linn) - postdoc at UPenn, formerly NC State grad student, part of the awesome statistics band (!) [@TheFifthMoment](https://twitter.com/TheFifthMoment)\n[@ledell](http://www.stat.berkeley.edu/~ledell/) (Erin LeDell) - grad student in Biostatistics at Berkeley working on machine learning, co-author of subsemble R package.\n[@atmccann](https://twitter.com/atmccann) (Allison McCann) - writer for FiveThirtyEight. Data viz person, my favorite post of hers is how to debug a jet\n[@ReginaNuzzo](https://twitter.com/@ReginaNuzzo) (Regina Nuzzo) - stats prof and freelance writer. Her piece on p-values in Nature just won the statistical reporting award.\n[@jrfAleks](https://twitter.com/jrfAleks) (Aleks Collingwood) - programme manager for the Joseph Rowntree Foundation. Working on poverty and aging.\n[@abarysh](https://twitter.com/@abarysh) (Anastasia Baryshnikova) - princeton Lewis-Sigler fellow, co-leader of major project on large international yeast knockout study.\n[@sharon000](Sharon%20Machlis) (Sharon Machlis) - online managing editor at Computerworld.\n[@2plus2make5](https://twitter.com/2plus2make5) (Emma Pierson) - Stanford undergrad, Rhodes Scholar, frequent contributor to FiveThirtyEight and other data blogs.\n[@mandyfmejia](https://twitter.com/mandyfmejia) (Mandy Mejia) - Johns Hopkins PhD student, brain imaging analyzer, also writes a great blog!\nI have also been informed that these Twitter lists are probably better than my post. But I’ll keep updating my list anyway cause I want to know who all the right people to follow are!\n\nhttps://twitter.com/ansate/lists/women-in-data\n\n\nhttps://twitter.com/BecomingDataSci/lists/women-in-data-science\n\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-09-04-why-the-three-biggest-positive-contributions-to-reproducible-research-are-the-ipython-notebook-knitr-and-galaxy/",
    "title": "Why the three biggest positive contributions to reproducible research are the iPython Notebook, knitr, and Galaxy",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-09-04",
    "categories": [],
    "contents": "\nThere is a huge amount of interest in reproducible research and replication of results. Part of this is driven by some of the pretty major mistakes in reproducibility we have seen in economics and genomics. This has spurred discussion at a variety of levels including at the level of the United States Congress.\nTo solve this problem we need the appropriate infrastructure. I think developing infrastructure is a lot like playing the lottery, only if the lottery required a lot more work to buy a ticket. You pour a huge amount of effort into building good infrastructure.  I think it helps if you build it for yourself like Yihui did for knitr:\n(also make sure you go read the blog post over at Data Science LA)\nIf lots of people adopt it, you are set for life. If they don’t, you did all that work for nothing. So you have to applaud all the groups who have made efforts at building infrastructure for reproducible research.\nI would contend that the largest positive contributions to reproducibility in sheer number of analyses made reproducible are:\n The knitr R package (or more recently rmarkdown) for creating literate webpages and documents in R.\niPython notebooks  for creating literate webpages and documents interactively in Python.\nThe Galaxy project for creating reproducible work flows (among other things) combining known tools.\nThere are similarities and differences between the different platforms but the one thing I think they all have in common is that they added either no or negligible effort to people’s data analytic workflows.\nknitr and iPython notebooks have primarily increased reproducibility among folks who have some scripting experience. I think a major reason they are so popular is because you just write code like you normally would, but embed it in a simple to use document. The workflow doesn’t change much for the analyst because they were going to write that code anyway. The document just allows it to be built into a more shareable document.\nGalaxy has increased reproducibility for many folks, but my impression is the primary user base are folks who have less experience scripting. They have worked hard to make it possible for these folks to analyze data they couldn’t before in a reproducible way. But the reproducibility is incidental in some sense. The main reason users come is that they would have had to stitch those pipelines together anyway. Now they have an easier way to do it (lowering workload) and they get reproducibility as a bonus.\nIf I was in charge of picking the next round of infrastructure projects that are likely to impact reproducibility or science in a positive way, I would definitely look for projects that have certain properties.\nFor scripters and experts I would look for projects that interface with what people are already doing (most data analysis is in R or Python these days), require almost no extra work, and provide some benefit (reproducibility or otherwise). I would also look for things that are agnostic to which packages/approaches people are using.\nFor non-experts I would look for projects that enable people to build pipelines  they were’t able to before using already standard tools and give them things like reproducibility for free.\nOf course I wouldn’t put me in charge anyway, I’ve never won the lottery with any infrastructure I’ve tried to build.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-08-20-a-very-brief-review-of-published-human-subjects-research-conducted-with-social-media-companies/",
    "title": "A (very) brief review of published human subjects research conducted with social media companies",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-08-20",
    "categories": [],
    "contents": "\nAs I wrote the other day, more and more human subjects research is being performed by large tech companies. The best way to handle the ethical issues raised by this research is still unclear. The first step is to get some idea of what has already been published from these organizations. So here is a brief review of the papers I know about where human subjects experiments have been conducted by companies. I’m only counting experiments here that have (a) been published in the literature and (b) involved experiments on users. I realized I could come up with surprisingly few.  I’d be interested to see more in the comments if people know about them.\nPaper: Experimental evidence of massive-scale emotional contagion through social networks\nCompany: Facebook\nWhat they did: Randomized people to get different emotions in their news feed and observed if they showed an emotional reaction.\nWhat they found: That there was almost no real effect on emotion. The effect was statistically significant but not scientifically or emotionally meaningful.\nPaper: Social influence bias: a randomized experiment\nCompany: Not stated but sounds like Reddit\nWhat they did: Randomly up-voted, down voted, or left alone posts to the social networking site. Then they observed whether there was a difference in the overall rating of posts within each treatment.\nWhat they found: Posts that were upvoted ended up with a final rating score (total upvotes - total downvotes) that was 25% higher.\nPaper: Identifying influential and susceptible members of social networks \nCompany: Facebook\nWhat they did: Using a commercial Facebook app,  they found users who adopted a product and randomized sending messages to their friends about the use of the product. Then they measured whether their friends decided to adopt the product as well.\nWhat they found: Many interesting things. For example: susceptibility to influence decreases with age, people over 31 are stronger influencers, women are less susceptible to influence than men, etc. etc.\n \nPaper: Inferring causal impact using Bayesian structural time-series models\nCompany: Google\nWhat they did: They developed methods for inferring the causal impact of an ad in a time series situation. They used data from an advertiser who showed ads to people related to keywords and measured how many visits there were to the advertiser’s website through paid and organic (non-paid) clicks.\nWhat they found: That the ads worked. But more importantly that they could predict the causal effect of the ad using their methods.\n \n \n \n \n \n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-08-19-swiftkey-and-johns-hopkins-partner-for-data-science-specialization-capstone/",
    "title": "SwiftKey and Johns Hopkins partner for Data Science Specialization Capstone",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-08-19",
    "categories": [],
    "contents": "\nI use SwiftKey on my Android phone all the time. So I was super pumped up I use [SwiftKey](http://swiftkey.com/en/) on my Android phone all the time. So I was super pumped up to run in October 2014. To enroll in the course you have to pass the other 9 courses in the Data Science Specialization.\nThe 9 courses have only been running for 4 months but already 200+ people have finished all 9! It has been unbelievable to see the response to the specialization and we are exited about taking it to the next level.\nAround the world, people are spending an increasing amount of time on their mobile devices for email, social networking, banking and a whole range of other activities. But typing on mobile devices can be a serious pain. SwiftKey, our corporate partner in this capstone, builds a smart keyboard that makes it easier for people to type on their mobile devices. One cornerstone of their smart keyboard is predictive text models. When someone types:\nI went to the\nthe keyboard presents three options for what the next word might be. For example, the three words might be gym, store, restaurant. In this capstone you will work on understanding and building predictive text models like those used by SwiftKey.\nThis course will start with the basics, analyzing a large corpus of text documents to discover the structure in the data and how words are put together. It will cover cleaning and analyzing text data, then building and sampling from a predictive text model. Finally, students will use the knowledge gained in our  Data Products course to build a predictive text product they can show off to their family, friends, and potential employers.\nWe are really excited to work with SwiftKey to take our Specialization to the next level! Here is Roger’s intro video for the course to get you fired up too.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:14:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-08-18-interview-with-copss-award-winner-martin-wainright/",
    "title": "Interview with COPSS Award winner Martin Wainwright",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-08-18",
    "categories": [],
    "contents": "\nEditor’s note: Martin Wainwright is the winner of the 2014 COPSS Award. This award is the most prestigious award in statistics, sometimes refereed to as the Nobel Prize in Statistics. Martin received the award for: “ For fundamental and groundbreaking contributions to high-dimensional statistics, graphical modeling, machine learning, optimization and algorithms, covering deep and elegant mathematical analysis as well as new methodology with wide-ranging implications for numerous applications.” He kindly agreed to be interviewed by Simply Statistics. \n\nSS: How did you find out you had received the COPSS prize?\nIt was pretty informal -– I received an email in February from\nRaymond Carroll, who chaired the committee. But it had explicit\ninstructions to keep the information private until the award ceremony\nin August.\nSS: You are in Electrical Engineering & Computer Science (EECS) and\nStatistics at Berkeley: why that mix of departments?\nJust to give a little bit of history, I did my undergraduate degree in\nmath at the University of Waterloo in Canada, and then my Ph.D. in\nEECS at MIT, before coming to Berkeley to work as a postdoc in\nStatistics. So when it came time to looking at faculty positions,\nhaving a joint position between these two departments made a lot of\nsense. Berkeley has always been at the forefront of having effective\njoint appointments of the “Statistics plus X” variety, whether X is\nEECS, Mathematics, Political Science, Computational Biology and so on.\nFor me personally, the EECS plus Statistics combination is terrific,\nas a lot of my interests lie at the boundary between these two areas,\nwhether it is investigating tradeoffs between computational and\nstatistical efficiency, connections between information theory and\nstatistics, and so on. I hope that it is also good for my students!\nIn any case, whether they enter in EECS or Statistics, they graduate\nwith a strong background in both statistical theory and methods, as\nwell as optimization, algorithms and so on. I think that this kind of\nmix is becoming increasingly relevant to the practice of modern\nstatistics, and one can certainly see that Berkeley consistently\nproduces students, whether from my own group or other people at\nBerkeley, with this kind of hybrid background.\nSS: What do you see as the relationship between statistics and machine\nlearning?\nThis is an interesting question, but tricky to answer, as it can\nreally depend on the person. In my own view, statistics is a very\nbroad and encompassing field, and in this context, machine learning\ncan be viewed as a particular subset of it, one especially focused on\nalgorithmic and computational aspects of statistics. But on the other\nhand, as things stand, machine learning has rather different cultural\nroots than statistics, certainly strongly influenced by computer\nscience. In general, I think that both groups have lessons to learn\nfrom each other. For instance, in my opinion, anyone who wants to do\nserious machine learning needs to have a solid background in\nstatistics. Statisticians have been thinking about data and\ninferential issues for a very long time now, and these fundamental\nissues remain just as important now, even though the application\ndomains and data types may be changing. On the other hand, in certain\nways, statistics is still a conservative field, perhaps not as quick\nto move into new application domains, experiment with new methods and\nso on, as people in machine learning do. So I think that\nstatisticians can benefit from the playful creativity and unorthodox\nexperimentation that one sees in some machine learning work, as well\nas the algorithmic and programming expertise that is standard in\ncomputer science.\nSS: What sorts of things is your group working on these days?\nI have fairly eclectic interests, so we are working on a range of\ntopics. A number of projects concern the interface between\ncomputation and statistics. For instance, we have a recent pre-print\n(with postdoc Sivaraman Balakrishnan and colleague Bin Yu) that tries\nto address the gap between statistical and computational guarantees in\napplications of the expectation-maximization (EM) algorithm for latent\nvariable models. In theory, we know that the global minimizer of the\n(nonconvex) likelihood has good properties, but the in practice, the\nEM algorithm only returns local optima. How to resolve this gap\nbetween existing theory and actual practice? In this paper, we show\nthat under pretty reasonable conditions-–that hold for various types\nof latent variable models-–the EM fixed points are as good as the\nglobal minima from the statistical perspective. This explains what is\nobserved a lot in practice, namely that when the EM algorithm is given\na reasonable initialization, it often returns a very good answer.\nThere are lots of other interesting questions at this\ncomputation/statistics interface. For instance, a lot of modern data\nsets (e.g., Netflix) are so large that they cannot be stored on a\nsingle machine, but must be split up into separate pieces. Any\nstatistical task must then be carried out in a distributed way, with\neach processor performing local operations on a subset of the data,\nand then passing messages to other processors that summarize the\nresults of its local computations. This leads to a lot of fascinating\nquestions. What can be said about the statistical performance of such\ndistributed methods for estimation or inference? How many bits do the\nmachines need to exchange in order for the distributed performance to\nmatch that of the centralized “oracle method” that has access to all\nthe data at once? We have addressed some of these questions in a\nrecent line of work (with student Yuchen Zhang, former student John\nDuchi and colleague Micheel Jordan).\nSo my students and postdocs are keeping me busy, and in addition, I am\nalso busy writing a couple of books, one jointly with Trevor Hastie\nand Rob Tibshirani at Stanford University on the Lasso and related\nmethods, and a second solo-authored effort, more theoretical in focus,\non high-dimensional and non-asymptotic statistics.\nSS: What role do you see statistics playing in the relationship\nbetween Big Data and Privacy?\nAnother very topical question: privacy considerations are certainly\nbecoming more and more relevant as the scale and richness of data\ncollection grows. Witness the recent controversies with the NSA, data\nmanipulation on social media sites, etc. I think that statistics\nshould have a lot to say about data and privacy. There has a long\nline of statistical work on privacy, dating back at least to Warner’s\nwork on survey sampling in the 1960s, but I anticipate seeing more of\nit over the next years. Privacy constraints bring a lot of\ninteresting statistical questions-–how to design experiments, how to\nperform inference, how should data be aggregated and what should be\nreleased and so on-–and I think that statisticians should be at the\nforefront of this discussion.\nIn fact, in some joint work with former student John Duchi and\ncolleague Michael Jordan, we have examined some tradeoffs between\nprivacy constraints and statistical utility. We adopt the framework\nof local differential privacy that has been put forth in the computer\nscience community, and study how statistical utility (in the form of\nestimation accuracy) varies as a function of the privacy level.\nObviously, preserving privacy means obscuring something, so that\nestimation accuracy goes down, but what is the quantitative form of\nthis tradeoff? An interesting consequence of our analysis is that in\ncertain settings, it identifies optimal mechanisms for preserving a\ncertain level of privacy in data.\nWhat advice would you give young statisticians getting into the\ndiscipline right now?\nIt is certainly an exciting time to be getting into the discipline.\nFor undergraduates thinking of going to graduate school in statistics,\nI would encourage them to build a strong background in basic\nmathematics (linear algebra, analysis, probability theory and so on)\nthat are all important for a deep understanding of statistical methods\nand theory. I would also suggest “getting their hands dirty”, that is\ndoing some applied work involving statistical modeling, data analysis\nand so on. Even for a person who ultimately wants to do more\ntheoretical work, having some exposure to real-world problems is\nessential. As part of this, I would suggest acquiring some knowledge\nof algorithms, optimization, and so on, all of which are essential in\ndealing with large, real-world data sets.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-08-15-crowdsourcing-resources-for-the-johns-hopkins-data-science-specialization/",
    "title": "Crowdsourcing resources for the Johns Hopkins Data Science Specialization",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-08-15",
    "categories": [],
    "contents": "\n\nSince we began offering the Johns Hopkins Data Science Specialization we’ve noticed the unbelievable passion that our students have about our courses and the generosity they show toward each other on the course forums. Many students have created quality content around the subjects we discuss, and many of these materials are so good we feel that they should be shared with all of our students. We also know there are tons of other great organizations creating material (looking at you Software Carpentry folks).\n\n\nWe’re excited to announce that we’ve created a site using GitHub Pages: http://datasciencespecialization.github.io/ to serve as a directory for content that the community has created. If you’ve created materials relating to any of the courses in the Data Science Specialization please send us a pull request and we will add a link to your content on our site. You can find out more about contributing here: https://github.com/DataScienceSpecialization/DataScienceSpecialization.github.io#contributing\n\n\nWe can’t wait to see what you’ve created and where the community can take this site!\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-08-13-swirl-and-the-little-data-scientists-predicament/",
    "title": "swirl and the little data scientist's predicament",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-08-13",
    "categories": [],
    "contents": "\n\nEditor’s note: This is a repost of “R and the little data scientist’s predicament”. A brief idea for an update is presented at the end in italics. \n\n\nI just read this fascinating post on _why, apparently a bit of a cult hero among enthusiasts of the Ruby programming language. One of the most interesting bits was The Little Coder’s Predicament, which boiled down essentially says that computer programming languages have grown too complex - so children/newbies can’t get the instant gratification when they start programming. He suggested a simplified “gateway language” that would get kids fired up about programming, because with a simple line of code or two they could make the computer do things like play some music or make a video.\n\n\nI feel like there is a similar ramp up with data scientists. To be able to do anything cool/inspiring with data you need to know (a) a little statistics, (b) a little bit about a programming language, and (c) quite a bit about syntax.\n\n\nWouldn’t it be cool if there was an R package that solved the little data scientist’s predicament? The package would have to have at least some of these properties:\n\n\nIt would have to be easy to load data sets, one line of not complicated code. You could write an interface for RCurl/read.table/download.file for a defined set of APIs/data sets so the command would be something like: load(“education-data”) and it would load a bunch of data on education. It would handle all the messiness of scraping the web, formatting data, etc. in the background.\n\n\nIt would have to have a lot of really easy visualization functions. Right now, if you want to make pretty plots with ggplot(), plot(), etc. in R, you need to know all the syntax for pch, cex, col, etc. The plotting function should handle all this behind the scenes and make super pretty pictures.\n\n\nIt would be awesome if the functions would include some sort of dynamic graphics (withsvgAnnotation or a wrapper for D3.js). Again, the syntax would have to be really accessible/not too much to learn.\n\n\nThat alone would be a huge start. In just 2 lines kids could load and visualize cool data in a pretty way they could show their parents/friends.\n\n\nUpdate: Now that Nick and co. have created swirl the technology is absolutely in place to have people do something awesome quickly. You could imagine taking the airplane data and immediately having them make a plot of all the flights using ggplot. Or any number of awesome government data sets and going straight to ggvis. Solving this problem is now no longer technically a challenge, it is just a matter of someone coming up with an amazing swirl module that immediately sucks students in. This would be a really awesome project for a grad student or even an undergrad with an interest in teaching. If you do do it, you should absolutely send it our way and we’ll advertise the heck out of it!\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-08-12-the-leek-group-guide-to-giving-talks/",
    "title": "The Leek group guide to giving talks",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-08-12",
    "categories": [],
    "contents": "\nI wrote a little guide to giving talks that goes along with my I wrote a little guide to giving talks that goes along with my , R packages, and reviewing guides. I posted it to Github and would be really happy to take any feedback/pull requests that folks might have. If you send a pull request please be sure to add yourself to the contributor list.\nLeek group guide to giving talks\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-08-11-stop-saying-scientists-discover-instead-say-prof-does-team-discovers/",
    "title": "Stop saying \"Scientists discover...\" instead say, \"Prof. Doe's team discovers...\"\n",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-08-11",
    "categories": [],
    "contents": "\nI was just reading an article about data science in the WSJ. They were talking about how data scientists with just 2 years experience can earn a whole boatload of money*. I noticed a description that seemed very familiar:\n\nAt e-commerce site operator Etsy Inc., for instance, a biostatistics Ph.D. who spent years mining medical records for early signs of breast cancer now writes statistical models to figure out the terms people use when they search Etsy for a new fashion they saw on the street.\n\nThis perfectly describes the resume of a student that worked with me here at Hopkins and is now tearing it up in industry. But it made me a little bit angry that they didn’t publicize her name. Now she may have requested her name not be used, but I think it is more likely that it is a case of the standard, “Scientists discover…” (see e.g. this article or this one or this one).\nThere is always a lot of discussion about how to push people to get into STEM fields, including a ton of misguided attempts that waste time and money. But here is one way that would cost basically nothing and dramatically raise the profile of scientists in the eyes of the public: use their names when you describe their discoveries.\nThe value of this simple change could be huge. In an era of selfies, reality TV, and the power of social media, emphasizing the value that individual scientists bring could have a huge impact on STEM recruiting. That paragraph above is a lot more inspiring to potential young data scientists when rewritten:\n\nAt e-commerce site operator Etsy Inc., for instance, Dr Hilary Parker,  a biostatistics Ph.D. who spent years mining medical records for early signs of breast cancer now writes statistical models to figure out the terms people use when they search Etsy for a new fashion they saw on the street.\n\n \n \n \n \n* Incidentally, I think it is a bit overhyped. I have rarely heard of anyone making $200k-$300k with that little experience, but maybe I’m wrong? I’d be interested to hear if people really were making that kind of $$ at that stage in their careers. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-08-07-its-like-tinder-but-for-peer-review/",
    "title": "It's like Tinder, but for peer review.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-08-07",
    "categories": [],
    "contents": "\nI have an idea for an app. You input the title and authors of a preprint (maybe even the abstract). The app shows the title/authors/abstract to people who work in a similar area to you. You could estimate this based on papers they have published that have similar key words to start.\nThen you swipe left if you think the paper is interesting and right if you think it isn’t. We could then aggregate the data on how many “likes” a paper gets as a measure of how “interesting” it is. I wonder if this would be a better measure of later citations/interestingness than the opinion of a small number of editors and referees.\nThis is obviously taking my proposal of a fast statistics journal to the extreme and would provide no measure of how scientifically sound the paper was. But in an age when scientific soundness is only one part of the equation for top journals, a measure of interestingness that was available before review could be of huge value to journals.\nIf done properly, it would encourage people to publish preprints. If you posted a preprint and it was immediately “interesting” to many scientists, you could use that to convince editors to get past that stage and consider your science. More things like this could happen:\n\n\nIs this the future? “We saw with interest your preprint on @biorxivpreprint. We encourage you to submit it to [well-established journal].”\n\n\n— Leonid Kruglyak (@leonidkruglyak) May 13, 2014\n\n\nSo anyone want to build it?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-08-06-if-you-like-ab-testing-here-are-some-other-biostatistics-ideas-you-may-like/",
    "title": "If you like A/B testing here are some other Biostatistics ideas you may like",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-08-06",
    "categories": [],
    "contents": "\nWeb companies are using A/B testing and experimentation regularly now to determine which features to push for advertising or improving user experience. A/B testing is a form of randomized controlled trial that was originally employed in psychology but first adopted on a massive scale in Biostatistics. Since then a large amount of work on trials and trial design has been performed in the Biostatistics community. Some of these ideas may be useful in the same context within web companies, probably a lot of them are already being used and I just haven’t seen published examples. Here are some examples:\nSequential study designs. Here the sample size isn’t fixed in advance (an issue that I imagine is pretty hard to do with web experiments) but as the experiment goes on, the data are evaluated and a stopping rule that controls appropriate error rates is used. Here are a couple of  good (if a bit dated) review on sequential designs [1] [2].\nAdaptive study designs. These are study designs that use covariates or responses to adapt the treatment assignments of people over time. With careful design and analysis choices, you can still control the relevant error rates. Here are a couple of reviews on adaptive trial designs [1] [2]\nNoninferiority trials These are trials designed to show that one treatment is at least as good as the standard of care. They are often implemented when a good placebo group is not available, often for ethical reasons. In light of the ethical concerns for human subjects research at tech companies  this could be a useful trial design. Here is a systematic review for noninferiority trials [1]\nIt is also probably useful to read about proportional hazards models and time varying coefficients. Obviously these are just a few ideas that might be useful, but talking to a Biostatistician who works on clinical trials (not me!) would be a great way to get more information.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-08-05-do-we-need-institutional-review-boards-for-human-subjects-research-conducted-by-big-web-companies/",
    "title": "Do we need institutional review boards for human subjects research conducted by big web companies?",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-08-05",
    "categories": [],
    "contents": "\nWeb companies have been doing human subjects research for a while now. Companies like Facebook and Google have employed statisticians for almost a decade (or more) and part of the culture they have introduced is the idea of randomized experiments to identify ideas that work and that don’t. They have figured out that experimentation and statistical analysis often beat out the opinion of the highest paid person at the company for identifying features that “work”. Here “work” may mean features that cause people to read advertising, or click on ads, or match up with more people.\nThis has created a huge amount of value and definitely a big interest in the statistical community. For example, today’s session on “Statistics: The Secret Weapon of Successful Web Giants” was standing room only.\n\n\nCan’t get into the session I wanted to attend, “Statistics: The Secret Weapon of Successful Web Giants” #JSM2014 pic.twitter.com/y6KNnPfDe2\n\n\n— Hilary Parker (@hspter) August 5, 2014\n\n\nBut at the same time, these experiments have raised some issues. Recently scientists from Cornell and Facebook published a study where they experimented with the news feeds of users. This turned into a PR problem for Facebook and Cornell because people were pretty upset they were being experimented on and weren’t being told about it. This has led defenders of the study to say: (a) Facebook is doing the experiments anyway, they just published it this time, (b) in this case very little harm was done, (c) most experiments done by Facebook are designed to increase profitability, at least this experiment had a more public good focused approach, and (d) there was a small effect size so what’s the big deal?\nOK Cupid then published a very timely blog postwith the title, “We experiment on human beings!”, probably at least in part to take advantage of the press around the Facebook experiment. This post was received with less vitriol than the Facebook study, but really drove home the point that large web companies perform as much human subjects research as most universities and with little or no oversight. \nThe same situation was the way academic research used to work. Scientists used their common sense and their scientific sense to decide on what experiments to run.  Most of the time this worked fine, but then things like the Tuskegee Syphillis Study happened. These really unethical experiments led to the National Research Act of 1974 which codified rules about Web companies have been doing human subjects research for a while now. Companies like Facebook and Google have employed statisticians for almost a decade (or more) and part of the culture they have introduced is the idea of randomized experiments to identify ideas that work and that don’t. They have figured out that experimentation and statistical analysis often beat out the opinion of the highest paid person at the company for identifying features that “work”. Here “work” may mean features that cause people to [read advertising](https://www.youtube.com/watch?v=E_F5GxCwizc), or [click on ads](http://www.cnet.com/news/google-starts-placing-ads-directly-in-gmail-inboxes/), or [match up with more people](http://blog.okcupid.com/index.php/we-experiment-on-human-beings/). to oversee research conducted on human subjects, to guarantee their protection. The IRBs are designed to consider the ethical issues involved with performing research on humans to balance protection of rights with advancing science.\nFacebook, OK Cupid, and other companies are not subject to IRB approval. Yet they are performing more and more human subjects experiments. Obviously the studies described in the Facebook paper and the OK Cupid post pale in comparison to the Tuskegee study. I also know scientists at these companies and know they are ethical and really trying to do the right thing. But it raises interesting questions about oversight. Given the emotional, professional, and economic value that these websites control for individuals around the globe, it may be time to discuss whether it is time to consider the equivalent of “institutional review boards” for human subjects research conducted by companies.\nCompanies who test drugs on humans such as Merck are subject to careful oversight and regulation to prevent potential harm to patients during the discovery process. This is obviously not the optimal solution for speed - understandably a major advantage and goal of tech companies. But there are issues that deserve serious consideration. For example, I think it is no where near sufficient to claim that by signing the terms of service that people have given informed consent to be part of an experiment. That being said, they could just stop using Facebook if they don’t like that they are being experimented on.\nOur reliance on these tools for all aspects of our lives means that it isn’t easy to just tell people, “Well if you don’t like being experimented on, don’t use that tool.” You would have to give up at minimum Google, Gmail, Facebook, Twitter, and Instagram to avoid being experimented on. But you’d also have to give up using smaller sites like OK Cupid, because almost all web companies are recognizing the importance of statistics. One good place to start might be in considering new and flexible forms of consent that make it possible to opt in and out of studies in an informed way, but with enough speed and flexibility not to slowing down the innovation in tech companies.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-07-29-introducing-people-to-r-14-years-and-counting/",
    "title": "Introducing people to R: 14 years and counting",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-07-29",
    "categories": [],
    "contents": "\nI’ve been introducing people to R for quite a long time now and I’ve been doing some reflecting today on how that process has changed quite a bit over time. I first started using R around 1998–1999 I think I first started talking about R informally to my fellow classmates (and some faculty) back when I was in graduate school at UCLA. There, the department was officially using Lisp-Stat (which I loved) and only later converted its courses over to R. Through various brown-bag lunches and seminars I would talk about R, and the main selling point at the time was “It’s just like S-PLUS but it’s free!” As it turns out, S-PLUS was basically abandoned by academics and its ownership changed hands a number of times over the years (it is currently owned by TIBCO). I still talk about S-PLUS when I talk about the history of R but I’m not sure many people nowadays actually have any memories of the product.\nWhen I got to Johns Hopkins in 2003 there wasn’t really much of a modern statistical computing class, so Karl Broman, Rafa Irizarry, Brian Caffo, Ingo Ruczinski, and I got together and started what we called the “KRRIB” class, which was basically a weekly seminar where one of us talked about a computing topic of interest. I gave some of the R lectures in that class and when I asked people who had heard of R before, almost no one raised their hand. And no one had actually used it before. My approach was pretty much the same at the time, although I left out the part about S-PLUS because no one had used that either. A lot of people had experience with SAS or Stata or SPSS. A number of people had used something like Java or C/C++ before and so I often used that a reference frame. No one had ever used a functional-style of programming language like Scheme or Lisp.\nOver time, the population of students (mostly first-year graduate students) slowly shifted to the point where many of them had been introduced to R while they were undergraduates. This trend mirrored the overall trend with statistics where we are seeing more and more students do undergraduate majors in statistics (as opposed to, say, mathematics). Eventually, by 2008–2009, when I’d ask how many people had heard of or used R before, everyone raised their hand. However, even at that late date, I still felt the need to convince people that R was a “real” language that could be used for real tasks.\nR has grown a lot in recent years, and is being used in so many places now, that I think its essentially impossible for a person to keep track of everything that is going on. That’s fine, but it makes “introducing” people to R an interesting experience. Nowadays in class, students are often teaching me something new about R that I’ve never seen or heard of before (they are quite good at Googling around for themselves). I feel no need to “bring people over” to R. In fact it’s quite the opposite–people might start asking questions if I weren’t teaching R.\nEven though my approach to introducing R has evolved over time, with the topics that I emphasize or de-emphasize changing, I’ve found there are a few topics that I always  stress to people who are generally newcomers to R. For whatever reason, these topics are always new or at least a little unfamiliar.\nR is a functional-style language. Back when most people primarily saw something like C as a first programming language, it made sense to me that the functional style of programming would seem strange. I came to R from Lisp-Stat so the functional aspect was pretty natural for me. But many people seem to get tripped up over the idea of passing a function as an argument or not being able to modify the state of an object in place. Also, it sometimes takes people a while to get used to doing things like lapply() and map-reduce types of operations. Everyone still wants to write a for loop!\nR is both an interactive system and a programming language. Yes, it’s a floor wax and a dessert topping–get used to it. Most people seem expect one or the other. SAS users are wondering why you need to write 10 lines of code to do what SAS can do in one massive PROC statement. C programmers are wondering why you don’t write more for loops. C++ programmers are confused by the weird system for object orientation. In summary, no one is ever happy.\nVisualization/plotting capabilities are state-of-the-art. One of the big selling points back in the “old days” was that from the very beginning R’s plotting and graphics capabilities where far more elegant than the ASCII-art that was being produced by other statistical packages (true for S-PLUS too). I find it a bit strange that this point has largely remained true. While other statistical packages have definitely improved their output (and R certainly has some areas where it is perhaps deficient), R still holds its own quite handily against those other packages. If the community can continue to produce things like ggplot2 and rgl, I think R will remain at the forefront of data visualization.\nI’m looking forward to teaching R to people as long as people will let me, and I’m interested to see how the next generation of students will approach it (and how my approach to them will change). Overall, it’s been just an amazing experience to see the widespread adoption of R over the past decade. I’m sure the next decade will be just as amazing.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-07-25-academic-statisticians-there-is-no-shame-in-developing-statistical-solutions-that-solve-just-one-problem/",
    "title": "Academic statisticians: there is no shame in developing statistical solutions that solve just one problem",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-07-25",
    "categories": [],
    "contents": "\n\nI think that the main distinction between academic statisticians and those calling themselves data scientists is that the latter are very much willing to invest most of their time and energy into solving specific problems by analyzing specific data sets. In contrast, most academic statisticians strive to develop methods that can be very generally applied across problems and data types. There is a reason for this of course:  historically statisticians have had enormous influence by developing general theory/methods/concepts such as the p-value, maximum likelihood estimation, and linear regression. However, these types of success stories are becoming more and more rare while data scientists are becoming increasingly influential in their respective areas of applications by solving important context-specific problems. The success of Money Ball and the prediction of election results are two recent widely publicized examples.\n\n\nA survey of papers published in our flagship journals make it quite clear that context-agnostic methodology are valued much more than detailed descriptions of successful solutions to specific problems. These applied papers tend to get published in subject matter journals and do not usually receive the same weight in appointments and promotions. This culture has therefore kept most statisticians holding academic position away from collaborations that require substantial time and energy investments in understanding and attacking the specifics of the problem at hand. Below I argue that to remain relevant as a discipline we need a cultural shift.\n\n\nIt is of course understandable that to remain a discipline academic statisticians can’t devote all our effort to solving specific problems and none to trying to the generalize these solutions. It is the development of these abstractions that defines us as an academic discipline and not just a profession. However, if our involvement with real problems is too superficial, we run the risk of developing methods that solve no problem at all which will eventually render us obsolete. We need to accept that as data and problems become more complex, more time will have to be devoted to understanding the gory details.\n\nBut what should the balance be?\n\nNote that many of the giants of our discipline were very much interested in solving specific problems in genetics, agriculture, and the social sciences. In fact, many of today’s most widely-applied methods were originally inspired by insights gained by answering very specific scientific questions. I worry that the balance between application and theory has shifted too far away from applications. An unfortunate consequence is that our flagship journals, including our applied journals, are publishing too many methods seeking to solve many problems but actually solving none.  By shifting some of our efforts to solving specific problems we will get closer to the essence of modern problems and will actually inspire more successful generalizable methods.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-07-16-jan-de-leeuw-owns-the-internet/",
    "title": "Jan de Leeuw owns the Internet",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-07-16",
    "categories": [],
    "contents": "\nOne of the best things to happen on the Internet recently is that Jan de Leeuw has decided to own the Twitter/Facebook universe. If you do not already, you should be following him. Among his many accomplishments, he founded the Department of Statistics at UCLA (my alma mater), which is currently thriving. On the occasion of the Department’s 10th birthday, there was a small celebration, and I recall Don Ylvisaker mentioning that the reason they invited Jan to UCLA way back when was because he “knew everyone and knew everything”. Pretty accurate description, in my opinion.\nJan’s been tweeting quite a bit of late, but recently had this gem:\n\n\nAs long as statistics continues to emphasize assumptions, models, and inference it will remain a minor subfield of data science.\n\n\n— Jan de Leeuw (@deleeuw_jan) July 15, 2014\n\n\nfollowed by\n\n\n@j8feng @rdpeng Statistics is the applied science that constructs and studies techniques for data analysis.\n\n\n— Jan de Leeuw (@deleeuw_jan) July 15, 2014\n\n\nI’m not sure what Jan’s thinking behind the first tweet was, but I think many in statistics would consider it a “good thing” to be a minor subfield of data science. Why get involved in that messy thing called data science where people are going wild with data in an unprincipled manner?\nThis is a situation where I think there is a large disconnect between what “should be” and what “is reality”. What should be is that statistics should include the field of data science. Honestly, that would be beneficial to the field of statistics and would allow us to provide a home to many people who don’t necessarily have one (primarily, people working not he border between two fields). Nate Silver made reference to this in his keynote address to the Joint Statistical Meetings last year when he said data science was just a fancy term for statistics.\nThe reality though is the opposite. Statistics has chosen to limit itself to a few areas, such as inference, as Jan mentions, and to willfully ignore other important aspects of data science as “not statistics”. This is unfortunate, I think, because unlike many in the field of statistics, I believe data science is here to stay. The reason is because statistics has decided not to fill the spaces that have been created by the increasing complexity of modern data analysis. The needs of modern data analyses (reproducibility, computing on large datasets, data preprocessing/cleaning) didn’t fall into the usual statistics curriculum, and so they were ignored. In my view, data science is about stringing together many different tools for many different purposes into an analytic whole. Traditional statistical modeling is a part of this (often a small part), but statistical thinking plays a role in all of it.\nStatisticians should take on the challenge of data science and own it. We may not be successful in doing so, but we certainly won’t be if we don’t try.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-06-30-piketty-in-r-markdown-we-need-some-help-from-the-crowd/",
    "title": "Piketty in R markdown - we need some help from the crowd",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-06-30",
    "categories": [],
    "contents": "\nThomas Piketty’s book Capital in the 21st Century was a surprise best seller and the subject of intense scrutiny. A few weeks ago the Financial Times claimed that the analysis was riddled with errors, leading to a firestorm of discussion. A few days ago the London School of economics posted a similar call to make the data open and machine readable saying.\n\nNone of this data is explicitly open for everyone to reuse, clearly licenced and in machine-readable formats.\n\nA few friends of Simply Stats  had started on a project to translate his work from the excel files where the original analysis resides into R. The people that helped were Alyssa Frazee, Aaron Fisher, Bruce Swihart, Abhinav Nellore, Hector Corrada Bravo, John Muschelli * Hector Corrada Bravo, and me. We haven’t finished translating all chapters, so we are asking anyone who is interested to help contribute to translating the book’s technical appendices into R markdown documents. If you are interested, please send pull requests to the gh-pages branch of this Github repo.\nAs a way to entice you to participate, here is one interesting thing we found. We don’t know enough economics to know if what we are finding is “right” or not, but one interesting thing I found is that the x-axes in the excel files are really distorted. For example here is Figure 1.1 from the Excel files where the ticks on the x-axis are separated by 20, 50, 43, 37, 20, 20, and 22 years.\n\n\n\n \nHere is the same plot with an equally spaced x-axis.\n\n\n\n\n\nI’m not sure if it makes any difference but it is interesting. It sounds like on measure, the Piketty analysis was mostly reproducible and reasonable.  But having the data available in a more readily analyzable format will allow for more concrete discussion based on the data. So consider contributing to our github repo.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-06-25-privacy-as-a-function-of-sample-size/",
    "title": "Privacy as a function of sample size",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-06-25",
    "categories": [],
    "contents": "\nThe U.S. Supreme Court just made a unanimous ruling in Riley v. California making it clear that police officers must get a warrant before searching through the contents of a cell phone obtained incident to an arrest. The message was put pretty clearly in the decision:\n\n Our answer to the question of what police must do before searching a cell phone seized incident to an arrest is accordingly simple — get a warrant.\n\nBut I was more fascinated by this quote:\n\nThe sum of an individual’s private life can be reconstructed through a thousand photographs labeled with dates, locations, and descriptions; the same cannot be said of a photograph or two of loved ones tucked into a wallet.\n\nSo n = 2 is not enough to recreate a private life, but n = 2,000 (with associated annotation) is enough.  I wonder what the minimum sample size needed is to officially violate someone’s privacy. I’d be curious get Cathy O’Neil’s opinion on that question, she seems to have thought very hard about the relationship between data and privacy.\nThis is another case where I think that, to some extent, the Supreme Court made a decision on the basis of a statistical concept. Last time it was correlation, this time it is inference. As I read the opinion, part of the argument hinged on how much information do you get by searching a cell phone versus a wallet? Importantly, how much can you infer from those two sets of data?\nIf any of the Supreme’s want a primer in statistics, I’m available.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-06-24-new-book-on-implementing-reproducible-research/",
    "title": "New book on implementing reproducible research",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-06-24",
    "categories": [],
    "contents": "\nI have mentioned this in a few places but my book edited with Victoria Stodden and Fritz Leisch, Implementing Reproducible Research, has just been published by CRC Press. Although it is technically in their “R Series”, the chapters contain information on a wide variety of useful tools, not just R-related tools. [](http://simplystatistics.org/wp-content/uploads/2014/06/9781466561595.jpg)I have mentioned this in a few places but my book edited with Victoria Stodden and Fritz Leisch, [Implementing Reproducible Research](http://www.crcpress.com/product/isbn/9781466561595), has just been published by CRC Press. Although it is technically in their “R Series”, the chapters contain information on a wide variety of useful tools, not just R-related tools. \nThere is also a supplementary web site hosted through Open Science Framework that contains a lot of additional information, including the list of chapters.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-06-23-the-difference-between-data-hype-and-data-hope/",
    "title": "The difference between data hype and data hope",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-06-23",
    "categories": [],
    "contents": "\nI was reading one of my favorite stats blogs, StatsChat, where Thomas points to this article in the Atlantic and highlights this quote:\n\nDassault Systèmes is focusing on that level of granularity now, trying to simulate propagation of cholesterol in human cells and building oncological cell models. “It’s data science and modeling,” Charlès told me. “Coupling the two creates a new environment in medicine.”\n\nI think that is a perfect example of data hype. This is a cool idea and if it worked would be completely revolutionary. But the reality is we are not even close to this. In very simple model organisms we can predict very high level phenotypes some of the time with whole cell modeling. We aren’t anywhere near the resolution we’d need to model the behavior of human cells, let alone the complex genetic, epigenetic, genomic, and environmental components that likely contribute to complex diseases. It is awesome that people are thinking about the future and the fastest way to science future is usually through science fiction, but this is way overstating the power of current or even currently achievable data science.\nSo does that mean data science for improving clinical trials right now should be abandoned?\nNo.\nThere is tons of currently applicable and real world data science being done in sequential analysis,  adaptive clinical trials, and dynamic treatment regimes. These are important contributions that are impacting clinical trials _right now _and where advances can reduce costs, save patient harm, and speed the implementation of clinical trials. I think that is the hope of data science - using statistics and data to make steady, realizable improvement in the way we treat patients.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-06-18-heads-up-if-you-are-going-to-submit-to-the-journal-of-the-national-cancer-institute/",
    "title": "Heads up if you are going to submit to the Journal of the National Cancer Institute",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-06-18",
    "categories": [],
    "contents": "\nUpdate (6/19/14): The folks at JNCI and OUP have kindly confirmed that they will consider manuscripts that have been posted to preprint servers. \nI just got this email about a paper we submitted to JNCI\n\nDear Dr. Leek:\nI am sorry that we will not be able to use the above-titled manuscript. Unfortunately, the paper was published online on a site called bioRXiv, The Preprint Server for Biology, hosted by Cold Spring Harbor Lab. JNCI does not publish previously published work.\nThank you for your submission to the Journal.\n\nI have to say I’m not totally surprised, but I am a little disappointed, the future of academic publishing is definitely not evenly distributed.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-06-16-the-future-of-academic-publishing-is-here-it-just-isnt-evenly-distributed/",
    "title": "The future of academic publishing is here, it just isn't evenly distributed",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-06-16",
    "categories": [],
    "contents": "\nAcademic publishing has always been a slow process. Typically you would submit a paper for publication and then wait a few months to more than a year (statistics journals can be slow!) for a review. Then you’d revise the paper in a process that would take another couple of months, resubmit it and potentially wait another few months while this second set of reviews came back.\nLately statistics and statistical genomics have been doing more of what math does and posting papers to the arxiv or to biorxiv. I don’t know if it is just me, but using this process has led to a massive speedup in the rate that my academic work gets used/disseminated. Here are a few examples of how crazy it is out there right now.\nI started a post on giving talks on Github. It was tweeted before I even finished!\n\n\n(not a joke) If @jtleek’s new guide turns out like any of the others in the series, it will be one to bookmark https://t.co/WGKjn6MINH\n\n\n— Stephen Turner (@genetics_blog) April 1, 2014\n\n\nI really appreciate the compliment, especially coming from someone whose posts I read all the time, but it was wild to me that I hadn’t even finished the post yet (still haven’t) and it was already public.\nAnother example is that we have posted several papers on biorxiv and they all get tweeted/read. When we posted the Ballgown paper it was rapidly discussed. The day after it was posted, there were already blog posts about the paper up.\nWe also have been working on another piece of software on Github that hasn’t been published yet, but have already had multiple helpful contributions from people outside our group.\nWhile all of this is going on, we have a paper out to review that we have been waiting to hear about for multiple months. So while open science is dramatically speeding up the rate at which we disseminate our results, the speed isn’t evenly distributed.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-06-13-what-i-do-when-i-get-a-new-data-set-as-told-through-tweets/",
    "title": "What I do when I get a new data set as told through tweets",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-06-13",
    "categories": [],
    "contents": "\nHilary Mason asked a really interesting question yesterday:\n\n\nData people: What is the very first thing you do when you get your hands on a new data set?\n\n\n— Hilary Mason (@hmason) June 12, 2014\n\n\nYou should really consider reading the whole discussion here it is amazing. But it also inspired me to write a post about what I do, as told by other people on Twitter. I apologize in advance if I missed your tweet, there was way too much good stuff to get them all.\nStep 0: Figure out what I’m trying to do with the data\nAt least for me I come to a new data set in one of three ways: (1) I made it myself, (2) a  collaborator created a data set with a specific question in mind, or (3) a collaborator created a data set and just wants to explore it. In the first case and the second case I already know what the question is, although sometimes in case (2) I still spend a little more time making sure I understand the question before diving in. @visualisingdata and I think alike here:\n\n\n@hmason this will sound textbooky but I stop, look and think about “what’s it about (phenomena, activity, entity etc). Look before see.\n\n\n— Andy Kirk (@visualisingdata) June 12, 2014\n\n\n  Usually this involves figuring out what the variables mean like @_jden does:\n\n\n@hmason try to figure out what the fields mean and how it’s coded — :sandwich emoji: (@_jden) June 12, 2014\n\n\nIf I’m working with a collaborator I do what @evanthomaspaul does:\n\n\n@hmason Interview the source, if possible, to know all of the problems with the data, use limitations, caveats, etc. — Evan Thomas Paul (@evanthomaspaul) June 12, 2014\n\n\nIf the data don’t have a question yet, I usually start thinking right away about what questions can actually be answered with the data and what can’t. This prevents me from wasting a lot of time later chasing trends. @japerk does something similar:\n\n\n@hmason figure out the format & how to read it. Then ask myself, what can be learned from this data? — Jacob (@japerk) June 12, 2014\n\n\nStep 1: Learn about the elephant Unless the data is something I’ve analyzed a lot before, I usually feel like the blind men and the elephant.\n\nSo the first thing I do is fool around a bit to try to figure out what the data set “looks” like by doing things like what @jasonpbecker does looking at the types of variables I have, what the first few observations and last few observations look like.\n\n\n@hmason sapply(df, class); head(df); tail(df) — Jason Becker (@jasonpbecker) June 12, 2014\n\n\nIf it is medical/social data I usually use this to look for personally identifiable information and then do what @peteskomoroch does:\n\n\n@hmason remove PII and burn it with fire — Peter Skomoroch (@peteskomoroch) June 12, 2014\n\n\nIf the data set is really big, I usually take a carefully chosen random subsample to make it possible to do my exploration interactively like @richardclegg\n\n\n@hmason unless it is big data in which case sample then import to R and look for NAs…  — Richard G. Clegg (@richardclegg) June 12, 2014\n\n\nAfter doing that I look for weird quirks, like if there are missing values or outliers like @feralparakeet\n\n\n@hmason ALL THE DESCRIPTIVES. Well, after reviewing the codebook, of course. — Vickie Edwards (@feralparakeet) June 12, 2014\n\n\nand like @cpwalker07\n\n\n@hmason count # rows, read every column header — Chris Walker (@cpwalker07) June 12, 2014\n\n\nand like @toastandcereal\n\n\n@hmason@mispagination jot down number of rows. That way I can assess right away whether I’ve done something dumb later on. — Jessica Balsam (@toastandcereal) June 12, 2014\n\n\nand like @cld276\n\n\n@hmason run a bunch of count/groupby statements to gauge if I think it’s corrupt. — Carol Davidsen (@cld276) June 12, 2014\n\n\nand @adamlaiacano\n\n\n@hmason summary() — Adam Laiacano (@adamlaiacano) June 12, 2014\n\n\nStep 2: Clean/organize I usually use the first exploration to figure out things that need to be fixed so that I can mess around with a tidy data set. This includes fixing up missing value encoding like @chenghlee\n\n\n.@hmason Often times, “fix” various codings, esp. for missing data (e.g., mixed strings & ints for coded vals; decide if NAs, “” are equiv.) — Cheng H. Lee (@chenghlee) June 12, 2014\n\n\nor more generically like: @RubyChilds\n\n\n@hmason clean it — Ruby ˁ˚ᴥ˚ˀ (@RubyChilds) June 12, 2014\n\n\nI usually do a fair amount of this, like @the_turtle too:\n\n\n@hmason Spend the next two days swearing because nobody cleaned it. — The Turtle  (@the_turtle) June 12, 2014\n\n\nWhen I’m done I do a bunch of sanity checks and data integrity checks like @deaneckles and if things are screwed up I got back and fix them:\n\n\n@treycausey @hmason Test really boring hypotheses. Like num_mobile_comments <= num_comments. — Dean Eckles (@deaneckles) June 12, 2014\n\n\n Step 3: Plot. That. Stuff. After getting a handle with mostly text based tables and output (things that don’t require a graphics device) and cleaning things up a bit I start with plotting everything like @hspter\n\n\n@hmason usually head(data) then straight to visualization. Have been working on some “unit tests” for data as well https://t.co/6Qd3URmzpe — Hilary Parker (@hspter) June 12, 2014\n\n\nAt this stage my goal is to get the maximum amount of information about the data set in the minimal amount of time. So I do not make the graphs pretty (I think there is a distinction between exploratory and expository graphics). I do histograms and jittered one d plots to look at variables one by one like @FisherDanyel\n\n\n@TwoHeadlines@hmason After looking at a few hundred random rows? Histograms & scatterplots of columns to understand what I have. — Danyel Fisher (@FisherDanyel) June 12, 2014\n\n\nTo compare the distributions of variables I usually use overlayed density plots like @sjwhitworth\n\n\n@hmason density plot all the things!\n\n\n— Stephen Whitworth (@sjwhitworth) June 12, 2014\n\n\nI make tons of scatterplots to look at relationships between variables like @wduyck\n\n\n@hmason plot scatterplots and distributions\n\n\n— Wouter Duyck (@wduyck) June 12, 2014\n\n\nI usually color/size the dots in the scatterplots by other variables to see if I can identify any confounding relationships that might screw up analyses downstream. Then, if the data are multivariate, I do some dimension reduction to get a feel for high dimensional structure. Nobody mentioned principal components or hierarchical clustering in the Twitter conversation, but I end up using these a lot to just figure out if there are any weird multivariate dependencies I might have missed.\nStep 4: Get a quick and dirty answer to the question from Step 1\nAfter I have a feel for the data I usually try to come up with a quick and dirty answer to the question I care about. This might be a simple predictive model (I usually use 60% training, 40% test) or a really basic regression model when possible, just to see if the signal is huge, medium or subtle. I use this as a place to start when doing the rest of the analysis. I also often check this against the intuition of the person who generated the data to make sure something hasn’t gone wrong in the data set.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-06-06-the-real-reason-reproducible-research-is-important/",
    "title": "The Real Reason Reproducible Research is Important",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-06-06",
    "categories": [],
    "contents": "\nReproducible research has been on my mind a bit these days, partly because it has been in the news with the Piketty stuff, and also perhaps because I just published a book on it and I’m teaching a class on it as we speak (as well as next month and the month after…).\nHowever, as I watch and read many discussions over the role of reproducibility in science, I often feel that many people miss the point. Now, just to be clear, when I use the word “reproducibility” or say that a study is reproducible, I do not mean “independent verification” as in a separate investigator conducted an independent study and came to the same conclusion as the original study (that is what I refer to as “replication”). By using the word reproducible, I mean that the original data (and original computer code) can be analyzed (by an independent investigator) to obtain the same results of the original study. In essence, it is the notion that the data analysis can be successfully repeated_. _Reproducibility is particularly important in large computational studies where the data analysis can often play an outsized role in supporting the ultimate conclusions.\nMany people seem to conflate the ideas of reproducible and correctness, but they are not the same thing. One must always remember that a study can be reproducible and still be wrong. By “wrong”, I mean that the conclusion or claim can be wrong. If I claim that X causes Y (think “sugar causes cancer”), my data analysis might be reproducible, but my claim might ultimately be incorrect for a variety of reasons. If my claim has any value, then others will attempt to replicate it and the correctness of the claim will be determined by whether others come to similar conclusions.\nThen why is reproducibility so important? Reproducibility is important because it is the only thing that an investigator can guarantee about a study.\nContrary to what most press releases would have you believe, an investigator cannot guarantee that the claims made in a study are correct (unless they are purely descriptive). This is because in the history of science, no meaningful claim has ever been proven by a single study. (The one exception might be mathematics, whether they are literally proving things in their papers.) So reproducibility is important not because it ensures that the results are correct, but rather because it ensures transparency and gives us confidence in understanding exactly what was done.\nThese days, with the complexity of data analysis and the subtlety of many claims (particularly about complex diseases), reproducibility is pretty much the only thing we can hope for. Time will tell whether we are ultimately right or wrong about any claims, but reproducibility is something we can know right now.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:13:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-06-03-post-piketty-lessons/",
    "title": "Post-Piketty Lessons",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-06-03",
    "categories": [],
    "contents": "\nThe latest crisis in data analysis comes to us (once again) from the field of Economics. Thomas Piketty, a French economist recently published a book titled Capital in the 21st Century that has been a best-seller. I have not read the book, but based on media reports, it appears to make the claim that inequality has increased in recent years and will likely increase into the future. The book argues that this increase in inequality is driven by capitalism’s tendency to reward capital more than labor. This is my non-economist’s understanding of the book, but the details specific claims of the book are not what I want to discuss here (there is much discussion elsewhere).\nAn interesting aspect of Piketty’s work, from my perspective, is that he has made all of his data and analysis available on the web. From what I can tell, his analysis was not trivial—data were collected and merged from multiple disparate sources and adjustments were made to different data series to account for various incompatibilities. To me, this sounds like a standard data analysis, in the sense that all meaningful data analyses are complicated. As noted by Nate Silver, data do not arise from a “virgin birth”, and in any example worth discussing, much work has to be done to get the data into a state in which statistical models can be fit, or even more simply, plots can be made.\nChris Giles, a journalist for the Financial Times, recently published a column (unfortunately blocked by paywall) in which he claimed that much of the analysis that Piketty had done was flawed or incorrect. In particular, he claimed that based on his (Giles’) analysis, inequality was not growing as much over time as Piketty claimed. Among other points, Giles claims that numerous errors were made in assembling the data and in Piketty’s original analysis.\nThis episode smacked of the recent Reinhart-Rogoff kerfuffle in which some fairly basic errors were discovered in those economists’ Excel spreadsheets. Some of those errors only made small differences to the results, but a critical methodological component, in which the data were weighted in a special way, appeared to have a significant impact on the results if alternate approaches were taken.\nPiketty has since responded forcefully to the FT’s column, defending all of the work he has done and addressing the criticisms one by one. To me, the most important result of the FT analysis is that Piketty’s work appears to be largely reproducible. Piketty made his data available, with reasonable documentation (in addition to his book), and Giles was able to come up with the same numbers Piketty came up with. This is a good thing. Piketty’s work was complex, and the only way to communicate the entirety of it was to make the data and code available.\nThe other aspects of Giles’ analysis are, from an academic standpoint, largely irrelevant to me, particularly because I am not an economist. The reason I find them irrelevant is because the objections are largely over whether he is correct or not. This is an obviously important question, but in any field, no single study or even synthesis can be determined to be “correct” at that instance. Time will tell, and if his work is “correct”, his predictions will be borne out by nature. It’s not so satisfying to have to wait many years to know if you are correct, but that’s how science works.\nIn the meantime, economists will have a debate over the science and the appropriate methods and data used for analysis. This is also how science works, and it is only (really) possible because Piketty made his work reproducible. Otherwise, the debate would be largely uninformed.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-05-28-the-big-in-big-data-relates-to-importance-not-size/",
    "title": "The Big in Big Data relates to importance not size",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-05-28",
    "categories": [],
    "contents": "\nIn the past couple of years several non-statisticians have asked me “what is Big Data exactly?” or “How big is Big Data?”. My answer has been “I think Big Data is much more about”data” than “big”. I explain below.\n\n\n\n\nSince 2011 Big Data has been all over the news. The New York Times, The Economist, Science, Nature, etc.. have told us that the Big Data Revolution is upon us (see google trends figure above). But was this really a revolution? What happened to the Massive Data Revolution (see figure above)? For this to be called a revolution, there must be some a drastic change, a discontinuity, or a quantum leap of some kind.  So has there been such a discontinuity in the rate of growth of data? Although this may be true for some fields (for example in genomics, next generation sequencing did introduce a discontinuity around 2007), overall, data size seems to have been growing at a steady rate for decades. For example, in the  graph below (see this paper for source) note the trend in internet traffic data (which btw dwarfs genomics data). There does seem to be a change of rate but during the 1990s which brings me to my main point.\n\nAlthough several fields (including Statistics) are having to innovate to keep up with growing data size, I don’t see this as something that new. But I do think that we are in the midst of a Big Data revolution.  Although the media only noticed it recently,  it started about 30 years ago. The discontinuity is not in the size of data, but in the percent of fields (across academia, industry and government) that use data. At some point in the 1980s with the advent of cheap computers, data were moved from the file cabinet to the disk drive. Then in the 1990s, with the democratization of the internet, these data started to become easy to share. All of the sudden, people could use data to answer questions that were previously answered only by experts, theory or intuition.\nIn this blog we like to point out examples but let me review a few. Credit card companies started using purchase data to detect fraud. Baseball teams started scraping data and evaluating players without ever seeing them. Financial companies started analyzing  stock market data to develop investment strategies. Environmental scientists started to gather and analyze data from air pollution monitors. Molecular biologists started quantifying outcomes of interest into matrices of numbers (as opposed to looking at stains on nylon membranes) to discover new tumor types and develop diagnostics tools. Cities started using crime data to guide policing strategies. Netflix started using costumer ratings to recommend movies. Retail stores started mining bonus card data to deliver targeted advertisements. Note that all the data sets mentioned were tiny in comparison to, for example, sky survey data collected by astronomers. But, I still call this phenomenon Big Data because the percent of people using data was in fact Big.\nbigdataI borrowed the title of this talk from a very nice presentation by Diego Kuonen\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-05-22-10-things-statistics-taught-us-about-big-data-analysis/",
    "title": "10 things statistics taught us about big data analysis",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-05-22",
    "categories": [],
    "contents": "\nIn my previous post I pointed out a major problem with big data is that applied statistics have been left out. But many cool ideas in applied statistics are really relevant for big data analysis. So I thought I’d try to answer the second question in my previous post: “When thinking about the big data era, what are some statistical ideas we’ve already figured out?” Because the internet loves top 10 lists I came up with 10, but there are more if people find this interesting. Obviously mileage may vary with these recommendations, but I think they are generally not a bad idea.\nIf the goal is prediction accuracy, average many prediction models together. In general, the prediction algorithms that most frequently win Kaggle competitions or the Netflix prize blend multiple models together. The idea is that by averaging (or majority voting) multiple good prediction algorithms you can reduce variability without giving up bias. One of the earliest descriptions of this idea was of a much simplified version based on bootstrapping samples and building multiple prediction functions - a process called bagging (short for bootstrap aggregating). Random forests, another incredibly successful prediction algorithm, is based on a similar idea with classification trees.\nWhen testing many hypotheses, correct for multiple testing This comic points out the problem with standard hypothesis testing when many tests are performed. Classic hypothesis tests are designed to call a set of data significant 5% of the time, even when the null is true (e.g. nothing is going on). One really common choice for correcting for multiple testing is to use the false discovery rate to control the rate at which things you call significant are false discoveries. People like this measure because you can think of it as the rate of noise among the signals you have discovered. Benjamini and Hochber gave the first definition of the false discovery rate and provided a procedure to control the FDR. There is also a really readable introduction to FDR by Storey and Tibshirani.\nWhen you have data measured over space, distance, or time, you should smooth This is one of the oldest ideas in statistics (regression is a form of smoothing and Galton popularized that a while ago). I personally like locally weighted scatterplot smoothing a lot.  This paperis a good one by Cleveland about loess. Here it is in a gif. But people also like smoothing splines, Hidden Markov Models, moving averages and many other smoothing choices.\nBefore you analyze your data with computers, be sure to plot it A common mistake made by amateur analysts is to immediately jump to fitting models to big data sets with the fanciest computational tool. But you can miss pretty obvious things like this if you don’t plot your data. There are too many plots to talk about individually, but one example of an incredibly important plot is the Bland-Altman plot, (called an MA-plot in genomics) when comparing measurements from multiple technologies. R provides tons of graphics for a reason and ggplot2 makes them pretty.\nInteractive analysis is the best way to really figure out what is going on in a data set This is related to the previous point; if you want to understand a data set you have to be able to play around with it and explore it. You need to make tables, make plots, identify quirks, outliers, missing data patterns and problems with the data. To do this you need to interact with the data quickly. One way to do this is to analyze the whole data set at once using tools like Hive, Hadoop, or Pig. But an often easier, better, and more cost effective approach is to use random sampling . As Robert Gentleman put it “make big data as small as possible as quick as possible”.\nKnow what your real sample size is.  It can be easy to be tricked by the size of a data set. Imagine you have an image of a simple black circle on a white background stored as pixels. As the resolution increases the size of the data increases, but the amount of information may not (hence vector graphics). Similarly in genomics, the number of reads you measure (which is a main determinant of data size) is not the sample size, it is the number of individuals. In social networks, the number of people in the network may not be the sample size. If the network is very dense, the sample size might be much less. In general the bigger the sample size the better and sample size and data size aren’t always tightly correlated.\nUnless you ran a randomized trial, potential confounders should keep you up at night Confounding is maybe the most fundamental idea in statistical analysis. It is behind the spurious correlations like these and the reason why nutrition studies are so hard. It is very hard to hold people to a randomized diet and people who eat healthy diets might be different than people who don’t in other important ways. In big data sets confounders might be technical variables about how the data were measured or they could be differences over time in Google search terms. Any time you discover a cool new result, your first thought should be, “what are the potential confounders?”\nDefine a metric for success up front Maybe the simplest idea, but one that is critical in statistics and decision theory. Sometimes your goal is to discover new relationships and that is great if you define that up front. One thing that applied statistics has taught us is that changing the criteria you are going for after the fact is really dangerous. So when you find a correlation, don’t assume you can predict a new result or that you have discovered which way a causal arrow goes.\nMake your code and data available and have smart people check it As several people pointed out about my last post, the Reinhart and Rogoff problem did not involve big data. But even in this small data example, there was a bug in the code used to analyze them. With big data and complex models this is even more important. Mozilla Science is doing interesting work on code review for data analysis in science. But in general if you just get a friend to look over your code it will catch a huge fraction of the problems you might have.\nProblem first not solution backward One temptation in applied statistics is to take a tool you know well (regression) and use it to hit all the nails (epidemiology problems). There is a similar temptation in big data to get fixated on a tool (hadoop, pig, hive, nosql databases, distributed computing, gpgpu, etc.) and ignore the problem of can we infer x relates to y or that x predicts y.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-05-07-why-big-data-is-in-trouble-they-forgot-about-applied-statistics/",
    "title": "Why big data is in trouble: they forgot about applied statistics",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-05-07",
    "categories": [],
    "contents": "\nThis year the idea that statistics is important for big data has exploded into the popular media. Here are a few examples, starting with the Lazer et. al paper in Science that got the ball rolling on this idea.\nThe parable of Google Flu: traps in big data analysis\nBig data are we making a big mistake?\nGoogle Flu Trends: the limits of big data\nEight (No, Nine!) Problems with Big Data\nAll of these articles warn about issues that statisticians have been thinking about for a very long time: sampling populations, confounders, multiple testing, bias, and overfitting. In the rush to take advantage of the hype around big data, these ideas were ignored or not given sufficient attention.\nOne reason is that when you actually take the time to do an analysis right, with careful attention to all the sources of variation in the data, it is almost a law that you will have to make smaller claims than you could if you just shoved your data in a machine learning algorithm and reported whatever came out the other side.\nThe prime example in the press is Google Flu trends. Google Flu trends was originally developed as a machine learning algorithm for predicting the number of flu cases based on Google Search Terms. While the underlying data management and machine learning algorithms were correct, a misunderstanding about the uncertainties in the data collection and modeling process have led to highly inaccurate estimates over time. A statistician would have thought carefully about the sampling process, identified time series components to the spatial trend, investigated why the search terms were predictive and tried to understand what the likely reason that Google Flu trends was working.\nAs we have seen, lack of expertise in statistics  has led to fundamental errors in both genomic science and economics. In the first case a team of scientists led by Anil Potti created an algorithm for predicting the response to chemotherapy. This solution was widely praised in both the scientific and popular press. Unfortunately the researchers did not correctly account for all the sources of variation in the data set and had misapplied statistical methods and ignored major data integrity problems. The lead author and the editors who handled this paper didn’t have the necessary statistical expertise, which led to major consequences and cancelled clinical trials.\nSimilarly, two economists Reinhart and Rogoff, published a paper claiming that GDP growth was slowed by high governmental debt. Later it was discovered that there was an error in an Excel spreadsheet they used to perform the analysis. But more importantly, the choice of weights they used in their regression model were questioned as being unrealistic and leading to dramatically different conclusions than the authors espoused publicly. The primary failing was a lack of sensitivity analysis to data analytic assumptions that any well-trained applied statisticians would have performed.\nStatistical thinking has also been conspicuously absent from major public big data efforts so far. Here are some examples:\nWhite House Big Data Partners Workshop - 0/19 statisticians\nNational Academy of Sciences Big Data Worskhop - 2/13 speakers statisticians\nMoore Foundation Data Science Environments - 0/3 directors from statistical background, 1/25 speakers at OSTP event about the environments was a statistician\nOriginal group that proposed NIH BD2K - 0/18 participants statisticians\nBig Data rollout from the White House - 0/4 thought leaders statisticians, 0/n participants statisticians.\nOne example of this kind of thinking is this insane table from the alumni magazine of the University of California which I found from this This year the idea that statistics is important for big data has exploded into the popular media. Here are a few examples, starting with the Lazer et. al paper in Science that got the ball rolling on this idea. (via Rafa, go watch his talk right now, it gets right to the heart of the issue).  It shows a fundamental disrespect for applied statisticians who have developed serious expertise in a range of scientific disciplines.\n\n\n\nAll of this leads to two questions:\nGiven the importance of statistical thinking why aren’t statisticians involved in these initiatives?\nWhen thinking about the big data era, what are some statistical ideas we’ve already figured out?\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-05-05-jhu-data-science-more-is-more/",
    "title": "JHU Data Science: More is More",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-05-05",
    "categories": [],
    "contents": "\nToday Jeff Leek, Brian Caffo, and I are launching 3 new courses on Coursera as part of the Johns Hopkins Data Science Specialization. These courses are\nExploratory Data Analysis\nReproducible Research\nStatistical Inference\nI’m particularly excited about Reproducible Research, not just because I’m teaching it, but because I think it’s essentially the first of its kind being offered in a massive open format. Given the rich discussions about reproducibility that have occurred over the past few years, I’m happy to finally be able to offer this course for free to a large audience.\nThese courses are launching in addition to the first 3 courses in the sequence: The Data Scientist’s Toolbox, R Programming, and Getting and Cleaning Data, which are also running this month in case you missed your chance in April.\nAll told we have 6 of the 9 courses in the Specialization available as of today. We’re really looking forward to next month where we will be launching the final 3 courses: Regression Models, Practical Machine Learning, and Developing Data Products. We also have some exciting announcements coming soon regarding the Capstone Projects.\nEvery course will be available every month, so don’t worry about missing a session. You can always come back next month.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-04-30-confession-i-sometimes-enjoy-reading-the-fake-journalconference-spam/",
    "title": "Confession: I sometimes enjoy reading the fake journal/conference spam",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-04-30",
    "categories": [],
    "contents": "\n\nI’ve spent a considerable amount of time setting up filters to avoid getting spam from fake journals and conferences. Unfortunately, they are exceptionally good at thwarting my defenses. This does not annoy me as much as I pretend because, secretly, I enjoy reading some of these emails. Here are three of my favorites.\n\n\nOver-the-top robot:\n\n\nIt gives us immense pleasure to invite you and your research allies to submit a manuscript for the journal “REDACTED”. The expertise of you in the never ending field of Gene Technology is highly appreciable. The level of intricacy shown by you in your work makes us even more proud, and we believe that your works should be known to mankind of science.\n\n\nSarcastic robot?\n\nFirst of all, congratulations on the publication of your highly cited original article < The human colon cancer methylome shows similar hypo- and hypermethylation at conserved tissue-specific CpG island shores > in the field of colon cancer, which has been cited more than 1 times and is in the world’s top one percent of papers. Such high number of citations reflects the high quality and influence of your paper.\n\n\n\nIntimidating robot:\n\n\n\nThis is Rocky…. Recently we have mailed you about the details of the conference. But we still have not received your response. So today we contact you again.\n\n\n\nNB: Although I am joking in this post, I do think these fake journals and conferences are a very serious problem. The fact that they are still around means enough money (mostly taxpayer money) is being spent to keep them in business. If you want to learn more, this blog does a good job on reporting on them and includes a list of culprits.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-04-22-picking-a-biostatistics-thesis-topic-for-real-world-impact-and-transferable-skills/",
    "title": "Picking a (bio)statistics thesis topic for real world impact and transferable skills",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-04-22",
    "categories": [],
    "contents": "\nOne of the things that was hardest for me in graduate school was starting to think about my own research projects and not just the ideas my advisor fed me. I remember that it was stressful because I didn’t quite know where to start. After having done this for a while and particularly after having read a bunch of papers by people who are way more successful than I am, I have come to the following algorithm as a means for finding a topic that will have real world impact and also give you skills to take on new problems in a flexible way.\n Find a scientific problem that hasn’t been solved with data (by far hardest part)\nDefine your metric for success\n Collect data/partner up with someone with data for that problem.\n Create a good solution to the problem\n Only invent new methods if you must\n(Optional) Write software and document the hell out of it\n(Optional) Respond to users and update as needed\nDon’t get (meanly) competitive\nThe first step is definitely the most important and the hardest. The balance is between big important problems that lots of people are working on but where the potential for innovation is low and small detailed problems where you won’t have serious competition but you will have limited impact. In general good ways to find scientific problems are the following. (1) Find close and real scientific/applications collaborators. Not real like you talk to them once a month, real like you have a weekly meeting, you try to understand how their data are collected or generated and you ask them specifically what problems prevent them from doing their job well then solve those problems. (2) You come up with a scientific question you have on your own. In mature research areas like genomics this requires a huge amount of reading to know what people have done before you, or to at least know what new technologies/data are becoming available. (3) You you could read a ton of papers and find one that produces interesting data you think could answer a question the authors haven’t asked. In general, the key is to put the problem first, before you even think about how to quantify or answer the question.\nNext you have to define your metric for success. This metric should be scientific. You should try to say, “if I could predict x at 70% accuracy I could solve scientific problem y” or “if I could infer the relationship between x and y I would know something about z”. The metric should be compared to the scientific standards in the field. As an example, screening tests for the general population often must be 99% sensitive and specific (or more) due to low prevalence. But in a sub population, sensitivity and specificity of 70% or 80% may be really useful.\nThen you find the data. Here the key quote comes from Tukey:\n\nThe data may not contain the answer. The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.\n\nMy experience is that when you start with the problem first, the data are often hard to come by, have quirks, or are not quite right for the problem you want to solve. Generating the perfect data is often very expensive, so a huge amount of the effort you will spend is either (a) generating the perfect data or (b) determining if the data you collected is “good enough” to answer the question. One important point here is that knowing when you have failed is the entire name of the game here. If you get stuck once, you should try again. If you get stuck 100 times, it might be time to look for a different data set or figure out why the problem is unanswerable with current data. Incidentally, this is the most difficult part of the approach I’m proposing for coming up with topics. Failure is both likely and frequent, but that is a good thing when you are in grad school if you can learn from it and learn to predict when you are going to fail.\nSince you’ve identified a problem that hasn’t been solved before in step 1, the first thing to try is to come up with a sensible solution using only the methods that already exist. In many cases, these existing methods will work pretty well. If they don’t, invent only as much statistical methodology and theory as you need to solve the problem. If you invent something new here, you should try it out on simple simulated examples and complex data where you either know the answer or can perform cross-validation/replication analysis.\nAt this point, if you have a basic solution to the problem, even if it is just the t-test, you are in great shape! You have solved a problem that is new and you are ready to publish. If you have invented some methods along the way, publish those, too!\nIn some cases the problems you solve will be focused on an area where lots of other people can collect similar data to answer similar problems. In this case, your most direct route to maximum impact is to write simple, usable, and really well documented software other people can use. Write it in R, make it free, give it a vignette and advertise it! If people use your software they will send you bug reports, patches, typos, fixes, and wish lists of things they want your software to do. The more you help people and respond, the more your software will get used and the more impact your method will have.\nStep 8 is often the hardest part. If you do something interesting, you will have a ton of competitors. People will write better and more precise methods down and will “beat” your method. That’s ok, in fact it is good! The more people that compare to your approach, the more you know you picked a good problem. In some cases, people will genuinely create better methods than you will. Learn from them and make your methods and software better. But try not to be upset that they wrote a paper about how their idea is so much better than yours, it is a high compliment they thought your idea was worth comparing to. This is one the author of the post hasn’t nailed down perfectly but I think the more you can do it the happier you will be.\nThe best part of this algorithm is that it gives you the problem first focus that will make it easy to transition if you do a postdoc with a different kind of data, or move to industry, or start with new collaborators.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-04-17-correlation-does-not-imply-causation-parental-involvement-edition/",
    "title": "Correlation does not imply causation (parental involvement edition)",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-04-17",
    "categories": [],
    "contents": "\nThe New York Times recently published an article on education titled “Parental Involvement Is Overrated”. Most research in this area supports the opposite view, but the authors claim that “evidence from our research suggests otherwise”.  Before you stop helping your children understand long division or correcting their grammar, you should learn about one of the most basic statistical concepts: correlation does not imply causation. The first two chapters of this very popular text book describes the problem and even Khan Academy has a class on it. As several of the commenters in the  NYT article point out, the authors fail to make this distinction.\nTo illustrate the problem, imagine you want to know how effective tutoring is for students in a math class you are teaching.  So you compare the test scores of students that received tutoring to those that don’t. You find that receiving tutoring is correlated with lower test scores. So do you conclude that tutoring causes lower grades? Of course not!  In this particular case we are confusing cause and effect: students that have trouble with math are much more likely to seek out tutoring and this is what drives the observed correlation. With that example in mind,  consider this quote from the New York Times article:\n\nWhen we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades…. Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse.\n\nA first question we would ask here is: how do we know that the children’s performance would not have been even worse had they not received help? I imagine the authors made use of controls: we compare the group that received the treatment (regular help with homework) to a control group that did not. But this brings up a more difficult question: how do we know that the treatment and control groups are comparable?\nIn a randomized controlled experiment, we would take a group of kids and randomly assign each one to the treatment group (will be helped with their homework) or control group (no help with homework). By doing this we can use probability calculations to determine the range of differences we expect to see by chance when the treatment has no effect.  Note that by chance one group may end up with a few more “better testers” than the other. However, if we see a big enough difference that can’t be explained by chance, then the alternative that the treatment is responsible for the observed differences becomes more believable.\nGiven all the prior research (and common sense) suggesting that parent involvement, in its many manifestations, is in fact helpful to students, many would consider it unethical to run a randomized controlled trial on this issue (you would knowingly hurt the control group). Therefore, the authors are left with no choice than to use an observational study to reach their conclusions. In this case, we have no control over who receives help and who doesn’t. Kids that require regular help with their homework are different in many ways to kids that don’t, even after correcting for all the factors mentioned. For example, one can envision how kids that have a mediocre teacher or have trouble with tests are more likely to be in the treatment group, while kids who naturally test well or go to schools that offer in-school tutoring are more likely to be in the control group.\nI am not an expert on education, but as a statistician I am skeptical of the conclusions of this data-driven article.  In fact, I would  recommend parents actually do get involved early on by, for example, teaching children that correlation does not imply causation.\nNote that I am not saying that observational studies are uninformative. If properly analyzed, observational data can be very valuable. For example, the data supporting smoking as a cause of lung cancer is all observational. Furthermore, there is an entire subfield within statistics (referred to as causal inference) that develops methodologies to deal with observational data. But unfortunately, observational data are commonly misinterpreted.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-04-10-the-ropensci-hackathon-ropenhack/",
    "title": "The #rOpenSci hackathon #ropenhack",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-04-10",
    "categories": [],
    "contents": "\nEditor’s note: This is a guest post by Alyssa Frazee, a graduate student in the Biostatistics department at Johns Hopkins and a participant in the recent rOpenSci hackathon. \nLast week, I took a break from my normal PhD student schedule to participate in a hackathon in San Francisco. The two-day event was hosted by rOpenSci, an organization committed to developing R tools for open science. Working with several wonderful people from the R community was inspiring, humbling, and incredibly fun. So many great things happened in a two-day whirlwind: it would be impossible now to capture the whole thing in a narrative that would do it justice. So instead of a play-by-play, here are some of the quotes from the event that I’ve recently been reflecting on:\n“The enemy isn’t R, Python, or Julia. The enemy is closed-source science.”\n\nThere have been some lively internet debates recently about mathematical and scientific computing languages. While conversations about these languages are interesting and necessary, the forest often gets lost for the trees: in the end, we are here to do good science, and we should use whatever makes that easiest. We should build strong, collaborative communities, both within languages and across them. A closed-source science mentality hinders this kind of collaboration. I thought one of the hackathon projects, an R kernel for the iPython notebook, especially exemplified a commitment to open science and to cross-language collaboration. It was so awesome to spend two days with R folks like this who genuinely enjoy working together, in any language, to make scientific computing better.\n\n“Pair debugging is fun!”\n\nThis quote perfectly captures one of my favorite things about hackathons: genuine group work! During my time in graduate school, I’ve done most of my programming solo. I think this is the nature of getting a PhD: the projects have to be yours, and all the other PhD students are working on their solo projects. So I really enjoyed the hackathon because it facilitated true pair/group work: two or more peers working on the same project, in the same room, at the same time. I like this work strategy for many reasons:\n\n\n•          The rate at which I learn new things is high, since it’s so easy to ask a question. Lots of time is saved by not having to sift through internet search results.\n\n\n•          Sometimes I find solo debugging to be pretty painful. But I think pair debugging is fun and satisfying: it’s like an inspirational sports movie. It’s you and me, the ragtag underdogs, against the computer, the evil bully from across town. Relatedly, the sweet sweet taste of victory is also shared.\n\n\n•          It’s easier to stay focused on the task at hand. I’m not as easily distracted by email/Twitter/Facebook/blogs/the rest of the internet when I’m not coding alone.\n\n\nMy academic sister, Hilary, and I did a good amount of pair debugging during the hackathon, and I kept finding myself thinking “I wish this would have been possible while we were both grad students!” I think we both had lots of fun working together. For a short discussion of more fun aspects of pairing, here’s a blog post I like. At the rOpenSci hackathon in particular, group work was especially awesome because we could ask questions in person to people who have written the libraries our projects depend on, or to RStudio developers, or to GitHub employees, or to potential users of the projects. Just some of the many joys of having lots of talented, friendly R programmers all in the same space!\n\n“Want me to write some unit tests for your unit tests?”\n\nDuring the hackathon, I primarily worked on a unit-testing package called testdat. Testdat provides functions that check for and fix common problems with tabular data, like UTF-8 characters and inconsistent missing data codes, with the overall goal of making data processing/cleaning more reproducible. The project was really good for a two-day hackathon, since it was small enough to almost finish in two days, and it was very modular: one person worked on the missing data checking functions, another worked on UTF-8 checking, a third wrote the tests for the finished functions (unit tests for unit tests!), etc. Also, it didn’t require a lot of background knowledge in a specific subject area or a deep dive into an existing codebase: all it required were some coding skills and perhaps a frustrating experience with messy data in the past (for motivation).\n\n\nFinding an appropriate project to work on was probably my biggest challenge at this hackathon. I spent the summer at Hacker School, where the days were structured similarly to how they were at the rOpenSci hackathon: there wasn’t really any structure. In both scenarios, the minimal structure was intentional. Lots of great collaborative work can happen with a few free days of hacking. But with two free days at the hackathon (versus Hacker School’s 50), it was much more important to choose a good project quickly and get coding. One way to do this would have been to arrive at the hackathon with a small project in hand (many people did this). My strategy, however, was to chat with a few different project groups for the first hour or two on day 1, and then stick with one of those groups for the rest of the time. It worked well – as I mentioned above, testdat was a great project – but I did feel some time pressure (internally!) to choose a small project quickly.\n\n\nFor a look at some of the other hackathon projects, check out rOpenSci’s GitHub page, the hackathon GitHub page, project-specific posts on the rOpenSci blog, or the hackathon’s live-tweet hashtag, #ropenhack.\n\n“Why are there so many Minnesotans here?”\n\nThere were at least four hackathon attendees (out of 35-40 total) that either currently live in or hail from Minnesota. Talk about overrepresentation! We are everywhere.\n\n“I love my job.”\n\nI’m a late-stage PhD student, so the job market is looming closer with every passing day. When I meet new people working in statistics, genomics, data science, or another related field, I like to ask them whether they like their current work, how it compares to other jobs they’ve had, etc. Hackathon attendees had all kinds of jobs: academic researcher, industry scientist, freelancer, student, etc. The majority of the responses to my inquiries about how they liked their work was “I love it.” The situation made the job market seem exciting, rather than intimidating: among the hackathon attendees and folks from the SF data science community that hung out with us for a dinner, the jobs themselves were pretty heterogeneous, but the general enjoyment of the work seemed consistently high.\n\n“What’s the future of R?”\n\nI suppose we should have known that existential questions like this would come up when 40 passionate R people spend two straight days together. Our discussion of the future of R didn’t really yield any definitive answers or predictions, but I think we have big dreams for what R’s future will look like: vibrant, open, collaborative, and scientifically driven. If the hackathon atmosphere was any indication of R’s future, I’m feeling pretty optimistic about where things are going.\n\nIn closing: we’re really grateful to the people and organizations that made the hackathon possible: rOpenSci, Karthik Ram, GitHub, the Sloan Foundation, and F1000 Research. Thanks for strengthening the R community, giving us the chance to meet each other outside of the internet, and helping us have a great time doing R, for science, together!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-04-07-writing-good-software-can-have-more-impact-than-publishing-in-high-impact-journals-for-genomic-statisticians/",
    "title": "Writing good software can have more impact than publishing in high impact journals for genomic statisticians",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-04-07",
    "categories": [],
    "contents": "\n\nEvery once in a while we see computational papers published in science journals with high impact factors.  Genomics related methods appear quite often in these journals. Several of my junior colleagues express frustration that all their papers get rejected from these journals. I tell them that the same is true for most of my papers and remind them of these examples:\n\n\nMethod\n\n\nJournal\n\n\nYear\n\n\n#Citations\n\n\nPLINK\n\n\nAJHG\n\n\n2007\n\n\n6481\n\n\nBioconductor\n\n\nGenome Biology\n\n\n2004\n\n\n5973\n\n\nRMA\n\n\nBiostatistics\n\n\n2003\n\n\n5674\n\n\nlimma\n\n\nSAGMB\n\n\n2004\n\n\n5637\n\n\nquantile normalization\n\n\nBioinformatics\n\n\n2003\n\n\n4646\n\n\nBowtie\n\n\nGenome Biology\n\n\n2009\n\n\n3849\n\n\nBWA\n\n\nBioinformatics\n\n\n2009\n\n\n3327\n\n\nLoess normalization\n\n\nNAR\n\n\n2002\n\n\n3313\n\n\nqvalues\n\n\nJRSS-B\n\n\n2002\n\n\n2758\n\n\ntophat\n\n\nBioinformatics\n\n\n2008\n\n\n1868\n\n\nvsn\n\n\nBioinformatics\n\n\n2002\n\n\n1398\n\n\nGCRMA\n\n\nJASA\n\n\n2004\n\n\n1397\n\n\nMACS\n\n\nGenome Biology\n\n\n2008\n\n\n1277\n\n\ndeseq\n\n\nGenome Biology\n\n\n2010\n\n\n1264\n\n\nCBS\n\n\nBiostatistics\n\n\n2004\n\n\n1051\n\n\nR/qtl\n\n\nBioinformatics\n\n\n2003\n\n\n1027\n\nLet me know of other examples in the comments.\nupdate: I added one more to the list.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-04-01-this-is-how-an-important-scientific-debate-is-being-used-to-stop-epa-regulation/",
    "title": "This is how an important scientific debate is being used to stop EPA regulation",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-04-01",
    "categories": [],
    "contents": "\n\nEnvironmental regulation in the United States has protected human health for over 40 years. Since the Clean Air Act was enacted in 1970, levels of outdoor air pollution have dropped dramatically, changing the landscape of once heavily-polluted cities like Los Angeles and Pittsburgh. A 2011 cost-benefit analysis conducted by the U.S. Environmental Protection Agency estimated that the 1990 amendments to the CAA prevented 160,000 deaths and 13 million lost work days in the year 2010 alone. They estimated that the monetary benefits of the CAA were 30 times greater than the costs of implementing the regulations.\n\n\nThe benefits of environmental regulations like the CAA significantly outweigh their costs. But there are still costs, and those costs must be borne by someone. The burden is usually put on the polluters, such as the automobile and power generation industries, which have long fought any notion of air pollution regulation as a threat to their existence. Initially, as air pollution and health studies were still emerging, opponents of regulation often challenged the science itself, claiming flaws in the methodology, the measurements, or the interpretation. But when study after study demonstrated a connection between outdoor air pollution and a variety of health problems, it became increasingly difficult for critics to mount a credible challenge. Lawsuits are another tactic used by industry, with one case brought by the American Trucking Association going all the way to the U.S. Supreme Court.\n\nThe latest attack comes from the House of Representatives in the form of the Secret Science Reform Act, or H.R. 4102. In summary, the proposed bill requires that every scientific paper cited by the EPA to justify a new rule or regulation needs to be reproducible. What exactly does this mean? To answer that question we need to take a brief diversion into some recent important developments in statistical science.\nThe idea behind reproducibility is simple. All the data used in a scientific paper and all the computer code used to analyze that data should be made available to other researchers and the public. It may be surprising that much of this data actually isn’t already available. The primary reason most data isn’t available is because, until recently, most people didn’t ask scientists for their data. The data was often small and collected for a specific purpose so other scientists and the general public just weren’t that interested. If a scientist were interested in checking the truth of a claim, she could simply repeat the experiment in her lab to see if the claim could be replicated.\nThe nature of science has changed quickly over the last three decades. There has been an explosion of data, fueled by the decreasing cost of data collection technologies and computing power. At the same time, increased access to sophisticated computing power has let scientists conduct more sophisticated analyses on their data. The massive growth in data and the increasing sophistication of the analyses has made communicating what was done in a scientific study more complicated.\nThe traditional medium of journal publications has proven to be inadequate for describing the important details of a data analysis. As a result, it has been said that scientific articles are merely the “advertising” for the research that was conducted. The real research is buried in the data and the computer code actually used to compute the results. Journals have traditionally not required that data or computer code be published along with papers. As a result, many important details may be lost and prevent key studies from being fully reproducible.\nThe explosion of data has also made completely replicating a large study by an independent scientist much more difficult and costly. A large study is expensive to conduct in the first place; there is usually little appetite or funding to repeat it.  The result is that much of published scientific research cannot be reproduced by other scientists because the necessary data and analytic details are not available to others.\nThe scientific community is currently engaged in a debate over how to improve reproducibility across all of science. You might be tempted to ask, why not just share the data? Even if we could get everyone to agree with that in principle, it’s not clear how to do it.\nImagine if everyone in the U.S. decided we were all going to share our movie collections, and suppose for the sake of this example that the movie industry did not object. How would it work? Numerous questions immediately arise. Where would all these movies be stored? How would they be transferred from one person to another? How would I know what movies everyone else had? If my movies are all on the old DVD format, do I need to convert them to some other format before I can share? My Internet connection is very slow, how can I download a 3 hour HD movie? My mother doesn’t use computers much, but she has a great movie collection that I think others should have access to. What should she do? And who is going to pay for all of this? While each question may have a reasonable answer, it’s not clear what is the optimal combination and how you might scale it to the entire country.\nSome of you may recall that the music industry had a brilliant sharing service that essentially allowed everyone to share their music collections. It was called Napster. Napster solved many of the problems raised above except for one – they failed to survive. So even when a decent solution is found, there’s no guarantee that it will always be there.\nAs outlandish as this example may seem, minor variations on these exact questions come up when we discuss how to share scientific data. The volume of data being produced today is enormous and making all of it available to everyone is not an easy task. That’s not to say it is impossible. If smart people get together and work constructively, it is entirely possible that a reasonable approach could be found. But at this point, a credible long-term solution has yet to emerge.\nThis brings us back to the Secret Science Reform Act. The latest tactic by opponents of air quality regulation is to force the EPA to ensure that all of the studies that it cites to support new regulations are reproducible. A cursory reading of the bill gives the impression that the sponsors are genuinely concerned about making science more transparent to the public. But when one reads the language of the bill in the context of ongoing discussions about reproducibility, it becomes clear that the sponsors of the bill have no such goal in mind. The purpose of H.R. 4102 is to prevent the Environmental Protection Agency from proposing new regulations.\nThe EPA develops rules and regulations on the basis of scientific evidence. For example, the Clean Air Act requires EPA to periodically review the scientific literature for the latest evidence on the health effects of air pollution. The science the EPA considers needs to be published in peer-reviewed journals. This makes the EPA a key consumer of scientific knowledge and it uses this knowledge to make informed decisions about protecting public health. What the EPA is not is a large funder of scientific studies. The entire budget for the Office of Research and Development at EPA is roughly $550 million (fiscal 2014), or less than 2 percent of the budget for the National Institutes of Health (about $30 billion for fiscal 2014). This means EPA has essentially no influence over the scientists behind many of the studies it cites because it funds very few of those studies. The best the EPA can do is politely ask scientists to make their data available. If a scientist refuses, there’s not much the EPA can use as leverage.\n\nThe latest controversy to come up involves the Harvard Six Cities study published in 1993. This landmark study found a large difference in mortality rates comparing cities with high and low air pollution, even after adjusting for smoking and other factors. The House committee has been trying to make the data for this study publicly available so that it can ensure that regulations are “backed by good science”. However, the Committee has either forgotten or never knew that this particular study has been fully reproduced by independent investigators. In 2005, independent investigators found that they were “…able to reproduce virtually all of the original numerical results, including the 26 percent increase in all-cause mortality in the most polluted city (Stubenville, OH) as compared to the least polluted city (Portage, WI). The audit and validation of the Harvard Six Cities Study conducted by the reanalysis team generally confirmed the quality of the data and the numerical results reported by the original investigators.”\n\nIt would be hard to find an air pollution study that has been subject to more scrutiny than the Six Cities studies. Even if you believed the Six Cities study was totally wrong, its original findings have been replicated numerous times since its publication, with different investigators, in different populations, using different analysis techniques, and in different countries. If you’re looking for an example where the science was either not reproducible or not replicable, sorry, but this is not your case study.\nUltimately, it is clear that the sponsors of this bill are cynically taking advantage of a genuine (but difficult) scientific debate over reproducibility to push a political agenda. Scientists are in agreement that reproducibility is important, but there is no consensus yet on how to make it happen for everyone. By forcing the EPA to ensure reproducibility of the science on which it bases regulation, lawmakers are asking the EPA to solve a problem that the entire scientific community has yet to figure out. The end result of passing a bill like H.R. 4102 is that the EPA will be forced to stop proposing any new regulation, handing a major victory to opponents of air quality standards and dealing a major blow to public health in the U.S.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-03-31-data-analysis-for-genomic-edx-course/",
    "title": "Data Analysis for Genomics edX Course",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-03-31",
    "categories": [],
    "contents": "\nMike Love (@mikelove) and I have been working hard the past couple of months preparing a free online edX course on data analysis for genomics. Our target audience are the postdocs, graduate students and research scientists that are tasked with analyzing genomics data, but don’t have any formal training. The eight week course will start with the very basics, but will ramp up rather quickly and end with real-life workflows for genome variation, RNA-seq, DNA methylation, and ChIP-seq.\nThroughout the course students will learn skills and concepts that provide a foundation for analyzing genomics data. Specifically, we will cover exploratory data analysis, basic statistical inference, linear regression, modeling with parametric distributions, empirical Bayes, multiple comparison corrections and smoothing techniques.\nIn the class we will make heavy use of computer labs. Almost every lecture is accompanied by an R markdown document that students can use to recreate the plots shown in the lectures. The html document resulting from these R markdown files will result in an html document that will serve as a text book for the class.\nQuestions will be discussed on online forums led by Stephanie Hicks (@stephaniehicks) and Jim MacDonald.\nIf you want to sign up, here is the link.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-03-26-a-non-comprehensive-comparison-of-prominent-data-science-programs-on-cost-and-frequency/",
    "title": "A non-comprehensive comparison of prominent data science programs on cost and frequency.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-03-26",
    "categories": [],
    "contents": "\n\nWe did a really brief comparison of a few notable data science\nprograms for a grant submission we were working on. I thought it was pretty fascinating, so I’m posting it here. A couple of notes about the table.\nOur program can be taken for free, which includes assessments. If you want the official certificate and to take the capstone you pay the above costs.\nUdacity’s program can also be taken for free, but if you want the official certificate, assessments, or tutoring you pay the above costs.\nThe asterisks denote programs where you get an official master’s degree.\nThe MOOC programs (Udacity’s and ours) offer the more flexibility in\nthe terms of student schedules. Ours is the most flexible with courses\nrunning every month. The in person programs having the least\nflexibility but obviously the most direct instructor time.\nThe programs are all quite different in the terms of focus, design,\nstudent requirements, admissions, instruction, cost and value.\nAs far as we know, ours is the only one where every bit of lecture\ncontent has been open sourced (https://github.com/DataScienceSpecialization)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-03-24-the-fact-that-data-analysts-base-their-conclusions-on-data-does-not-mean-they-ignore-experts/",
    "title": "The fact that data analysts base their conclusions on data does not mean they ignore experts",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-03-24",
    "categories": [],
    "contents": "\nPaul Krugman recently joined the new FiveThirtyEight hating bandwagon. I am not crazy about the new website either (although I’ll wait more than one weeks before judging) but in a recent post Krugman creates a false dichotomy that is important to correct. Krugmanam states that “[w]hat [Nate Silver] seems to have concluded is that there are no experts anywhere, that a smart data analyst can and should ignore all that.” I don’t think that is what Nate Silver, nor any other smart data scientist or applied statistician has concluded. Note that to build his election prediction model, Nate had to understand how the electoral college works, how polls work, how different polls are different, the relationship between primaries and presidential election, among many other details specific to polls and US presidential elections. He learned all of this by reading and talking to experts. Same is true for PECOTA where data analysts who know quite a bit about baseball collect data to create meaningful and predictive summary statistics. As Jeff said before, the key word in “Data Science” is not Data, it is Science.\nThe one example Krugman points too as ignoring experts appears to be written by someone who, according to the article that Krugman links to, was biased by his own opinions, not by data analysis that ignored experts. However, in Nate’s analysis of polls and baseball data it is hard to argue that he let his bias affect his analysis. Furthermore, it is important to point out that he did not simply stick data into a black box prediction algorithm. Instead he did what most of us applied statisticians do: we build empirically inspired models but guided by expert knowledge.\nps - Krugman links to a Paul Krugman recently [joined](http://krugman.blogs.nytimes.com/2014/03/23/tarnished-silver/?_php=true&_type=blogs&_php=true&_type=blogs&_r=1&) the new FiveThirtyEight hating [bandwagon](http://www.salon.com/2014/03/18/nate_silvers_new_fivethirtyeight_is_getting_some_high_profile_bad_reviews/). I am not crazy about the new website either (although I’ll wait more than one weeks before judging) but in a recent post Krugman creates a false dichotomy that is important to correct. Krugmanam states that “[w]hat [Nate Silver] seems to have concluded is that there are no experts anywhere, that a smart data analyst can and should ignore all that.” I don’t think that is what Nate Silver, nor any other smart data scientist or applied statistician has concluded. Note that to build his election prediction model, Nate had to understand how the electoral college works, how polls work, how different polls are different, the relationship between primaries and presidential election, among many other details specific to polls and US presidential elections. He learned all of this by reading and talking to experts. Same is true for PECOTA where data analysts who know quite a bit about baseball collect data to create meaningful and predictive summary statistics. As Jeff said before, [the key word in “Data Science” is not Data, it is Science](http://simplystatistics.org/2013/12/12/the-key-word-in-data-science-is-not-data-it-is-science/). piece which has another false dichotomy as the title: “Creativity vs. Quants”. He should try doing it before assuming there is no creativity involved in extracting information from data.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-03-20-the-8020-rule-of-statistical-methods-development/",
    "title": "The 80/20 rule of statistical methods development",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-03-20",
    "categories": [],
    "contents": "\nDeveloping statistical methods is hard and often frustrating work. One of the under appreciated rules in statistical methods development is what I call the 80/20 rule (maybe could even by the 90/10 rule). The basic idea is that the first reasonable thing you can do to a set of data often is 80% of the way to the optimal solution. Everything after that is working on getting the last 20%. (Edit: Rafa points out that once again I’ve reverse-scooped a bunch of people and this is already a thing that has been pointed out many times. See for example the Pareto principle and this post also called the 80:20 rule)\nSometimes that extra 20% is really important and sometimes it isn’t. In a clinical trial, where each additional patient may cost a large amount of money to recruit and enroll, it is definitely worth the effort. For more exploratory techniques like those often used when analyzing high-dimensional data it may not. This is particularly true because the extra 20% usually comes at a cost of additional assumptions about the way the world works. If your assumptions are right, you get the 20%, if they are wrong, you may lose and it isn’t always clear how much.\nHere is a very simple example of the 80/20 rule from frequentist statistics - in my experience similar ideas hold in machine learning and Bayesian inference as well. Suppose that I collect some observations  and want to test whether the mean of the observations is greater than 0. Suppose I know that the data are normal and that the variance is equal to 1. Then the absolute best statistical test (called the uniformly most powerful test) you could do rejects the hypothesis the mean is zero if  versus the alternative that the probability is greater than 0.5 . Or you could use the one sided t-test. Or you could use the Wilcoxon test. These are suboptimal if you know the data are Normal with variance one.\nI tried each of these tests with a sample of size  at the  level. In the plot below I show the ratio of power between each non-optimal test and the optimal z-test (you could do this theoretically but I’m lazy so did it with simulation, code here, colors by RSkittleBrewer).\n\n\n\nThe tests get to 80% of the power of the z-test for different sizes of the true mean (0.6 for Wilcoxon, 0.5 for the t-test, and 0.85 for the sign test). Overall, these methods very quickly catch up to the optimal method.\nIn this case, the non-optimal methods aren’t much easier to implement than the optimal solution. But in many cases, the optimal method requires significantly more computation, memory, assumptions, theory, or some combination of the four. The hard decision is whether to create a new method is whether the 20% is worth it. This is obviously application specific.\nAn important corollary of the 80/20 rule is that you can have a huge impact on new technologies if you are the first to suggest an already known 80% solution. For example, the first person to suggest hierarchical clustering or the singular value decomposition for a new high-dimensional data type will often get a large number of citations. But that is a hard way to make a living - you aren’t the only person who knows about these methods and the person who says it first soaks up a huge fraction of the credit. So the only way to take advantage of this corollary is to spend your time constantly trying to figure out what the next big technology will be. And you know what they say about prediction being hard, especially about the future.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-03-19-the-end-of-the-world-challenge/",
    "title": "The time traveler's challenge.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-03-19",
    "categories": [],
    "contents": "\nEditor’s note: This has nothing to do with statistics. \nI do a lot of statistics for a living and would claim to know a relatively large amount about it. I also know a little bit about a bunch of other scientific disciplines, a tiny bit of engineering, a lot about pointless sports trivia, some current events, the geography of the world (vaguely) and the geography of places I’ve lived (pretty well).\nI have often wondered, if I was transported back in time to a point before the discovery of say, how to make a fire, how much of human knowledge I could recreate. In other words, what would be the marginal effect on the world of a single person (me) being transported back in time. I could propose Newton’s Laws, write down a bunch of the basis of calculus, and discover the central limit theorem. I probably couldn’t build an internal combustion engine - I know the concept but don’t know enough of the details. So the challenge is this.\n If you were transported back 4,000 or 5,000 years, how much could you accelerate human knowledge?\nWhen I told Leah J. about this idea she came up with an even more fascinating variant.\nSuppose that I told you that in 5 days you were going to be transported back 4,000 or 5,000 years but you couldn’t take anything with you. What would you read about on Wikipedia? \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-03-14-enar-is-in-baltimore-heres-what-to-do/",
    "title": "ENAR is in Baltimore - Here's What To Do",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-03-14",
    "categories": [],
    "contents": "\nThis year’s meeting of the Eastern North American Region of the International Biometric Society (ENAR) is in lovely Baltimore, Maryland. As local residents Jeff and I thought we’d put down a few suggestions for what to do during your stay here in case you’re not familiar with the area.\nVenue\nThe conference is being held at the Marriott in the Harbor East area of the city, which is relatively new and a great location. There are a number of good restaurants right in the vicinity, including Wit & Wisdom in the Four Seasons hotel across the street and Pabu, an excellent Japanese restaurant that I personally believe is the best restaurant in Baltimore (a very close second is Woodberry Kitchen, which is a bit farther away near Hampden). If you go to Pabu, just don’t get sushi; try something new for a change. Around Harbor East you’ll also find a Cinghiale (excellent northern Italian restaurant), Charleston (expensive southern food), Lebanese Taverna, and Ouzo Bay. If you’re sick of restaurants, there’s also a Whole Foods. If you want a great breakfast, you can walk just a few blocks down Aliceanna street to the Blue Moon Cafe. Get the eggs Benedict. If you get the Cap’n Crunch French toast, you will need a nap afterwards.\nJust east of Harbor East is an area called Fell’s Point. This is commonly known as the “bar district” and it lives up to its reputation. Max’s in Fell’s Point (on the square) has an obscene number of beers on tap. The Heavy Seas Alehouse on Central Avenue has some excellent beers from the local Heavy Seas brewery and also has great food from chef Matt Seeber. Finally, the Daily Grind coffee shop is a local institution.\nAround the Inner Harbor\nOutside of the immediate Harbor East area, there are a number of things to do. For kids, there’s Port Discovery, which my 3-year-old son seems to really enjoy. There’s also the National Aquarium where the Tuesday networking event will be held. This is also a great place for kids if you’re bringing family. There’s a neat little park on Pier 6 that is small, but has a number of kid-related things to do. It’s a nice place to hang out when the weather is nice. Around the other side of the harbor is the Maryland Science Center, another kid-fun place, and just west of the Harbor down Pratt Street is the B&O Railroad Museum, which I think is good for both kids and adults (I like trains).\nUnfortunately, at this time there’s no football or baseball to watch.\nAround Baltimore\nThere are a lot of really interesting things to check out around Baltimore if you have the time. If you need to get around downtown and the surrounding areas there’s the Charm City Circulator which is a free bus that runs every 15 minutes or so. The Mt. Vernon district has a number of cultural things to do. For classical music fans there’s the wonderful Baltimore Symphony Orchestra directed by Marin Alsop. The Peabody Institute often has some interesting concerts going on given by the students there. There’s the Walters Art Museum, which is free, and has a very interesting collection. There are also a number of good restaurants and coffee shops in Mt. Vernon, like Dooby’s (excellent dinner) and Red Emma’s  (lots of Noam Chomsky).\nThat’s all I can think of right now. If you have other questions about Baltimore while you’re here for ENAR tweet us up at @simplystats.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-03-14-using-bioconductor-to-find-some-empirical-evidence-in-support-of-%cf%80-being-a-normal-number/",
    "title": "How to use Bioconductor to find empirical evidence in support of π being a normal number",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-03-14",
    "categories": [],
    "contents": "\nHappy π day everybody!\nI wanted to write some simple code (included below) to the test parallelization capabilities of my  new cluster. So, in honor of  π day, I decided to check for evidence that π is a normal number. A normal number is a real number whose infinite sequence of digits has the property that picking any given random m digit pattern is 10−m. For example, using the Poisson approximation, we can predict that the pattern “123456789” should show up between 0 and 3 times in the first billion digits of π (it actually shows up twice starting, at the 523,551,502-th and  773,349,079-th decimal places).\nTo test our hypothesis, let Y1, …, Y100 be the number of “00”, “01”, …,“99” in the first billion digits of  π. If  π is in fact normal then the Ys should be approximately IID binomials with N=1 billon and p=0.01.  In the qq-plot below I show Z-scores (Y - 10,000,000) /  √9,900,000) which appear to follow a normal distribution as predicted by our hypothesis. Further evidence for π being normal is provided by repeating this experiment for 3,4,5,6, and 7 digit patterns (for 5,6 and 7 I sampled 10,000 patterns). Note that we can perform a chi-square test for the uniform distribution as well. For patterns of size 1,2,3 the p-values were 0.84, 0.89, 0.92, and 0.99.\n\nAnother test we can perform is to divide the 1 billion digits into 100,000 non-overlapping segments of length 10,000. The vector of counts for any given pattern should also be binomial. Below I also include these qq-plots.\n\nThese observed counts should also be independent, and to explore this we can look at autocorrelation plots:\n\nTo do this in about an hour and with just a few lines of code (included below), I used the Bioconductor Biostrings package to match strings and the foreach function to parallelize.\nlibrary(Biostrings)\nlibrary(doParallel)\nregisterDoParallel(cores = 48)\nx=scan(\"pi-billion.txt\",what=\"c\")\nx=substr(x,3,nchar(x)) ##remove 3.\nx=BString(x)\nn<-length(x)\np <- 1/(10^d)\npar(mfrow=c(2,3))\nfor(d in 2:4){\n    if(d<5){\n      patterns<-sprintf(paste0(\"%0\",d,\"d\"),seq(0,10^d-1))\n    } else{\n      patterns<-sprintf(paste0(\"%0\",d,\"d\"),sample(10^d,10^4)-1)\n    }\n    res <- foreach(pat=patterns,.combine=c) %dopar% countPattern(pat,x)\n    z <- (res - n*p ) / sqrt( n*p*(1-p) )\n    qqnorm(z,xlab=\"Theoretical quantiles\",ylab=\"Observed z-scores\",main=paste(d,\"digits\"))\n    abline(0,1)\n    ##correction: original post had length(res)\n    if(d<5) print(1-pchisq(sum ((res - n*p)^2/(n*p)),length(res)-1)) \n}\n###Now count in segments\nd <- 1\nm <-10^5\n\npatterns <-sprintf(paste0(\"%0\",d,\"d\"),seq(0,10^d-1))\nres <- foreach(pat=patterns,.combine=cbind) %dopar% {\n    tmp<-start(matchPattern(pat,x))\n    tmp2<-floor( (tmp-1)/m)\n    return(tabulate(tmp2+1,nbins=n/m))\n}\n##qq-plots\npar(mfrow=c(2,5))\np <- 1/(10^d)\nfor(i in 1:ncol(res)){\n    z <- (res[,i] - m*p) / sqrt( m*p*(1-p)  )\n     qqnorm(z,xlab=\"Theoretical quantiles\",ylab=\"Observed z-scores\",main=paste(i-1))\n    abline(0,1)\n}\n##ACF plots\npar(mfrow=c(2,5))\nfor(i in 1:ncol(res)) acf(res[,i])\nNB: A normal number has the above stated property in any base. The examples above a for base 10.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-03-12-oh-no-the-leekasso/",
    "title": "Oh no, the Leekasso....",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-03-12",
    "categories": [],
    "contents": "\nAn astute reader (Niels Hansen, who is visiting our department today) caught a bug in my code on Github for the Leekasso. I had:\nlm1 = lm(y ~ leekX)\npredict.lm(lm1,as.data.frame(leekX2))\nUnfortunately, this meant that I was getting predictions for the training set on the test set. Since I set up the test/training sets the same, this meant that I was actually getting training set error rates for the Leekasso. Neils Hansen noticed the bug and reran the fixed code with this term instead:\nlm1 = lm(y ~ ., data = as.data.frame(leekX))\npredict.lm(lm1,as.data.frame(leekX2))\nHe created a heatmap subtracting the average accuracy of the Leekasso/Lasso and showed they are essentially equivalent.\n\nThis is a bummer, the Leekasso isn’t a world crushing algorithm. On the other hand, I’m happy that just choosing the top 10 is still competitive with the optimized lasso on average. More importantly, although I hate being wrong, I appreciate people taking the time to look through my code.\nJust out of curiosity I’m taking a survey. Do you think I should publish this top10 predictor thing as a paper? Or do you think it is too trivial?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:12:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-03-07-per-capita-gdp-versus-years-since-women-received-right-to-vote/",
    "title": "Per capita GDP versus years since women received right to vote",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-03-07",
    "categories": [],
    "contents": "\nBelow is a plot of per capita GPD (in log scale) against years since women received the right to vote for 42 countries. Is this cause, effect, both or neither? We all know correlation does not imply causation, but I see many (non statistical) arguments to support both cause and effect here. Happy International Women’s Day ! \nThe data is from here and here. I removed countries where women have had the right to vote for less than 20 years.\npd -What’s with Switzerland?\nupdate - R^2 and p-value added to graph\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-03-05-plos-one-i-have-an-idea-for-what-to-do-with-all-your-profits-buy-hard-drives/",
    "title": "PLoS One, I have an idea for what to do with all your profits: buy hard drives",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-03-05",
    "categories": [],
    "contents": "\nI’ve been closely following the fallout from PLoS One’s new policy for data sharing. The policy says, basically, that if you publish a paper, all data and code to go with that paper should be made publicly available at the time of publishing and include an explicit data sharing policy in the paper they submit.\nI think the reproducibility debate is over. Data should be made available when papers are published. The Potti scandal and the Reinhart/Rogoff scandal have demonstrated the extreme consequences of lack of reproducibility and the reproducibility advocates have taken this one home. The question with reproducibility isn’t “if” anymore it is “how”.\nThe transition toward reproducibility is likely to be rough for two reasons. One is that many people who generate data lack training in handling and analyzing data, even in a data saturated field like genomics. The story is even more grim in areas that haven’t been traditionally considered “data rich” fields.\nThe second problem is a cultural and economic problem. It involves the fundamental disconnect between (1) the incentives of our system for advancement, grant funding, and promotion and (2) the policies that will benefit science and improve reproducibility. Most of the debate on social media seems to conflate these two issues. I think it is worth breaking the debate down into three main constituencies: journals, data creators, and data analysts.\nJournals with requirements for data sharing\nData sharing, especially for large data sets, isn’t easy and it isn’t cheap. Not knowing how to share data is not an excuse - to be a modern scientist this is one of the skills you have to have. But if you are a journal that makes huge profits and you want open sharing, you should put up or shut up. The best way to do that would be to pay for storage on something like AWS for all data sets submitted to comply with your new policy. In the era of cheap hosting and standardized templates, charging $1,000 or more for an open access paper is way too much. It costs essentially nothing to host that paper online and you are getting peer review for free. So you should spend some of your profits paying for the data sharing that will benefit your journal and the scientific community.\nData creators\nIt is really hard to create a serious, research quality data set in almost any scientific discipline. If you are studying humans, it requires careful adherence to rules and procedures for handling human data. If you are in ecology, it may involve extensive field work. If you are in behavioral research, it may involve careful review of thousands of hours of video tape.\nThe value of one careful, rigorous, and interesting data set is hard to overstate. In my field, the data Leonid Kruglyak’s group generated measuring gene expression and genetics in a careful yeast experiment spawned an entirely new discipline within both genomics and statistics.\nThe problem is that to generate one really good data set can take months or even years. It is definitely possible to publish more than one paper on a really good data set. But after the data are generated, most of these papers will have to do with data analysis, not data generation. If there are ten papers that could be published on your data set and your group publishes the data with the first one, you may get to the second or third, but someone else might publish 4-10.\nThis may be good for science, but it isn’t good for the careers of data generators. Ask anyone in academics whether you’d rather have 6 citations from awesome papers or 6 awesome papers and 100% of them will take the papers.\nI’m completely sympathetic to data generators who spend a huge amount of time creating a data set and are worried they may be scooped on later papers. This is a place where the culture of credit hasn’t caught up with the culture of science. If you write a grant and generate an amazing data set that 50 different people use - you should absolutely get major credit for that in your next grant. However, you probably shouldn’t get authorship unless you intellectually contributed to the next phase of the analysis.\nThe problem is we don’t have an intermediate form of credit for data generators that is weighted more heavily than a citation. In the short term, this lack of a proper system of credit will likely lead data generators to make the following (completely sensible) decision to hold their data close and then publish multiple papers at once - like ENCODE did. This will drive everyone crazy and slow down science - but it is the appropriate career choice for data generators until our system of credit has caught up.\nData analysts\nI think that data analysts who are pushing for reproducibility are genuine in their desire for reproducibility. I also think that the debate is over. I think we can contribute to the success of the reproducibility transition by figuring out ways to give stronger and more appropriate credit to data generators. I don’t think authorship is the right approach. But I do think that it is the right approach to loudly and vocally give credit to people who generated the data you used in your purely data analytic paper. That includes making sure the people that are responsible for their promotion and grants know just how incredibly critical it is that they keep generating data so you can keep doing your analysis.\nFinally, I think that we should be more sympathetic to the career concerns of folks who generate data. I have written methods and made the code available. I have then seen people write very similar papers using my methods and code - then getting credit/citations for producing a very similar method to my own. Being I’ve been closely following the fallout from PLoS One’s [new policy for data sharing](http://www.plos.org/data-access-for-the-open-access-literature-ploss-data-policy/). The policy says, basically, that if you publish a paper, all data and code to go with that paper should be made publicly available at the time of publishing and include an explicit data sharing policy in the paper they submit. like this is incredibly frustrating. If you’ve ever had that experience imagine what it would feel like to spend a whole year creating a data set and then only getting one publication.\nI also think that the primary use of reproducibility so far has been as a weapon. It has been used (correctly) to point out critical flaws in research. It has also been used as a way to embarrass authors who don’t (and even some who do) have training in data analysis. The transition to fully reproducible science can either be a painful fight or a smoother transition. One thing that would go a long way would be to think of code review/reproducibility not like peer review, but more like pull requests and issues on Github. The goal isn’t to show how the other person did it wrong, the goal is to help them do it right.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-02-26-data-science-is-hard-but-so-is-talking/",
    "title": "Data Science is Hard, But So is Talking",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-02-26",
    "categories": [],
    "contents": "\nJeff, Brian, and I had to record nine separate introductory videos for our Data Science Specialization and, well, some of us were better at it than others. It takes a bit of practice to read effectively from a teleprompter, something that is exceedingly obvious from this video.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-02-21-heres-why-the-scientific-publishing-system-can-never-be-fixed/",
    "title": "Here's why the scientific publishing system can never be \"fixed\"\n",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-02-21",
    "categories": [],
    "contents": "\nThere’s been much discussion recently about how the scientific publishing system is “broken”. Just the latest one that I saw was a tweet from Princeton biophysicist Josh Shaevitz:\n\n\nEditor at a ‘fancy’ journal to my postdoc “This is amazing work that will change the field. No one will publish it.” Sci. pubs are broken.\n\n\n— Joshua Shaevitz (@shaevitz) February 13, 2014\n\n\nOn this blog, we’ve talked quite a bit about the publishing system, including in this interview with Michael Eisen. Jeff recently posted about changing the reviewing system (again). We have a few other posts on this topic. Yes, we like to complain like the best of them.\nBut there’s a simple fact: The scientific publishing system, as broken as you may find it to be, can never truly be fixed.\nHere’s the tl;dr\nThe collection of scientific publications out there make up a marketplace of ideas, hypotheses, theorems, conjectures, and comments about nature.\nEach member of society has an algorithm for placing a value on each of those publications. Valuation methodologies vary, but they often include factors like the reputation of the author(s), the journal in which the paper was published, the source of funding, as well as one’s own personal beliefs about the quality of the work described in the publication.\nGiven a valuation methodology, each scientist can rank order the publications from “most valuable” to “least valuable”.\nFixing the scientific publication system would require forcing everyone to agree on the same valuation methodology for all publications.\nThe Marketplace of Publications\nThe first point is that the collection of scientific publications make up a kind of market of ideas. Although we don’t really “trade” publications in this market, we do estimate the value of each publication and label some as “important” and some as not important. I think this is important because it allows us to draw analogies with other types of markets. In particular, consider the following question: Can you think of a market in any item where each item was priced perfectly, so that every (rational) person agreed on its value? I can’t.\nConsider the stock market, which might be the most analyzed market in the world. Professional investors make their entire living analyzing the companies that are listed on stock exchanges and buying and selling their shares based on what they believe is the value of those companies. And yet, there can be huge disagreements over the valuation of these companies. Consider the current Herbalife drama, where investors William Ackman and Carl Icahn (and Daniel Loeb) are taking complete opposite sides of the trade (Ackman is short and Icahn is long). They can’t both be right about the valuation; they must have different valuation strategies. Everyday, the market’s collective valuation of different companies changes, reacting to new information and perhaps to irrational behavior. In the long run, good companies survive while others do not. In the meantime, everyone will argue about the appropriate price.\nJournals are in some ways like the stock exchanges of yore. There are very prestigious ones (e.g. NYSE, the “big board”) and there are less prestigious ones (e.g. NASDAQ) and everyone tries to get their publication into the prestigious journals. Journals have listing requirements–you can’t just put any publication in the journal. It has to meet certain standards set by the journal. The importance of being listed on a prestigious stock exchange has diminished somewhat over the years. The most valuable company in the world trades on the NASDAQ.  Similarly, although Science, Nature, and the New England Journal of Medicine are still quite sought after by scientists, competition is increasing from journals (such as those from the Public Library of Science) who are willing to publish papers that are technically correct and let readers determine their importance.\nWhat’s the “Fix”?\nNow let’s consider a world where we obliterate journals like Nature and Science and that there’s only the “one true journal”. Suppose this journal accepts any publication that satisfies some basic technical requirements (i.e. not content-based) and then has a sophisticated rating system that allows readers to comment on, rate, and otherwise evaluate each publication. There is no pre-publication peer review. Everything is immediately published. Problem solved? Not really, in my opinion. Here’s what I think would end up happening:\nPeople would have to (slightly) alter their methodology for ranking individual scientists. They would not be able to say “so-and-so has 10 Nature papers, so he must be good”. But most likely, another proxy for actually reading the appears would arise. For example, “My buddy from University of Whatever put this paper in his top-ten list, so it must be good”. As Michael Eisen said in our interview, the ranking system induced by journals like Science and Nature is just an abstract hierarchy; we can still reproduce the hierarchy even if Science/Nature don’t exist.\nIn the current system, certain publications often “get stuck” with overly inflated valuations and it is often difficult to effectively criticize such publications because there does not exist an equivalent venue for informed criticism on par with Science and Nature. These publications achieve such high valuations partly because they are published in high-end journals like Nature and Science, but partly it is because some people actually believe they are valuable. In other words, it is possible to create a “bubble” where people irrationally believe a publication is valuable, just because everyone believes it’s valuable. If you destroy the current publication system, there will still be publications that are “over-valued”, just like in every other market. And furthermore, it will continue to be difficult to criticize such publications. Think of all the analysts that were yelling about how the housing market was dangerously inflated back in 2007. Did anyone listen? Not until it was too late.\nWhat Can be Done?\nI don’t mean for this post to be depressing, but I think there’s a basic reality about publication that perhaps is not fully appreciated. That said, I believe there are things that can be done to improve science itself, as well as the publication system.\nRaise the ROC curves of science. Efforts in this direction make everyone better and improve our ability to make more important discoveries.\nIncrease the reproducibility of science. This is kind of the “Sarbanes-Oxley” of science. For the most part, I think the debate about whether science should be made more reproducible is coming to a close (or it is for me). The real question is how do we do it, for all scientists? I don’t think there are enough people thinking about this question. It will likely be a mix of different strategies, policies, incentives, and tools.\nDevelop more sophisticated evaluation technologies for publications. Again, to paraphrase Michael Eisen, we are better able to judge the value of a pencil on Amazon than we are able to judge a scientific publication. The technology exists for improving the system, but someone has to implement it. I think a useful system along these lines would go a long way towards de-emphasizing the importance of “vanity journals” like Nature and Science.\nMake open access more accessible. Open access journals have been an important addition to the publication universe, but they are still very expensive (the cost has just been shifted). We need to think more about lowering the overall cost of publication so that it is truly open access.\nUltimately, in a universe where there are finite resources, a system has to be developed to determine how those resources should be distributed. Any system that we can come up with will be flawed as there will by necessity have to be winners and losers. I think there are serious efforts that need to be made to make the system more fair and more transparent, but the problem will never truly be “fixed” to everyone’s satisfaction.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-02-19-why-do-we-love-r-so-much/",
    "title": "Why do we love R so much?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-02-19",
    "categories": [],
    "contents": "\nWhen Jeff, Brian, and I started the Johns Hopkins Data Science Specialization we decided early on to organize the program around using R. Why? Because we love R, we use it everyday, and it has an incredible community of developers and users. The R community has created an ecosystem of packages and tools that lets R continue to be relevant and useful for real problems.\nWe created a short video to talk about one of the reasons we love R so much.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-02-18-k-means-clustering-in-a-gif/",
    "title": "k-means clustering in a GIF",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-02-18",
    "categories": [],
    "contents": "\nk-means is a simple and intuitive clustering approach. Here is a movie showing how it works:\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-02-17-repost-ronald-fisher-is-one-of-the-few-scientists-with-a-legit-claim-to-most-influential-scientist-ever/",
    "title": "Repost: Ronald Fisher is one of the few scientists with a legit claim to most influential scientist ever",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-02-17",
    "categories": [],
    "contents": "\nEditor’s Note: Ronald  This is a repost of the post “R.A. Fisher is the most influential scientist ever” with a picture of my pilgrimage to his  gravesite in Adelaide, Australia. \nYou can now see profiles of famous scientists on Google Scholar citations. Here are links to a few of them (via Ben L.). Von Neumann, Einstein, Newton, Feynman\nBut their impact on science pales in comparison (with the possible exception of Newton) to the impact of one statistician: R.A. Fisher. Many of the concepts he developed are so common and are considered so standard, that he is never cited/credited. Here are some examples of things he invented along with a conservative number of citations they would have received calculated via Google Scholar*.\nP-values - 3 million citations\nAnalysis of variance (ANOVA) - 1.57 million citations\nMaximum likelihood estimation - 1.54 million citations\nFisher’s linear discriminant 62,400 citations\nRandomization/permutation tests 37,940 citations\nGenetic linkage analysis 298,000 citations\nFisher information 57,000 citations\nFisher’s exact test 237,000 citations\nA couple of notes:\nThese are seriously conservative estimates, since I only searched for a few variants on some key words\nThese numbers are BIG, there isn’t another scientist in the ballpark. The guy who wrote the “most highly cited paper” got 228,441 citations on GS. His next most cited paper? 3,000 citations. Fisher has at least 5 papers with more citations than his best one.\nThis page says Bert Vogelstein has the most citations of any person over the last 30 years. If you add up the number of citations to his top 8 papers on GS, you get 57,418. About as many as to the Fisher information matrix.\nI think this really speaks to a couple of things. One is that Fisher invented some of the most critical concepts in statistics. The other is the breadth of impact of statistical ideas across a range of disciplines. In any case, I would be hard pressed to think of another scientist who has influenced a greater range or depth of scientists with their work.\nUpdate: I recently when to Adelaide to give a couple of talks on Bioinformatics, Statistics and MOOCs. My host Gary informed me that Fisher was buried in Adelaide. I went to the cathedral to see the memorial and took this picture. I couldn’t get my face in the picture because the plaque was on the ground. You’ll have to trust me that these are my shoes.\n\n\n\nCalculations of citations #####################\nAs described in a previous post\n# of GS results for “Analysis of Variance” + # for “ANOVA” - “Analysis of Variance”\n# of GS results for “maximum likelihood”\n# of GS results for “linear discriminant”\n# of GS results for “permutation test” + # for ”permutation tests” - “permutation test”\n# of GS results for “linkage analysis”\n# of GS results for “fisher information” + # for “information matrix” - “fisher information”\n# of GS results for “fisher’s exact test” + # for “fisher exact test” - “fisher’s exact test”\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-02-14-on-the-scalability-of-statistical-procedures-why-the-p-value-bashers-just-dont-get-it/",
    "title": "On the scalability of statistical procedures: why the p-value bashers just don't get it.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-02-14",
    "categories": [],
    "contents": "\nExecutive Summary\nThe problem is not p-values it is a fundamental shortage of data analytic skill.\nIn general it makes sense to reduce researcher degrees of freedom for non-experts, but any choice of statistic, when used by many untrained people, will be flawed.\nThe long term solution is to require training in both statistics and data analysis for anyone who uses data but particularly journal editors, reviewers, and scientists in molecular biology, medicine, physics, economics, and astronomy.\nThe Johns Hopkins Specialization in Data Science runs every month and can be easily integrated into any program. Other, more specialized, online courses and short courses make it possible to round this training out in ways that are appropriate for each discipline.\nScalability of Statistical Procedures\nThe P-value is in the news again. Nature came out with a piece talking about how scientists are naive about the use of P-values among other things. P-values have known flaws which have been regularly discussed. If you want to see some criticisms just Google “NHST”. Despite their flaws, from a practical perspective it is and oversimplification to point to the use of P-values as the critical flaw in scientific practice. The problem is not that people use P-values poorly it is that the vast majority of data analysis is not performed by people properly trained to perform data analysis. \nData are now abundant in nearly every discipline from astrophysics, to biology, to the social sciences, and even in qualitative disciplines like literature. By scientific standards, the growth of data came on at a breakneck pace. Over a period of about 40 years we went from a scenario where data was measured in bytes to terabytes in almost every discipline. Training programs haven’t adapted to this new era. This is particularly true in genomics where within one generation we went from a data poor environment to a data rich environment. Executive Summary were trained before data were widely available and used.\nThe result is that the vast majority of people performing statistical and data analysis are people with only one or two statistics classes and little formal data analytic training under their belt. Many of these scientists would happily work with a statistician, but as any applied statistician at a research university will tell you, it is impossible to keep up with the demand from our scientific colleagues. Everyone is collecting major data sets or analyzing public data sets; there just aren’t enough hours in the day.\nSince most people performing data analysis are not statisticians there is a lot of room for error in the application of statistical methods. This error is magnified enormously when naive analysts are given too many “researcher degrees of freedom”. If a naive analyst can pick any of a range of methods and does not understand how they work, they will generally pick the one that gives them maximum benefit.\nThe short-term solution is to find a balance between researcher degrees of freedom and “recipe book” style approaches that require a specific method to be applied. In general, for naive analysts, it makes sense to lean toward less flexible methods that have been shown to work across a range of settings. The key idea here is to evaluate methods in the hands of naive users and see which ones work best most frequently, an idea we have previously called “evidence based data analysis”.\nAn incredible success story of evidence based data analysis in genomics is the use of the limma package for differential expression analysis of microarray data. Limma can be beat in certain specific scenarios, but it is robust to such a wide number of study designs, sample sizes, and data types that the choice to use something other than limma should only be exercised by experts.\nThe trouble with criticizing p-values without an alternative\nP-values are an obvious target of wrath by people who don’t do day to day statistical analysis because the P-value is the most successful statistical procedure ever invented. If every person who used a P-value cited the inventor, P-values would have, very conservatively, 3 million citations. That’s an insane amount of use for one statistic.\nWhy would such a terrible statistic be used by so many people? The reason is that it is critical that we have some measure of uncertainty we can assign to data analytic results. Without such a measure, the only way to determine if results are real or not is to rely on people’s intuition, which is a notoriously unreliable metric when uncertainty is involved. It is pretty clear science would be much worse off if we decided if results were reliable based on peoples’ gut feeling about the data.\nP-values can and are misinterpreted, misused, and abused both by naive analysts and by statisticians. Sometimes these problems are due to statistical naiveté, sometimes they are due to wishful thinking and career pressure, and sometimes they are malicious. The reason is that P-values are complicated and require training to understand.\nCritics of the P-value argue in favor of a large number of the procedures to be used in place of P-values. But when considering the scale at which the methods must be used to address the demands of the current data rich world, many alternatives would result in similar flaws. This is in no way proves the use of P-values is a good idea, but it does prove that coming up with an alternative is hard. Here are a few potential alternatives.\nMethods should only be chosen and applied by true data analytic experts. Pros: This is the best case scenario. Cons: Impossible to implement broadly given the level of statistical and data analytic expertise in the community \nThe full prior, likelihood and posterior should be detailed and complete sensitivity analysis should be performed. Pros: In cases where this can be done this provides much more information about the model and uncertainty being considered. Cons: The model requires more advanced statistical expertise, is computationally much more demanding, and can not be applied in problems where model based approaches have not been developed. Yes/no decisions about credibility of results still come down to picking a threshold or allowing more researcher degrees of freedom.\nA direct Bayesian approach should be used reporting credible intervals and Bayes estimators. Pros: In cases where the model can be fit, can be used by non-experts, provides scientific measures of uncertainty like confidence intervals. Cons: The prior allows a large number of degrees of freedom when not used by an expert, sensitivity analysis is required to determine the effect of the prior, many more complex models can not be implemented, results are still sample size dependent.\nReplace P-values with likelihood ratios. Pros: In cases where it is available may reduce some of the conceptual difficulty with the null hypothesis. Cons: Likelihood ratios can usually only be computed exactly for cases with few or no nuisance parameters, likelihood ratios run into trouble for complex alternatives, they are still sample size dependent, the a likelihood ratio threshold is equivalent to a p-value threshold in many cases.\nWe should use Confidence Intervals exclusively in the place of p-values.  Pros: A measure and variability on the scale of interest will be reported. We can evaluate effect sizes on a scientific scale.  Cons: Confidence intervals are still sample size dependent and can be misleading for large samples, significance levels can be chosen to make intervals artificially wide/small, if used as a decision making tool there is a one-to-one mapping between a confidence interval and a p-value threshold.\nWe should use Bayes Factors instead of p-values. Pros: They can compare the evidence (loosely defined) for both the null and alternative. They can incorporate prior information. Cons: Priors provide researcher degrees of freedom, cutoffs may still lead to false/true positives, BF’s still depend on sample size.\nThis is not to say that many of these methods have advantages over P-values. But at scale any of these methods will be prone to abuse, misinterpretation and error. For example, none of them by default deals with multiple testing. Reducing researcher degrees of freedom is good when dealing with a lack of training, but the consequence is potential for mistakes and all of these methods would be ferociously criticized if used as frequently as p-values.\nThe difference between data analysis and statistics\nMany disciplines including medicine and molecular biology usually require an introductory statistics or machine learning class during their program. This is a great start, but is not sufficient for the modern data saturated era. The introductory statistics or machine learning class is enough to teach someone the language of data analysis, but not how to use it. For example, you learn about the t-statistic and how to calculate it. You may also learn the asymptotic properties of the statistic. But you rarely learn about what happens to the t-statistic when there is an unmeasured confounder. You also don’t learn how to handle non iid data, sample mixups, reproducibility, most of scripting, etc.\nIt is therefore critical that if you plan to use or understand data analysis you take both the introductory course and at least one data analysis course. The data analysis course should cover study design, more general data analytic reasoning, non-iid data, biased sampling, basics of non-parametrics, training vs test sets, prediction error, sources of likely problems in data sets (like sample mixups), and reproducibility. These are the concepts that appear regularly when analyzing real data that don’t usually appear in the first course in statistics that most medical and molecular biology professionals see. There are awesome statistical educators who are trying hard to bring more of this into the introductory stats world, but it is just too much to cram into one class.\nWhat should we do\nThe thing that is the most frustrating about the frequent and loud criticisms of P-values is that they usually point out what is wrong with P-values, but don’t suggest what we should do about it.  When they do make suggestions, they frequently ignore the fundamental problems:\nStatistics are complicated and require careful training to understand properly. This is true regardless of the choice of statistic, philosophy, or algorithm.\nData is incredibly abundant in all disciplines and shows no sign of slowing down.\nThere is a fundamental shortage of training in statistics and data analysis \nGiving untrained analysts extra researcher degrees of freedom is dangerous.\nThe most direct solution to this problem is increased training in statistics and data analysis. Every major or program in a discipline that regularly analyzes data (molecular biology, medicine, finance, economics, astrophysics, etc.) should require at minimum an introductory statistics class and a data analysis class. If the expertise doesn’t exist to create these sorts of courses there are options. For example, we have introduced a series of 9 courses that run every month that cover most of the basic topics that are common across disciplines.\nhttp://jhudatascience.org/\nhttps://www.coursera.org/specialization/jhudatascience/1\nI think of particular interest given the NIH Director’s recent comments on reproducibility is our course on Reproducible Research. There are also many more specialized resources that are very good and widely available that will build on the base we created with the data science specialization.\nFor scientific software engineering/reproducibility: Software Carpentry.\nFor data analysis in genomics: Rafa’s Data Analysis for Genomics Class.\nFor Python and computing: The Fundamentals of Computing Specialization\nEnforcing education and practice in data analysis is the only way to resolve the problems that people usually attribute to P-values. In the short term, we should at minimum require all the editors of journals who regularly handle data analysis to show competency in statistics and data analysis.\n_Correction: _After seeing Katie K.’s comment on Facebook I concur that P-values were not directly referred to as “worse than useless”, so to more fairly represent the article, I have deleted that sentence.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-02-13-loess-explained-in-a-gif/",
    "title": "loess explained in a GIF",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-02-13",
    "categories": [],
    "contents": "\nLocal regression (loess) is one of the statistical procedures I most use. Here is a movie showing how it works\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-02-10-monday-datastatistics-link-roundup-11014/",
    "title": "Monday data/statistics link roundup (2/10/14)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-02-10",
    "categories": [],
    "contents": "\nI’m going to try Monday’s for the links. Let me know what you think.\nThe Guardian is reading our blog. A week after Rafa posts that everyone should learn to code for career preparedness, the Guardian gets on the bandwagon.\nNature Methods published a paper on a webtool for creating boxplots (via Simina B.). The nerdrage rivaled the quilt plot. I’m not opposed to papers like this being published, in fact it is an important part of making sure we don’t miss out on the good software when it comes. There are two important things to keep in mind though: (a) Nature Methods grades on a heavy “innovative” curve which makes it pretty hard to publish papers there, so publishing papers like this could cause frustration among people who would submit there and (b) if you use the boxplots from using this tool you must cite the relevant software that generated the boxplot.\nThis story about Databall (via Rafa.) is great, I love the way that it talks about statisticians as the leaders on a new data type. I also enjoyed reading the paper the story is about. One interesting thing about that paper and many of the papers at the Sloan Sports Conference is that the data are proprietary (via Chris V.) so the code/data/methods are actually not available for most papers (including this one). In the short term this isn’t a big deal, the papers are fun to read. In the long term, it will dramatically slow progress. It is almost always a bad long term strategy to make data private if the goal is to maximize value.\nThe P-value curve for fixing publication bias (via Rafa). I think it is an interesting idea, very similar to our approach for the science-wise false discovery rate. People who liked our paper will like the P-value curve paper. People who hated our paper for the uniformity under the null assumption will hate that paper for the same reason (via David S.)\nHopkins discovers bones are the best (via Michael R.).\nAwesome scientific diagrams in tex. Some of these are ridiculous.\nMary Carillo goes crazy on backyard badminton. This is awesome. If you love the Olympics and the internet, you will love this (via Hilary P.)\nB’more Biostats has been on a tear lately. I’ve been reading [I’m going to try Monday’s for the links. Let me know what you think.\nThe Guardian is reading our blog. A week after Rafa posts that everyone should learn to code for career preparedness, the Guardian gets on the bandwagon.\nNature Methods published a paper on a webtool for creating boxplots (via Simina B.). The nerdrage rivaled the quilt plot. I’m not opposed to papers like this being published, in fact it is an important part of making sure we don’t miss out on the good software when it comes. There are two important things to keep in mind though: (a) Nature Methods grades on a heavy “innovative” curve which makes it pretty hard to publish papers there, so publishing papers like this could cause frustration among people who would submit there and (b) if you use the boxplots from using this tool you must cite the relevant software that generated the boxplot.\nThis story about Databall (via Rafa.) is great, I love the way that it talks about statisticians as the leaders on a new data type. I also enjoyed reading the paper the story is about. One interesting thing about that paper and many of the papers at the Sloan Sports Conference is that the data are proprietary (via Chris V.) so the code/data/methods are actually not available for most papers (including this one). In the short term this isn’t a big deal, the papers are fun to read. In the long term, it will dramatically slow progress. It is almost always a bad long term strategy to make data private if the goal is to maximize value.\nThe P-value curve for fixing publication bias (via Rafa). I think it is an interesting idea, very similar to our approach for the science-wise false discovery rate. People who liked our paper will like the P-value curve paper. People who hated our paper for the uniformity under the null assumption will hate that paper for the same reason (via David S.)\nHopkins discovers bones are the best (via Michael R.).\nAwesome scientific diagrams in tex. Some of these are ridiculous.\nMary Carillo goes crazy on backyard badminton. This is awesome. If you love the Olympics and the internet, you will love this (via Hilary P.)\nB’more Biostats has been on a tear lately. I’ve been reading](http://lcolladotor.github.io/2014/02/05/DropboxAndGoogleDocsFromR/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+FellgernonBit+%28Fellgernon+Bit%29) on uploading files to Dropbox/Google drive from R, Mandy’s post explaining quantitative MRI, Yenny’s post on data sciences, John’s post on graduate school open houses, and Alyssa’s post on vectorization. If you like Simply Stats you should be following them on Twitter here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-02-05-just-a-thought-on-peer-reviewing-i-cant-help-myself/",
    "title": "Just a thought on peer reviewing - I can't help myself.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-02-05",
    "categories": [],
    "contents": "\nToday I was thinking about reviewing, probably because I was handling a couple of papers as AE and doing tasks associated with reviewing several other papers. I know that this is idle thinking, but suppose peer review was just a drop down ranking with these 6 questions.\nHow close is this paper to your area of expertise?\nDoes the paper appear to be technically right?\nDoes the paper use appropriate statistics/computing?\nIs the paper interesting to people in your area?\nIs the paper interesting to a broad audience?\nAre the appropriate data and code available?\nEach question would be rated on a 1-5 star scale. 1 stars = completely inadequate, 3 stars = acceptable, 5 stars = excellent. There would be an optional comments box that would only be used for major/interesting thoughts and anything that got above 3 stars for questions 2, 3, and 6 was published. Incidentally, you could do this for free on Github if the papers were written in markdown, that would reduce the substantial costs of open-access publishing.\nNo doubt peer review would happen faster this way. I was wondering, would it be any worse?\n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-02-04-my-online-course-development-workflow/",
    "title": "My Online Course Development Workflow",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-02-04",
    "categories": [],
    "contents": "\nOne of the nice things about developing 9 new courses for the JHU Data Science Specialization in a short period of time is that you get to learn all kinds of cool and interesting tools. One of the ways that we were able to push out so much content in just a few months was that we did most of the work ourselves, rather than outsourcing things like video production and editing. You could argue that this results in a poorer quality final product but (a) I disagree; and (b) even if that were true, I think the content is still valuable.\nThe advantage of learning all the tools was that it allowed for a quick turn-around from the creation of the lecture to the final exporting of the video (often within a single day). For a hectic schedule, it’s nice to be able to write slides in the morning, record some video in between two meetings in the afternoon, and the combine/edit all the video in the evening. Then if you realize something doesn’t work, you can start over the next day and have another version done in less than 24 hours.\nI thought it might be helpful to someone out there to detail the workflow and tools that I use to develop the content for my online courses.\nI use Camtasia for Mac to do all my screencasting/recording. This is a nice tool and I think has more features than your average screen recorder. That said, if you just want to record your screen on your Mac, you can actually use the built-in Quicktime software. I used to do all of my video editing in Camtasia but now it’s pretty much glorified screencasting software for me.\nFor talking head type videos I use my iPhone 5S mounted on a tripod. The iPhone produces surprisingly good 1080p HD 30 fps video and is definitely sufficient for my purposes (see here for a much better example of what can be done). I attach the phone to an Apogee microphone to pick up better sound. For some of the interviews that we do on Simply Statistics I use two iPhones (A 5S and a 4S, my older phone).\nTo record my primary sound (i.e. me talking), I use the Zoom H4N portable recorder. This thing is not cheap but it records very high-quality stereo sound. I can connect it to my computer via USB or it can record to a SD card.\nFor simple sound recording (no video or screen) I use Audacity.\nAll of my lecture videos are run through Final Cut Pro X on my 15-inch MacBook Pro with Retina Display. Videos from Camtasia are exported in Apple ProRes format and then imported into Final Cut. Learning FCPX is not for the faint-of-heart if you’re not used to a nonlinear editor (as I was not). I bought this excellent book to help me learn it, but I still probably only use 1% of the features. In the end using a real editor was worth it because it makes merging multiple videos much easier (i.e. multicam shots for screencasts + talking head) and editing out mistakes (e.g. typos on slides) much simpler. The editor in Camtasia is pretty good but if you have more then one camera/microphone it becomes infeasible.\nI have an 8TB Western Digital Thunderbolt drive to store the raw video for all my classes (and some backups). I also use two 1TB Thunderbolt drives to store video for individual classes (each 4-week class borders on 1TB of raw video). These smaller drives are nice because I can just throw them in my bag and edit video at home or on the weekend if I need to.\nFinished videos are shared with a Dropbox for Business account so that Jeff, Brian, and I can all look at each other’s stuff. Videos are exported to H.264/AAC and uploaded to Coursera.\nFor developing slides, Jeff, Brian, and I have standardized around using Slidify. The beauty of using slidify is that it lets you write everything in Markdown, a super simple text format. It also make it simpler to manage all the course material in Git/GitHub because you don’t have to lug around huge PowerPoint files. Everything is  a light-weight text file. And thanks to Ramnath’s incredible grit and moxie, we have handy tools to easily export everything to PDF and HTML slides (HTML slides hosted via GitHub Pages).\nThe first courses for the Data Science Specialization start on April 7th. Don’t forget to sign up!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-02-03-the-three-tables-for-genomics-collaborations/",
    "title": "The three tables for genomics collaborations",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-02-03",
    "categories": [],
    "contents": "\nCollaborations between biologists and statisticians are very common in genomics. For the data analysis to be fruitful, the statistician needs to understand what samples are being analyzed. For the analysis report to make sense to the biologist, it needs to be properly annotated with information such as gene names, genomic location, etc… In a recent post, Jeff laid out his guide for such collaborations, here I describe an approach that has helped me in mine.\nIn many of my past collaborations, sharing the experiment’s key information,  in a way that facilitates data analysis, turned out to be more time consuming than the analysis itself. One reason is that the data producers annotated samples in ways that was imposible to decipher without direct knowledge of the experiment (e.g using lab specific codes in the filenames, or colors in excel files).  In the early days of microarrays, a group of researchers noticed this problem and created a markup language to describe and communicate information about  experiments electronically.\nThe Bioconductor project took a less ambitious  approach and created classes specifically designed to store the minimal information needed to perform an analysis. These classes can be thought of as three tables, stored as flat text files, all of which are relatively easy to create for biologists.\nThe first table contains the experimental data with rows representing features (e.g. genes) and the columns representing samples.\nThe second table contains the sample information. This table contains a row for each column in the experimental data table. This table contains at least two columns. The first contains an identifier that can be used to match the rows of this table to the columns of the first table. The second contains the main outcome of interest, e.g. case or control, cancer or normal. Other commonly included columns are the filename of the original raw data associated with each row, the date the experiment was processed, and other information about the samples.\nThe third table contains the feature information. This table contains a row for each row in the experimental table. The table contains at least two columns. The first contains an identifier that can be used to match the rows of this table to the row of the first table. The second will contain an annotation that makes sense to biologists, e.g. a gene name. For technologies that are widely used (e.g. Affymetrix gene expression arrays) these table are readily available.\nWith these three relatively simple files in place less time is spent “figuring out” the data and the statisticians can focus their energy on data analysis while the biologists can focus their energy on interpreting the results. This approach seems to have been the inspiration for the MAGE-TAB format.\nNote that with newer technologies, statisticians prefer to get access to the raw data. In this case, instead of an experimental data table (table 1), they will want the original raw data files. The sample information then must contain a column with the filenames so that sample annotation can be properly matched.\nNB: These three tables are not a complete description of an experiment and are not intended as an alternative to standards such as MAGE and MIAME. But in many cases, they provide the very minimum information needed to carry out a rudimentary analysis. Note that Bioconductor provides tools to import information from MAGE-ML and other related formats.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-01-29-not-teaching-computing-and-statistics-in-our-public-schools-will-make-upward-mobility-even-harder/",
    "title": "Not teaching computing and statistics in our public schools will make upward mobility even harder",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-01-29",
    "categories": [],
    "contents": "\nIn his book Average Is Over, Tyler Cowen predicts that as automatization becomes more common, modern economies will eventually be composed of two groups: 1) a highly educated minority involved in the production of  automated services and 2) a vast majority earning very little but enough to consume some of the low-priced products created by group 1.  Not everybody will agree with this view, but we can’t ignore the fact that automatization has already eliminated many middle class jobs in, for example, manufacturing and the automotive industries. New technologies, such as driverless cars and online retailers, will very likely eliminate many more jobs (e.g. drivers and retail clerks) than they create (programmers and engineers).\nComputer literacy is essential for working with automatized systems. Programming and learning from data are perhaps the most useful skill for creating these systems. Yet the current default curriculum includes neither computer science nor statistics. At the same time, there are plenty of resources for motivated parents with means to get their children to learn these subjects. Kids whose parents don’t have the wherewithal to take advantage of these educational resources will be at an even greater disadvantage than they are today. This disadvantage is made worse by the fact that many of the aforementioned resources are free and open to the world  (Codeacademy, Khan Academy, EdX, and Coursera for example) which means that a large pool of students that previously had no access to this learning material will also be competing for group 1 jobs. If we want to level the playing field we should start by updating the public school curriculum so that, in principle, everybody has the opportunity to compete.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-01-28-marie-curie-says-stop-hating-on-quilt-plots-already/",
    "title": "Marie Curie says stop hating on quilt plots already.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-01-28",
    "categories": [],
    "contents": "\n\n“There are sadistic scientists who hurry to hunt down error instead of establishing the truth.” -Marie Curie (http://en.wikiquote.org/wiki/Marie_Curie)\n\nThanks to Kasper H. for that quote. I think it is a perfect fit for today’s culture of academic put down as academic contribution. One perfect example is the explosion of hate against the quilt plot. A quilt plot is a heatmap with several parameters selected in advance; that’s it. This simplification of R’s heatmap function appeared in the journal PLoS One. They say (though not up front and not clearly enough for my personal taste) that they know it is just a heatmap.\nOver the course of the next several weeks quilt plots went viral. Here are a few example tweets. It was also > “There are sadistic scientists who hurry to hunt down error instead of establishing the truth.” -Marie Curie (http://en.wikiquote.org/wiki/Marie_Curie) on people’s blogs and even in the scientist. So I did an experiment. I built a table of frequencies in R like this and applied the heatmap function in R, then the quilt.plot function in fields, then the function written by the authors of the paper with as minimal tweeking as possible.\nset.seed(12345)\nlibrary(fields)\nx = matrix(rbinom(25,size=4,prob=0.5),nrow=5)\npt = prop.table(x)\nheatmap(pt)\nquilt.plot(x=rep(1:5,5),y=rep(1:5,5),z=pt)\nquilt(pt,1:5,1:5,zlabel=\"Proportion\")\n\nHere are the results:\nheatmap\n\nquilt.plot\n\nquilt\n\nIt is clear that out of the box and with no tinkering, the new plot makes something nicer/more interpretable. The columns/rows are where I expect and the scale is there and nicely labeled. Everyone who has ever made heatmaps in R has some bit of code that looks like this:\nimage(t(bdat)[,nrow(bdat):1],col=colsb(9),breaks=quantile(as.vector(as.matrix(dat)),probs=seq(0,1,length=10)),xaxt=\"n\",yaxt=\"n\",xlab=\"\",ylab=\"\")\n\n\nTo hack together a heatmap in R that looks like you expect. It is a total pain. Obviously the quilt plot paper has a few flaws:\nIt tries to introduce the quilt plot as a new idea.\nIt doesn’t just come out and say it is a hack of the heatmap function, but tries to dance around it.\nIt produces code, but only as images in word files. I had to retype the code to make my plot.\nThat being said here are a couple of other true things about the paper:\nThe code works if you type it out and apply it.\nThey produced code.\nThe paper is open access.\nThe paper is correct technically.\nThe hack is useful for users with few R skills.\nSo why exactly isn’t it a paper? It smacks of academic elitism to claim that this isn’t good enough because it isn’t a “new idea”. Not every paper discovers radium. Some papers are better than others and that is ok. I think the quilt plot being published isn’t a problem, maybe I don’t like the way it is written exactly, but they do acknowledge the heat map, they do produce correct, relevant code, and it does solve a problem people actually have. That is better than a lot of papers that appear in more prestigious journals. Arsenic life anyone?\nI think it is useful to have a forum where people can post correct, useful, but not necessarily ground breaking results and get credit for them, even if the credit is modest. Otherwise we might miss out on useful bits of code. Frank Harrell has a bunch of functions that tons of people use but he doesn’t get citations, you probably have heard of the Hmisc package if you use R.\nBut did you know Karl Broman has a bunch of really useful functions in his personal R package, qqline2 is great. I know Rafa has a bunch of functions he has never published because they seem “too trivial” but I use them all the time. Every scientist who touches code has a personal library like this. I’m not saying the quilt plot is in that category. But I am saying that it is stupid not to have a public forum for making these functions available to other scientists. But that won’t happen if the “quilt plot backlash” is what people see when they try to get published credit for simple code that solves real problems.\nHacks like the quilt plot can help people who aren’t comfortable with R write reproducible scripts without having to figure out every plotting parameter. Keeping in mind that the vast majority of data analysis is not done by statisticians, it seems like these little hacks are an important part of science. If you believe in figshare, github, open science, and shareable code, you shouldn’t be making fun of the quilt plotters.\nMarie Curie says so.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-01-28-swirl-2/",
    "title": "Announcing the Release of swirl 2.0",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-01-28",
    "categories": [],
    "contents": "\nEditor’s note: This post was written by Nick Carchedi, a Master’s degree student in the Department of Biostatistics at Johns Hopkins. He is working with us to develop the Data Science Specialization as well as software for interactive learning of R and statistics.\nOfficial swirl website: swirlstats.com\nOn September 27, 2013, I wrote a guest blog post on Simply Statistics to announce the creation of Statistics with Interactive R Learning (swirl), an R package for teaching and learning statistics and R simultaneously and interactively. Over the next several months, I received a tremendous amount of feedback from all over the world. Two things became clear: 1) there were many opportunities for improvement on the original design and 2) there’s an incredible demand globally for new and better ways of learning statistics and R.\nIn the spirit of R and open source software, I shared the source code for swirl on GitHub. As a result, I quickly came in contact with several very talented individuals, without whom none of what I’m about to share with you would have been possible. Armed with invaluable feedback and encouragement from early adopters of swirl 1.0, my new team and I pursued a complete overhaul of the original design.\nToday, I’m happy to announce the result of our efforts: swirl 2.0.\nLike the first version of the software, swirl 2.0 guides students through interactive tutorials in the R console on a variety of topics related to statistics and R. The user selects from a menu of courses, each of which is broken up by topic into shorter lessons. Lessons, in turn, are a dialog between swirl and the user and are composed of text output, multiple choice and text-based questions, and (most importantly) questions that require the user to enter actual R code at the prompt. Responses are evaluated for correctness based on instructor-specified answer tests and appropriate feedback is given immediately to the user.\nIt’s helpful to think of swirl as the synthesis of two separate parts: content and platform. Content is authored by instructors in R Markdown files. The platform is then responsible for delivering this content to the user and interpreting the user’s responses in an interactive and engaging way.\nOur primary focus for swirl 2.0 was to build a more robust and extensible platform for delivering content. Here’s a (nontechnical) summary of new and revised features:\nA library of answer tests an instructor can deploy to check user input for correctness\nIf stuck, a user can skip a question, causing swirl to enter the correct answer on their behalf\nDuring a lesson, a user can pause instruction to play around or practice something they just learned, then use a special keyword to regain swirl’s attention when ready to resume\nswirl “sees” user input the same way R “sees” it, which allows swirl to understand the composition of a user’s input on a much deeper level (thanks, Hadley)\nUser progress is saved between sessions\nMore readable output that adjusts to the width of the user’s display (thanks again, Hadley)\nExtensible framework allows others to easily extend swirl’s functionality\nInstructors can author content in a special flavor of R markdown\n(For a more technical understanding of swirl’s features and inner workings, we encourage readers to consult our GitHub repository.)\nAlthough improving the platform was our first priority for this release, we’ve made some improvements to existing content and, more importantly, added the beginnings of a new course: Intro to R. Intro to R is our response to the overwhelming demand for a more accessible and interactive way to learn the R language. We’ve included the first three lessons of the course and plan to add many more over the coming months as our focus turns to creating more high quality content.\nOur ultimate goal is to have the statistics and R communities use swirl as a platform to deliver their own content to students interactively. We’ve heard from many people who have an interest in creating their own content and we’re working hard to make the process of creating content as simple and enjoyable as possible.\nThe goal of swirl is not to be flashy, but rather to provide the most authentic learning environment possible. We accomplish this by placing students directly on the R prompt, within the very same environment they’ll use for data analysis when they are not using swirl. We hope you find swirl to be a valuable tool for learning and teaching statistics and R.\nIt’s important to stress that, as with any new software, we expect there will be bugs. At this point, users should still consider themselves “early adopters”. For bug reports, suggested enhancements, or to learn more about swirl, please visit our website.\nContributors:\nMany people have contributed to this project, either directly or indirectly, since its inception. I will attempt to list them all here, in no particular order. I’m sincerely grateful to each and everyone one of you.\nBill & Gina: swirl is as much theirs as it is mine at this point. Their contributions are the only reason the project has evolved so much since the release of swirl 1.0.\nBrian: Challenged me to turn my idea for swirl into a working prototype. Coined the “swirl” acronym. swirl would still be an idea in my head without his encouragement.\nJeff: Pushes me to think big picture and provides endless encouragement. Reminds me that a great platform is worthless without great content.\nRoger: Encouraged me to separate platform and content, a key paradigm that allowed swirl to mature from a messy prototype to something of real value. Introduced me to Git and GitHub.\nLauren & Ethan: Helped with development of the earliest instructional content.\nRamnath: Provided a model for content authoring via slidify “flavor” of R Markdown.\nHadley: Made key suggestions for improvement and provided an important proof of concept. His work has had a profound influence on swirl’s development.\nPeter: Our discussions led to a better understanding of some key ideas behind swirl 2.0.\nSally & Liz: Beta testers and victims of my endless rants during stats tutoring sessions.\nKelly: Most talented graphic designer I know and mastermind behind the swirl logo. First line of defense against bad ideas, poor design, and crappy websites. Visit her website.\nMom & Dad: Beta testers and my #1 fans overall.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-01-21-the-johns-hopkins-data-science-specialization-on-coursera/",
    "title": "The Johns Hopkins Data Science Specialization on Coursera",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-01-21",
    "categories": [],
    "contents": "\nWe are very proud to announce the the Johns Hopkins Data Science Specialization on Coursera. You can see the official announcement from the Coursera folks here. This is the main reason Simply Statistics has been a little quiet lately.\nThe three of us (Brian Caffo, Roger Peng, and Jeff Leek) along with a couple of incredibly hard working graduate students (Nick Carchedi of swirl fame and Sean Kross) have put together nine new one-month classes to run on the Coursera platform. The classes are:\nThe Data Scientist’s Toolbox  - A basic introduction to data and data science and a  basic guide to R/Rstudio/Github/Command Line Interface.\nR Programming  - Introduction to R programming, from installing R to types, to functions, to control structures.\nGetting and Cleaning Data - An introduction to getting data from the web, from images, from APIs, and from databases. The course also covers how to go from raw data to tidy data.\n\nExploratory Data Analysis - This course covers plotting in base graphics, lattice, ggplot2 and clustering and other exploratory techniques. It also covers how to think about exploring data you haven’t seen.\n\n\nReproducible Research  - This is one of the unique courses to our sequence. It covers how to think about reproducible research, evidence based data analysis, reproducible research checklists and knitr, markdown, R markdown, etc.\n\n\nStatistical Inference  - This course covers the fundamentals of statistical inference from a practical perspective. The course covers both the technical details and important ideas like confounding.\n\n\nRegression Models  - This course covers the fundamentals of linear and generalized linear regression modeling. It also serves as an introduction to how to “think about” relating variables to each other quantitatively.\n\n\nPractical Machine Learning  - This course will cover the basic conceptual ideas in machine learning like in/out of sample errors, cross validation, and training and test sets. It will also cover a range of machine learning algorithms and their practical implementation.\n\n\nDeveloping Data Products  - This course will cover how to develop tools for communicating data, methods, and analyses with other people. It will cover building R packages, Shiny, and Slidify, among other things.\n\nThere will also be a specialization project - consisting of a 10th class where students will work on projects conducted with industry, government, and academic partners.\nThe classes represent some of the content we have previously covered in our popular Coursera classes and a ton of brand new content for this specialization. Here are some things that I think make our program stand out:\nWe will roll out 3 classes at a time starting in April. Once a class is running, it will run every single month concurrently.\nThe specialization offers a bunch of unique content, particularly in the courses Getting and Cleaning Data, Reproducible Research, and Developing Data Products.\nAll of the content is being developed open source and open-access on Github. You are welcome to check it out as we develop it and contribute!\nYou can take the first 9 courses of the specialization entirely for free.\nYou can choose to pay a very modest fee to get “Signature Track” certification in every course.\nI have also created a little page that summarizes some of the unique aspects of our program. Scroll through it and you’ll find sharing links at the bottom. Please share with your friends, we think this is pretty cool: http://jhudatascience.org\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-01-19-sunday-datastatistics-link-roundup-1192014/",
    "title": "Sunday data/statistics link roundup (1/19/2014)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-01-19",
    "categories": [],
    "contents": "\nTesla is hiring a data scientist. That is all.\nI’m not sure I buy the idea that Python is taking over for R among people who actually do regular data science.  I think it is still context dependent. A huge fraction of genomics happens in R and there is a steady stream of new packages that allow R users to push farther and farther back into the processing pipeline. On the other hand, I think language diversity is clearly a plus for someone who works with data. Not that I’d know…\nThis is an awesome talk on why to pursue a Ph.D.. It gives a really level headed and measured discussion, specifically focused on computational programs (I think I got to it via Alyssa F.’s blog).\nEn Español - A blog post about a study of genetic risk factors among Hispanic/Latino populations (via Rafa).\nWhere have all the tenured women gone? This is a major issue and deserves much more press than it gets (via Sherri R.).\nNot related to statistics really, but these image captures from Google streetview are wild. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-01-17-missing-not-at-random-data-makes-some-facebook-users-feel-sad/",
    "title": "Missing not at random data makes some Facebook users feel sad",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-01-17",
    "categories": [],
    "contents": "\nThis article, published last week, explained how “some younger users of Facebook say that using the site often leaves them feeling sad, lonely and inadequate”. Being a statistician gives you an advantage here because we know that naive estimates from missing not at random (MNAR) data can be very biased. The posts you see on Facebook are not a random sample from your friends’ lives. We see pictures of their vacations, abnormally flattering pictures of themselves, reports on their major achievements, etc… but no view of the mundane typical daily occurrences. Here is a simple cartoon explanation of how MNAR data can give you a biased view of whats really going on. Suppose your life occurrences are rated from 1 (worst) to 5 (best), this table compares what you see to what is really going on after 15 occurrences:\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:11:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-01-16-edge-org-asks-famous-scientists-what-scientific-concept-to-throw-out-they-say-statistics/",
    "title": "edge.org asks famous scientists what scientific concept to throw out &#038; they say statistics",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-01-16",
    "categories": [],
    "contents": "\nI don’t think I’ve ever been forwarded one link on the web more than I have been forwarded the edge.org post on “What scientific idea is ready for retirement?”. Here are a few of the comments with my responses. I’m going to keep them brief because I think the edge.org crowd pushes people to say outrageous things, so it isn’t even clear they mean what they say.\nI think the whole conceit of the question is a little silly. If you are going to retire a major scientific idea you better have a replacement or at least a guess at what we could do next. The question totally ignores the key question of: “Suppose we actually did what you suggested, what would we do instead?”\nOn getting rid of big clinical trials\n\nIt is a commonly held but erroneous belief that a larger study is always more rigorous or definitive than a smaller one, and a randomized controlled trial is always the gold standard . However, there is a growing awareness that size does not always matter and a randomized controlled trial may introduce its own biases. We need more creative experimental designs.\n\nMy response: Yes clinical trials work. Yes bigger trials and randomized trials are more definitive. There is currently no good alternative for generating causal statements that doesn’t require quite severe assumptions. The “creative experimental designs” has serious potential to be abused by folks who say things like “Well my friend Susie totally said that diet worked for her…”. The author says we should throw out RCT with all the benefits they have provided because it is hard to get women to adhere to a pretty serious behavioral intervention over an 8 year period. If anything, this makes us consider what is a reasonable intervention, not the randomized trial part.\nOn bailing on statistical independence assumptions\n\nIt is time for science to retire the fiction of statistical independence…..So the overwhelming common practice is simply to assume that sampled events are independent. An easy justification for this is that almost everyone else does it and it’s in the textbooks. This assumption has to be one of the most widespread instances of groupthink in all of science.\n\nMy response: There are a huge number of statistical methods for dealing with non-independent data. Statisticians have been working on this for decades with blocking, stratification, random effects, deep learning, multilevel models, GEE, Garch models, etc. etc., etc. It’s a fact that statistical independence is a fiction, but sometimes it is a useful one.\nOn bailing on the p-value (or any other standardized statistical procedure)\n\nNot for a minute should anyone think that this procedure has much to do with statistics proper… A 2011 paper in_Nature Neuroscience_ presented an analysis of neuroscience articles in Science, Nature, Nature Neuroscience, Neuron and The Journal of Neuroscience showed that although 78 did as they should, 79 used the incorrect procedure.\n\nMy response: P-values on their own and P-values en-masse are both annoying and not very helpful. But we need a way to tell whether those effect sizes you observed are going to replicate or not. P-values are probably not the best thing for measuring that (maybe you should try to estimate it directly?). But any procedure you scale up to 100,000’s of thousands of users is going to cause all sorts of problems. If you give people more dimensions to call their result “real” or “significant” you aren’t going to reduce false positives. At scale we need fewer researcher degrees of freedom not more.\nOn science not being self-correcting\n\nThe pace of scientific production has quickened, and self-correction has suffered. Findings that might correct old results are considered less interesting than results from more original research questions. Potential corrections are also more contested. As the competition for space in prestigious journals has become increasingly frenzied, doing and publishing studies that would confirm the rapidly accumulating new discoveries, or would correct them, became a losing proposition. ublic registration of the design and analysis plan of a study before it is begun. Clinical trials researchers have done this for decades, and in 2013 researchers in other areas rapidly followed suit. Registration includes the details of the data analyses that will be conducted, which eliminates the former practice of presenting the inevitable fluctuations of multifaceted data as robust results. Reviewers assessing the associated manuscripts end up focusing more on the soundness of the study’s registered design rather than disproportionately favoring the findings. This helps reduce the disadvantage that confirmatory studies usually have relative to fishing expeditions. Indeed, a few journals have begun accepting articles from well-designed studies even before the results come in.\n\nWait, I thought there was a big rise in retraction rates that has everyone freaking out. Isn’t there a website just dedicated to outing and shaming people who retract stuff?  I think registry of study designs for confirmatory research is a great idea. But I wonder what the effect would be on reducing the opportunity for scientific mistakes that turn into big ideas. This person needs to read the ROC curves of science. Any basic research system that doesn’t allow for a lot of failure is never going to discover anything interesting.\nBig effects are due to multiple small effects\n\nSo, do big effects tend to have big explanations, or many explanations? There is probably no single, simple and uniformly correct answer to this question. (It’s a hopeless tree!) But, we can use a simple model to help make an educated guess.\n\nThe author simulates 200 variables each drawn from a N(0,i) for i=1…5. The author finds that most of the largest values come from the N(0,5) not the N(0,1). This says nothing about simple or complex phenomena. It says a lot about how a N(0,5) is more variable than a N(0,1). This does not address the issue of whether hypotheses are correct or not.\nBonus round: On abandoning evolution\n\nIntelligent design and other Creationist critiques have been easily shrugged off and the facts of evolution well established in the laboratory, fossil record, DNA record and computer simulations. If evolutionary biologists are really Seekers of the Truth, they need to focus more on finding the mathematical regularities of biology, following in the giant footsteps of Sewall Wright, JBS Haldane, Ronald  Fisher and so on.\n\nAmong many other things, this person needs a course in statistics. The people he is talking about focused on quantifying uncertainty about biology, not certainty or mathematical regularity.\nOne I actually agree with: putting an end to the idea that Big Data solves all problems\n\nNo, I don’t literally mean that we should stop believing in, or collecting, Big Data. But we should stop pretending that Big Data is magic.\n\nThat guy must be reading our blog. The key word in data science is science, after all.\nOn focusing on the variance rather than the mean\n\nOur focus on averages should be retired. Or, if not retired, we should give averages an extended vacation. During this vacation, we should catch up on another sort of difference between groups that has gotten short shrift: we should focus on comparing the difference in variance (which captures the spread or range of measured values) between groups.\n\nI actually like most of this article, but the format for the edge.org pieces killed it. The author says we should stop caring about the mean or make it secondary. I completely agree we should consider the variance - the examples he points out are great. But we should also always keep in mind the first moment before we move on to the second, so not “retire” just “add to”.\n \n No one asked me but here is what I’d throw out\nSweeping generalizations without careful theory, experimentation, and good data\nOversimplifying questions that don’t ask for potential solutions that deal with the complexity of the real world.\nSensationalism by scientists about science\nSensationalism by journalists about science\nAbsolutist claims about uncertain data\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-01-13-sunday-datastatistics-link-roundup-1132014/",
    "title": "Sunday data/statistics link roundup (1/12/2014)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-01-13",
    "categories": [],
    "contents": "\nWell it technically is Monday, but I never went to sleep so that still counts as Sunday right?\nAs a person who has taught a couple of MOOCs I’m used to getting some pushback from people who don’t like the whole concept. But I’m still happy that I’m not the only one who thinks they are a pretty good idea and still worth doing. I think that both the hype and the backlash are too much. They hype claimed it would completely end the university as we know it. The backlash says it will have no impact. I think more likely it will have a major impact on people who traditionally don’t attend colleges. That’s ok with me. I think this post gets it about right.\nThe Leekasso is finally dethroned! Korbinian Strimmer used my simulation code and compared it to CAT scores in the sda package coupled with Higher Criticism feature selection. Here is the accuracy plot. Looks like Leekasso is competitive with CAT-Leekasso, but CAT+HC wins. Big win for Github there and thanks to Korbinian for taking the time to do the simulation!\nJack Andraka is getting some pushback from serious scientists on the draft of his paper describing the research he outlined in his TED talk. He is taking the criticism like a pro, which says a lot about the guy. From reading the second hand reviews, it sounds like his project was like most good science projects  - it made some interesting progress but needs a lot of grinding before it turns into something real. The hype made it sound too good to be true. I hope that he will just ignore the hype machine from here on in and keep grinding (via Rafa).\nI’ve probably posted this before, but here is the illustrated guide to a Ph.D. Lest you think that little bump doesn’t matter, don’t forget to scroll to the bottom and read this.\nThe bmorebiostat bloggers (http://bmorebiostat.com/), if you aren’t following them, you should be.\nPotentially cool website for accessing treasury data.\nOk its 5am. I need a githug and then off to bed.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-01-08-the-top-10-predictor-takes-on-the-debiased-lasso-still-the-champ/",
    "title": "The top 10 predictor takes on the debiased Lasso - still the champ!",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-01-08",
    "categories": [],
    "contents": "\nAfter reposting on the comparison between the lasso and the always top 10 predictor (leekasso) I got some feedback that the problem could be I wasn’t debiasing the Lasso (thanks Tim T. on Twitter!). The idea behind debiasing (as I understand it) is to use the Lasso to do feature selection and then fit model without shrinkage to “debias” the coefficients. The debiased model is then used for prediction. Noah Simon, who knows approximately infinitely more about this than I do, kindly provided some code for fitting a debiased Lasso. He is not responsible for any mistakes/silliness in the simulation, he was just nice enough to provide some debiased Lasso code. He mentions a similar idea appears in the relaxo package if you set .\nI used the same simulation set up as before and tried out the Leekasso, the Lasso and the Debiased Lasso. Here are the accuracy results (more red = higher accuracy):\n\n\n\n\nThe results suggest the debiased Lasso still doesn’t work well under this design. Keep in mind as I mentioned in my previous post that the Lasso may perform better under a different causal model.\n\n\nUpdate:  Code available here on Github if you want to play around.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-01-07-preparing-for-tenure-track-job-interviews-2/",
    "title": "Preparing for tenure track job interviews",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2014-01-07",
    "categories": [],
    "contents": "\nEditor’s note: This is a slightly modified version of a previous post.\nIf you are in the job market you will soon be receiving (or already received) an invitation for an interview. So how should you prepare?  You have two goals. The first is to make a good impression. Here are some tips:\nDuring your talk, do NOT go over your allotted time. Practice your talk at least twice. Both times in front of a live audiences that asks questions.\nKnow your audience. If it’s a “math-y” department, give a more “math-y” talk. If it’s an applied department, give a more applied talk. But (sorry for the cliché) be yourself. Don’t pretend to be interested in something you are not as this almost always backfires.\nLearn about the faculty’s research interests. This will help during the one-on-one meetings.\n Be ready to answer the question “what do you want to teach?” and “where do you see yourself in five years?”\nI can’t think of any department where it is necessary to wear a suit (correct me if I’m wrong in the comments). In some places you might feel uncomfortable wearing a suit while those interviewing you are in shorts and t-shirt.\nSecond, and just as important, you want to figure out if you like the department you are visiting. Do you want to spend the next 5, 10, 50 years there?  Make sure to find out as much as you can to answer this question. Some questions are more appropriate for junior faculty, the more sensitive ones for the chair. Here are some example questions I would ask:\nWhat are the expectations for promotion? Would you promote someone publishing exclusively in subject matter journals such as Nature, Science, Cell, PLoS Biology, American Journal of Epidemiology ? Somebody publishing exclusively in Annals of Statistics? Is being a PI on an R01 a requirement for tenure?\nWhat are the expectations for teaching/service/collaboration? How are teaching and committee service assignments made?\nHow did you connect with your collaborators? How are these connections made?\nWhat percent of my salary am I expected to cover? Is it possible to do this by being a co-investigator?\nWhere do you live? How are the schools? How is the commute?\nHow many graduate students does the department have? How are graduate students funded? If I want someone to work with me, do I have to cover their stipend/tuition?\nHow is computing supported? This varies a lot from place to place. Some departments share amazing systems. Ask how costs are shared? How is the IT staff? Is R supported? In others you might have to buy your own hardware. Get all the details.\nSpecific questions for the junior Faculty:\nAre the expectations for promotion made clear to you? Do you get feedback on your progress? Do the senior faculty mentor you? Do the senior faculty get along? What do you like most about the department? What can be improved? In the last 10 years, what percent of junior faculty get promoted?\nQuestions for the chair:\nWhat percent of my salary am I expected to cover? How soon? Is their bridge funding? What is a standard startup package? Can you describe the promotion process in detail? What space is available for postdocs? (for hard money place) I love teaching, but can I buy out teaching with grants?\nI am sure I missed stuff, so please comment away….\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-01-05-sunday-datastatistics-link-roundup-1514/",
    "title": "Sunday data/statistics link roundup (1/5/14)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-01-05",
    "categories": [],
    "contents": "\nIf you haven’t seen lolmythesis it is pretty incredible. 1-2 line description of thesis projects. I think every student should be required to make one of these up before they defend. The best I could come up with for mine is, “We built a machine sensitive enough to measure the abundance of every gene in your body at once; turns out it measures other stuff too.”\nAn interesting article about how different direct to consumer genetic tests give different results. It doesn’t say, but it would be interesting if the raw data were highly replicable and the interpretations were different. If the genotype calls themselves didn’t match up that would be much worse on some level. I agree people have a right to their genetic data. On the other hand, I think it is important to remember that even people with Ph.D’s and 15 years experience have trouble interpreting the results of a GWAS. To assume the average individual will understand their genetic risk is seriously optimistic (via Rafa).\nThe 10 commandments of egoless programming.These are so important on big collaborative projects like my group has been working on the last year or so. Fortunately my students and postdocs are much better at being egoless than I am (I am an academic with a blog so it isn’t like you couldn’t see the ego coming ).\nThis is a neat post on parsing and analyzing data from a Garmin. The analysis even produces an automated report! I love it when people do cool things like this with their own data in R.\nSuper interesting advice page for potential graduate students from a faculty member at Duke Biology. This is particularly interesting in light of the ongoing debate about the viability of the graduate education pipeline highlighted in this recent article. I think it is important for graduate students in Ph.D. programs to know that not every student goes to an academic position. This has been true for a long time in Biostatistics, where many people end up in industry positions. That also means it is the obligation of Ph.D. programs to prepare students for a variety of jobs. Fortunately, most Ph.D.s in Biostatistics have experience processing data, working with collaborators, and developing data products so are usually also really prepared for industry.\nThis old video of Tukey and Friedman is awesome and mind-blowing (via Mike L.).\nCool site that lets you try to balance Baltimore’s budget. This type of thing would be even cooler if there were Github like pull requests where you could make new suggestions as well.\nMy student Alyssa has a very interesting post on teaching R to a non-programmer in one hour. Take the Frazee Challenge and list what you would teach.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-01-04-repost-prediction-the-lasso-vs-just-using-the-top-10-predictors/",
    "title": "Repost: Prediction: the Lasso vs. just using the top 10 predictors",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2014-01-04",
    "categories": [],
    "contents": "\nEditor’s note: This is a previously published post of mine from a couple of years ago (!). I always thought about turning it into a paper. The interesting idea (I think) is how the causal model matters for whether the lasso or the marginal regression approach works better. Also check it out, the Leekasso is part of the SuperLearner package.\nOne incredibly popular tool for the analysis of high-dimensional data is the lasso. The lasso is commonly used in cases when you have many more predictors than independent samples (the n « p) problem. It is also often used in the context of prediction.\nSuppose you have an outcome Y and several predictors X1,…,XM, the lasso fits a model:\nY = B0 + B1 X1 + B2 X2 + … + BM XM + E\nsubject to a constraint on the sum of the absolute value of the B coefficients. The result is that: (1) some of the coefficients get set to zero, and those variables drop out of the model, (2) other coefficients are “shrunk” toward zero. Dropping some variables is good because there are a lot of potentially unimportant variables. Shrinking coefficients may be good, since the big coefficients might be just the ones that were really big by random chance (this is related to Andrew Gelman’s type M errors).\nI work in genomics, where n«p problems come up all the time. Whenever I use the lasso or when I read papers where the lasso is used for prediction, I always think: “How does this compare to just using the top 10 most significant predictors?” I have asked this out loud enough that some people around here started calling it the “Leekasso” to poke fun at me. So I’m going to call it that in a thinly veiled attempt to avoid Stigler’s law of eponymy (actually Rafa points out that using this name is a perfect example of this law, since this feature selection approach has been proposed before at least once).\nHere is how the Leekasso works. You fit each of the models:\nY = B0 + BkXk + E\ntake the 10 variables with the smallest p-values from testing the Bk coefficients, then fit a linear model with just those 10 coefficients. You never use 9 or 11, the Leekasso is always 10.\nFor fun I did an experiment to compare the accuracy of the Leekasso and the Lasso.\nHere is the setup:\nI simulated 500 variables and 100 samples for each study, each N(0,1)\nI created an outcome that was 0 for the first 50 samples, 1 for the last 50\nI set a certain number of variables (between 5 and 50) to be associated with the outcome using the model Xi = b0i + b1iY + e (this is an important choice, more later in the post)\nI tried different levels of signal to the truly predictive features\nI generated two data sets (training and test) from the exact same model for each scenario\nI fit the Lasso using the lars package, choosing the shrinkage parameter as the value that minimized the cross-validation MSE in the training set\nI fit the Leekasso and the Lasso on the training sets and evaluated accuracy on the test sets.\nThe R code for this analysis is available here and the resulting data is here.\nThe results show that for all configurations, using the top 10 has a higher out of sample prediction accuracy than the lasso. A larger version of the plot is here.\n\nInterestingly, this is true even when there are fewer than 10 real features in the data or when there are many more than 10 real features ((remember the Leekasso always picks 10).\nSome thoughts on this analysis:\nThis is only test-set prediction accuracy, it says nothing about selecting the “right” features for prediction.\nThe Leekasso took about 0.03 seconds to fit and test per data set compared to about 5.61 seconds for the Lasso.\nThe data generating model is the model underlying the top 10, so it isn’t surprising it has higher performance. Note that I simulated from the model: Xi = b0i + b1iY + e, this is the model commonly assumed in differential expression analysis (genomics) or voxel-wise analysis (fMRI). Alternatively I could have simulated from the model: Y = B0 + B1 X1 + B2 X2 + … + BM XM + E, where most of the coefficients are zero. In this case, the Lasso would outperform the top 10 (data not shown). This is a key, and possibly obvious, issue raised by this simulation. When doing prediction differences in the true “causal” model matter a lot. So if we believe the “top 10 model” holds in many high-dimensional settings, then it may be the case that regularization approaches don’t work well for prediction and vice versa.\nI think what may be happening is that the Lasso is overshrinking the parameter estimates, in other words, you give up too much bias for a gain in variance. Alan Dabney and John Storey have a really nice paper discussing shrinkage in the context of genomic prediction that I think is related.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-01-03-the-supreme-court-takes-on-pollution-source-apportionment-and-realizes-its-hard/",
    "title": "The Supreme Court takes on Pollution Source Apportionment...and Realizes It's Hard",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2014-01-03",
    "categories": [],
    "contents": "\nRecently, the U.S. Supreme Court heard arguments in the cases EPA v. EME Homer City Generation and _American Lung Association v EME Homer City Generation. _SCOTUSblog has a nice summary of the legal arguments, for the law buffs out there.\nThe basic problem is that the way air pollution is regulated, the EPA and state and local agencies monitor the air pollution in each state. When the levels of pollution are above the national ambient air quality standards at the monitors in that state, the state is considered in “non-attainment” (i.e. they have not attained the standard). Otherwise, they are in attainment.\nBut what if your state doesn’t actually generate any pollution, but there’s all this pollution blowing in from another state? Pollution knows no boundaries and in that case, the monitors in your state will be in non-attainment, and it isn’t even your fault! The Clean Air Act has something called the “good neighbor” policy that was designed to address this issue. From SCOTUSblog:\n\nOne of the obligations that states have, in drafting implementation plans [to reduce pollution], is imposed by what is called the “good neighbor” policy.  It dates from 1963, in a more elemental form, but its most fully developed form requires each state to include in its plan the measures necessary to prevent the migration of their polluted air to their neighbors, if that would keep the neighbors from meeting EPA’s quality standards.\n\nThe problem is that if you live in a state like Maryland, your air pollution is coming from a bunch of states (Pennsylvania, Ohio, etc.). So who do you blame? Well, the logical thing would be to say that if Pennsylvania contributes to 90% of Maryland’s interstate air pollution and Ohio contributes 10%, then Pennsylvania should get 90% of the blame and Ohio 10%. But it’s not so easy because air pollution doesn’t have any special identifiers on it to indicate what state it came from. This is the source apportionment problem in air pollution and it involves trying to back-calculate where a given amount of pollution came from (or what was its source). It’s not an easy problem.\nEPA realized the unfairness here and devised the State Air Pollution Rule, also known as the “Transport Rule”. From SCOTUSblog:\n\nWhat the Transport Rule sought to do is to set up a regime to limit cross-border movement of emissions of nitrogen oxides and sulfur dioxide.  Those substances, sent out from coal-fired power plants and other sources, get transformed into ozone and “fine particular matter” (basically, soot), and both are harmful to human health, contributing to asthma and heart attacks.  They also damage natural terrain such as forests, destroy farm crops, can kill fish, and create hazes that reduce visibility.\nBoth of those pollutants are carried by the wind, and they can be transported very large distances — a phenomenon that is mostly noticed in the eastern states.\n\nThere are actually a few versions of this problem. One common one involves identifying the source of a particle (i.e. automobile, power plans, road dust) based on its chemical composition. The idea here is that at any given monitor, there are particles blowing in from all different types of sources and so the pollution you measure is a mixture of all these sources. Making some assumptions about chemical mass balance, there are ways to statistically separate out the contributions from individual sources based on a the chemical composition of the total mass measurement. If the particles that we measure, say, have a lot of ammonium ions and we know that particles generated by coal-burning power plants have a lot of ammonium ions, then we might infer that the particles came from a coal-burning power plant.\nThe key idea here is that different sources of particles have “chemical signatures” that can be used to separate out their various contributions. This is already a difficult problem, but at least here, we have some knowledge of the chemical makeup of various sources and can incorporate that knowledge into the statistical analysis.\nIn the problem at the Supreme Court, we’re not concerned with particles from various types of sources, but rather from different locations. But, for the most part, different states don’t have “chemical signatures” or tracer elements, so it’s hard to identify whether a given particle (or other pollutant) blowing in the wind came from Pennsylvania versus Ohio.\nSo what did EPA do? Well, instead of figuring out where the pollution came from, they decided that states would reduce emissions based on how much it would cost to control those emissions. The states objected because the cost of controlling emissions may well have nothing to do with how much pollution is actually being contributed downwind.\nThe legal question involves whether or not EPA has the authority to devise a regulatory plan based on costs as opposed to actual pollution contribution. I will let people who actually know the law address that question, but given the general difficulty of source apportionment, I’m not sure EPA could have come up with a much better plan.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-12-30-some-things-r-can-do-you-might-not-be-aware-of/",
    "title": "Some things R can do you might not be aware of",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-12-30",
    "categories": [],
    "contents": "\nThere is a lot of noise around the “R versus Contender X” for Data Science. I think the two main competitors right now that I hear about are Python and Julia. I’m not going to weigh into the debates because I go by the motto: “Why not just use something that works?”\nR offers a lot of benefits if you are interested in statistical or predictive modeling. It is basically unrivaled in terms of the breadth of packages for applied statistics.  But I think sometimes it isn’t obvious that R can handle some tasks that you used to have to do with other languages. This misconception is particularly common among people who regularly code in a different language and are moving to R. So I thought I’d point out a few cool things that R can do. Please add to the list in the comments if I’ve missed things that R can do people don’t expect.\nR can do regular expressions/text processing: Check out stringr, tm, and a large number of other natural language processing packages.\nR can get data out of a database: Check out RMySQL, RMongoDB, rhdf5, ROracle, MonetDB.R (via Anthony D.).\nR can process nasty data: Check out plyr, reshape2, Hmisc\nR can process images: EBImage is a good general purpose tool, but there are also packages for various file types like jpeg.\nR can handle different data formats: XML and RJSONIO handle two common types, but you can also read from Excel files with xlsx or handle pretty much every common data storage type (you’ll have to search R + data type) to find the package.\nR can interact with APIs: Check out RCurl and httr for general purpose software, or you could try some specific examples like twitteR. You can create an api from R code using yhat.\nR can build apps/interactive graphics: Some pretty cool things have already been built with shiny, rCharts interfaces with a ton of interactive graphics packages.\nR can create dynamic documents: Try out [There is a lot of noise around the “R versus Contender X” for Data Science. I think the two main competitors right now that I hear about are Python and Julia. I’m not going to weigh into the debates because I go by the motto: “Why not just use something that works?”\nR offers a lot of benefits if you are interested in statistical or predictive modeling. It is basically unrivaled in terms of the breadth of packages for applied statistics.  But I think sometimes it isn’t obvious that R can handle some tasks that you used to have to do with other languages. This misconception is particularly common among people who regularly code in a different language and are moving to R. So I thought I’d point out a few cool things that R can do. Please add to the list in the comments if I’ve missed things that R can do people don’t expect.\nR can do regular expressions/text processing: Check out stringr, tm, and a large number of other natural language processing packages.\nR can get data out of a database: Check out RMySQL, RMongoDB, rhdf5, ROracle, MonetDB.R (via Anthony D.).\nR can process nasty data: Check out plyr, reshape2, Hmisc\nR can process images: EBImage is a good general purpose tool, but there are also packages for various file types like jpeg.\nR can handle different data formats: XML and RJSONIO handle two common types, but you can also read from Excel files with xlsx or handle pretty much every common data storage type (you’ll have to search R + data type) to find the package.\nR can interact with APIs: Check out RCurl and httr for general purpose software, or you could try some specific examples like twitteR. You can create an api from R code using yhat.\nR can build apps/interactive graphics: Some pretty cool things have already been built with shiny, rCharts interfaces with a ton of interactive graphics packages.\nR can create dynamic documents: Try out](http://yihui.name/knitr/) or slidify.\nR can play with Hadoop: Check out the rhadoop wiki.\nR can create interactive teaching modules: You can do it in the console with swirl or on the web with Datamind.\nR interfaces very nicely with C if you need to be hardcore (also maybe? interfaces with Python): Rcpp, enough said. Also read the tutorial. I haven’t tried the rPython library, but it looks like a great idea.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-12-20-a-non-comprehensive-list-of-awesome-things-other-people-did-this-year/",
    "title": "A non-comprehensive list of awesome things other people did this year.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-12-20",
    "categories": [],
    "contents": "\nEditor’s Note: I made this list off the top of my head and have surely missed awesome things people have done this year. If you know of some, you should make your own list or add it to the comments! I have also avoided talking about stuff I worked on or that people here at Hopkins are doing because this post is supposed to be about other people’s awesome stuff. I wrote this post because a blog often feels like a place to complain, but we started Simply Stats as a place to be pumped up about the stuff people were doing with data. \nI emailed Hadley Wickham about some trouble we were having memory profiling. He wrote back immediately, then wrote an R package, then wrote this awesome guide. That guy is ridiculous.\nJared Horvath wrote this incredibly well-written and compelling argument for the scientific system that has given us a wide range of discoveries.\nYuwen Liu and colleagues wrote this really interesting paper on power for RNA-seq studies comparing biological replicates and sequencing depth. Shows pretty conclusively to go for more replicates (music to a statisticians ears!).\nYoav Benjamini and Yotam Hechtlingler wrote an amazing discussion of the paper we wrote about the science-wise false discovery rate. It contributes new ideas about estimation/control in that context.\nSherri Rose wrote a fascinating article about statistician’s role in big data. One thing I really liked was this line: “This may require implementing commonly used methods, developing a new method, or integrating techniques from other fields to answer our problem.” I really like the idea that integrating and applying standard methods in new and creative ways can be viewed as a statistical contribution.\nKarl Broman gave his now legendary talk (part1/part2) on statistical graphics that I think should be required viewing for anyone who will ever plot data on a Google Hangout with the Iowa State data viz crowd. They had some technical difficulties during the broadcast so Karl B. took it down. Join me in begging him to put it back up again despited the warts.\nEverything Thomas Lumley wrote on notstatschat, I follow that blog super closely. I love this scrunchable poster he pointed to and this post on Statins and the Causal Markov property.\nI wish I could take Joe Blitzstein’s data science class. Particularly check out the reading list, which I think is excellent.\nLev Muchik, Sinan Aral, and Sean Taylor brought the randomized control trial to social influence bias on a massive scale. I love how RCT are finding their ways into the new, sexy areas.\nGenevera Allen taught a congressman about statistical brain mapping and holy crap he talked about it on the floor of the house.\nLior Pachter starting mixing it up on his blog. I don’t necessarily agree with all of his posts but it is hard to deny the influence that his posts have had on real science. I definitely read it regularly.\nMarie Davidian, President of the ASA, has been on a tear this year, doing tons of cool stuff, including landing the big fish, Nate Silver, for JSM. Super impressive to watch the energy. I’m also really excited to see what Bin Yu works on this year as president of IMS.\nThe Stats 2013 crowd has done a ridiculously good job of getting the word out about statistics this year. I keep seeing statistics pop up in places like the WSJ, which warms my heart.\nOne way I judge a paper is by how angry/jealous I am that I didn’t think of or write that paper. This paper on the reproducibility of RNA-seq experiments was so good I was seeing red. I’ll be reading everything that Tuuli Lappalainen’s new group at the New York Genome Center writes.\nHector Corrada Bravo and the crowd at UMD wrote this paper about differential abundance in microbial communities that also made me crazy jealous. Just such a good idea done so well.\nChad Myers and Curtis Huttenhower continue to absolutely tear it up on networks and microbiome stuff. Just stop guys, you are making the rest of us look bad…\nI don’t want to go to Stanford I want to go to Johns Hopkins.\nRamnath keeps Ramnathing (def. to build incredible things at a speed that we can’t keep up with by repurposing old tools in the most creative way possible) with rCharts.\nNeo Chung and John Storey invented the jackstraw for testing the association between measured variables and principal components. It is an awesome idea and a descriptive name.\nI wasn’t at Bioc 2013, but I heard from two people who I highly respect and it takes a lot to impress that Levi Waldron gave one of the best talks they’d ever seen. The paper isn’t up yet (I think) but here is the R package with the data he described.  His survHd package for fast coxph fits (think rowFtests but with Cox) is also worth checking out.\nJohn Cook kept cranking out interesting posts, as usual. One of my favorites talks about how one major component of expertise is the ability to quickly find and correct inevitable errors (for example, in code).\nLarry Wasserman’s Simpson’s Paradox post should be required reading. He is shutting down Normal Deviate, which is a huge bummer.\nAndrew Gelman and I don’t always agree on scientific issues, but there is no arguing that he and the stan team have made a pretty impressive piece of software with stan. Richard McElreath also wrote a slick interface that makes fitting a fully Bayesian model match the syntax of lmer.\nSteve Pierson and Ron Wasserstein from ASA are also doing a huge service for our community in tackling the big issues like interfacing statistics to government funding agencies. Steve’s Twitter feed has been a great resource for keeping track of deadlines for competitions, grants, and other deadlines.\nJoshua Katz built these amazing dialect maps that have been all over the news. Shiny Apps are getting to be serious business.\nSpeaking of RStudio, they keep rolling out the goodies, my favorite recent addition is interactive debugging.\nI’ll close with David Duvenaud’s HarlMCMC shake:\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-12-16-a-summary-of-the-evidence-that-most-published-research-is-false/",
    "title": "A summary of the evidence that most published research is false",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-12-16",
    "categories": [],
    "contents": "\nOne of the hottest topics in science has two main conclusions:\nMost published research is false\nThere is a reproducibility crisis in science\nThe first claim is often stated in a slightly different way: that most results of scientific experiments do not replicate. I recently got caught up in this debate and I frequently get asked about it.\nSo I thought I’d do a very brief review of the reported evidence for the two perceived crises. An important point is all of the scientists below have made the best effort they can to tackle a fairly complicated problem and this is early days in the study of science-wise false discovery rates. But the take home message is that there is currently no definitive evidence one way or another about whether most results are false.\nPaper: Why most published research findings are false. Main idea: People use hypothesis testing to determine if specific scientific discoveries are significant. This significance calculation is used as a screening mechanism in the scientific literature. Under assumptions about the way people perform these tests and report them it is possible to construct a universe where most published findings are false positive results. Important drawback: The paper contains no real data, it is purely based on conjecture and simulation.\nPaper: Drug development: Raise standards for preclinical research. Main idea: Many drugs fail when they move through the development process. Amgen scientists tried to replicate 53 high-profile basic research findings in cancer and could only replicate 6. Important drawback: This is not a scientific paper. The study design, replication attempts, selected studies, and the statistical methods to define “replicate” are not defined. No data is available or provided.\nPaper: An estimate of the science-wise false discovery rate and application to the top medical literature. Main idea: The paper collects P-values from published abstracts of papers in the medical literature and uses a statistical method to estimate the false discovery rate proposed in paper 1 above. Important drawback: The paper only collected data from major medical journals and the abstracts. P-values can be manipulated in many ways that could call into question the statistical results in the paper.\nPaper: Revised standards for statistical evidence. Main idea: The P-value cutoff of 0.05 is used by many journals to determine statistical significance. This paper proposes an alternative method for screening hypotheses based on Bayes factors. Important drawback: The paper is a theoretical and philosophical argument for simple hypothesis tests. The data analysis recalculates Bayes factors for reported t-statistics and plots the Bayes factor versus the t-test then makes an argument for why one is better than the other.\nPaper: Contradicted and initially stronger effects in highly cited research Main idea: This paper looks at studies that attempted to answer the same scientific question where the second study had a larger sample size or more robust (e.g. randomized trial) study design. Some effects reported in the second study do not match the results exactly from the first. Important drawback: The title does not match the results. 16% of studies were contradicted (meaning effect in a different direction). 16% reported smaller effect size, 44% were replicated and 24% were unchallenged. So 44% + 24% + 16% = 86% were not contradicted. Lack of replication is also not proof of error.\nPaper: Modeling the effects of subjective and objective decision making in scientific peer review. Main idea: This paper considers a theoretical model for how referees of scientific papers may behave socially. They use simulations to point out how an effect called “herding” (basically peer-mimicking) may lead to biases in the review process. Important drawback: The model makes major simplifying assumptions about human behavior and supports these conclusions entirely with simulation. No data is presented.\nPaper: Repeatability of published microarray gene expression analyses. Main idea: This paper attempts to collect the data used in published papers and to repeat one randomly selected analysis from the paper. For many of the papers the data was either not available or available in a format that made it difficult/impossible to repeat the analysis performed in the original paper. The types of software used were also not clear. Important drawback: This paper was written about 18 data sets in 2005-2006. This is both early in the era of reproducibility and not comprehensive in any way. This says nothing about the rate of false discoveries in the medical literature but does speak to the reproducibility of genomics experiments 10 years ago.\nPaper: Investigating variation in replicability: The “Many Labs” replication project. (not yet published) Main idea: The idea is to take a bunch of published high-profile results and try to get multiple labs to replicate the results. They successfully replicated 10 out of 13 results and the distribution of results you see is about what you’d expect (see embedded figure below). Important drawback: The paper isn’t published yet and it only covers 13 experiments. That being said, this is by far the strongest, most comprehensive, and most reproducible analysis of replication among all the papers surveyed here.\nI do think that the reviewed papers are important contributions because they draw attention to real concerns about the modern scientific process. Namely\nWe need more statistical literacy\nWe need more computational literacy\nWe need to require code be published\nWe need mechanisms of peer review that deal with code\nWe need a culture that doesn’t use reproducibility as a weapon\nWe need increased transparency in review and evaluation of papers\nSome of these have simple fixes (more statistics courses, publishing code) some are much, much harder (changing publication/review culture).\nThe Many Labs project (Paper 8) points out that statistical research is proceeding in a fairly reasonable fashion. Some effects are overestimated in individual studies, some are underestimated, and some are just about right. Regardless, no single study should stand alone as the last word about an important scientific issue. It obviously won’t be possible to replicate every study as intensely as those in the Many Labs project, but this is a reassuring piece of evidence that things aren’t as bad as some paper titles and headlines may make it seem.\n\n\n\nMany labs data. Blue x’s are original effect sizes. Other dots are effect sizes from replication experiments (http://rolfzwaan.blogspot.com/2013/11/what-can-we-learn-from-many-labs.html)\n\n\nThe Many Labs results suggest that the hype about the failures of science are, at the very least, premature. I think an equally important idea is that science has pretty much always worked with some number of false positive and irreplicable studies. This was beautifully described by Jared Horvath in this blog post from the Economist.  I think the take home message is that regardless of the rate of false discoveries, the scientific process has led to amazing and life-altering discoveries.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-12-15-sunday-datastatistics-link-roundup-121513/",
    "title": "Sunday data/statistics link roundup (12/15/13)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-12-15",
    "categories": [],
    "contents": "\nRafa (in Spanish) clarifying some of the problems with the anti-GMO crowd.\nJoe Bliztstein, most recently of #futureofstats fame, talks up data science in the Harvard Crimson (via Rafa). As has been pointed out by Rebecca Nugent when she stopped to visit us, class sizes in undergrad stats programs are blowing up!\nIf you missed it, Michael Eisen dropped by to chat about open access (part 1/part 2). We talked about Randy Schekman, a recent Nobel prize winner who says he isn’t publishing in Nature/Science/Cell anymore. Professor Schekman did a Reddit AMA where he got grilled pretty hard about pushing a glamour open access journal eLife, while dissing N/S/C, where he published a lot of stuff before winning the Nobel.\nThe article I received most the last couple of weeks is this one. In it, Peter Higgs says he wouldn’t have had time to think deeply to perform the research that led to the Boson discovery in the modern publish or perish academic system. But he got the prize, at least in part, because of the people who conceived/built/tested the theory in the Large Hadron Collider. I’m much more inclined to believe someone would have come up with the Boson theory in our current system than someone would have built the LHC in a system without competitive pressure.\nI think this post raises some interesting questions about the Obesity Paradox that says overweight people with diabetes may have lower risk of death than normal weight people. The analysis is obviously tongue-in-cheek, but I’d be interested to hear what other people think about whether it is a serious issue or not.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-12-13-simply-statistics-interview-with-michael-eisen-co-founder-of-the-public-library-of-science-part-22/",
    "title": "Simply Statistics Interview with Michael Eisen, Co-Founder of the Public Library of Science (Part 2/2)",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-12-13",
    "categories": [],
    "contents": "\nHere is Part 2 of our Jeff’s and my interview with Michael Eisen, Co-Founder of the Public Library of Science.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-12-12-simply-statistics-interview-with-michael-eisen-co-founder-of-the-public-library-of-science/",
    "title": "Simply Statistics Interview with Michael Eisen, Co-Founder of the Public Library of Science (Part 1/2)",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-12-12",
    "categories": [],
    "contents": "\nJeff and I had a chance to interview Michael Eisen, a co-founder of the Public Library of Science, HHMI Investigator, and a Professor at UC Berkeley. We talked with him about publishing in open access and how young investigators might publish in open access journals under the current system. Watch part 1 of the interview above.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-12-12-the-key-word-in-data-science-is-not-data-it-is-science/",
    "title": "The key word in \"Data Science\" is not Data, it is Science",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-12-12",
    "categories": [],
    "contents": "\nOne of my colleagues was just at a conference where they saw a presentation about using data to solve a problem where data had previously not been abundant. The speaker claimed the data were “big data” and a question from the audience was: “Well, that isn’t really big data is it, it is only X Gigabytes”.\nWhile that exact question would elicit groans from most people who work with data, I think it highlights one of the key problems with the thinking around data science. Most people hyping data  science have focused on the first word: data. They care about volume and velocity and whatever other buzzwords describe data that is too big for you to analyze in Excel. This hype about the size (relative or absolute) of the data being collected fed into the second category of hype - hype about tools. People threw around EC2, Hadoop, Pig, and had huge debates about Python versus R.\nBut the key word in data science is not “data”; it is “science”. Data science is only useful when the data are used to answer a question. That is the science part of the equation. The problem with this view of data science is that it is much harder than the view that focuses on data size or tools. It is much, much easier to calculate the size of a data set and say “My data are bigger than yours” or to say, “I can code in Hadoop, can you?” than to say, “I have this really hard question, can I answer it with my data?”.\nA few reasons it is harder to focus on the science than the data/tools are:\nJohn Tukey’s quote: “The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.”. You may have 100 Gb and only 3 Kb are useful for answering the real question you care about. \nWhen you start with the question you often discover that you need to collect new data or design an experiment to confirm you are getting the right answer.\nIt is easy to discover “structure” or “networks” in a data set. There will always be correlations for a thousand reasons if you collect enough data. Understanding whether these correlations matter for specific, interesting questions is much harder.\nOften the structure you found on the first pass is due to a phenomena (measurement error, artifacts, data processing) that doesn’t answer an interesting question.\nThe issue is that the hype around big data/data science will flame out (it already is) if data science is only about “data” and not about “science”. The long term impact of data science will be measured by the scientific questions we can answer with the data.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-12-11-are-moocs-fundamentally-flawed-or-is-it-a-problem-with-statistical-literacy/",
    "title": "Are MOOC's fundamentally flawed? Or is it a problem with statistical literacy?",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-12-11",
    "categories": [],
    "contents": "\nPeople know I have taught a MOOC on Data Analysis, so I frequently get emails about updates on the “state of MOOCs”. It definitely feels like the wild west of education is happening right now. If you make an analogy to air travel, I would say we are about here:\n\n\n\n\n\nSo of course I feel like it is a bit premature for quotes like this:\n\n\n\nTwo years after a Stanford professor drew 160,000 students from around the globe to a free online course on artificial intelligence, starting what was widely viewed as a revolution in higher education, early results for such large-scale courses are disappointing, forcing a rethinking of how college instruction can best use the Internet.\n\n\n\nThese headlines are being driven in large part by Sebastian Thrun, the founder of Udacity, which has had some trouble with their business model. One reason is that they seem to have had the most trouble with luring instructors from the top schools to their platform.\n\n\nBut the main reason that gets cited for the “failure” of MOOCs is this experiment performed at San Jose State. I previously pointed out one major flaw with the study design: that the students in the two comparison groups were not comparable.\n\n\nHere are a few choice quotes from the study:\n\n\nPoor response rate:\n\n\n\nWhile a major effort was made to increase participation in the survey research within this population, the result was disappointing (response rates of 32% for Survey 1; 34% for Survey 2, and 32% for Survey 3).\n\n\n\nNot a representative sample:\n\n\n\nThe research team compared the survey participants to the entire student population and found significant differences. Most importantly, students who succeeded are over-represented among the survey respondents.\n\n\n\nDifficulties with data collection/processing:\n\n\n\nWhile most of the data were provided by the end of the Spring 2013 semester, clarifications, corrections and data transformations had to be made for many weeks thereafter, including resolving accuracy questions that arose once the analysis of the Udacity platform data began\n\n\n\nThese ideas alone point to an incredibly suspect study that is not the fault of the researchers in question. They were working with the data the best they could, but the study design and data are deeply flawed. The most egregious, of course, is the difference in populations between the students who matriculated and didn’t (Tables 1-4 show the dramatic differences in population).\n\n\nMy take home message is that if this study were submitted to a journal it would be seriously questioned on both scientific and statistical grounds. Before we rush to claim that the whole idea of MOOCs are flawed, I think we should wait for more thorough, larger, and well-designed studies are performed.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-12-05-nyc-crime-rates-by-yearcommissioner/",
    "title": "NYC crime rates by year/commissioner",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-12-05",
    "categories": [],
    "contents": "\nNYC mayor-elect Bill de Blasio is expected to name William J. Bratton to lead the NYPD. Bratton has been commissioner before (1994-1996) so I was curious to see the crime rates during his tenure, which was within the period that saw an impressive drop (1990-2010). Here is the graph of violent crimes per 100,000 inhabitants for NYC NY state for year 1965-2012 (divided by commissioner). Will Bratton be able to continue the trend? The graph suggests to me that they have hit a “floor” (1960s levels!).\n\nData is here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-12-04-advice-for-stats-students-on-the-academic-job-market-2/",
    "title": "Advice for students on the academic job market",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-12-04",
    "categories": [],
    "contents": "\nEditor’s note: This is a slightly modified version of a previous post.\nJob hunting season is upon us. Openings are already being posted here, here, and here. So you should have your CV, research statement, and web page ready. I highly recommend having a web page. It doesn’t have to be fancy. Here, here, and here are some good ones ranging from simple to a bit over the top. Minimum requirements are a list of publications and a link to a CV. If you have written software, link to that as well.\nThe earlier you submit the better. Don’t wait for your letters. Keep in mind two things: 1) departments have a limit of how many people they can invite and 2) admissions committee members get tired after reading 200+ CVs.\nIf you are seeking an academic job your CV should focus on the following: PhD granting institution, advisor (including postdoc advisor if you have one), and papers. Be careful not to drown out these most important features with superflous entries. For papers, include three sections: 1-published, 2-under review, and 3-under preparation. For 2, include the journal names and if possible have tech reports available on your web page. For 3, be ready to give updates during the interview. If you have papers for which you are co-first author be sure to highlight that fact somehow.\nSo what are the different types of jobs? Before listing the options I should explain the concept of hard versus soft money. Revenue in academia comes from tuition (in public schools the state kicks in some extra $), external funding (e.g. NIH grants), services (e.g. patient care), and philanthropy (endowment). The money that comes from tuition, services, and philanthropy is referred to as hard money. Within an institution, roughly the same amount is available every year and the way its split among departments rarely changes. When it does, it’s because your chair has either lost or won a long hard-fought zero-sum battle. Research money comes from NIH, NSF, DoD, etc.. and one has to write grants to raise funding (which pay part or all of your salary). These days about 10% of grant applications are funded, so it is certainly not guaranteed. Although at the institution level the law of large numbers kicks in, at the individual level it certainly doesn’t. Note that the break down of revenue varies widely from institution to institution. Liberal arts colleges are almost 100% hard money while research institutes are almost 100% soft money.\nSo to simplify, your salary will come from teaching (tuition) and research (grants). The percentages will vary depending on the department. Here are five types of jobs:\nSoft money university positions: examples are Hopkins and Harvard Biostat. A typical breakdown is 75% soft/25% hard. To earn the hard money you will have to teach, but not that much. In my dept we teach 48 classroom hours a year (equivalent to one one-semester class). To earn the soft money you have to write, and eventually get, grants. As a statistician you don’t necessarily have to write your own grants, you can partner up with other scientists that need help with their data. And there are many! Salaries are typically higher in these positions. Stress levels are also higher given the uncertainty of funding. I personally like this as it keeps me motivated, focused, and forces me to work on problems important enough to receive NIH funding.\n1a) Some schools of medicine have Biostatistics units that are 100% soft money. One does not have to teach, but, unless you have a joint appointment, you won’t have access to grad students. Still these are tenure track jobs. Although at 100% soft what does tenure mean?  I should mention at MD Anderson, one only needs to raise 50% of ones salary and the other 50% is earned via service (statistical consulting to the institution). I imagine there are other places like this, as well as institutions that use endowments to provide some hard money.\nHard money positions: examples are Berkeley and Stanford Stat. A typical break down is 75% hard/25% soft. You get paid a 9 month salary. If you want to get paid in the summer and pay students, you need a grant. Here you typically teach two classes a semester but many places let you “buy out” of teaching if you can get grants to pay your salary. Some tension exists when chairs decide who teaches the big undergrand courses (lots of grunt work) and who teaches the small seminar classes where you talk about your own work.\n2a) Hard money postions: Liberal arts colleges will cover as much as 100% of your salary from tuition. As a result, you are expected to teach much more. Most liberal arts colleges weigh teaching as much (or more) than research during promotion although there is a trend towards weighing research more.\nResearch associate positions: examples are jobs in schools of medicine in departments other than Stat/Biostat. These positions are typically 100% soft and are created because someone at the institution has a grant to pay for you. These are usually not tenure track positons and you rarely have to teach. You also have less independence since you have to work on the grant that funds you.\nIndustry: typically 100% hard. There are plenty of for-profit companies where one can have fruitful research careers. AT & T, Google, IBM, Microsoft, and Genentech are all examples of companies with great research groups. Note that S, the language that R is based on, was born in Bell Labs. And one of the co-creators of R now does his research at Genentech. Salaries are typically higher in industry and cafeteria food can be quite awesome. The drawbacks are no access to students and lack of independence (although not always).\nGovernment jobs: The FDA and NIH are examples of agencies that have research positions. The NCI’s Biometric Research Branch is an example. I would classify these as 100% hard. But it is different than other hard money places in that you have to justify your budget every so often. Service, collaborative, and independent research is expected. A drawback is that you don’t have access to students although you can get joint appointments. Hopkins Biostat has a couple of NCI researchers with joint appointments.\nOk, that is it for now. Later this month we will blog about job interviews.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-12-03-on-the-future-of-the-textbook/",
    "title": "On the future of the textbook",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-12-03",
    "categories": [],
    "contents": "\nThe latest issue of Technological Innovations in Statistics Education is focused on the future of the textbook. Editor Rob Gould has put together an interesting list of contributions as well as discussions from the leaders in the field of statistics education. Articles include\nThe Course as Textbook: A Symbiotic Relationship in the Introductory Statistics Class by Zieffler, Isaak, and Garfield\nOpenIntro Statistics: an Open-source Textbook by Cetinkaya-Rundel, Diez, and Barr\nTextbooks 2.0 by Webster West\nGo check it out!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-12-02-academics-should-not-feel-guilty-for-maximizing-their-potential-by-leaving-their-homeland/",
    "title": "Academics should not feel guilty for maximizing their potential by leaving their homeland",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-12-02",
    "categories": [],
    "contents": "\nIn a New York Times op-ed titled Migration Hurts the Homeland, Paul Collier tells us that\n\n\nWhat’s good for migrants from poor places is not always good for the countries they’re leaving behind.\n\n\n\nHe makes the argument that those that favor open immigration don’t realize that they are actually hurting “the poor” more than they are helping. This post is not about the issue of whether migration is bad for the homeland (I know of others that make the opposite claim) but rather about the opinions I have formed by leaving my homeland to become an academic in a US research university.\n\n\nLet me start by pointing out that an outstanding 470 Nobel prizes have been handed out to residents of the US or the UK. About 25% of these are to immigrants. These Nobel laureates include academics born in Egypt, Venezuela, and Mexico. In contrast, only one of the 20 prizes handed to Italy was to an immigrant (none in the last 50 years). I view my university as international, not american.\n\n\nThroughout my career I have encountered several foreign graduate students/postdocs that ponder passing on academic jobs in the US to go back and help the homeland. I was one of them and I admire the commitment of those who decide to go back. However, I think it’s important to point out that the accomplishments of those that take jobs in American research universities are in large part due to the unique support that these universities provide. This is particularly true in the sciences were research success depends on low teaching loads, lab infrastructure, high-performance computers, administrative support for grant submission, and talented collaborators.\n\n\nThe latter is by far the most important for applied statisticians like myself who depend on subject matter experts that provide quantitative challenges. Having a critical mass of such innovators is key. Although I will never know for sure, I am quite certain that most of what I have accomplished would not have happened had I returned home.\n\n\nIt is also important to point out that my homeland benefits from what I have learned during 15 years working in top research universities. I am always looking for an excuse to visit my friends and family and I also enjoy giving back to my alma mater. This has greatly increased my interactions through workshops, academic talks, participation in advisory boards, and many other informal exchanges.\n\n\nSo, if you are an up-and-coming academic deciding if you should go back or not, do not let guilt factor into the decision. Humanity benefits from you  maximizing your potential. Your homeland will benefit in indirect ways as well.\n\n\nps - Do people from Idaho feel guilty for leaving their brain-drained state?\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-12-01-sunday-datastatistics-link-roundup-12213/",
    "title": "Sunday data/statistics link roundup (12/2/13)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-12-01",
    "categories": [],
    "contents": "\nI’m in Australia for Bioinfo Summer 2013! First time in Australia and excited about the great lineup of speakers and to meet a bunch of people at the University of Adelaide. \nAn interesting post about how CS has become the de facto language of our times. They specifically talk about CS50 at Harvard. I think in terms of being an informed citizen CS and Statistics are quickly being added to Reading, Writing, and Arithmetic as the required baseline knowledge (link via Alex N.)\nA long but fascinating read by Gary King about restructuring the social sciences with a focus on ending the quantitative/qualitative divide. I think a similar restructuring has been going on in biology for a while. It is nearly impossible to be a modern molecular biologist without at least some basic training in statistics. Similarly statisticians are experiencing an inverted revolution where we are refocusing on applications and some basic scientific experience is becoming a required component of being a statistician (link via Rafa).\nThis is how you make a splash in data science. Rochester is hiring 20! faculty across multiple disciplines. It will be interesting to see how that works out (link via Rafa). This goes along with the recent announcement of the Moore foundation funding Berkeley, UW, and NYU to build data science cultures/environments.\nPLoS is rich and they have to figure out what to do! They are a non-profit, but their journal PLoS One publishes about 30k papers a year at about 1k a pop. That is some serious money, which they need to figure out how to spend pronto. My main suggestion: fund research to figure out a way to put peer reviewing on the same level as publishing in terms of academic credit (link via Simina B.)\nA group of psychologists got together and performed replication experiments for 13 major effects. They replicated 11/13 (of course depending on your definition of replication). Hopefully these results are a good first step toward reducing the mania around the “replication crisis” and refocusing attention back on real solutions.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:10:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-26-statistical-zealots/",
    "title": "Statistical zealots",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-11-26",
    "categories": [],
    "contents": "\nYesterday my data sharing policy went a little bit viral. It hit the front page of Hacker News and was a trending repo on Github. I was reading the comments on Hacker News and came across this gem:\n\nSo, while I can imagine there are good Frequentists Statisticians out there, I insist that frequentism itself is bogus.\n\nThis is the extension of a long standing debate about the relative merits of frequentist and Bayesian statistical methods. It is interesting that I largely only see one side of the debate being played out these days. The Bayesian zealots have it in for the frequentists in a big way. The Hacker News comments are one example, but here are a Yesterday [my data sharing policy](https://github.com/jtleek/datasharing) went a little bit viral. It hit the front page of Hacker News and was a trending repo on Github. I was [reading the comments on Hacker News](https://news.ycombinator.com/item?id=6793291) and came across this gem: more. Interestingly, even the “popular geek press” is getting in the game.\n\n\n\nI think it probably deserves a longer post but here are my thoughts on statistical zealotry:\n\nUser effect >>>>>>>>>>>>>>>>> Philosophy effect. The person doing the statistics probably matters more than the statistical philosophy. I would prefer Andrew Gelman analyzed my data than a lot of frequentists. Similarly, I’d prefer that John Storey analyzed my data than a lot of Bayesians. \nI agree with Noahpinion that this is likely mostly a philosophy battle than a real practical applications battle.\nI like Rob Kass’s idea that we should move away from frequentist vs. Bayesian to pragmatism. I think most real applied statisticians have already done this, if for no other reason than being pragmatic helps you get things done.\nPapers like this one that claim total victory for one side or the other all have one thing in common: they rarely use real data to verify their claims. The real world is messy and one approach never wins all the time.\nMy final thought on this matter is: never trust people with an agenda bearing extreme counterexamples.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-22-simply-statistics-interview-with-daphnekoller-co-founder-of-coursera/",
    "title": "Simply Statistics interview with Daphne Koller, Co-Founder of Coursera",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-11-22",
    "categories": [],
    "contents": "\nJeff and I had an opportunity to sit down with Daphne Koller, Co-Founder of Coursera and Rajeev Motwani Professor of Computer Science at Stanford University. Jeff and I both teach massive open online courses using the Coursera platform and it was great to be able to talk with Professor Koller about the changing nature of education today.\nSome highlights:\n[1:35] On the origins of Coursera: “I actually came to that realization when listening to talk about YouTube, and realizing that, why does it make sense for me to come and deliver the same lecture year after year after year where I could package it in much smaller bite size chunks that were much more fun and much more cohesive and then use the class time for engaging with students in more meaningful ways.\n[7:22] On the role of MOOCs in academia: “Sometimes I have these discussions with some people in academic institutions who say that they feel that by engaging, for example, with MOOCs or blogs or social media they are diverting energy from what is their primary function which is teaching of their registered students…. But I think for most academic institutions, if I had to say what the primary function of an academic institution is, it’s the creation and dissemination of knowledge…. The only way society is going to move forward is if more people are better educated.”\n[10:15] On teaching: “I think that teaching is a scholarly work as well, a kind of distillation of knowledge that has to occur in order to put together a really great course.”\n[11:19] On teaching to the world. “Teaching, and quality of teaching, that used to be something that you could hide away from everyone…here, we’re suddenly in a world where teaching is really visible to everyone, and as a consequence, good teaching is going to be visible as a role model.”\n[13:33] On work/life balance: “It’s been insane. It’s also been somewhat surreal…. Sometimes I look at my life and I’m saying really, I mean, who’s life is this?”\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-21-future-of-statistics-take-home-messages-futureofstats/",
    "title": "Future of Statistics take home messages. #futureofstats",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-11-21",
    "categories": [],
    "contents": "\nA couple weeks ago we had the Future of Statistics Unconference. You can still watch it online here. Rafa also attended the Future of Statistical Sciences Workshop and wrote a great summary which you can read here.\nI decided to write a summary of take home messages from our speakers at the Unconference. You can read it on Github here. I put it on Github for two reasons:\nI agree with Hadley’s statement that the future of statistics is on Github.\nI summarized them based on my interpretation and would love collaboration on the document. If you want to add your new thoughts/summaries, add a new section with your bullet pointed ideas and send me a pull request!\nI sent our speakers a gift for presenting in the Unconference (if you were a speaker and didn’t get yours, email me!). Hadley posted the front on Twitter. Here is the back:\n\n\n\n\n\nP.S. Stay tuned for the future of Simply Statistics Unconferences.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-21-you-must-be-at-least-20-years-old-for-this-job/",
    "title": "You must be at least 20 years old for this job",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-11-21",
    "categories": [],
    "contents": "\nThe New York Times is recruiting a chief data scientist.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-18-feeling-optimistic-after-the-future-of-the-statistical-sciences-workshop/",
    "title": "Feeling optimistic after the Future of the Statistical Sciences Workshop",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-11-18",
    "categories": [],
    "contents": "\nLast I week I participated in the Future of the Statistical Sciences Workshop. I arrived feeling somewhat pessimistic about the future of our discipline. My pessimism stemmed from the emergence of the term Data Science and the small role academic (bio)statistics department are playing in the excitement and initiatives surrounding it.  Data Science centers/departments/initiatives are propping up in universities without much interaction with (bio)statistics departments. Funding agencies, interested in supporting Data Science, are not always including academic statisticians in the decision making process.\nAbout 100 participants, including many of our discipline’s leaders, attended the workshop. It was organized in sessions and about a dozen talks; some about the future, others featuring collaborations between statisticians and subject matter experts. The collaborative talks provided great examples of the best our field has to offer and the rest generated provocative discussions. In most of these discussions the disconnect between our discipline and  Data Science was raised as cause for concern.\nSome participants thought Data Science is just another fad like Data Mining was 10-20 years ago. I actually disagree because I view the recent increase in  the number of fields that have suddenly become data-driven as a historical discontinuity. For example, we first posted about statistics versus data science back in 2011.\nAt the workshop, Mike Jordan explained that the term was coined up by industry for practical reasons: emerging companies needed a work force that could solve problems with data and statisticians were not fitting the bill. However, at the workshop there was consensus that our discipline needs a jolt to meet these new challenges. The take away messages were all in line with ideas we have been promoting here in Simply Statistics (here is a good summary post from Jeff):\nWe need to engage in real present-day problems (problem first not solution backward)\nComputing should be a big part of our PhD curriculum (here are some suggestions)\nWe need to deliver solutions (and stop whining about not being listened to); be more like engineers than mathematicians. (here is a related post by Roger, in statistical genomics this has been the de facto rule for a while.)\nWe need to improve our communication skills (in talks or on Twitter)\nThe fact that there was consensus on these four points gave me reason to feel optimistic about our future.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-16-what-should-statistics-do-about-massive-open-online-courses/",
    "title": "What should statistics do about massive open online courses?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-11-16",
    "categories": [],
    "contents": "\nMarie Davidian, the President of the American Statistical Association, writes about the JHU Biostatistics effort to deliver massive open online courses. She interviewed Jeff, Brian Caffo, and me and summarized our thoughts.\n\nAll acknowledge that the future is unknown. How MOOCs will affect degree programs remains to be seen. Roger notes that the MOOCs he, Jeff, Brian, and others offer seem to attract many students who would likely not enter a degree program at Hopkins, regardless, so may be filling a niche that will not result in increased degree enrollments. But Brian notes that their MOOC involvement has brought extensive exposure to the Hopkins Department of Biostatistics—for many people the world over, Hopkins biostatistics is statistics.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-15-whats-the-future-of-inference/",
    "title": "What's the future of inference?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-11-15",
    "categories": [],
    "contents": "\nRob Gould reports on what appears to have been interesting panel discussion on the future of statistics hosted by the UCLA Statistics Department. The panelists were Songchun Zhu (UCLA Statistics), Susan Paddock (RAND Corp.), and Jan de Leeuw (UCLA Statistics).\nHe describes Jan’s thoughts on the future of inference in the field of statistics:\n\nJan said that inference as an activity belongs in the substantive field that raised the problem.  Statisticians should not do inference.  Statisticians might, he said, design tools to help specialists have an easier time doing inference. But the inferential act itself requires intimate substantive knowledge, and so the statistician can assist, but not do.\n\nI found this comment to be thought provoking. First of all, it sounds exactly like something Jan would say, which makes me smile. In principle, I agree with the premise. In order to make a reasonable (or intelligible) inference you have to have some knowledge of the substantive field. I don’t think that’s too controversial. However, I think it’s incredibly short-sighted to conclude therefore that statisticians should not be engaged in inference. To me, it seems more logical that statisticians should go learn some science. After all, we keep telling the scientists to learn some statistics.\nIn my experience, it’s not so easy to draw a clean line between the person analyzing the data and the person drawing the inferences. It’s generally not possible to say to someone, “Hey, I just analyze the data, I don’t care about your science.” For starters, that tends to make for bad collaborations. But more importantly, that kind of attitude assumes that you can effectively analyze the data without any substantive knowledge. That you can just “crunch the numbers” and produce a useful product.\nUltimately, I can see how statisticians would want to stay away from the inference business. That part is hard, it’s controversial, it involves messy details about sampling, and opens one up to criticism. And statisticians love to criticize other people. Why would anyone want to get mixed up with that? This is why machine learning is so attractive–it’s all about prediction and in-sample learning.\nHowever, I think I agree with Daniela Witten, who at our recent Unconference, said that the future of statistics is inference. That’s where statisticians are going to earn their money.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-14-the-leek-group-guide-to-sharing-data-with-a-statistician-to-speed-collaboration/",
    "title": "The Leek group guide to sharing data with a data analyst to speed collaboration",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-11-14",
    "categories": [],
    "contents": "\nMy group collaborates with many different scientists and the number one determinant of how fast we can turn around results is the status of the data we receive from our collaborators. If the data are well organized and all the important documentation is there, it dramatically speeds up the analysis time.\nI recently had the experience where a postdoc requesting help with an analysis provided an amazing summary of the data she wanted analyzed. It has made me want to prioritize her analysis in my queue and it inspired me to write a how-to guide that will help scientific/business collaborators get speedier results from their statistician colleagues.\nHere is the Leek group guide to sharing data with statisticians/data analysts.\nAs usual I put it on Github because I’m sure this first draft will have mistakes or less than perfect ideas. I would love help in making the guide more comprehensive and useful. If you issue a pull request make sure you add yourself to list of contributors at the end.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-13-original-source-code-for-apple-ii-dos/",
    "title": "Original source code for Apple II DOS",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-11-13",
    "categories": [],
    "contents": "\nSomeone needs to put this on GitHub right now.\n\nThanks Paul Laughton for your donation of this superb collection of early to mid-1978 documents including the letters, agreements, specifications (including hand-written code and schematics), and two original source code listing for the creation of the Apple II “DOS” (Disk Operating System).This was, of course, Apple’s first operating system, written not by Steve Wozniak (“Woz”) but by an external contractor (Paul Laughton working for Shepardson Microsystems). Woz lacked the skills to write an OS (as did anyone then at Apple). Paul authored the actual Apple II DOS to its release in the fall of 1978.\n\nUpdate: At this point I see some GitHub stub accounts, but no real code (yet).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-12-future-of-statistical-sciences-workshop-is-happening-right-now-fssw2013/",
    "title": "Future of Statistical Sciences Workshop is happening right now #FSSW2013",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-11-12",
    "categories": [],
    "contents": "\nASA Executive Director Ron Wasserstein is tweeting like mad man. If you’re not in London, catch up on what’s happening at the hashtag #FSSW2013.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-12-survival-analysis-for-hard-drives/",
    "title": "Survival analysis for hard drives",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-11-12",
    "categories": [],
    "contents": "\nHow long do hard drives last?\n\nBackblaze has kept up to 25,000 hard drives constantly online for the last four years. Every time a drive fails, they note it down, then slot in a replacement. After four years, Backblaze now has some amazing data and graphs that detail the failure rate of hard drives over the first four years of their life.\n\nI guess it’s easier to do this with hard drives than it is for people.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-11-apples-touch-id-and-a-worldwide-lesson-in-sensitivity-and-specificity/",
    "title": "Apple's Touch ID and a worldwide lesson in sensitivity and specificity",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-11-11",
    "categories": [],
    "contents": "\nI’ve been playing with my new iPhone 5s for the last few weeks, and first let me just say that it’s an awesome phone. Don’t listen to whatever Jeff says. It’s probably worth it just for the camera, but I’ve been particularly interested in the behavior of Apple’s fingerprint sensor (a.k.a. Touch ID). Before the phone came out, there were persistent rumors of a fingerprint sensor from now-defunct AuthenTec, and I wondered how the sensor would work given that it was unlikely to be perfect.\nApple reportedly sold 9 million iPhone 5c and 5s models over the opening weekend alone. Of those, about 7 million were estimated to be the 5s model which includes the fingerprint sensor (the 5c does not include it). So now millions of people have been using this thing and I’m getting the sense that many people are experiencing the same behavior I’ve observed over the last few weeks.\nThe sensor appears to have a high specificity. If you put the wrong finger, or the wrong person’s finger on the sensor, it will not let you unlock the phone. I haven’t seen a single instance of a false positive here, which seems like a good thing.\nThe sensor’s sensitivity is modest. Given the correct finger, the sensor seems to have a sensitivity of between 50-80% based on my completely unscientific guestimation. It seems to depend a little on the finger. I don’t know if this is high or low based on other fingerprint sensors, but it’s mildly annoying to have to switch fingers or type in the passcode more often than I was expecting to have to do that.\nBehavior seems to change depending on the task. This is pure speculation, but it seems the sensor is a bit more open to false positives if you’re using it to buy a song on iTunes. Although I haven’t actually seen it happen, it feels like I don’t have to place my finger on the sensor so perfectly if I’m just purchasing a song or an app.\nIf my experiences in any way reflect reality, it seems to make sense. Apple had to make some choices on what cutoffs to make for false positives and negatives, and I think they erred on the side of security. Having a high specificity is critical because that prevents a bad guy from accessing the phone. A low sensitivity is annoying, but not critical because the correct user could always type in a passcode. As for modifying the behavior based on the task, that seems to make sense too because you can’t buy songs or apps without first unlocking the phone.\nOverall, I think Apple did a good job with the fingerprint sensor, especially for  version 1.0. I’m guessing they’re making improvements in the technology/software as we speak and will want to improve the sensitivity before they start using it for more tasks or applications.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-11-out-with-big-data-in-with-hyperdata/",
    "title": "Out with Big Data, in with Hyperdata",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-11-11",
    "categories": [],
    "contents": "\nBig data is so last year.\n\n\nCollecting data from all sorts of odd places and analyzing it much faster than was possible even a couple of years ago has become one of the hottest areas of the technology industry. The idea is simple: With all that processing power and a little creativity, researchers should be able to find novel patterns and relationships among different kinds of information.\n\n\nFor the last few years, insiders have been calling this sort of analysis Big Data. Now Big Data is evolving, becoming more “hyper” and including all sorts of sources. Start-ups like Premise and ClearStory Data, as well as larger companies like General Electric, are getting into the act.\n\n\n…\n\n\n“Hyperdata comes to you on the spot, and you can analyze it and act on it on the spot,” said Bernt Wahl, an industry fellow at the Center for Entrepreneurship and Technology at the University of California, Berkeley. “It will be in regular business soon, with everyone predicting and acting the way Amazon instantaneously changes its prices around.”\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-05-how-to-host-a-conference-on-google-hangouts-on-air/",
    "title": "How to Host a Conference on Google Hangouts on Air",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-11-05",
    "categories": [],
    "contents": "\nWe recently hosted the first ever Simply Statistics Unconference on the Future of Statistics. In preparing for the event, we learned a lot about how to organize such an event and frankly we wished there had been a bit more organized documentation on how to do this. The various Google web sites were full of nice videos demonstrating how cool the technology is, but not much in the way of specific instructions on how to get it done.\nI posted on GitHub my step-by-step list of instructions for how to set up and run a conference on Google Hangouts on Air in the hopes that someone would find it useful. I’m also happy accept corrections if something there is not right.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-11-03-sunday-datastatistics-link-roundup-11313/",
    "title": "Sunday data/statistics link roundup (11/3/13)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-11-03",
    "categories": [],
    "contents": "\nThere has been a big knockdown-dragout battle in the blogosphere over how GTEX is doing their analysis. Read the original post here, my summary here, and the response from GTEX here. I agree that the criticism bordered on hyperbolic but also think that methods matter. I also think that consortia are under pressure to get something out and have to use software that works, I’m sympathetic cause that must be a tough position to be in, but it is important to remember software runs != software works well.\nChris Bosh thinks you should learn to code. Me too. I wonder if the Heat will give me a contract now?\nTerry Speed wins the Prime Minister’s Prize for science. Here is an awesome interview with him. Watch to the end to find out how he is gonna spend all the money.\nLearn faster with the Feynman technique. tl;dr = practice teaching what you are trying to learn.\nVia Tim T. Jr. check out this interactive version of Simpson’s paradox. Super slick and educational.\nStats used to determine the Gold Glove (in part).\nAn angry newcomer’s guide to data types in R, dangit!\nAccidental aRt - accidentally beautiful creations in R.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-30-unconference-on-the-future-of-statistics-live-stream-futureofstats/",
    "title": "Unconference on the Future of Statistics (Live Stream) #futureofstats",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-10-30",
    "categories": [],
    "contents": "\nThe Unconference on the Future of Statistics will begin at 12pm EDT today. Watch the live stream here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-29-how-to-participate-in-futureofstats-unconference/",
    "title": "How to participate in #futureofstats Unconference",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-10-29",
    "categories": [],
    "contents": "\nTomorrow is the Unconference on the Future of Statistics from 12PM-1PM EDT. There are two ways that you can get in the game:\nAsk questions for our speakers on Twitter with the hashtag #futureofstats. Don’t wait, start right now, Roger, Rafa, and I are monitoring the hashtag and collecting questions. We will pick some to ask the speakers tomorrow during the Unconference. \nIf you have an idea about the future of statistics write it up, post it on Github, on Blogger, on WordPress, on your personal website, then tweet it with the hashtag #futureofstats. We will do our best to collect these and post them with the video so your contributions will be part of the Unconference.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-29-tukey-talks-turkey-futureofstats/",
    "title": "Tukey Talks Turkey #futureofstats",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-10-29",
    "categories": [],
    "contents": "\nI’ve been digging up old “future of statistics” writings from the past in anticipation of our Unconference on the Future of Statistics this Wednesday 12-1pm EDT. Last week I mentioned Daryl Pregibon’s experience trying to build statistical expertise into software. One classic is “The Future of Data Analysis” by John Tukey published in the Annals of Mathematical Statistics in 1962.\nPerhaps the most surprising aspect of this paper is how relevant it remains today. I think perhaps with just a few small revisions it could easily be published in a journal today and few people would find it out of place.\nIn Section 3 titled “How can new data analysis be initiated?” he describes directions in which statisticians should go to grow the field of data analysis. But the advice itself is quite general and probably should be heeded by any junior statistician just starting out in research.\n\nHow is novelty most likely to begin and grow? Not through work on familiar problems, in terms of familiar frameworks, and starting with the results of applying familiar processes to the observations. Some or all of these familiar constraints must be given up in each piece of work which may contribute novelty.\n\nTukey’s article serves as a coherent and comprehensive roadmap for the development of data analysis as a field. He suggests that we should study how people analyze data and uncover “what works” and what doesn’t. However, he appears to draw the line at suggesting that such study should result in a single way of analyzing a given type of data. Rather, statisticians should maintain some flexibility in modeling and analysis. I personally think the reality should be somewhere the middle. Too much flexibility can lead to problems, but rigidity is not the solution.\nIt is interesting, from my perspective, that given how clear and coherent Tukey’s roadmap was in 1962, how much of it was essentially ignored. In fact, the field pretty much went the other direction towards more mathematical elegance (I’m guessing Tukey sensed this would happen). His article is uncomfortable to read, because it’s full of problems that arise in real data that are difficult to handle with standard approaches. He has an uncanny ability to make up methods that look totally bizarre on first glance but are totally reasonable after some thought.\nI honestly can’t think of a better way to end this post than to quote Tukey himself.\n\nThe future of data analysis can involve great progress, the overcoming of real difficulties, and the provision of a great service to all fields of science and technology. Will it? That remains to us, to our willingness to take up the rocky road of real problems in preference to the smooth road of unreal assumptions, arbitrary criteria, and abstract results without real attachments. Who is for the challenge?\n\nRead the paper. And then come join us at 12pm EDT tomorrow.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-28-simply-statistics-future-of-statistics-speakers-two-truths-one-lie-futureofstats/",
    "title": "Simply Statistics Future of Statistics Speakers - Two Truths, One Lie #futureofstats",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-10-28",
    "categories": [],
    "contents": "\nOur online conference live-streamed on Youtube is going to happen on October 30th 12PM-1PM Baltimore (UTC-4:00) time. You can find more information here or sign up for email alerts here. I get bored with the usual speaker bios at conferences so I am turning our speaker bios into a game. Below you will find three bullet pointed items of interest about each of our speakers. Two of them are truths and one is a lie. See if you can spot the lies and sign up for the unconference!\nHadley Wickham\nCreated the ggplot2/devtools packages.\nDeveloped R’s first class system.\nIs chief scientist at RStudio.\nDaniela Witten\nDeveloped the most popular method for inferring Facebook connections.\nCreated the Spacejam algorithm for inferring networks.\nMade the Forbes 30 under 30 list twice as a rising scientific star.\nJoe Blitzstein \nA Professor of the Practice of Statistics at Harvard University.\nCreated the first statistical method for automatically teaching the t-test.\nHis statistics 101 course is frequently in the top 10 courses on iTunes U.\nHongkai Ji\nDeveloped the hmChIP database of over 2,000 ChIP-Seq and ChIP-Chip data samples.\nCoordinated the analysis of the orangutan genome project.\nAnalyzed data to help us understand sonic-hedgehog mediated neural patterning.\nSinan Aral\nCoined the phrase “social networking potential”.\nRan a large randomized study that determined the value of upvotes.\nDiscovered that peer influence is dramatically overvalued in product adoption.\nHilary Mason\nIs a co-founder of DataGotham and HackNY\nDeveloped computational algorithms for identifying the optimal cheeseburger\nFounded the first company to create link sorting algorithms.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-27-sunday-datastatistics-link-roundup-102713/",
    "title": "Sunday data/statistics link roundup (10/27/13)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-10-27",
    "categories": [],
    "contents": "\nPubmed Commons is a new post-publication commenting system. I think this is a great idea and I hope it succeeds. Right now it is in “private beta” so only people with Pubmed Commons accounts can post/view comments. But you can follow along with who is making comments via this neat twitter bot. I think the main feature this lacks to be a hugely successful experiment is some way to give real, tangible academic credit to commenters. One very obvious way would be by assigning DOIs to every comment and making the comments themselves Pubmed searchable. Then they could be listed as contributions on CVs - a major incentive.\nA post on the practice of asking potential hires tricky math problems  - even if they are going to be hired to do something else (like software engineering). This happens all the time in academia as well - often the exams we give/questions we ask aren’t neatly aligned with the ultimate goals of a program (producing innovative/determined researchers).\nThis is going to be a short Sunday Links because my Coursera class is starting again tomorrow.\nDon’t forget that next week is the [ 1. Pubmed Commons is a new post-publication commenting system. I think this is a great idea and I hope it succeeds. Right now it is in “private beta” so only people with Pubmed Commons accounts can post/view comments. But you can follow along with who is making comments via this neat twitter bot. I think the main feature this lacks to be a hugely successful experiment is some way to give real, tangible academic credit to commenters. One very obvious way would be by assigning DOIs to every comment and making the comments themselves Pubmed searchable. Then they could be listed as contributions on CVs - a major incentive.\nA post on the practice of asking potential hires tricky math problems  - even if they are going to be hired to do something else (like software engineering). This happens all the time in academia as well - often the exams we give/questions we ask aren’t neatly aligned with the ultimate goals of a program (producing innovative/determined researchers).\nThis is going to be a short Sunday Links because my Coursera class is starting again tomorrow.\nDon’t forget that next week is the](https://plus.google.com/events/cd94ktf46i1hbi4mbqbbvvga358) on Wednesday, October 30th at noon Baltimore time!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-25-back-to-the-future-of-statistical-software-futureofstats/",
    "title": "(Back to) The Future of Statistical Software #futureofstats",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-10-25",
    "categories": [],
    "contents": "\nIn anticipation of the upcoming Unconference on the Future of Statistics next Wednesday at 12-1pm EDT, I thought I’d dig up what people in the past had said about the future so we can see how things turned out. In doing this I came across an old National Academy of Sciences report from 1991 on the Future of Statistical Software. This was a panel discussion hosted by the National Research Council and summarized in this volume. I believe you can download the entire volume as a PDF for free from the NAS web site.\nThe entire volume is a delight to read but I was particularly struck by Daryl Pregibon’s presentation on “Incorporating Statistical Expertise into Data Analysis Software” (starting on p. 51). Pregibon describes his (unfortunate) experience trying to develop statistical software which has the ability to incorporate expert knowledge into data analysis. In his description of his goals, it’s clear in retrospect that he was incredibly ambitious to attempt to build a kind of general-purpose statistical analysis machine. In particular, it was not clear how to incorporate subject matter information.\n\n\n\n<div>\n  <div>\n    <p>\n      [T]he major factor limiting the number of people using these tools was the recognition that (subject matter) context was hard to ignore and even harder to incorporate into software than the statistical methodology itself. Just how much context is required in an analysis? When is it used? How is it used? The problems in thoughtfully integrating context into software seemed overwhelming.\n    <\/p>\n  <\/div>\n<\/div>\n\n\n\nPregibon skirted the problem of integrating subject matter context into statistical software.\n\n\n\n<div>\n  <div>\n    <p>\n      I am not talking about integrating context into software. That is ultimately going to be important, but it cannot be done yet. The expertise of concern here is that of carrying out the plan, the sequence of steps used once the decision has been made to do, say, a regression analysis or a one-way analysis of variance. Probably the most interesting things statisticians do take place before that.\n    <\/p>\n  <\/div>\n<\/div>\n\n\n\nStatisticians (and many others) tend to focus on the application of the “real” statistical method–the regression model, lasso shrinkage, or support vector machine. But as much painful experience in a variety of fields has demonstrated, much what happens before the application of the key model is as important, or even more important.\nPregibon makes an important point that although statisticians are generally resistant to incorporating their own expertise into software, they have no problem writing textbooks about the same topic. I’ve observed the same attitude when I talk about evidence-based data analysis. If I were to guess, the problem is that textbooks are still to a certain extent abstract, while software is 100% concrete.\n\n\n\nInitial efforts to incorporate statistical expertise into software were aimed at helping inexperienced users navigate through the statistical software jungle that had been created…. Not surprisingly, such ideas were not enthusiastically embraced by the statistics community. Few of the criticisms were legitimate, as most were concerned with the impossibility of automating the “art” of data analysis. Statisticians seemed to be making a distinction between providing statistical expertise in textbooks as opposed to via software. [emphasis added]\n\n\n\nIn short, Pregibon wanted to move data analysis from an art to a science, more than 20 years ago! He stressed that data analysis, at that point in time, was not considered a process worth studying. I found the following paragraph interesting and worth considering in now, over 20 years later. He talks about the reasons for incorporating statistical expertise into software.\n\n\n\n<div>\n  <div>\n    <p>\n      The third [reason] is to study the data analysis process itself, and that is my motivating interest. Throughout American or even global industry, there is much advocacy of statistical process control and of understanding processes. <strong>Statisticians have a process they espouse but do not know anything about<\/strong>. It is the process of putting together many tiny pieces, the process called data analysis, and is not really understood. Encoding these pieces provides a platform from which to study this process that was invented to tell people what to do, and about which little is known. [emphasis added]\n    <\/p>\n  <\/div>\n<\/div>\n\n\n\n\n\nI believe we have come quite far since 1991, but I don’t think we no much more about the process of data analysis, especially in newer areas that involve newer data. The reason is because the field has not put much effort into studying the whole data analysis process.  I think there is still a resistance to studying this process, in part because it involves “stooping” to analyze data and in part because it is difficult to model with mathematics. In his presentation, Pregibon suggests that resampling methods like the bootstrap might allow us to skirt the mathematical difficulties in studying data analysis processes.\n\n\nOne interesting lesson Pregibon relates during the development of REX, an early system that failed, involves the difference between the end-goals of statisticians and non-statisticians:\n\n\n\n  <p>\n    Several things were learned from the work on REX. The first was that statisticians wanted more control. There were no users, rather merely statisticians looking over my shoulder to see how it was working. Automatically, people reacted negatively. They would not have done it that way. In contrast, non-statisticians to whom it was shown loved it. They wanted less control. In fact they did not want the system--they wanted answers.\n  <\/p>\n<\/div>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:09:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-23-the-leek-group-guide-to-reviewing-scientific-papers/",
    "title": "The Leek group guide to reviewing scientific papers",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-10-23",
    "categories": [],
    "contents": "\nThere has been a lot of discussion of peer review on this blog and elsewhere. One thing I realized is that no one ever formally taught me the point of peer review or how to write a review.\nLike a lot of other people, I have been frustrated by the peer review process. I also now frequently turn to my students to perform supervised peer review of papers, both for their education and because I can’t handle the large number of peer review requests I get on my own.\nSo I wrote this guide on how to write a review of  a scientific paper on Github. Last time I did this with R packages a bunch of people contributed to make the guide better. I hope that the same thing will happen this time.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-22-blog-posts-that-impact-real-science-software-review-and-gtex/",
    "title": "Blog posts that impact real science - software review and GTEX",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-10-22",
    "categories": [],
    "contents": "\nThere was a flurry of activity on social media yesterday surrounding a blog post by Lior Pachter. He was speaking about the GTEX project - a large NIH funded project that has the goal of understanding expression variation within and among human beings. The project has measured gene expression in multiple tissues of over 900 individuals.\nIn the post, the author claims that the GTEX project is “throwing away” 90% of its data. The basis for this claim is a simulation study using the parameters from one of the author’s papers. The claim of 90% is based on the fact that  increasing the number of mRNA fragments leads to increasing correlation in abundance measurements in the simulation study. In order to get the same Spearman correlation as other methodologies have at 10M fragments, the software being used by GTEX needs 100M fragments.\nThis post and the associated furor raises three issues:\nThe power and advantage of blog posts and social media as a form of academic communication.\nThe importance of using published software.\nExtreme critiques deserve as much scrutiny as extreme claims.\nThe first point is obvious; the post was rapidly disseminated and elicited responses from the leaders of the GTEX project. Interestingly, I think the authors got an early view of the  criticisms they would face from reviewers through the blog post. The short term criticism is probably not fun to deal with but it might save them time later.\nI think the criticism about using software that has not been fully vetted through the publication/peer review process is an important one. For such a large scale project, you’d like to see the primary analysis being done with “community approved” software.  The reason is that we just don’t know if it is better or worse because no one published a study on the software.  It would be interesting to see how the bottom up approach would have faired here. The good news for GTEX here is that for future papers they will either get out a more comprehensive comparison or they will switch software - either of which will improve their work.\nRegarding point 2, Pachter did a “back of the envelope” calculation that suggested the Flux software wasn’t performing well. These back of the envelope calculations are very important - if you can’t solve the easy case, how can you expect to solve the hard case. Lost in all of the publicity about the 90% number is that Pachter’s blog post hasn’t been vetted, either. Here are a few questions that immediately jumped to my mind when reading the blog post:\nWhy use Spearman correlation as the important measure of agreement?\nWhat is the correlation between replicates?\nWhat parameters did he use for the Flux calculation?\nWhere is his code so we can see if there were any bugs (I’m sure he is willing to share, but I don’t see a link)?\nThat 90% number seems very high, I wonder if varying the simulation approach/parameter settings/etc. would show it isn’t quite that bad\nThrowing away 90% of you data might not matter if you get the right answer to the question you care about at the end. Can we evaluate something closer to what we care about? A list of DE genes/transcripts, for example?\nWhenever a scientist sees a claim as huge as “throwing away 90% of the data” they should be skeptical. This is particularly true in genomics, where huge effects are often due to bugs or artifacts. So in general, it is important that we apply the same level of scrutiny to extreme critiques as we do to extreme claims.\nMy guess is ultimately, the 90% number may end up being an overestimate of how bad the problem is. On the other hand, I think it was hugely useful for Pachter to point out the potential issue and give GTEX the chance to respond. If nothing else, it points out (1) the danger of using  unpublished methods when good published alternatives exist and (2) that science moves faster in the era of blog posts and social media.\nDisclaimers: I work on RNA-seq analysis although I’m not an author on any of the methods being considered. I have spoken at a GTEX meeting, but am not involved in the analysis of the data. Most importantly, I have not analyzed any data and am in no position to make claims about any of the software in question. I’m just making observations about the sociology of this interaction.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-22-pubmed-commons-is-launching/",
    "title": "PubMed commons is launching",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-10-22",
    "categories": [],
    "contents": "\nPubMed, the main database of life sciences and biomedical literature, is now allowing comments and upvotes. Here is more information and the twitter handle is @PubMedCommons.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-21-why-are-the-best-relievers-not-used-when-they-are-most-needed/",
    "title": "Why are the best relievers not used when they are most needed?",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-10-21",
    "categories": [],
    "contents": "\nDuring Saturday’s ALCS game 6 the Red Sox’s manager John Farrell took out his starter in the 6th inning. They were leading by 1, but had runners on first and second with no outs. This is a hard situation to get out of without giving up a run. The chances of scoring with an average pitcher are about 64%. I am sure that with a top of the line pitcher, like Koji Uehara, this number goes down substantially. So what does a typical manager do in this situation? Because managers like to save their better relievers for the end, and it’s only the 6th inning, they will bring in a mediocre one instead. This is what Farrell did and 2 batters latter the score was 2-1 Tigers. To really understand why this is bad move, the chances of a mediocre pitcher giving up runs when starting an inning is about 28%.  So why not bring in your best reliever when the game is actually on the line? Here is an article by John Dewan with a good in -depth discussion.  Note that the Red Sox won the game 5-2 and Koji Uehara was brought in the ninth inning to get 3 outs with the bases empty and a 3 run lead.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-18-platforms-and-integration-in-statistical-research-part-22/",
    "title": "Platforms and Integration in Statistical Research (Part 2/2)",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-10-18",
    "categories": [],
    "contents": "\nIn my last post, I talked about two general approaches to conducting statistical research: platforms and integration. In this followup I thought I’d describe the characteristics of certain fields that suggesting taking one approach over another.\nI think in practice, most statisticians will dedicate some time to both the platform and integrative approaches to doing statistical research because different approaches work better in different situations. The question then is not “Which approach is better?” but rather “What characteristics of a field suggest one should take a platform / integrative approach in order to have the greatest impact?” I think one way to answer this question is to make an analogy with transaction costs a la the theory of the firm. (This kind of analogy also plays a role in determining who best to collaborate with but that’s a different post).\nIn the context of an academic community, I think if it’s easy to exchange information, for example, about data, then building platforms that are widely used makes sense. For example, if everyone uses a standardized technology for collecting a certain kind of data, then it’s easy to develop a platform that applies some method to that data. Regression methodology works in any field that can organize their data into a rectangular table. On the other hand, if information exchange is limited, then building platforms is more difficult and closer collaboration may be required with individual investigators. For example, if there is no standard data collection method or if everyone uses a different proprietary format, then it’s difficult to build a platform that generalizes to many different areas.\nThere are two case studies with which I am somewhat familiar that I think are useful for demonstrating these characteristics.\nGenomics. I think genomics is an area where you can see statisticians definitely taking both approaches. However, I’m struck by the intense focus on the development of methods and data analysis pipelines, particularly in order to adapt to the ever-changing ’omics technologies that are being developed. Part of the reason is that for a given type of data, there are relatively few companies developing the technology for collecting the data. Here, it is possible to develop a method or pipeline to deal with a new kind of data generated by a new technology in the early stages of when that data are being produced. If your method works well relative to others, then it’s possible for your method to become essentially a standard approach that everyone uses for that technology. So there’s a pretty big incentive to be the person who develops a platform for a data collection technology on which everyone builds their research. It is helpful if you can get early access to new technologies so you can get a peek at the data before everyone else and get a head start on developing the methods. Another aspect of genomics is that the field is quite open relative to others, in that there is quite a bit of information sharing. With the enormous amount of publicly available data out there, there’s a very large population of potential users of your method/software. Those people who don’t collect their own primary data can still take your method and apply it to data that’s already out there. Therefore, I think from a statistician’s point of view, genomics is a field that presents many opportunities to build platforms that will be used by many people addressing many different types of questions.\nEnvironmental Health. The area of environmental health, where I generally operate, is a much smaller field than genomics. You can see this by looking at things like journal impact factors and h-indices. It does not have the same culture as genomics and relatively little data is shared openly and there are typically no requirements from journals to make data available upon publication. Data are often very expensive and time-consuming to collect, particularly if you are running large cohorts and are monitoring things like personal exposure. There are no real standardized methods for data collection and many formats are proprietary. Statisticians in this area tend to be attached to larger groups who run studies or collect human health and exposure data. It’s relatively hard to be an independent statistician here because you need access to a collaborator who has relevant expertise, resources, and data. The lack of publicly available health data severely limits the participation of statisticians outside biomedical research institutions where the data are collected primarily. I would argue that in environmental health, the integrative approach is more fruitful because (1) in order to do the work in the first place you already need be working closely with people collecting the health data; (2) there is a general lack of information sharing and standardization with respect to data collection; (3) if you develop a new tool, there is not a particularly large audience available to adopt those tools; (4) because studies are not unified by shared technologies, as in genomics, it’s often difficult to usefully generalize methodology from one study to the next. While I think it’s possible to develop general methodology for a certain type of study in this field, the impact is inherently limited due to the small size of the field.\nIn the end I think areas that are ripe for the platform approach to statistical research are those that are very open and have culture of information sharing, have a large community of active methodologists, and have a lot of useful publicly available data. Areas that do not have these qualities might be better served by an integrative approach where statisticians work more directly with scientific collaborators and focus on the specific questions and problems of a given study.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-17-the-fivethirtyeight-effect-watching-walthickey-gain-twitter-followers-in-real-time/",
    "title": "The @fivethirtyeight effect - watching @walthickey gain Twitter followers in real time",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-10-17",
    "categories": [],
    "contents": "\nLast night Nate Silver announced that he had hired Walt Hickey away from Business Insider to be an editor for the new http://www.fivethirtyeight.com/ website with a couple of tweets:\n\n\nSuper excited to announce that 538 is hiring @WaltHickey, the talented young writer/journalist/data geek from Business Insider.\n\n\n— Nate Silver (@NateSilver538) October 16, 2013\n\n\n\n\n.@WaltHickey will have a similarly broad range for 538, bringing a data-driven view toward all types of things. Give him a follow!\n\n\n— Nate Silver (@NateSilver538) October 16, 2013\n\n\nI knew about Walt because he syndicated one of my posts about Fox News Graphics on Business Insider. But he clearly wasn’t as well known as Nate S. who is probably the face of statistical analysis to most people in the world. So I figured the announcement might increase Walt’s following on Twitter.\nAfter goofing around a bit with the Twitter api and the twitteR R package. I managed to start sampling the number of followers for Walt H. This started about an hour or so (I think) after the announcement was made, here is a plot of Walt’s followers over about two hours.\n\nOver the two hours he gained almost 1,000 followers! We can also take a look at the rate he was gaining followers.\nn\nSo he was gaining followers at around 10-15 per minute on average at 7:30 yesterday. It cooled off over those two hours, but he was still getting a few followers a minute. To put those numbers in perspective, our Twitter account @simplystats, gets on average about 10 new followers a day.\nSo there you have it, the real time (albeit two hours too late) 538 bump in Twitter followers.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-15-platforms-and-integration-in-statistical-research-part-12/",
    "title": "Platforms and Integration in Statistical Research (Part 1/2)",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-10-15",
    "categories": [],
    "contents": "\nIn the technology world today, one of the common topics of interest is the so-called “war” between Apple and Google (or Android). This war is ostensibly over dominance of the mobile phone industry, where Apple sells the most popular phone but Google/Android (as an operating system) controls over half the market. (Android phones themselves are manufactured by a variety of companies and no one of those companies sells more phones than Apple.)\nApple vs. Google (vs. Microsoft)\nApple’s model is to own the entire (or most of the) development of the phone. They build the hardware, the software, and create the design. They also control the App Store for selling their own software and third party software, distribute the music from their iTunes store, and distribute the e-books through their iBookstore. They even have their own proprietary messaging platform. This “walled-garden” approach is a hallmark of Apple and its famously controlling founder Steve Jobs. Rather than “walled-garden”, I would call it more of an “integrative” approach, where Apple essentially has its fingers in all the relevant pies, controlling every aspect.\nThe Google/Android approach is more modular and involves controlling the platform on which pretty much every phone could theoretically be built. Until recently, Google did not build their own phones, but rather let other companies build the phones and use Google’s operating system as the software for the phone. The model here is similar to the Unix philosophy, which is to “do one thing well”. Google is really good at developing Android and handset manufacturers are really good at building phones. There’s no point in one company doing two things moderately well when you could have two companies each do one thing really well. Here, Google focuses on the platform, the Android operating system, and tries to spread it as far and wide as possible to cover the most possible phones, tablets, watches, whatever mobile device is relevant.\nFor us older people, the more relevant “war” is between Microsoft and everyone else. Microsoft built one of the most legendary platforms in computer history-–the Windows operating system. For decades this platform was (and continues to be) the dominant operating system for personal desktop computers. Although Microsoft never really focused on building its own hardware, Microsoft’s impact on the PC world through its control of Windows is undeniable. Unfortunately, an asterisk must be put next to all of this history because we now know that much of this dominance was achieved through criminal activity.\nTheory and Methods vs. Applications\nThere’s much debate in the technology world over which approach is better, the Apple integrative model or the Google/Microsoft modular platform model. I think this “debate” exists because it’s fun to argue about Apple vs. Google and it gives technology reporters something to write about. When the dust settles (if ever) I think the answer will be “it depends”.\nIn the statistical community I find there’s often an analogous debate that goes on regarding which is the more important form of statistical activity, theory/methods or applications. In a nutshell (perhaps even a cartoon nutshell) there’s a sense that theoretical or abstract methodological development has a greater impact because it is broadly generalizable to many different areas. Applications work is less impactful because it is focused on a specific area and any lessons learned that might be applicable to other areas would only be realized much later.\nWe could spend a lot of time debating the specific arguments here (and I have already spent that time!) but I think a better way to frame this debate is to use the analogy of Apple and Google, that is between integrative statistical research and platforms research. In particular, I think the “theory vs. applications” moniker is a bit outdated and does not cover many of the recent developments in the field of statistics.\nPlatforms in Statistics\nWhen I was in graduate school and learning about being a statistician, it was pretty much hammered into my brain that the ultimate goal of a statistician is to build a platform. It was not described to me in those words, but that was the essential message. The basic idea was that you would develop a new method that was as general as possible so that it could be applied to a wide variety of fields, from agriculture to zoology. Ideally, you would demonstrate that this method was better than any other method through some sort of theorem.\nThey ultimate platform in statistics might be the t-test, or perhaps the p-value. Those two statistical methods are used in some form in almost any scientific context you could possibly imagine. I’d argue that the p-value is the Microsoft Windows of science. (Much like with Windows, you could argue this is for better or for worse.) Other essential platforms in statistics might be linear regression, generalized linear models, the bootstrap, the EM algorithm, etc. If you could be the developer of one of these platforms your impact would be tremendous because everyone in every discipline would use it. That’s why Ronald Fisher should be the most influential scientist ever.\nI think the notion of a platform, rather than theory/methods, is a much more useful context here because it more accurately describes why these things are so important. Generalized linear models may be interesting because they represent an abstract concept of linear relationships, but it’s useful because it’s a platform on which a ton of other research in many other fields can be built. If we accept the idea that something is important because it serves as a platform on which many other things can be built, then I think this idea encompasses more than what might be published in the pages of the Journal of the American Statistical Association or the Annals of Statistics.\nIn particular, I think one of the greatest statistical platforms developed in the last 10 to 15 years is R. If you consider what R really is, yes it’s a software package that does statistical calculations, but primarily it’s a platform on which an enormous community of people can build things. The Comprehensive R Archive Network is the “App Store” through which statisticians can develop and distribute their tools. R itself has been extended (through packages) and applied to almost every imaginable scientific discipline. Take one look at the Task Views section to get a sense of the diversity of areas to which R has been applied. Entire sub-projects (i.e. Bioconductor) have been developed around using R in specific fields of research. At this point the impact of R on both the sciences and on statistics is as undeniable as the t-test.\nIntegrative Statistical Research\nIntegrative research in statistics is something that I think harks back to a much earlier era in the history of statistics, the era in which the field of statistics didn’t really exist. Before the field really had solidified itself as a separate discipline, people “doing statistics” came from all areas of science as well as mathematics. Here, the statistician was involved in all aspects of research and not just walled-off in a separate area dreaming up abstract methods. Many methods were later abstracted and generalized, but this largely grew out of an initial need to solve a specific problem.\nAs the field matured and separate Departments of Statistics started to appear, the discipline moved more towards a platform approach by focusing more on abstraction and generalizable approaches. It’s easy to see why this move would occur. If you’re trying to distinguish your discipline as being separate from other disciplines (and therefore deserving of separate resources), you need to demonstrate a unique contribution that is separate from the other fields and, in a sense, wall yourself off a little from the others. Given that computers were generally available at the time this move began, mathematics was the most useful and easily accessible tool to build new statistical platforms.\nToday, I think the field of statistics is moving back towards the old model of integrating closer with scientists in other disciplines. In particular, we are seeing more and more people “invading” the field from other related areas like computer science, just like the old days. Personally, I think these “outsiders” should be welcomed under our tent as they bring unique insights to our field and provide a richness not otherwise obtainable.\nWith the integrative statistical research model we see more statisticians “embedded” into the sciences, in the the thick of it, so to speak, with involvement in every aspect. They publish in discipline-specific journals and in some cases are flat-out leading large-scale scientific collaborations. The reasons for this are many, but I think are centered around advances in computer technology that has allowed for the rapid collection of large and complex datasets and the easy implementation of sophisticated models. The heterogeneity and unique complexity of these different datasets has required statisticians to dig deeper into the field and learn more of the substantive details before a useful contribution can be made. This accumulation of deep knowledge of a field occurs at the expense of being able to work in many different fields at once, or as John Tukey said, to “play in everyone’s backyard”.\nThe integrative approach to statistical research is exciting because it allows for the statistician to have a direct impact on a scientific discipline rather than a more indirect one through developing platforms. However, the approach is resource intensive in that it requires an interdisciplinary research environment with good collaborators in the relevant disciplines. As such, it may only be possible to take the integrative approach in certain institutions and environments. I think a similar argument could be made with respect to conducting platform research but I think there are many cases there where it was not strictly necessary.\nIn the next post, I’ll talk a bit (and give examples) about where I think the platform and integrative approaches may be more or less fruitful.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-15-teaching-least-squares-to-a-5th-grader-by-calibrating-a-programmable-robot/",
    "title": "Teaching least squares to a 5th grader by calibrating a programmable robot",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-10-15",
    "categories": [],
    "contents": "\nThe Lego Mindstorm kit provides software and hardware to create programmable robots. A very simple first task is figuring out how to make the robot move any given distance. You get to program the number of wheel rotations. The video below shows how one can use this to motivate and teach least squares. We assumed the formula was distance = K × rotations, collected data for 1,2…, 10 rotations, then used R to motivate (via plots) and calculate the least squares estimate of K.\nNot shown in the video is my explanation of how we could also use the formula circumference  = pi x diameter to figure out K and a discussion about which approach is better.  Next project will be to calibrate turns which are achieved by rotating the wheels in opposite directions. This time I will use both the geometric approach (compute the wheel circumference and the circumference defined by robot turns) and the statistical approach.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-14-why-did-lars-peter-hansen-win-the-nobel-prize-generalized-method-of-moments-explained/",
    "title": "A general audience friendly explanation for why Lars Peter Hansen won the Nobel Prize",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-10-14",
    "categories": [],
    "contents": "\nLars Peter Hansen won the Nobel Prize in economics for creating the generalized method of moments. A rather technical explanation of the idea appears on Wikipedia. These are a good set of lecture notes on gmms if you like math. I went over to Marginal Revolution to see what was being written about the Nobel Prize winners. Clearly a bunch of other people were doing the same thing as the site was pretty slow to load. Here is what Tyler C. says about Hansen. In describing Hansen’s work he says:\n\nFor years now journalists have asked me if Hansen might win, and if so, how they might explain his work to the general reading public.  Good luck with that one.\n\nAlex T. does a good job of explaining the idea, but it still seems a bit technical for my tastes. Guan Y.  does another good, and a little less technical explanation here, but it is still a little rough if you aren’t an economist. So I took a shot at an even more “general audience friendly” version below.\nA very common practice in economics (and most other scientific disciplines) is to collect experimental data on two (or more) variables and to try to figure out if the variables are related to each other. A huge amount of statistical research is dedicated to this relatively simple-sounding problem. Lars Hansen won the Nobel Prize for his research on this problem because:\nEconomists (and scientists) hate assumptions they can’t justify with data and want to use the fewest number possible. The recent Rogoff and Reinhart controversy illustrates this idea. They wrote a paper that suggested public debt was bad for growth. But when they estimated the relationship between variables they made assumptions (chose weights) that have been questioned widely - suggesting that public debt might not be so bad after all. But not before a bunch of politicians used this result to justify austerity measures which had a huge impact on the global economy.\nEconomists (and mathematicians) love to figure out the “one true idea” that encompasses many ideas. When you show something about the really general solution, you get all the particular cases for free. This means that all the work you do to show some statistical procedure is good helps not just you in a general sense, but all the specific cases that are examples of the general things you are talking about.\nI’m going to use a really silly example to illustrate the idea. Suppose that you collect information on the weight of animals bodies and the weight of their brains. You want to find out how body weight and brain weight are related to each other. You collect the data, they might look something like this:\nSo it looks like if you have a bigger body you have a bigger brain (except for poor old Triceratops who is big but has a small brain). Now you want to say something quantitative about this. For example:\n\nAnimals that are 1 kilogram larger have a brain that is on average k kilograms larger.\n\nHow do you figure that out? Well one problem is that you don’t have infinite money so you only collected information on a few animals. But you don’t want to say something just about the animals you measured - you want to change the course of science forever and say something about the relationship between the two variables for all animals.\nThe best way to do this is to make some assumptions about what the measurements of brain and body weight look like if you could collect all of the measurements. It turns out if you assume that you know the complete shape of the distribution in this way, it becomes pretty straightforward (with a little math) to estimate the relationship between brain and body weight using something called maximum likelihood estimation. This is probably the most common way that economists or scientists relate one variable to another (the inventor of this approach is still waiting for his Nobel).\nThe problem is you assumed a lot to get your answer. For example, here are the data from just the brains that we have collected. It is pretty hard to guess exactly what shape the data from the whole world would look like.\n\nThis presents the next problem: how do we know that we have the “right one”?\nWe don’t.\nOne way to get around this problem is to use a very old idea called the  method of moments. Suppose we believe the equation:\n\nAverage in World Body Weight = k * Average in World Brain Weight\n\n\nIn other words, if we take any animal in the world on average it’s brain weights 5 kilos then its body will on average be (k * 5) kilos. The relationship is only “on average” because there are a bunch of variables we didn’t measure and they may affect the relationship between brain and body weight. You can see it in the scatterplot because the two values don’t lie on the same line.\n\n\nOne way to estimate k is to just replace the numbers you wish you knew with the numbers you have in your population:\n\nAverage in Data you Have Body Weight = k * Average in Data you Have Brain Weight\nSince you have the data the only thing you don’t know in the equation is k, so you can solve the equation and get an estimate. The nice thing here is we don’t have to say much about the shape of the data we expect for body weight or brain weight. We just have to believe this one equation.  The key insight here is that you don’t have to know the whole shape of the data, just one part of it (the average).  An important point to remember is that you are still making some assumptions here (that the average is a good thing to estimate, for example) but they are definitely fewer assumptions than you make if you go all the way and specify the whole shape, or distribution, of the data.\nThis is a pretty oversimplified version of the problem that Hansen solved. In reality when you make assumptions about the way the world works you often get more equations like the one above than variables you want to estimate. Solving all of those equations is now complicated because the answers from different equations  might contradict each other (the technical word is overdetermined).\nHansen showed that in this case you can take the equations and multiply them by a set of weights. You put more weight on equations you are more sure about, then add them up. If you choose the weights well, you avoid the problem of having too many equations for two few variables. This is the thing he won the prize for - the generalized method of moments.\nThis is all a big deal because the variables that economists measure frequently aren’t very pretty. One common way they aren’t pretty is that they are often measured over time, with complex relationships between values at different time points. That means it is hard to come up with realistic assumptions about what the data may look like.\nBy proposing an approach that doesn’t require as many assumptions Hansen satisfied criteria (1) for things economists like. And, if you squint just right at the equations he proposed, you can see they actually are a general form of a bunch of other estimation techniques like maximum likelihood estimation and instrumental variables, which made it easier to prove theoretical results and satisfied criteria (2) for things economists like.\n -—\nDisclaimer: This post was written for a general audience and may cause nerd-rage in those who see (important) details I may have skimmed over. \n_Disclaimer #2: I’m not an economist. So I can’t talk about economics. T__here are reasons gmm is useful economically that I didn’t even talk about here._\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-13-sunday-datastatistics-link-roundup-101313/",
    "title": "Sunday data/statistics link roundup (10/13/13)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-10-13",
    "categories": [],
    "contents": "\nA really interesting comparison between educational and TV menus (via Rafa). On a related note, it will be interesting to see how/whether the traditional educational system will be disrupted. I’m as into the MOOC thing as the next guy, but I’m not sure I buy a series of pictures from your computer as “validation” you took/know the material for a course. Also I’m not 100% sure about what this is, but it has the potential to be kind of awesome - the Moocdemic.\nThis piece of “investigative journalism” had the open-access internet up in arms. The piece shows pretty clearly that there are bottom-feeding journals who will use unscrupulous tactics and claim peer review while doing no such thing. But it says basically nothing about open access as far as I can tell. On a related note, a couple of years ago we developed an economic model for peer review, then tested the model out. In a very contrived/controlled system we showed peer review improves accuracy, even when people aren’t incentivized to review.\nRelated to our guest post on NIH study sections is this pretty depressing piece in Nature.\nOne of JHU Biostat’s NSF graduate research fellows was interviewed by Amstat News.\nJenny B. has some great EDA lectures you should check out.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-11-why-do-we-still-teach-a-semester-of-trigonometry-how-about-engineering-instead/",
    "title": "Why do we still teach a semester of trigonometry? How about engineering instead?",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-10-11",
    "categories": [],
    "contents": "\nArthur Benjamini says we should teach statistics before calculus.  He points out that most of what we do in high school math is preparing us for calculus. He makes the point that while physicists, engineers and economists need calculus, in the digital age, discrete math, probability and statistics are much more relevant to everyone else. I agree with him and was happy to see Statistics as part of the common core. However, other topics I wish were there, such as engineering, programming, and finance, are missing.\nThis saturday I took my 5th grader to a 3 hour robotics workshop. We both enjoyed it thoroughly. We built and programmed two-wheeled robots to, among other things, go around a table. To make this happen we learned about measurement error,  how to use a protractor, that C =   ∏ d, a bit of algebra, how to use grid searches, if-else conditionals, and for-loops. Meanwhile during a semester of high school trigonometry we learn this (do you remember that 2 sin^2 x = 1-cos 2x  ? ). Of course it is important to know trigonometry, but do we really need to learn to derive and memorize these identities that are rarely use and are readily available from a smartphone? One could easily teach the fundamentals as part of an applied class such as robotics. We can ask questions like: if while turning you make a mistake of 0.5 degrees, by how much will your robot miss its mark after traveling one meter? We can probably teach the fundamentals of trigonometry in about 2 weeks, later using these concepts in applied problems.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-10-cancelled-nih-study-sections-a-subtle-yet-disastrous-effect-of-the-government-shutdown/",
    "title": "Cancelled NIH study sections: a subtle, yet disastrous, effect of the government shutdown",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-10-10",
    "categories": [],
    "contents": "\nEditor’s note: _This post is contributed by Debashis Ghosh. Debashis is the chair of __the Biostatistical Methods and Research Design (BMRD) study sections at the National Institutes of Health (NIH).  BMRD’s focus is statistical methodology._\nI write today to discuss effects of the government shutdown that will likely have disastrous long-term effects on the state of biomedical and scientific research.  A list of the sections can be found at http://public.csr.nih.gov/StudySections/Standing/Pages/default.aspx.   These are panels of distinguished scientists in their fields that meet three times a year to review grant submissions to the NIH by investigators.  For most professors and scientists that work in academia, these grants provide the means of conducting research and funding for staff such as research associates, postdocs and graduate students.  At most universities and medical schools in the U.S., having an independent research grant is necessary for assistant professors to be promoted and get tenure (of course, there is some variation in this across all universities).\nYesterday, I was notified by NIH that the BMRD October meeting was cancelled and postponed until further notice.  I could not communicate with NIH staff about this because they are on furlough, meaning that they are not able to send or receive email or other communications.   This means that our study section will not be reviewing grants in October.  People who receive funding from NIH grants are familiar with the usual routine of submitting grants three times a year and getting reviewed approximately 6 months after submission.  This process has now stopped because of the government shutdown, and it is unclear when it will restart.  The session I chair is but one of 160 regular study sections and many of them would be meeting in October.  In fact, I was involved with a grant submitted to another study section that would have met on October 8, but this meeting did not happen.\nThe stoppage has many detrimental consequences.  Because BMRD will not be reviewing the submitted grants at the scheduled time, they will lack a proper scientific evaluation.  The NIH review process separates the scientific evaluation of grants from the actual awarding of funding.   While there have been many criticisms of the process, it has also been acknowledged that that the U.S. scientific research community has been the leader in the world, and NIH grant review has played a role in this status.   With the suspension of activities, the status that the U.S. currently enjoys is in peril.   It is interesting to note that now many countries are attempting to install a review process similar to the one at NIH (R. Nakamura, personal communication).\nThe effects of the shutdown are perilous for the investigators that are submitting grants.  Without the review, their grants cannot be evaluated and funded.  This lag in the funding timeline stalls research, and in scientific research a slow stall now is more disastrous in the long term.   The type of delay described here will mean layoffs for lab technicians and research associates that are funded by grants needing renewal as well as a hiring freeze for new lab personnel using newly funded grants.   This delay and loss of labor will diminish the existing scientific knowledge base in the U.S., which leads to a loss of the competitive advantage we have enjoyed as a nation for decades in science.\nEconomically, the delay has a huge impact as well.   Suppose there is a delay of three months in funding decisions.  In the case of NIH grants, this is tens of millions of dollars that is not being given out for scientific research for a period of three months. The rate of return of these grants has been estimated to be 25 – 40 percent a year (http://www.faseb.org/portals/0/pdfs/opa/2008/nih_research_benefits.pdf), and the findings from these grants have the potential to benefit 1,000s of patients a year by increasing their survival or improving the quality of their lives.   In the starkest possible terms, more medical patients will die and suffer because the government shutdown is forcing the research that provides new methods of diagnosis and treatment to grind to a halt.\nNote: The opinions expressed here represent my own and not those of my employer, Penn State University, nor those of the National Institutes of Health nor the Center for Scientific Review.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-09-the-care-and-feeding-of-your-scientist-collaborator/",
    "title": "The Care and Feeding of Your Scientist Collaborator",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-10-09",
    "categories": [],
    "contents": "\nEditor’s Note: This post written by Roger Peng is part of a two-part series on Scientist-Statistician interactions. The first post was written by Elizabeth C. Matsui, an Associate Professor in the Division of Allergy and Immunology at the Johns Hopkins School of Medicine.\nThis post is a followup to Elizabeth Matsui’s previous post for scientists/clinicians on collaborating with biostatisticians. Elizabeth and I have been working for over half a decade and I think the story of how we started working together is perhaps a brief lesson on collaboration in and of itself. Basically, she emailed someone who didn’t have time, so that person emailed someone else who didn’t have time, so that person emailed someone else who didn’t have time, so that person emailed me, who as a mere assistant professor had plenty of time! A few people I’ve talked to are irked by this process because it feels like you’re someone’s fourth choice. But personally, I don’t care. I’d say almost all my good collaborations have come about this way. To me, it either works or it doesn’t work, regardless of where on the list you were when you were contacted.\nI’ve written before about how to find good collaborators (although I neglected to mention the process described above), but this post tries to answer the question, “Now that I’ve found this good collaborator, what do I do with her/him?” Her are some thoughts I’ve accumulated over the years.\nUnderstand that a scientist is not a fountain from which “the numbers” flow. Most statisticians like to work with data, and some even need it to demonstrate the usefulness of their methods or theory. So there’s a temptation to go “find a scientist” to “give you some data”. This is starting off on the wrong foot. If you picture your collaborator as a person who hands over the data and then you never talk to that person again (because who needs a clinician for a JASA paper?), then things will probably not end up so great. And I think there are two ways in which the experience will be sub-optimal. First, your scientist collaborator may feel miffed that you basically went off and did your own thing, making her/him less inclined to work with you in the future. Second, the product you end up with (paper, software, etc.) might not have the same impact on science as it would have had if you’d worked together more closely. This is the bigger problem: see #5 below.\nAll good collaborations involve some teaching: Be patient, not patronizing. Statisticians are often annoyed that “So-and-so didn’t even know this” or “they tried to do this with a sample size of 3!” True, there are egregious cases of scientists with a lack of basic statistical knowledge, but in my experience, all good collaborations involve some teaching. Otherwise, why would you collaborate with someone who knows exactly the same things that you know? Just like it’s important to take some time to learn the discipline that you’re applying statistical methods to, it’s important to take some time to describe to your collaborator how those statistical methods you’re using really work. Where does the information in the data come from? What aspects are important; what aspects are not important? What do parameter estimates mean in the context of this problem? If you find you can’t actually explain these concepts, or become very impatient when they don’t understand, that may be an indication that there’s a problem with the method itself that may need rethinking. Or maybe you just need a simpler method.\nGo to where they are. This bit of advice I got from Scott Zeger when I was just starting out at Johns Hopkins. His bottom line was that if you understand where the data come from (as in literally, the data come from this organ in this person’s body), then you might not be so flippant about asking for an extra 100 subjects to have a sufficient sample size. In biomedical science, the data usually come from people. Real people. And the job of collecting that data, the scientist’s job, is usually not easy. So if you have a chance, go see how the data are collected and what needs to be done. Even just going to their office or lab for a meeting rather than having them come to you can be helpful in understanding the environment in which they work. I know it can feel nice (and convenient) to have everyone coming to you, but that’s crap. Take the time and go to where they are.\nTheir business is your business, so pitch in. A lot of research (and actually most jobs) involves doing things that are not specifically relevant to your primary goal (a paper in a good journal). But sometimes you do those things to achieve broader goals, like building better relationships and networks of contacts. This may involve, say, doing a sample size calculation once in a while for a new grant that’s going in. That may not be pertinent to your current project, but it’s not that hard to do, and it’ll help your collaborator a lot. You’re part of a team here, so everyone has to pitch in. In a restaurant kitchen, even the Chef works the line once in a while. Another way to think of this is as an investment. Particularly in the early stages there’s going to be a lot of ambiguity about what should be done and what is the best way to proceed. Sometimes the ideal solution won’t show itself until much later (the so-called “j-shaped curve” of investment). In the meantime, pitch in and keep things going.\nYour job is to advance the science. In a good collaboration, everyone should be focused on the same goal. In my area, that goal is improving public health. If I have to prove a theorem or develop a new method to do that, then I will (or at least try). But if I’m collaborating with a biomedical scientist, there has to be an alignment of long-term goals. Otherwise, if the goals are scattered, the science tends to be scattered, and ultimately sub-optimal with respect to impact. I actually think that if you think of your job in this way (to advance the science), then you end up with better collaborations. Why? Because you start looking for people who are similarly advancing the science and having an impact, rather than looking for people who have “good data”, whatever that means, for applying your methods.\nIn the end, I think statisticians need to focus on two things: Go out and find the best people to work with and then help them advance the science.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-08-the-care-and-feeding-of-the-biostatistician/",
    "title": "The Care and Feeding of the Biostatistician",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-10-08",
    "categories": [],
    "contents": "\nEditor’s Note: This guest post was written by Elizabeth C. Matsui, an Associate Professor in the Division of Pediatric Allergy and Immunology at the Johns Hopkins School of Medicine.\nI’ve been collaborating with Roger for several years now and we have had quite a few discussions about characteristics of a successful collaboration between a clinical investigator and a biostatistician.  I can’t remember for certain, but think that this cartoon may have been the impetus for some of our discussions. I have joked that I should write a guide for clinical investigators entitled, “The Care and Feeding of the Biostatistician.”  Fortunately, Roger has a good sense of humor and appreciates the ironic title, so asked me to write down a few thoughts for Simply Statistics.  Forging successful collaborations may seem less important than other skills such as grant writing, but successful collaboration is an important determinant of career success, and for many people, an enormous source of career satisfaction. And in the current scientific environment in which large, complex datasets and sophisticated quantitative and data visualization methods are becoming increasingly common, collaboration with biostatisticians is necessary to harness the full potential of your data and to have the greatest scientific impact.  In some cases, not engaging a biostatistical collaborator may put you at risk of making statistical missteps that could result in erroneous results.\nBe respectful of time. This tenet, of course, is applicable to all collaborations, but may be a more common stumbling block for clinical investigators working with biostatisticians. Most power estimates and sample size calculations, for example, are more complex than appreciated by the clinical investigator.  A discussion about the research question, primary outcome, etc. is required and some thought has to go into determining the most appropriate approach before your biostatistician collaborator has even laid hands on the keyboard and fired up R.  At a minimum, engage your biostatistician collaborator earlier than you might think necessary, and ideally, solicit their input during the planning stages.  Engaging a biostatistician sooner rather than later not only fosters good will, but will also improve your science. A biostatistician’s time, like yours, is valuable, so respect their time by allocating an appropriate level of salary support on grants.  Most academicians I come across appreciate that budgets are tight, so they understand that they may not get the level of salary support that they think is most appropriate.  However, “finding room” in the budget for 1% salary support for a biostatistician sends the message that the biostatistician is an afterthought, a necessity for a sample size calculation and a competitive grant application, but in the end, just a formality.   Instead, dedicate sufficient salary support in your grant to support the level of biostatistical effort that will be needed.  This sends the message that you would like your biostatistician collaborator to be an integral part of the investigator team and provides an opportunity for the kind of regular, ongoing interactions that are needed for productive collaborations.\nUnderstand that a biostatistician is not a computational tool.  Although sample size and power calculations are probably the most common service solicited from biostatisticians, and biostatisticians can be enormously helpful in this arena, they have the most impact when they are engaged in discussions about study designs and analytic approaches for a scientific question. Their quantitative approach to scientific problems provides a fresh perspective that can increase the scientific impact of your work.  My sense is that this is also much more interesting work for a biostatistician than sample size and power calculations, and engaging them in interesting work goes a long way towards cementing a mutually productive collaboration.\nMake an effort to learn the language of biostatistics. Technical jargon is a serious impediment to successful collaboration.  Again, this is true of all cross-discipline collaborations, but may be particularly true in collaborations with biostatisticians.  The field has a penchant for eponymous methods (Hosmer-Lemeshow, Wald, etc.) and terminology that is entertaining, but not intuitive (jackknife, bootstrapping, lasso).  While I am not suggesting that a clinical investigator needs to enroll in biostatistics courses (why gain expertise in a field when your collaborator provides this expertise), I am advocating for educating yourself about the basic concepts and terminology of statistics.  Know what is meant by: distribution of a variable, predictor variable, outcome variable, and variance, for example. There are some terrific “Biostatistics 101”-type lectures and course materials online that are excellent resources.  But also lean on your biostatistician collaborator by asking him/her to explain terminology and teach you these basics and do not be afraid to ask questions.\nWhen all else fails (and even when all else doesn’t fail), draw pictures. In truth, this is often the place where I start when I first engage a biostatistician. Showing your biostatistician collaborator what you expect your data to look like in a figure or conceptual diagram simplifies communication as it avoids use of jargon and biostatisticians can readily grasp the key information they need from a figure or diagram to come up with a sample size estimate or analytic approach.\nTeach them your language.  Clinical medicine is also rife with jargon, and just as biostatistical jargon can make it difficult to communicate clearly with a biostatistician, so can clinical jargon.  Avoid technical jargon where possible, and define terminology where it is not possible.  Educate your collaborator about the background, context and rationale for your scientific question and encourage questions.\nGenerously share your data and ideas.  In many organizations, biostatisticians are very interested in developing new methods, applying more sophisticated methods to an “old” problem, and/or answering their own scientific questions. Do what you can to support these career interests, such as sharing your data and your ideas. Sharing data opens up avenues for increasing the impact of your work, as your biostatistician collaborator has opportunities to develop quantitative approaches to answering research questions related to your own interests.  Sharing data alone is not sufficient, though. Discussions about what you see as the important, unanswered questions will help provide the necessary background and context for the biostatistician to make the most of the available data.  As highlighted in a recent book, giving may be an important and overlooked component of success, and I would argue, also a cornerstone of a successful collaboration.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-07-the-leek-group-policy-for-developing-sustainable-r-packages/",
    "title": "The Leek group policy for developing sustainable R packages",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-10-07",
    "categories": [],
    "contents": "\nAs my group has grown over the past few years and I have more people writing software, I have started to progressively freak out more and more about how to make sure that the software is sustainable as students graduate and move on to bigger and better things. I am also concerned with maintaining quality of the software we are developing in a field where the pace of development/discovery is so high.\nAs a person who simultaneously (a) has no formal training in CS or software development and (b) believes that if there is no software there is no paper I am worried about creating a bunch of unsustainable software. So I solicited the advice of people around here who know more about it than I do and I collected my past experience with creating software and how I screwed it up. I put it all together in the Leek group guide to building and maintaing software packages.\nThe guide covers (among other things):\nWhen to start building a package\nHow to version the package\nHow to document the package\nWhat not to include\nHow to build unit tests\nHow to create a vignette\nThe commitment I expect in terms of software maintenance\nI put it on Github because I’m still not 100% sure I got it right. The policy takes effect as of now. But I would welcome feedback/pull requests on how we can improve the policy to make it better and reduce the probability that I end up with a bunch of broken packages when all my awesome students, who are much better coders than me, eventually graduate.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-06-sunday-datastatistics-link-roundup-1062013/",
    "title": "Sunday data/statistics link roundup (10/6/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-10-06",
    "categories": [],
    "contents": "\nA fascinating read about applying decision theory to mathematical proofs. They talk about Type I and Type II errors and everything. \nStatistical concepts explained through dance. Even for a pretty culture-deficient dude like me this is cool.\nLots of good talks from the WIN Workshop, including by one of our speakers for the Unconference on the Future of Statistics.\nThe best advice for graduate students (or any academics) I have seen in my time writing the Sunday Links. You must try, and then you must ask (via Seth F.).\nAlberto C. has a MOOC on infographics and visualization that looks pretty cool. That way you can avoid this kind of thing.\nThis picture is awesome. Nothing to do with statistics. (via @AstroKatie).\nIf you aren’t reading Thomas L.’s notstatschat, you should be.\nKarl B. has an interesting presentation on open access that is itself open access. First Beamer theme I’ve seen that didn’t make me want to cover my eyes in sadness. My only problem is I wish open access publishing wasn’t so expensive. Can’t we just use a blog/figshare to publish journals that are almost as good. This dude says peer review is old news anyway.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-04-repost-finding-good-collaborators/",
    "title": "Repost: Finding good collaborators",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-10-04",
    "categories": [],
    "contents": "\nEditor’s note: Simply Statistics is still freaking out about the government shut down and potential impending economic catastrophe if the debt ceiling isn’t raised. Since anything new we might write seems trivial compared to what is going on in Washington, we are reposting an awesome old piece by Roger on finding good collaborators. \nThe job of the statistician is almost entirely about collaboration. Sure, there’s theoretical work that we can do by ourselves, but most of the impact that we have on science comes from our work with scientists in other fields. Collaboration is also what makes the field of statistics so much fun.\nSo one question I get a lot from people is “how do you find good collaborations”? Or, put another way, how do you find good collaborators? It turns out this distinction is more important than it might seem.\nMy approach to developing collaborations has evolved over time and I consider myself fairly lucky to have developed a few very productive and very enjoyable collaborations. These days my strategy for finding good collaborations is to look for good collaborators. I personally find it important to work with people that I like as well as respect as scientists, because a good collaboration is going to involve a lot of personal interaction. A place like Johns Hopkins has no shortage of very intelligent and very productive researchers that are doing interesting things, but that doesn’t mean you want to work with all of them.\nHere’s what I’ve been telling people lately about finding collaborations, which is a mish-mash of a lot of advice I’ve gotten over the years.\nFind people you can work with. I sometimes see situations where a statistician will want to work with someone because he/she is working on an important problem. Of course, you want to be working on a problem that interests you, but it’s only partly about the specific project. It’s very much about the person. If you can’t develop a strong working relationship with a collaborator, both sides will suffer. If you don’t feel comfortable asking (stupid) questions, pointing out problems, or making suggestions, then chances are the science won’t be as good as it could be.\nIt’s going to take some time. I sometimes half-jokingly tell people that good collaborations are what you’re left with after getting rid of all your bad ones. Part of the reasoning here is that you actually may not know what kinds of people you are most comfortable working with. So it takes time and a series of interactions to learn these things about yourself and to see what works and doesn’t work. Of course, you can’t take forever, particularly in academic settings where the tenure clock might be ticking, but you also can’t rush things either. One rule I heard once was that a collaboration is worth doing if it will likely end up with a published paper. That’s a decent rule of thumb, but see my next comment.\nIt’s going to take some time. Developing good collaborations will usually take some time, even if you’ve found the right person. You might need to learn the science, get up to speed on the latest methods/techniques, learn the jargon, etc. So it might be a while before you can start having intelligent conversations about the subject matter. Then it takes time to understand how the key scientific questions translate to statistical problems. Then it takes time to figure out how to develop new methods to address these statistical problems. So a good collaboration is a serious long-term investment which has some risk of not working out.  There may not be a lot of papers initially, but the idea is to make the early investment so that truly excellent papers can be published later.\nWork with people who are getting things done. Nothing is more frustrating than collaborating on a project with someone who isn’t that interested in bringing it to a close (i.e. a published paper, completed software package). Sometimes there isn’t a strong incentive for the collaborator to finish (i.e she/he is already tenured) and other times things just fall by the wayside. So finding a collaborator who is continuously getting things done is key. One way to determine this is to check out their CV. Is there a steady stream of productivity? Papers in good journals? Software used by lots of other people? Grants? Web site that’s not in total disrepair?\nYou’re not like everyone else. One thing that surprised me was discovering that just because someone you know works well with a specific person doesn’t mean that you will work well with that person. This sounds obvious in retrospect, but there were a few situations where a collaborator was recommended to me by a source that I trusted completely, and yet the collaboration didn’t work out. The bottom line is to trust your mentors and friends, but realize that differences in personality and scientific interests may determine a different set of collaborators with whom you work well.\nThese are just a few of my thoughts on finding good collaborators. I’d be interested in hearing others’ thoughts and experiences along these lines.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-30-statistical-ode-to-mariano-rivera/",
    "title": "Statistical Ode to Mariano Rivera",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-09-30",
    "categories": [],
    "contents": "\nMariano Rivera is an outlier in many ways. The plot below shows one of them: top 10 pitchers ranked by postseason saves.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-29-sunday-datastatistics-link-roundup-92913/",
    "title": "Sunday data/statistics link roundup (9/29/13)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-09-29",
    "categories": [],
    "contents": "\nThe links are back! Read on.\nSusan Murphy - a statistician - wins a Macarthur Award. Great for the field of statistics (via Dan S. and Simina B., among others).\nRelated: an Interview with David Donoho about the Shaw Prize. Statisticians are blowing up! (via Rafa)\nHope that the award winners don’t lose momentum! (via Andrew J.)\nHopkins grad students take to the Baltimore Sun to report yet more ongoing negative effects of sequestration. Particularly appropriate in light of the current mayhem around keeping the government open. (via Rafa)\nGreat BBC piece featuring David Spiegelhalter on the science of chance. I rarely watch Youtube videos that long all the way through, but I made it to the end of this one.\nLove how Yahoo finance has recognized the agonized cries of statisticians and is converting pie charts to bar charts. (via Rafa - who has actually given up on the issue).\nDon’t use Hadoop - your data aren’t that big.\nDon’t forget to sign up for the future of statistics unconference October 30th Noon-1pm eastern. We have an awesome lineup of speakers and over 500 people RSVP’d on google plus alone. It’s going to be a thing.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-27-announcing-statistics-with-interactive-r-learning-software-environment/",
    "title": "Announcing Statistics with Interactive R Learning Software Environment",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-09-27",
    "categories": [],
    "contents": "\n\nEditor’s note: This post was written by Nick Carchedi, a Master’s degree student in the Department of Biostatistics at Johns Hopkins. He is working with us to develop software for interactive learning of R and statistics. \n\n\nInspired by the relative lack of computer-based platforms for learning statistics and the R programming language, we at Johns Hopkins Biostatistics have created a new R package designed to teach both topics simultaneously and interactively. Accordingly, we’ve named the package swirl, which stands for “Statistics with Interactive R Learning”. We sought to model swirl after other highly successful interactive learning platforms such as Codecademy, Code School, and Khan Academy, but with a specific focus on teaching statistics and R. Additionally, we wanted users to learn these topics within the same environment in which they would be applying them, namely the R console.\n\n\nIf you’re reading this article, then you probably already have an appreciation for the R language and there’s no need to beat that drum any further. Staying true to the R culture, the swirl package is totally open-source and free for anyone to use, modify, or improve. Furthermore, anyone with something to teach can use the platform to create their own interactive content for the world to use.\n\n\nA typical swirl session has a user load the package from the R console, choose from a menu of options the course he or she would like to take, then work through 10-15 minute interactive modules, each covering a particular topic. A module generally alternates between instructional text output to the user and prompts for the user to answer questions. One question may ask for the result of a simple numerical calculation, while another requires the user to enter an actual R command (which is parsed and executed, if correct) to perform a requested task. Multiple choice, text-based and approximate numerical answers are also fair game. Whenever the user answers a question incorrectly, immediate feedback is given in the form of a hint before prompting her to try again. Finally, plots, figures, and even videos may be incorporated into a module for the sake of reinforcing the methods or concepts being taught.\n\n\nWe believe that this form of interactive learning, or learning by doing, is essential for true mastery of topics as challenging and complex as statistics and statistical computing. While we are aware of a handful of other platforms for learning R interactively, our goal was to focus on the teaching of R and statistics simultaneously. As far as we know, swirl is the only platform of its kind and almost certainly the only one that takes place within the R console.\n\n\nWhen we developed the swirl package, we wanted from the start to allow other people to extend and customize it to their particular needs. The beauty of the swirl platform is that anyone can create their own content and have it included in the package for all users to access. We have designed pre-formatted templates (color-coded spreadsheets) that instructors can fill out with their own content according to a fairly simple set of instructions. Once instructors send us the completed templates, we then load the content into the package so that anyone with the most recent version of swirl on their computer can access the content. We’ve tried to make the process of content creation as simple and painless as possible so that the statistics and computing communities are encouraged to share their knowledge with the world through our platform.\n\n\nThe package currently includes only a few sample modules that we’ve created in-house, primarily serving as demonstrations of how the platform works and how a typical module may appear to users. In the future, we envision a vibrant and dynamic collection of full courses and short modules that users can vote up or down based on the quality of their experience with each. In such a scenario, the very best courses would naturally float to the top and the less effective courses would fall out of favor and perhaps be recommended for revision.\n\n\nIn addition to making more content available to future users, we hope to one day transition swirl from being an interactive learning environment to one that is truly adaptive to the individual needs of each user. Perhaps this future version of our software would support a more intricate web of content, intelligently navigating users among topics based on a dynamic, data-driven interpretation of their strengths, weaknesses, competencies, and knowledge gaps. With the right people on board, this could become a reality.\n\n\nWe’ve created this package with the hope that the statistics and computing communities find it to be a valuable educational tool. We’ve got the basic infrastructure in place, but we recognize that there is a great deal of room for improvement. The swirl package is still very much in development and we are actively seeking feedback on how we can make it better. Please visit the swirl website to download the package or for more information on the project. We’d love for you to give it a try and let us know what you think.\n\n\nGo to swirl website: http://swirlstats.com\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-26-how-could-code-review-discourage-code-disclosure-reviewers-with-motivation/",
    "title": "How could code review discourage code disclosure? Reviewers with motivation.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-09-26",
    "categories": [],
    "contents": "\n appeared a couple of days ago in Nature describing Mozilla’s efforts to implement code review for scientific papers. As anyone who follows our blog knows, we are in favor of reproducible research, in favor of disclosing code, and in favor of open science.\nSo people were surprised when they saw this quote from Roger at the end of the Nature piece:\n\n“One worry I have is that, with reviews like this, scientists will be even more discouraged from publishing their code. We need to get more code out there, not improve how it looks.”\n\nNot surprisingly a bunch of reproducible research/open science people were quick to jump on this quote:\n\n\n.@kaythaney re code review story, http://t.co/7rlAsmLuPw comment by @simplystats seems off to me… must be more nuance there \n\n— Titus Brown (@ctitusbrown) September 24, 2013\n\n\n\n\n@nickbarnes @cboettig @ctitusbrown agree. comment lead with this backfiring / discouraging others to make code available, which seemed off.\n\n\n— Kaitlin Thaney (@kaythaney) September 25, 2013\n\n\nNow, Roger’s quote was actually a little more nuanced and it was posted after a pretty in-depth discussion on Twitter:\n\n\n@ctitusbrown @cboettig @kaythaney @nickbarnes see whole @simplystats quote on prof. code review discouraging sharing pic.twitter.com/pNQWT9Safz\n\n\n— Erika Check Hayden (@Erika_Check) September 25, 2013\n\n\nBut I think the real source of confusion was best summed up by Titus B.:\n\n\n.@cboettig @kaythaney @nickbarnes As one of my grad students said to me, “I don’t understand why ‘must share code’ is a radical opinion.”\n\n\n— Titus Brown (@ctitusbrown) September 25, 2013\n\n\nThat is the key issue. People are surprised that sharing code would be anything but an obvious thing to do. To people who share code all the time, this is an obvious no-brainer. My bias is clearly in that camp as well. I require reproducibility of my students analyses, I discuss reproducible research when I teach, I take my own medicine by making my analyses reproducible, and I frequently state in reviews that papers are only acceptable after the code is available.\nSo what’s the big deal?\nIn an incredibly interesting coincidence, I had a paper come out the same week in Biostatistics that has been uh…little controversial.\nIn this case, our paper was published with discussion. For people outside of statistics, a discussant and a reviewer are different things. The paper first goes through peer review in the usual way. Then, once it is accepted for publication, it is sent out to discussants to read and comment on.\nA couple of discussants were very, very motivated to discredit our approach. Despite this, because we believe in open science, stating our assumptions, and being reproducible, we made all of the code we used and data we collected available for the discussants (and for everyone else). In an awesome win for open science, many of the discussants used/evaluated our code in their discussions.\nOne of the very motivated discussants identified an actual bug in the code. This bug caused the journal names to be scrambled in Figures 3 and 4. The bug (thank goodness!) did not substantively alter the methods, the results or the conclusions of our paper. On top of it, the cool thing about having our code on github meant we could carefully look it over, fix the bug, and push the changes to the repository (and update the paper) so the discussant could see the revised version as soon as we pushed it.\nWe were happy that the discussant didn’t find any more substantial bugs (because we knew they were motivated to review our code for errors as carefully as possible). We were also happy to make the changes, admit our mistake and move on.\nAn interesting thing happened though. The motivated discussant wanted to discredit our approach. So they included in the supplement how they noticed the bug (totally fair game, it was a bug). But they also included their email exchange with the editor about the bug and this quote:\n\nAs all seasoned methodologists know, minor coding errors causing total havoc is quite common (I have seen it happen in my own work).  I think that it is ironic that a paper that claims to prove the reliability of the literature had completely messed up the two main figures that represent the core of all its data and its main results.\n\nA couple of points here: (1) the minor bug didn’t wreak havoc with our results, it didn’t change any conclusions and it didn’t affect our statistics and (2) the statement is clearly designed for the sole purpose of embarrassing us (the authors) and discrediting our work.\nThe problem here is that the code reviewer deeply cares about us being wrong. This incident highlights one reason for Roger’s concerns. I feel we acted in pretty good faith here to try to be honest about our assumptions and open with our code. We also responded quickly and thoroughly to the report of a bug. But the discussant used the fact that we had a bug at all to try to discredit our whole analysis with sarcasm. This sort of thing could absolutely discourage a person from releasing code.\nOne thing the discussant is absolutely right about is that most code will have minor bugs. Personally, I’m very grateful to the discussant for catching the bug before the work was published and I’m happy that we made the code available and corrected our mistake.\nBut the key risk here is that people who demand reproducible code do so only so they can try to embarrass analysts and discredit science they don’t like. \nIf we want people to make code available, be willing to admit mistakes, and continuously update their code then we don’t just need code review. We need a policy and commitment from the community to not just use reproducible research as a vehicle for embarrassment and discrediting each other. We need a policy that:\nDoesn’t discourage people from putting code up before papers are published for fear of embarrassment.\nAcknowledges minor bugs happen and doesn’t penalize people for admitting them/fixing them.\nPrevents people from publishing when they have major typos, but doesn’t humiliate them.\nDefines specific, positive ways that code sharing can benefit the community (collaboration) rather than only reporting errors that are discovered when code is made available.\nRecognizes that most scientists are not professional software developers and focuses review on the scientific correctness/reproducibility of code, rather than technical software development skills.\nOne way I think we could address a lot of these issues is not to think of it as code review, but as code evaluation and update. That is one thing I really like about Mozilla’s approach - they report their findings to the authors and let them respond. The only thing that would be better is if Mozilla actually created patches/bug fixes for the code and issued pull requests that the authors could incorporate. \nUltimately, I hope we can focus on a way to make scientific software correct, not just point out how it is wrong.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:08:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-25-is-most-science-false-the-titans-weigh-in/",
    "title": "Is most science false? The titans weigh in.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-09-25",
    "categories": [],
    "contents": "\nSome of you may recall that a few months ago my colleague and I posted a paper to the ArXiv on estimating the rate of false discoveries in the scientific literature. The paper was picked up by the Tech Review and led to a post on Andrew G.’s blog, on Discover blogs, and on our blog. One other interesting feature of our paper was that we put all the code/data we collected on Github.\nAt the time this whole thing blew up our paper still wasn’t published. After the explosion of interest we submitted the paper to Biostatistics. They liked the paper and actually solicited formal discussion of our approach by other statisticians. We were then allowed to respond to the discussions.\nOverall, it was an awesome experience at Biostatistics - they did a great job of doing a thorough, but timely, review. They  got some amazing discussants. Finally, they made our paper open-access. So much goodness. (conflict of interest disclaimer - I am an associate editor for Biostatistics)\nHere are the papers that came out which I think are all worth reading:\nOur paper\nDiscussion by Benjamini and colleagues\nDiscussion by D.R. Cox (!)\nDiscussion by Gelman and colleagues\nDiscussion by Goodman\nDiscussion by Ioannidis\nDiscussion by Schuemie and colleagues\nOur rejoinder\nI’m very proud of our paper and the rejoinder. The discussants were very passionate and added a huge amount of value, particularly in the collection/analysis of our data and additional data they collected.\nI think it is 100% worth reading all of the papers over at Biostatistics but for the tldr crowd here are some take home messages I have from the experience and summarizing the discussion above:\nPosting to ArXiv can be a huge advantage for a paper like ours but be ready for the heat.\nBiostatistics (the journal) is awesome. Great job of reviewing/editing in a timely way and great job of organizing the discussion!\nWhen talking about the science-wise false discovery rate you have to bring data.\nWe proposed the first formal framework for evaluating the science-wise false discovery rate which lots of people care about (and there are a ton of ideas in the discussion about ways to estimate it better).\nI think based on our paper and the discussion that it is pretty unlikely that most published research is false. But that probably varies by your definition of false/what you mean by most/the journal type/the field you are considering/the analysis type/etc.\nThis is a question people care about. A lot.\nFinally, I think this is the most important quote from our rejoinder:\n\nWe are encouraged, however, that several of the discussants collected additional data to evaluate the impact of the above decisions on the SWFDR estimates. The discussion illustrates the powerful way that data collection can be used to move the theoretical and philosophical discussion on to a more concrete, scientific footing—discussing the specific strengths and weaknesses of a particular empirical approach. Moreover, the interesting additional data collected by the discussants on study types, journals, and endpoints demonstrate that data beget data and lead to a stronger and more directed conversation.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-24-how-i-view-an-academic-talk-like-a-sports-game/",
    "title": "How I view an academic talk: like a sports game",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-09-24",
    "categories": [],
    "contents": "\nI know this is a little random/non-statisticsy but I have been thinking about it a lot lately. Over the last couple of weeks I have been giving a bunch of talks and guest lectures here locally around the Baltimore/DC area. Each one of them was to a slightly different audience.\nAs I was preparing/giving all of these talks I realized I have a few habits that I have developed in the way I view the talks and in the way that I give them. I 100% agree with Hilary M. that a talk should entertain more than it should teach. I also try to give talks that I would like to see myself.\nAnother thing I realized is that I view talks in a very specific way. I see them as a sports game. From the time I was a kid until the end of graduate school I was on sports teams. I love playing/watching all kinds of sports and I definitely miss playing competitively.\nUnfortunately, being a faculty member doesn’t leave much time for sports. So now, the only chance I have to get up and play is during a talk. Here are the ways that I see the two activities as being similar:\nThey both require practice. I played a lot of sports with this guy who liked the quote, “Practice doesn’t make perfect, perfect practice makes perfect”. I feel the same way.\nThey are both a way to entertain. I rarely played in front of crowds as big as the groups I speak to these days, but whenever there was an audience I would always get way more pumped up.\nThere is some competition to both. In terms of talks, there is always at least one audience member who wants to challenge your ideas. I see this exchange as a game, rather than something I dread. Sometimes I win (my answers cover all the questions) and sometimes I lose (I missed something important). Usually, being prepared is associated with better practice.\nI get a rush off of both playing in games and giving talks. Part of that is self fueled. I like to listen to pump up music right before I give a talk or play a game.\nOne thing I wish is that more talks were joint talks. One thing I love about sports is playing on a team. The preparation of a talk is always done with a team - usually the students/postdocs/collaborators working on the project. But I wish presentations were more often a team activity. It makes it more fun to celebrate if the talk went well and less painful if I flub when I give a talk with someone else. Plus it is fun to cheer on your team mate.\nDoes anyone else think of talks this way? Or do you have another way of thinking about talks?\n \n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-23-the-limiting-reagent-for-big-data-is-often-small-well-curated-data/",
    "title": "The limiting reagent for big data is often small, well-curated data",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-09-23",
    "categories": [],
    "contents": "\nI’ve been working on “big” data in genomics since I was a first year student in graduate school (a longer time than I’d rather admit). At the time, “big” meant microarray studies with a couple of hundred patients. Of course, that is now a really small drop in the pond compared to the huge sequencing data sets, like the one published recently in Nature.\nDespite the exploding size of these genomic data sets, the discovery process is almost always limited by the quality and quantity of useful metadata that go along with them. In the trauma study I referenced above, the genomic data was both costly and hard to collect. But the bigger, more impressive feat was to collect the data from trauma patients at relatively precise time points after they had been injured. Along with the genomic data a host of clinical data was also collected and aligned with the genomic data.\nThe key insights derived from the data were the relationships between low-dimensional and high-dimensional measurements. \nThis is actually relatively common:\nIn computer vision you need quality labeled images to use as a training set (this type of manual labeling is so common it forms the basis for major citizen science projects like zooniverse)\nIn genome-wide association studies you need accurate phenotypes.\nIn the analysis of social networks like the Framingham Heart Survey, you need to collect data on obesity levels, etc.\nOne common feature of these studies is that they are examples of what computer scientists call _supervised learning. _Most hypothesis-driven research falls into this type of study. It is important to recognize that these studies can only work with painstaking and careful collection of small data. So in many cases, the limits to the insights we can obtain from big data are imposed by how much schlep we are willing to put in to get small data.\n \n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-17-announcing-the-simply-statistics-unconference-on-the-future-of-statistics-futureofstats/",
    "title": "Announcing the Simply Statistics Unconference on the Future of Statistics #futureofstats",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-09-17",
    "categories": [],
    "contents": "\nSign up here!\n\nWe here at Simply Statistics are pumped about the Statistics 2013 Future of Statistical Sciences Workshop (Nov. 11-12). It is a great time to be a statistician and discussing the future of our discipline is of utmost importance to us. In fact, we liked the idea so much that we decided to get in the game ourselves.\n\n\nWe are super excited to announce the first ever “Unconference” hosted by Simply Statistics. The unconference will focus on the Future of Statistics and will be held October 30th from 12-1pm EST. The unconference will be hosted on Google Hangouts and will be simultaneously live-streamed on YouTube. After the unconference is over we will maintain a recorded version for viewing on YouTube. Our goal is to compliment and continue the discussion inspired by the Statistics 2013 Workshop.\n\n\nThis unconference will feature some of the most exciting and innovative statistical thinkers in the field discussing their views on the future of the field and focusing on issues that affect junior statisticians the most: education, new methods, software development, collaborations with natural sciences/social sciences, and the relationship between statistics and industry.\n\n\nThe confirmed presenters are:\n\nDaniela Witten, Assistant Professor, Department of Biostatistics, University of Washington\nHongkai Ji, Assistant Professor, Department of Biostatistics, Johns Hopkins University\nJoe Blitzstein, Professor of the Practice, Department of Statistics, Harvard University\nSinan Aral, Associate Professor, MIT Sloan School of Management\nHadley Wickham, Chief Scientist, RStudio\nHilary Mason, Chief Data Scientist at Accel Partners\nFollow us on Twitter or sign up for the Unconference at http://simplystatistics.org/unconference. In the month or so leading up to the conference we would also love to hear from you about your thoughts on the future of statistics. Let us know about your ideas on Twitter with the hashtag #futureofstats, we’ll be compiling the information and will make it available along with the talks so that you can tell us what you think the future is.\nTell your friends, tell your family, it is on!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-16-data-analysis-in-the-top-9-courses-in-lifetime-enrollment-at-coursera/",
    "title": "Data Analysis in the top 9 courses in lifetime enrollment at Coursera!",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-09-16",
    "categories": [],
    "contents": "\nHoly cow I just saw this, my Coursera class is in the top 9 by all time enrollment!\n\n\nTop 9 courses on #coursera by lifetime enrollment current as of 9/16- check out & enroll: http://t.co/2X0EJoetoC! pic.twitter.com/d0Sko1KhoD\n\n\n— Coursera (@coursera) September 16, 2013\n\n\nOnly problem is those pesky other classes ahead of me. Help me take down Creativity, Innovation and Change (what good is all that anyway by signing up here!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-13-so-youre-moving-to-baltimore/",
    "title": "So you're moving to Baltimore",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-09-13",
    "categories": [],
    "contents": "\nEditor’s Note: This post was written by Brian Caffo, occasional Simply Statistics contributor and Director of Graduate Studies in the Department of Biostatistics at Johns Hopkins. This was written primarily for incoming graduate students, but if you’re planning on moving to Baltimore anyway, feel free to use it to your advantage!\nCongratulations on picking Hopkins Biostatistics for your graduate studies. Now that you’re either here or coming to to Baltimore, I’m guessing that you’ll need some start-up knowledge for this quirky, fun city. Here’s a guide of to some of my favorite Baltimore places and traditions.\nPut more in the comments!\n\nEvents\n\n\nFirst, let me discuss some sporting events that you should be aware of.  Absolutely top on the list is going to a baseball game at Camden Yards to watch the Orioles. There’s lots of games on days, nights and weekends and for the most part, tickets are easy to get and relatively cheap. Going to the (twice Super Bowl champion) NFL Ravens is a bit harder and more expensive, but well worth the splurge once during your studies. Then you can come back to your research on investigating the long term impact of football head trauma.\n\n**** ****The Editor’s Note: This post was written by Brian Caffo, occasional Simply Statistics contributor and Director of Graduate Studies in the Department of Biostatistics at Johns Hopkins. This was written primarily for incoming graduate students, but if you’re planning on moving to Baltimore anyway, feel free to use it to your advantage! horse race is another that’s worth going to at least once.  The Preakness takes place on a Saturday and is a very popular event; this can translate to big crowds.  If you don’t like big crowds but would like to see what all the fuss is about, you may enjoy the Black Eye Susan Stakes; this is a day of racing at Pimlico on Friday before the Preakness where the crowds are smaller, it costs $5 to get into the track and you can enjoy the celebratory atmosphere of the Preakness.  Another fun event is the Baltimore Grand Prix which happens every Labor day weekend (at least for the next few years).  Since you’re at Hopkins, try to go catch a lacrosse game. The Hopkins team is consistently among the best. If you’re a distance runner, there’s the Baltimore Marathon. Also, I hesitate to include this with sports, but I can’t get enough of the Kinetic Sculpture “Race”, the most fun Baltimore event that I can think of.  And we would be doing Hilary Parker a disservice if we failed to mention the Charm City Roller Girls!\n\nThe main non-sporting event that I like are all of the festivals. Every year, especially during the summer, every neighborhood has a festival. Honfest in Hampden is surely the one not to be missed (but there are festivals in every notable neighborhood including the Fells Point Festival) At Christmas time, there’s the Miracle on 34th Street right nearby and 36th street (the Avenue) is a fun place to go out for shopping and eating, regardless of whether Honfest is going on.  During the summer months, a local radio station sponsors “First Thursdays” where they put on a free concert series at the Washington Monument in Mt. Vernon.\n\n\nThings to do during the day\n\n\nProbably you’ll visit the Harbor as one of the first things you do. Make sure to hit the National Aquarium, the Visionary Arts Museum and the Maryland Science Center (not all in one day). Downtown there’s the Walters Art Museum and the Baltimore Museum of Art on the Johns Hopkins Homewood campus. Go see Fort McHenry, where Francis Scott Key wrote the National Anthem. The Museum of African American History and Culture is right near the Inner Harbor on Pratt Street.\n\n\nIf you’re outdoorsy, Patapsco and Gunpowder Falls appear to be good places nearby. Catoctin Park is nearby with Camp David tucked in it somewhere; you’ll know you’ve found it when the secret service tackles you. If you don’t want to travel too far, just outside the northern border of the city is Robert E. Lee park which has a nice hiking trail and a dog park. When you’re done there you can grab lunch at the Haute Dog.\n\n**** ****If you have kids, the Baltimore Zoo is a really nice outdoor zoo that is a great place to go if the weather is nice. It’s in Druid Hill Park, which is also a great place to go running or biking. If you’re willing to drive an hour or more, the outdoor options are basically endless.\nDC and Philly are easy day trips using the train and Annapolis is an easy drive. If you go to the DC, only schedule a few museums right near one and another, otherwise you’ll spend the whole day walking. On a nice day, the National Zoo is fantastic (and free). The MARC train goes to DC from Penn Station and is under $10 each way, but it only runs in the morning and evening. Outside of those times you can take the Amtrak train. If you drive, it’s usually about an hour one-way, depending on where you’re going.\n\nThings to do during the night\n\n\nI have little kids. How would I know? My answer is, fight about bedtime and collapse. However, if I was forced to come up with something, I would say go to Patterson Park Lanes and do Duckpin Bowling. Make sure to reserve a lane earlier on in the week if you want to go at night on a weekend.\n\nFrom my outside vantage point, there appears to be tons of nightlife. The best places appear to be in upscale city areas, like Fells Point, Canton, downtown, Harbor East, Federal Hill. Also, catch a show at the Editor’s Note: This post was written by Brian Caffo, occasional Simply Statistics contributor and Director of Graduate Studies in the Department of Biostatistics at Johns Hopkins. This was written primarily for incoming graduate students, but if you’re planning on moving to Baltimore anyway, feel free to use it to your advantage! or Center Stage or any one of the many local theatres. The best places to go to movies are the Senator, Rotunda, the Charles and the Landmark at Harbor East.\n**** ****The Baltimore Symphony is one of the top orchestras in the country and usually has interesting programs. You can usually just show up a few minutes before the concert and get a good (cheap) ticket. There’s also opera at the Lyric Opera House, but Ingo will tell you that the real stuff is in DC at the National Opera.\n\nThings to eat\n\n\nThere’s too many restaurants to discuss. So, I’ll talk about some recommendations. If you have to have deli food, go to Attman’s on Lombard street. If you need authentic Chinese food, go to Hunan Taste in Catonsville. All of the Korean restaurants are just north of North Avenue on Charles; try Jong Kak.  If you’re a locavore and want to go out for a nice dinner, there’s a lot of choices. I like the Woodberry Kitchen and Clementine. If you want to break the bank, go to the Charleston, probably the fanciest restaurant in the city. Also, make sure to hit the big Farmer’s Market on Sunday at least once. The best place to go drink beer and eat crabs is LP Steamers. If you want a crab cake the size of a softball, go to Faidley’s in Lexington Market. Lexington Market is its own spectacle that you should try at least once. If you need an Italian Deli, there’s several (Mastellone’s is my favorite, but this list at least omits Isabella’s in Little Italy and Ceriello in Belvedere Square).\n\n\nWhat you eat\n\n\nYou’re a Baltimoron now, so you drink Natty Boh, eat Utz Potato chips and Berger cookies. (Don’t question; this is what you do now.) In the summer, go get an egg cream snowball with marshmallow.  If you want high end local beer, I like Heavy Seas and Union Craft.  If you’re a coffee drinker, you drink Zeke’s coffee now.\n\n\nBaltimore stuff\n\n\nSo you need to know a few things so you don’t look the fool. I’ve created a Baltimore cheat sheet. Normally I wouldn’t suggest cheating, but feel free to write this on your hand or something.\n\n\n\nThe O’s  are the baseball team (Orioles, named after a species of bird that lives around here); they have a rich history and are in a division with poser glamour bankroll teams: the Yankees and Red Sox. You do not like the Yankees or Red Sox now.\n\n\n\n\nCal Ripken Jr is a former O’s player who broke a famous record for number of consecutive games played.\n\n\n\n\nThe Ravens are the football team (named after the poem from Edgar Allan Poe see below). They have been very good for a while. There was an issue where the old team, the Baltimore Colts, left Baltimore for Indianapolis and Baltimore subsequently got Cleveland’s team and named it the Ravens. So, now you don’t like Indianapolis Colts fans and people from Cleveland don’t like you.\n\n\n\n\nLacrosse is a sport that exists and Hopkins is good at it.\n\n\n\n\nThurgood Marshall, the first black US Supreme court justice, was born here. The airport is named after him.\n\n\n\n\nThe author Edgar Allan Poe lived, worked, died and was buried here. You can go visit his grave.\n\n\n\n\nThe most famous baseball player ever, Babe Ruth, was born, grew up and started in baseball here. He really liked duckpin bowling, so the story goes.\n\n\n\n\nOlympic swimmer Michael Phelps grew up, lives and trains here.\n\n\n\n\nJohn Waters is a famous film director of cult classics is from Baltimore and the city is prominent in many of his movies.\n\n\n\n\nHL Mencken was a celebrated intellectual and writer.\n\n\n\n\nFrederick Douglass, the abolitionist and intellectual was born and lived near here.\n\n\n\n\nThere was a wonderfully done and controversial television program from HBO, The Wire, by David Simon, that everyone talks about around here. It’s filmed in and is about Baltimore.\n\n\n\n\nThere is a Baltimore accent, but you may miss it at first. People say hon as a term of endearment, pronounce Baltimore as Bawlmer  and Washington as Warshington, among other things. Think about all of the time you can save for research now, by omitting several pesky syllables.\n\n\n\nThat’s it for now. We’ll do another one on Hopkins and research in the area.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-12-help-needed-for-establishing-an-asa-statistical-genetics-and-genomics-section/",
    "title": "Help needed for establishing an ASA statistical genetics and genomics section",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-09-12",
    "categories": [],
    "contents": "\nTo promote research and education in statistical genetics and genomics, some of us in the community would like to establish a statistical genetics and genomics section of the American Statistical Association (ASA). Having an ASA section gives us certain advantages, such as having allocated invited sessions at JSM, young investigator and student awards, and senior investigator awards in statistical genetics and genomics, as well as a community to interact and exchange information.\nWe need at least 100 ASA members  to pledge that they will join the section (if you are in more than 3 sections already you will be asked to pay a nominal fee of less than $10). If you are interested please fill a row in the following google doc by November 1st:\nhttps://docs.google.com/spreadsheet/ccc?key=0AtD3gd8kGN45dE9BZ1pTYWtCa0M2VWhKckRoUE9KLVE#gid=0\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-05-implementing-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-3/",
    "title": "Implementing Evidence-based Data Analysis: Treading a New Path for Reproducible Research (Part 3)",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-09-05",
    "categories": [],
    "contents": "\nLast week I talked about how we might be able to improve data analyses by moving towards “evidence-based” data analysis and to use data analytic techniques that are proven to be useful based on statistical research rather. My feeling was this approach attacks the most “upstream” aspect of data analysis before problems have the chance to filter down into things like publications, or even worse, clinical decision-making.\nIn this third (and final!) post on this topic I wanted to describe a little how we could implement evidence-based data analytic pipelines. Depending on your favorite  software system you could imagine a number of ways to do this. If the pipeline were implemented in R, you could imagine it as an R package. The precise platform is not critical at this point; I would imagine most complex pipelines would involve multiple different software systems tied together.\nBelow is a rough diagram of how I think the various pieces of an evidence-based data analysis pipeline would fit together.\nThere are a few key elements of this diagram that I’d like to stress:\n Inputs are minimal. You don’t want to allow for a lot of inputs or arguments that can be fiddled with. This reduces the number of degrees of freedom and hopefully reduces the amount of hacking. Basically, you want to be able to input the data and perhaps some metadata.\nAnalysis comes in stages. There are multiple stages in any analysis, not just the part where you fit a model. Everything is important and every stage should use the best available method.\nThe stuff in the red box does not involve manual intervention. The point is to not allow tweaking, fudging, and fiddling. Once the data goes in, we just wait for something to come out the other end.\nMethods should be benchmarked. For each stage of the analysis, there is a set of methods that are applied. These methods should, at a minimum, be benchmarked via a standard group of datasets. That way, if another method comes a long, we have an objective way to evaluate whether the new method is better than the older methods. New methods that improve on the benchmarks can replace the existing methods in the pipeline.\nOutput includes a human-readable report. This report summarizes what the analysis was and what the results were (including results of any sensitivity analysis). The material in this report could be included in the “Methods” section of a paper and perhaps in the “Results” or “Supplementary Materials”. The goal would be to allow someone who was not intimately familiar with the all of the methods used in the pipeline to be able to walk away with a report that he/she could understand and interpret. At a minimum, this person could take the report and share it with their local statistician for help with interpretation.\nThere is a defined set of output parameters. Each analysis pipeline should, in a sense, have an “API” so that we know what outputs to expect (not the exact values, of course, but what kinds of values). For example, if a pipeline fits a regression model at the end the regression parameters are the key objects of interest, then the output could be defined as a vector of regression parameters. There are two reasons to have this: (1) the outputs, if the pipeline is deterministic, could be used for regression testing in case the pipeline is modified; and (2) the outputs could serve as inputs into another pipeline or algorithm.\nClearly, one pipeline is not enough. We need many of them for different problems. So what do we do with all of them?\nI think we could organize them in a central location (kind of a specialized GitHub) where people could search for, download, create, and contribute to existing data analysis pipelines. An analogy (but not exactly a model) is the Cochrane Collaboration which serves as a repository for evidence-based medicine. There are already a number of initiatives along these lines, such as the Galaxy Project for bioinformatics. I don’t know whether it’d be ideal to have everything in one place or have a number of sub-projects for specialized areas.\nEach pipeline would have a leader (or “friendly dictator”) who would organize the contributions and determine which components would go where. This could obviously be contentious, more some in some areas than in others, but I don’t think any more contentious than your average open source project (check the archives of the Linus Kernel or Git mailing lists and you’ll see what I mean).\nSo, to summarize, I think we need to organize lots of evidence-based data analysis pipelines and make them widely available. If I were writing this 5 or 6 years ago, I’d be complaining about a lack of infrastructure out there to support this. But nowadays, I think we have pretty much everything we need in terms of infrastructure. So what are we waiting for?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-04-repost-a-proposal-for-a-really-fast-statistics-journal/",
    "title": "Repost: A proposal for a really fast statistics journal",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-09-04",
    "categories": [],
    "contents": "\nEditor’s note: This is a repost of a previous Simply Statistics column that seems to be relevant again in light of Marie Davidian’s really important column on the peer review process. You should also check out Yihui’s thoughts on this, which verge on the creation of a very fast/dynamic stats journal.  \nI know we need a new journal like we need a good poke in the eye. But I got fired up by the recent discussion of open science (by Paul Krugman and others) and the seriously misguided Research Works Act- that aimed to make it illegal to deposit published papers funded by the government in Pubmed central or other open access databases.\n\nI also realized that I spend a huge amount of time/effort on the following things: (1) waiting for reviews (typically months), (2) addressing reviewer comments that are unrelated to the accuracy of my work - just adding citations to referees papers or doing additional simulations, and (3) resubmitting rejected papers to new journals - this is a huge time suck since I have to reformat, etc. Furthermore, If I want my papers to be published open-access I also realized I have to pay at minimum $1,000 per paper.So I thought up my criteria for an ideal statistics journal. It would be accurate, have fast review times, and not discriminate based on how interesting an idea is. I have found that my most interesting ideas are the hardest ones to get published.  This journal would:\n\n\nBe open-access and free to publish your papers there. You own the copyright on your work.\n\n\nThe criteria for publication would be: (1) it has to do with statistics, computation, or data analysis, (2) is the work is technically correct.\n\n\nWe would accept manuals, reports of new statistical software, and full length research articles.\n\n\nThere would be no page limits/figure limits.\n\n\nThe journal would be published exclusively online.\n\n\nWe would guarantee reviews within 1 week and publication immediately upon review if criteria (1) and (2) are satisfied\n\n\nPapers would receive a star rating from the editor - 0-5 stars. There would be a place for readers to also review articles\n\n\nAll articles would be published with a tweet/like button so they can be easily distributed\n\n\n\n\n\nTo achieve such a fast review time, here is how it would work. We would have a large group of Associate Editors (hopefully 30 or more). When a paper was received, it would be assigned to an AE. The AEs would agree to referee papers within 2 days. They would use a form like this:\n\n\n\n\n\n\nReview of: Jeff’s Paper\n\n\nTechnically Correct: Yes\n\n\nAbout statistics/computation/data analysis: Yes\n\n\nNumber of Stars: 3 stars\n\n\n3 Strengths of Paper (1 required):\n\n\nThis paper revolutionizes statistics\n\n\n3 Weakness of Paper (1 required):\n\n\nThe proof that this paper revolutionizes statistics is pretty weak\n\n<li>\n  because he only includes one example.\n<\/li>\n\n\n\n\n\nThat’s it, super quick, super simple, so it wouldn’t be hard to referee. As long as the answers to the first two questions were yes, it would be published.\n\n\n\n\n\nSo now here’s my questions:\n\n\n\n\n\n\nWould you ever consider submitting a paper to such a journal?\n\n\nWould you be willing to be one of the AEs for such a journal?\n\n\nIs there anything you would change?\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-01-sunday-datastatistics-link-roundup-9113/",
    "title": "Sunday data/statistics link roundup (9/1/13)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-09-01",
    "categories": [],
    "contents": "\nThere has been a lot of discussion of the importance of open access on Twitter. I am 100% in favor of open access (I do wish it was less expensive), but I also think that sometimes people lose sight of other important issues for junior scientists that go beyond open access. Dr. Isis has a great example of this on her blog. \nSherri R. has a great list of resources for stats minded folks at the undergrad, grad, and faculty levels.\nThere he goes again. Another awesome piece by Rafa on someone else’s blog. It is in Spanish but the google translate does ok. Be sure to check out questions 3 and 4.\nA really nice summary of Nate Silver’s talk at JSM and a post-talk interview (in video format) are available here. Pair with this awesome Onion piece (both links via Marie D.)\nA really nice post that made the rounds in the economics blogosphere talking about the use of mathematics in econ. This seems like a pretty relevant quote, “Instead, it was just some random thing that someone made up and wrote down because A) it was tractable to work with, and B) it sounded plausible enough so that most other economists who looked at it tended not to make too much of a fuss.”\nMore on hiring technical people. This is related to Google saying their brainteaser interview questions don’t work. Check out the list here of things that this person found useful in hiring technical people that could be identified easily.  I like how typos and grammatical errors were one of the best predictors.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-30-aaas-st-fellows-for-big-data-and-analytics/",
    "title": "AAAS S&amp;T Fellows for Big Data and Analytics",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-08-30",
    "categories": [],
    "contents": "\nThanks to Steve Pierson of the ASA for letting us know that the AAAS Science and Technology Fellowship program has a new category for “Big Data and Analytics”. For those not familiar, AAAS organizes the S&T Fellowship program to get scientists involved in the policy-making process in Washington and at the federal agencies. In general, the requirements for the program are\n\nApplicants must have a PhD or an equivalent doctoral-level degree at the time of application. Individuals with a master’s degree in engineering and at least three years of post-degree professional experience also may apply. Some programs require additional experience. Applicants must be U.S. citizens. Federal employees are not eligible for the fellowships.\n\nFurther details are on the AAAS web site.\nI’ve met a number of current and former AAAS fellows working on Capitol Hill and at the various agencies and I have to say I’ve been universally impressed. I personally think getting more scientists into the federal government and involved with the policy-making process is a Good Thing. If you’re a statistician looking to have a different kind of impact, this might be for you.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-29-the-return-of-the-stat-computing-for-data-analysis-data-analysis-back-on-coursera/",
    "title": "The return of the stat - Computing for Data Analysis &#038; Data Analysis back on Coursera!",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-08-29",
    "categories": [],
    "contents": "\nIt’s the return of the stat. Roger and I are going to be re-offering our Coursera courses:\nComputing for Data Analysis (starts Sept 23)\nSign up here.\nData Analysis (starts Oct 28)\nSign up here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-28-evidence-based-data-analysis-treading-a-new-path-for-reproducible-research-part-2/",
    "title": "Evidence-based Data Analysis: Treading a New Path for Reproducible Research (Part 2)",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-08-28",
    "categories": [],
    "contents": "\n\nLast week I posted about how I thought the notion of reproducible research did not go far enough to address the question of whether you could trust that a given data analysis was conducted appropriately. From some of the discussion on the post, it seems some of you thought I believed therefore that reproducibility had no value. That’s definitely not true and I’m hoping I can clarify my thinking in this followup post.\n\n\nJust to summarize a bit from last week, one key problem I find with requiring reproducibility of a data analysis is that it comes only at the most “downstream” part of the research process, the post-publication part. So anything that was done incorrectly has already happened and the damage has been done to the analysis. Having code and data available, importantly, makes it possible to discover these problems, but only after the fact. I think this results in two problems: (1) It may take a while to figure out what exactly the problems are (even with code/data) and how to fix them; and (2) the problems in the analysis may have already caused some sort of harm.\n\n\nOpen Source Science?\n\n\nFor the first problem, I think a reasonable analogy for reproducible research is open source software. There the idea is that source code is available for all computer programs so that we can inspect and modify how a program runs. With open source software “all bugs are shallow”. But the key here is that as long as all programmers have the requisite tools, they can modify the source code on their own, publish their corrected version (if they are fixing a bug), others can review it and accept or modify, and on and on. All programmers are more or less on the same footing, as long as they have the ability to hack the code. With distributed source code management systems like git, people don’t even need permission to modify the source tree. In this environment, the best idea wins.\n\n\nThe analogy with open source software breaks down a bit with scientific research because not all players are on the same footing. Typically, the original investigator is much better equipped to modify the “source code”, in this case the data analysis, and to fix any problems. Some types of analyses may require tremendous resources that are not available to all researchers. Also, it might take a long time for others who were not involved in the research, to fully understand what is going on and how to make reasonable modifications. That may involve, for example, learning the science in the first place, or learning how to program a computer for that matter. So I think making changes to a data analysis and having them accepted is a slow process in science, much more so than with open source software. There are definitely things we can do to improve our ability to make rapid changes/updates, but the implementation of those changes are only just getting started.\n\n\nFirst Do No Harm\n\n\nThe second problem, that some sort of harm may have already occurred before an analysis can be fully examined is an important one. As I mentioned in the previous post, merely stating that an analysis is reproducible doesn’t say a whole lot about whether it was done correctly. In order to verify that, someone knowledgeable has to go into the details and muck around to see what is going on. If someone is not available to do this, then we may never know what actually happened. Meanwhile, the science still stands and others may build off of it.\n\n\nIn the Duke saga, one of the most concerning aspects of the whole story was that some of Potti’s research was going to be used to guide therapy in a clinical trial. The fact that a series of flawed data analyses was going to be used as the basis of choosing what cancer treatments people were going to get was very troubling. In particular, one of these flawed analyses reversed the labeling of the cancer and control cases!\n\nTo me, it seems that waiting around for someone like Keith Baggerly to come around and spend close to 2,000 hours reproducing, inspecting, and understanding a series of analyses is not an efficient system. In particular, when actual human lives may be affected, it would be preferable if the analyses were done right in the first place, without the “statistics police” having to come in and check that everything was done properly.\nEvidence-based Data Analysis\n\nWhat I think the statistical community needs to invest time and energy into is what I call “evidence-based data analysis”. What do I mean by this? Most data analyses are not the simple classroom exercises that we’ve all done involving linear regression or two-sample t-tests. Most of the time, you have to obtain the data, clean that data, remove outliers, impute missing values, transform variables and on and on, even before you fit any sort of model. Then there’s model selection, model fitting, diagnostics, sensitivity analysis, and more. So a data analysis is really pipeline of operations where the output of one stage becomes the input of another.\n\n\nThe basic idea behind evidence-based data analysis is that for each stage of that pipeline, we should be using the best method, justified by appropriate statistical research that provides evidence favoring one method over another. If we cannot reasonable agree on a best method for a given stage in the pipeline, then we have a gap that needs to be filled. So we fill it!\n\n\nJust to clarify things before moving on too far, here’s a simple example.\n\n\nEvidence-based Histograms\n\n\nConsider the following simple histogram.\n\n\n\n\n\nThe histogram was created in R by calling hist(x) on some Normal random deviates (I don’t remember the seed so unfortunately it is not reproducible). Now, we all know that a histogram is a kind of smoother, and with any smoother, the critical parameter is the smoothing parameter or the bandwidth. Here, it’s the size of the bin or the number of bins.\n\nNotice that when I call ‘hist’ I don’t actually specify the number of bins. Why not? Because in R, the default is to use Sturges’ formula for the number of bins. Where does that come from? Well, there is a paper in the Journal of the American Statistical Association in 1926 by H. A. Sturges that justifies why such a formula is reasonable for a histogram (it is a very short paper, those were the days). R provides other choices for choosing the number of bins. For example, David Scott wrote a paper in Biometrika that justified bandwith/bin size based in integrated mean squared error criteria.\nThe point is that R doesn’t just choose the default number of bins willy-nilly, there’s actual research behind that choice and evidence supporting why it’s a good choice. Now, we may not all agree that this default is the best choice at all times, but personally I rarely modify the default number of bins. Usually I just want to get a sense of what the distribution looks like and the default is fine. If there’s a problem, transforming the variable somehow often is more productive than modifying the number of bins. What’s the best transformation? Well, it turns out there’s research on that too.\nEvidence-based Reproducible Research\n\nNow why can’t we extend the idea behind the histogram bandwidth to all data analysis? I think we can. For every stage of a given data analysis pipeline, we can have the “best practices” and back up those practices with statistical research. Of course it’s possible that such best practices have not yet been developed. This is common in emerging areas like genomics where the data collection technology is constantly changing. That’s fine, but in more mature areas, I think it’s possible for the community to agree on a series of practices that work, say, 90% of the time.\n\n\nThere are a few advantages to evidence-based reproducible research.\n\nIt reduces the “researcher degrees of freedom”. Researchers would be disincentivized from choosing the method that produces the “best” results if there is already a generally agreed upon approach. If a given data analysis required a different approach, the burden would be on the analyst to justify why a deviation from the generally accepted approach was made.\nThe methodology would be transparent because the approach would have been vetted by the community. I call this “transparent box” analysis, as opposed to black box analysis. The analysis would be transparent so you would know exactly what is going on, but it would “locked in a box” so that you couldn’t tinker with it to game the results.\nYou would not have the lonely data analyst coming up with their own magical method to analyze the data. If a researcher claimed to have conducted an analysis using an evidence-based pipeline, you could at least have a sense that something reasonable was done. You would still need reproducibility to ensure that the researcher was not misrepresenting him/herself, but now we would have two checks on the analysis, not just one.\nMost importantly, evidence-based reproducible research attacks the furthest upstream aspect of the research, which is the analysis itself. It guarantees that generally accepted approaches are used to analyze the data from the very beginning and hopefully prevents problems from occurring rather than letting them propagate through the system.\n\nWhat can we do to bring evidence-based data analysis practices to all of the sciences? I’ll write about what I think we can do in the next post.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-27-interview-with-ani-eloyan-and-betsy-ogburn/",
    "title": "Interview with Ani Eloyan and Betsy Ogburn",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-08-27",
    "categories": [],
    "contents": "\nJeff and I interview Ani Eloyan and Betsy Ogburn, two new Assistant Professors in the Department of Biostatistics here.\nJeff and I talk to Ani and Betsy about their research interests and finally answer the burning question: “What is the future of statistics?”\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-26-statistics-meme-sad-p-value-bear/",
    "title": "Statistics meme: Sad p-value bear",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-08-26",
    "categories": [],
    "contents": "\n\n\n\nSad p-value bear wishes you had a bigger sample size.\n\n\nI was just at a conference where the idea for a sad p-value bear meme came up (in the spirit of Biostatistics Ryan Gosling). This should not be considered an endorsement of p-values or p-value hacking.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-24-did-faulty-software-shut-down-the-nasdaq/",
    "title": "Did Faulty Software Shut Down the NASDAQ?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-08-24",
    "categories": [],
    "contents": "\nThis past Thursday, the NASDAQ stock exchange shut down for just over 3 hours due to some technical problems. It’s still not clear what the problem was because NASDAQ officials are being tight-lipped. NASDAQ has had a bad run of problems recently, the most visible was the botching of the Facebook initial public offering.\nStock trading these days is a highly technical business involving complex algorithms and multiple exchanges spread across the country. Poorly coded software or just plain old bugs have the potential to take down an entire exchange and paralyze parts of the financial system for hours.\nMary Jo White, the Chairman of the SEC is apparently getting involved.\n\nThursday evening, Ms. White said in a statement that the paralysis at the Nasdaq was “serious and should reinforce our collective commitment to addressing technological vulnerabilities of exchanges and other market participants.”\nShe said she would push ahead with recently proposed rules that would add testing requirements and safeguards for trading software. So far, those rules have faced resistance from the exchange companies. Ms. White said that she would “shortly convene a meeting of the leaders of the exchanges and other major market participants to accelerate ongoing efforts to further strengthen our markets.”\n\nHaving testing requirements for trading software is an interesting idea. It’s easy to see why the industry would be against it. Trading is a fast moving business and my guess is software is updated/modified constantly to improve performance or to provide people and edge. If you had to get approval or run a bunch of tests every time you wanted to deploy something, you’d quickly get behind the curve.\nBut is there an issue of safety here? If a small bug in the computer code on which the exchange relies can take down the entire system for hours, isn’t that a problem of “financial safety”? Other problems, like the notorious “flash crash” of 2010 where the Dow Jones Industrial Average dropped 700 points in minutes, have the potential to affect regular people, not just hedge fund traders.\nIt’s not unprecedented to subject computer code to higher scrutiny. Code that flies airplanes or runs air-traffic control systems is all tested and reviewed rigorously before being put into production and I think most people would consider that reasonable. Are financial markets the next area? What about scientific research?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-23-if-you-are-near-dcbaltimore-come-see-jeff-talk-about-coursera/",
    "title": "If you are near DC/Baltimore, come see Jeff talk about Coursera",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-08-23",
    "categories": [],
    "contents": "\nI’ll be speaking at the Data Science Maryland meetup. The title of my presentation is “Teaching Data Science to the Masses”. The talk is at 6pm on Thursday, Sept. 19th. More info here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-23-stratifying-pisa-scores-by-poverty-rates-suggests-imitating-finland-is-not-necessarily-the-way-to-go-for-us-schools/",
    "title": "Stratifying PISA scores by poverty rates suggests imitating Finland is not necessarily the way to go for US schools",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-08-23",
    "categories": [],
    "contents": "\nFor the past several years a steady stream of news articles and opinion pieces have been praising the virtues of Finish schools and exalting the US to imitate this system. One data point supporting this view comes from the most recent PISA scores (2009) in which Finland outscored the US 536 to 500. Several people have pointed out that this is an apples (huge and diverse) to oranges (small and homogeneous) comparison. One of the many differences that makes the comparison complicated is that Finland has less students living in poverty ( 3%) than the US (20%). This post defending US public school teachers makes this point with data. Here I show these data in graphical form. The plot on the left shows PISA scores versus the percent of students living in poverty for several countries. There is a pattern suggesting that higher poverty rates are associated with lower PISA scores. In the plot on the right, US schools are stratified by % poverty (orange points). The regression line is the same. Some countries are added (purple) for comparative purposes (the post does not provide their poverty rates).   Note that US school with poverty rates comparable to Finland’s (below 10%) outperform Finland and schools in the 10-24% range aren’t far behind. So why should these schools change what they are doing? Schools with poverty rates above 25% are another story. Clearly the US has lots of work to do in trying to improve performance in these schools,  but is it safe to assume that Finland’s system would work for these student populations?\n\nNote that I scraped data from this post and not the original source.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-22-chris-lane-u-s-tourism-boycotts-and-large-relative-risks-on-small-probabilities/",
    "title": "Chris Lane, U.S. tourism boycotts, and large relative risks on small probabilities",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-08-22",
    "categories": [],
    "contents": "\nChris Lane was tragically killed (link via Leah J.) in a shooting in Duncan, Oklahoma. According to the reports, it sounds like it was apparently a random and completely senseless act of violence. It is horrifying to think that those kids were just looking around for someone to kill because they were bored.\nGun violence in the U.S. is way too common and I’m happy about efforts to reduce the chance of this type of event. But I noticed this quote in the above linked CNN article from the former prime minister of Australia, Tim Fischer:\n\nPeople thinking of going to the USA for business or tourist trips should think carefully about it given the statistical fact you are 15 times more likely to be shot dead in the USA than in Australia per capita per million people.\n\nThe CNN article suggests he is calling for a boycott of U.S. tourism. I’m guessing he got his data from a table like this. According to the table, the total firearm related deaths per one million in Australia is 10.6 and in the U.S. 103. So the ratio is something like 10 times. If you restrict to homicides, the rates are 1.3 per million for Australia and 36 per million for the U.S. Here the ratio is almost 36 times.\nSo the question is, should you boycott the U.S. if you are an Australian tourist? Well, the percentage of people killed in firearm related deaths is 0.0036% in the U.S. and 0.00013% for Australia. So it is incredibly unlikely that you will be killed by a firearm in either country. The issue here is that with small probabilities, you can get huge relative risks, even when both outcomes are very unlikely in an absolute sense. The Chris Lane killing is tragic and horrifying, but I’m not sure a tourism boycott for the purposes of safety is justified.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-21-treading-a-new-path-for-reproducible-research-part-1/",
    "title": "Treading a New Path for Reproducible Research: Part 1",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-08-21",
    "categories": [],
    "contents": "\n\nDiscussions about reproducibility in scientific research have been on the rise lately, including on this blog. There are many underlying trends that have produced this increased interest in reproducibility: larger and larger studies being harder to replicate independently, cheaper data collection technologies/methods producing larger datasets, cheaper computing power allowing for more sophisticated analyses (even for small datasets), and the rise of general computational science (for every “X” we now have “Computational X”).\n\nFor those that haven’t been following, here’s a brief review of what I mean when I say “reproducibility”. For the most part in science, we focus on what I and some others call “replication”. The purpose of replication is to address the validity of a scientific claim. If I conduct a study and conclude that “X is related to Y”, then others may be encouraged to replicate my study–with independent investigators, data collection, instruments, methods, and analysis–in order to determine whether my claim of “X is related to Y” is in fact true. If many scientists replicate the study and come to the same conclusion, then there’s evidence in favor of the claim’s validity. If other scientists cannot replicate the same finding, then one might conclude that the original claim was false. In either case, this is how science has always worked and how it will continue to work.\n\nReproducibility, on the other hand, focuses on the validity of the data analysis. In the past, when datasets were small and the analyses were fairly straightforward, the idea of being able to reproduce a data analysis was perhaps not that interesting. But now, with computational science, where data analyses can be extraodinarily complicated, there’s great interest in whether certain data analyses can in fact be reproduced. By this I mean is it possible to take someone’s dataset and come to the same numerical/graphical/whatever output that they came to. While this seems theoretically trivial, in practice it’s very complicated because a given data analysis, which typically will involve a long pipeline of analytic operations, may be difficult to keep track of without proper organization, training, or software.\n\nWhat Problem Does Reproducibility Solve?\n\nIn my opinion, reproducibility cannot really address the validity of a scientific claim as well as replication. Of course, if a given analysis is not reproducible, that may call into question any conclusions drawn from the analysis. However, if an analysis is reproducible, that says practically nothing about the validity of the conclusion or of the analysis itself.\n\n\nIn fact, there are numerous examples in the literature of analyses that were reproducible but just wrong. Perhaps the most nefarious recent example is the Potti scandal at Duke. Given the amount of effort (somewhere close to 2000 hours) Keith Baggerly and his colleagues had to put into figuring out what Potti and others did, I think it’s reasonable to say that their work was not reproducible. But in the end, Baggerly was able to reproduce some of the results–this was how he was able to figure out that the analysis were incorrect. If the Potti analysis had not been reproducible from the start, it would have been impossible for Baggerly to come up with the laundry list of errors that they made.\n\n\nThe Reinhart-Rogoff kerfuffle is another example of analysis that ultimately was reproducible but nevertheless questionable. While Herndon did have to do a little reverse engineering to figure out the original analysis, it was nowhere near the years-long effort of Baggerly and colleagues. However, it was Reinhart-Rogoff’s unconventional weighting scheme (fully reproducible, mind you) that drew all of the attention and strongly influenced the analysis.\n\n\nI think the key question we want to answer when seeing the results of any data analysis is “Can I trust this analysis?” It’s not possible to go into every data analysis and check everything, even if all the data and code were available. In most cases, we want to have a sense that the analysis was done appropriately (if not optimally). I would argue that requiring that analyses be reproducible does not address this key question.\n\nWith reproducibility you get a number of important benefits: transparency, data and code for others to analyze, and an increased rate of transfer of knowledge. These are all very important things. Data sharing in particular may be important independent of the need to reproduce a study if others want to aggregate datasets or do meta-analyses. But reproducibility does not guarantee validity or correctness of the analysis.\nPrevention vs. Medication\n\nOne key problem with the notion of reproducibility is the point in the research process at which we can apply it as an intervention. Reproducibility plays a role only in the most downstream aspect of the research process–post-publication. Only after a paper is published (and after any questionable analyses have been conducted) can we check to see if an analysis was reproducible or conducted in error.\n\n\n\n\n\nAt this point it may be difficult to correct any mistakes if they are identified. Grad students have graduated, postdocs have left, people have moved on. In the Potti case, letters to the journal editors were ignored. While it may be better to check the research process at the end rather than to never check it, intervening at the post-publication phase is arguably the most expensive place to do it. At this phase of the research process, you are merely “medicating” the problem, to draw an analogy with chronic diseases. But fundamental data analytic damage may have already been done.\n\n\nThis medication aspect of reproducibility reminds me of a famous quotation from R. A. Fisher:\n\n\nTo consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.\n\n\nReproducibility allows for the statistician to conduct the post mortem of a data analysis. But wouldn’t it have been better to have prevented the analysis from dying in the first place?\n\n\nMoving Upstream\n\n\nThere has already been much discussion of changing the role of reproducibility in the publication/dissemination process. What if a paper had to be deemed reproducible before it was published? The question here is who will reproduce the analysis? We can’t trust the authors to do it so we have to get an independent third party. What about peer reviewers? I would argue that this is a pretty big burden to place on a peer reviewer who is already working for free. How about one of the Editors? Well, at the journal Biostatistics, that’s exactly what we do. However, our policy is voluntary and only plays a role after a paper has been accepted through the usual peer review process. At any rate, from a business perspective, most journal owners will be reluctant to implement any policy that might reduce the number of submissions to the journal.\n\nWhat Then?\n\nTo summarize, I believe reproducibility of computational research is very important, primarily to increase transparency and to improve knowledge sharing. However, I don’t think reproducibility in and of itself addresses the fundamental question of “Can I trust this analysis?”. Furthermore, reproducibility plays a role at the most downstream part of the research process (post-publication) where it is costliest to fix any mistakes that may be discovered. Ultimately, we need to think beyond reproducibility and to consider developing ways to ensure the quality of data analysis from the start.\n\n\nHow can we address the key problem concerning the validity of a data analysis? I’ll talk about what I think we should do in Part 2 of this post.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-20-a-couple-of-requests-for-the-statistics2013-future-of-statistics-workshop/",
    "title": "A couple of requests for the @Statistics2013 future of statistics workshop",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-08-20",
    "categories": [],
    "contents": "\nStatistics 2013 is hosting a workshop on the future of statistics. Given the timing and the increasing popularity of our discipline I think its a great idea to showcase the future of our field.\nI just have two requests:\n\n\n<div dir=\"ltr\" id=\":2o3\">\n  <ol>\n    <li>\n      Please invite more junior people to speak who are doing cutting edge work that will define the future of our field.\n    <\/li>\n    <li>\n      Please focus the discussion on some of the real and very urgent issues facing our field.\n    <\/li>\n  <\/ol>\n<\/div>\n\n\nRegarding #1  the list of speakers appears to be only very senior people. I wish there were more junior speakers because: (1) the future of statistics will be defined by people who are just starting their careers now and (2) there are some awesome super stars who are making huge advances in, among other things, the theory of machine learning, high-throughput data analysis, data visualization, and software creation. I think including at least one person under 40 on the speaking list would bring some fresh energy.*\nRegarding #2 I think there are a few issues that are incredibly important for our field as we move forward. I hope that the discussion will cover some of these:\nProblem first not solution backward. It would be awesome if there was a whole panel filled with people from industry/applied statistics talking about the major problems where statisticians are needed and how we can train statisticians to tackle those problems. In particular it would be cool to see discussion of: (1) should we remove some math and add some software development to our curriculum?, (2) should we rebalance our curriculum to include more machine learning?, (3) should we require all students to do rotations in scientific or business internships?, (4) should we make presentation skills a high priority skill along with the required courses in math stats/applied stats?\nStraight up embracing online education. We are teaching MOOCs here at Simply Stats. But that is only one way to embrace online education. What about online tutorials on Github. Or how about making educational videos for software packages?\nGood software is now the most important contribution of statisticians. The most glaring absence from the list of speakers and panels is that there is no discussion of software! I have gone so far as to say if you (or someone else) aren’t writing software for your methods, they don’t really exist. We need to have a serious discussion as a field about how the future of version control, reproducibility, data sharing, etc. are going to work. This seems like the perfect venue.\nHow we can forge better partnerships with industry and other data generators? Facebook, Google, Bitly, Twitter, Fitbit etc. are all collecting huge amounts of data. But there is no data sharing protocol like there was for genomics. Similarly, much of the imaging data in the world is tied up in academic and medical institutes. Fresh statistical eyes can’t be placed on these problems until the data are available in easily accessible, analyzable formats. How can we forge partnerships that make the data more valuable to the companies/institutes creating them and add immense value to young statisticians?\nThese next two are primarily targeted at academics:\nHow we can speed up our publication process? For academic statisticians this is a killer and major problem. I regularly wait 3-5 months for papers to be reviewed for the first time at the fastest stat journals. Some people still wait years. By then, the highest impact applied problems have moved on with better technology, newer methodology etc.\nHow we can make our promotion process/awards process more balanced between theoretical and applied contributions? I think both are very important, but right now, on balance, papers in JASA are much more highly rated than Bioconductor packages with 10,000+ users. Both are hard work, both represent important contributions and both should be given strong weight (for example in rating ASA Fellows).\nAnyway, I hope the conference is a huge success. I was pumped to see all the chatter on Twitter when Nate Silver spoke at JSM. That was a huge win for the organizers of the event. I am really hopeful that with the important efforts of the organizers of these big events that we will see a continued trend toward a bigger and bigger impact of statistics.\n_* Rafa is invited, but he’s over 40 :-).**_\n** Rafa told me to mention he’s barely over 40.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-13-wanted-neuro-quants/",
    "title": "WANTED: Neuro-quants",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-08-13",
    "categories": [],
    "contents": "\nOur good colleagues Brian Caffo, Martin Lindquist, and Ciprian Crainiceanu have written a nice editorial for the HuffPo on the need for statisticians in neuroimaging.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:07:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-09-embarrassing-typos-reveal-the-dangers-of-the-lonely-data-analyst/",
    "title": "Embarrassing typos reveal the dangers of the lonely data analyst",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-08-09",
    "categories": [],
    "contents": "\nA silly, but actually very serious, error in the supplementary material of a recent paper in Organometallics is causing a stir on the internets (I saw it on Andrew G.’s blog). The error in question is a comment in the supplementary material of the paper:\n\nEmma, please insert NMR data here! where are they? and for this compound, just make up an elemental analysis . . .\n\nAs has been pointed out on the chemistry blogs, this is actually potentially a pretty serious problem. Apparently, the type of analysis in question is relatively easy to make up or at minimum, there are a lot of researcher degrees of freedom.\nThis error reminds me of another slip-up, this one from a paper in BMC Bioinformatics. Here is the key bit, from the abstract:\n\nIn this study, we have used (insert statistical method here) to compile unique DNA methylation signatures from normal human heart, lung, and kidney using the\n\nThese slip-ups seem pretty embarrassing/funny at first pass. I will also admit that in some ways, I’m pretty sympathetic as a person who advises students and analysts. The comments on intermediate drafts of papers frequently say things like, “put this analysis here” or “fill in details here”. I think if one slipped through the cracks and ended up in the abstract or supplement of a paper I was publishing, I’d look pretty silly to.\nBut there are some more important issues here that relate to the issue of analysts/bioinformaticians/computing experts being directed by scientists. In some cases the scientists might not understand statistics, which has its own set of problems. But often the scientists know exactly what they are talking about; the analyst and their advisor/boss just need to communicate about what is acceptable and what isn’t acceptable in practice. This is beautifully covered in this post on advice for lonely bioinformaticians. I would extend that to all students/lonely analysts in any field. Finally, in the era of open science and collaboration, it is pretty clear that it is important to make sure that statements made in the margins of drafts can’t be misinterpreted and to check for typos in final submitted drafts of papers. Always double check for typos. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-08-data-scientist-is-just-a-sexed-up-word-for-statistician/",
    "title": "Data scientist is just a sexed up word for statistician",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-08-08",
    "categories": [],
    "contents": "\nA couple of cool things happened at this years JSM.\nTwitter adoption went way up and it was much easier for people (like me) who weren’t there to keep track of all the action by monitoring the #JSM2013 hashtag.\nNate Silver gave the keynote and [A couple of cool things happened at this years JSM.\nTwitter adoption went way up and it was much easier for people (like me) who weren’t there to keep track of all the action by monitoring the #JSM2013 hashtag.\nNate Silver gave the keynote and](https://twitter.com/rafalab/status/364480835577073664/photo/1) showed up.\nNate Silver is hands down the rockstar of our field. I mean, no other statistician changing jobs would make the news at the Times, at ESPN, and on pretty much every other major news source.\nSilver’s talk at JSM focused on 11 principles of statistical journalism, which are covered really nicely here by Joseph Rickert from Revolution. After his talk, he answered questions Tweeted from the audience. He brought the house down (I’m sure in person, but definitely on Twitter) with his response to a question about data scientists versus statisticians with the perfectly weighted response for the audience:\n\nData scientist is just a sexed up word for statistician\n\nOf course statisticians love to hear this but data scientists didn’t necessarily agree.\n\n\nNot at #JSM2013, but intersect of self-ID’ed statisticians w/ self-ID’ed data scis is ~ null. Not sure who’s losing in the “sexed up” dept.\n\n\n— Drew Conway (@drewconway) August 5, 2013\n\n\n\n\n@hspter not sure that describes what I do.\n\n\n— josh attenberg (@jattenberg) August 6, 2013\n\n\n\n\n@jattenberg @hspter Me either. \n\n— Hilary Mason (@hmason) August 6, 2013\n\n\nI’ve talked about the statistician/data scientist divide before and how I think that we need better marketing as statisticians. I think it is telling that some of the very accomplished, very successful people tweeting about Nate’s quote are uncomfortable being labeled statistician. The reason, I think, is that statisticians have a reputation for focusing primarily on theory and not being willing to do the schlep.\nI do think there is some cachet to having the “hot job title” but eventually solving real problems matters more. Which leads me to my favorite part of Nate’s quote, the part that isn’t getting nearly as much play as it should:\n\nJust do good work and call yourself whatever you want.\n\nI think that as statisticians we should embrace a “big tent” approach to labeling. But rather than making it competitive by saying data scientists aren’t that great they are just “sexed up” statisticians, we should make it inclusive, “data scientists are statisticians because being a statistician is awesome and anyone who does cool things with data is a statistician”. People who build websites, or design graphics, or make reproducible documents, or build pipelines, or hack low-level data are all statisticians and we should respect them all for their unique skills.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-07-simply-statistics-jsm2013-picks-for-wednesday/",
    "title": "Simply Statistics #JSM2013 Picks for Wednesday",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-08-07",
    "categories": [],
    "contents": "\nSorry for the delay with my session picks for Wednesday. Here’s what I’m thinking of:\n8:30-10:20am: Bayesian Methods for Causal Inference in Complex Settings (CC-520a) or Developments in Statistical Methods for Functional and Imaging Data (CC-522bc)\n10:30am-12:20pm: Spatial Statistics for Environmental Health Studies (CC-510c) or Big Data Exploration with Amazon (CC-516c)\n2-3:50pm: There are some future stars in the session Environmental Impacts on Public and Ecological Health (CC-512h) and Statistical Challenges in Cancer Genomics with Next-Generation Sequencing and Microarrays (CC-514a)\n4-5:50pm: Find out who won the COPSS award! (CC-517ab)\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-06-simply-statistics-jsm2013-picks-for-tuesday/",
    "title": "Simply Statistics #JSM2013 Picks for Tuesday",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-08-06",
    "categories": [],
    "contents": "\nIt seems like Monday was a big hit at JSM with Nate Silver’s talk and all. Rafa estimates that there were about 1 million people there (+/- 1 million). Ramnath Vaidyanathan has a nice summary of the talk and the Q&A afterwards. Among other things, Silver encouraged people to start a blog and communicate directly with the public. Couldn’t agree more! Thanks to all who live-tweeted at #JSM2013. I felt like I was there.\nOn to Tuesday! Here’s where I’d like to go:\n8:30-10:20am: Spatial Uncertainty in Public Health Problems (CC-513b); and since Nate says education is the next important area, Statistical Knowledge for Teaching: Research Results and Implications for Professional Development (CC-520d)\n10:30am-12:20pm: Check out the latest in causal inference at Fresh Perspectives on Causal Inference (CC-512f) and come see the future of statistics at the ****SBSS Student Paper Travel Award Winners II**** (CC-520d)\n2-3:50pm: There’s a cast of all-stars over in the Biased Epidemiological Study Designs: Opportunities and Challenges (CC-511c) session and a visualization session with an interesting premise Painting a Picture of Life in the United States (CC-510a)\n4-5:50pm: Only two choices here, so take your pick (or flip a coin).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-05-simply-statistics-jsm2013-picks-for-monday/",
    "title": "Simply Statistics #JSM2013 Picks for Monday",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-08-05",
    "categories": [],
    "contents": "\nI’m sadly not able to attend the Joint Statistical Meetings this year (where Nate Silver is the keynote speaker!) in the great city of Montreal. I’m looking forward to checking out the chatter on #JSM2013 but in the meantime, here are the sessions I would have attended if I’d been there. If I pick more than one session for a given time slot, I assume you can run back and forth between the two.\n8:30-10:20am: Kasper Hansen is presenting in Statistical Methods for High-Dimensional Data: Presentations by Junior Researchers (CC-515c) and there are some great people in The Profession of Statistics and Its Impact on the Media (CC-516d)\n10:30am-12:20pm: There are some heavy hitters in the Showcase of Analysis of Correlated Measurements (CC-511d); this session has a great title Herd Immunity: Teaching Techniques for the Health Sciences (CC-515b)**\n** * 2-3:50pm: I have a soft spot in my heart for a good MCMC session like Challenges in Using Markov Chain Monte Carlo in Modern Applications (CC-510d); I also have a soft spot for visualization and Simon Urbanek - Visualizing Big Data Interactively (CC-510b) * 4-5:50pm: I would check out Nate Silver’s talk (CC-517ab)\nHave fun!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-04-sunday-datastatistics-link-roundup-8413/",
    "title": "Sunday data/statistics link roundup (8/4/13)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-08-04",
    "categories": [],
    "contents": "\nThe $4 million teacher. I love the idea that teaching is becoming a competitive industry where the best will get the kind of pay they really really deserve. I can’t think of another profession where the ratio of (if you are good at how much influence you have on the world)/(salary) is so incredibly large. MOOC’s may contribute to this, that is if they aren’t felled by the ecological fallacy (via Alex N.).\nThe NIH is considering requiring replication of results (via Rafa). Interestingly, the article talks about reproducibility, as opposed to replication, throughout most of the text.\nR  jobs on the rise! Pair that with this rather intense critique of Marie Davidian’s interview about big data because she didn’t mention R. I think R/software development is definitely coming into its own as a critical part of any statistician’s toolbox. As that happens we need to take more and more care to include relevant training in version control, software development, and documentation for our students.\nNot technically statistics, but holy crap a 600,000 megapixel picture?\nA short history of data science. Not many card-carrying statisticians make the history, which is a shame, given all the good they have contributed to the development of the foundations of this exciting discipline (via Rafa).\nFor those of you at JSM 2013, make sure you wear out that hashtag (#JSM2013) for those of us on the outside looking in. Watch out for the Lumley 12 and make sure you check out Shirley’s talk, Lumley and Hadley together, this interesting looking ethics session, and Martin doing his fMRI thang,  among others….\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-02-that-causal-inference-came-out-of-nowhere/",
    "title": "That causal inference came out of nowhere",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-08-02",
    "categories": [],
    "contents": "\nThis is a study of breastfeeding and its impact on IQ that has been making the rounds on a number of different media outlets. I first saw it on the Wall Street Journal where I was immediately drawn to this quote:\n\nThey then subtracted those factors using a statistical model. Dr. Belfort said she hopes that “what we have left is the true connection” with nursing and IQ.\n\nAs the father of a young child this was of course pretty interesting to me so I thought I’d go and check out the paper itself. I was pretty stunned to see this line right there in the conclusions:\n\nOur results support a causal relationship of breastfeeding duration with receptive language and verbal and nonverbal intelligence later in life.\n\nI immediately thought: “man how did they run a clinical trial of breastfeeding”. It seems like it would be a huge challenge to get past the IRB. So then I read a little bit more carefully how they performed the analysis. It was a prospective study, where they followed the children over time, then performed a linear regression analysis to adjust for a number of other factors that might influence childhood intelligence. Some examples include mother’s IQ, soci0-demographic information, and questionaires about delivery.\nThey then fit a number of regression models with different combinations of covariates and outcomes. They did not attempt to perform any sort of causal inference to make up for the fact that the study was not randomized. Moreover, they did not perform multiple hypothesis testing correction for all of the combinations of effects they observed. The actual reported connections represent just a small fraction of all the possible connections they tested.\nSo I was pretty surprised when they said:\n\nIn summary, our results support a causal relationship of breastfeeding in infancy with receptive language at age 3 and with verbal and nonverbal IQ at school age.\n\n\nI’m as optimistic as science as they come. But where did that causal inference come from?\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-01-the-roc-curves-of-science/",
    "title": "The ROC curves of science",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-08-01",
    "categories": [],
    "contents": "\nAndrew Gelman’s recent post on what he calls the “scientific mass production of spurious statistical significance” reminded me of a thought I had back when I read John Ioannidis’ paper claiming that most published research finding are false. Many authors, which I will refer to as the pessimists, have joined Ioannidis in making similar claims and repeatedly blaming the current state of affairs on the mindless use of frequentist inference. The gist of my thought is that, for some scientific fields, the pessimist’s criticism is missing a critical point: that in practice, there is an inverse relationship between increasing rates of true discoveries and decreasing rates of false discoveries and that true discoveries from fields such as the biomedical sciences provide an enormous benefit to society. Before I explain this in more detail, I want to be very clear that I do think that reducing false discoveries is an important endeavor and that some of these false discoveries are completely avoidable. But, as I describe below, a general solution that improves the current situation is much more complicated than simply abandoning the frequentist inference that currently dominates.\nFew will deny that our current system, with all its flaws, still produces important discoveries. Many of the pessimists’ proposals for reducing false positives seem to be, in one way or another, a call for being more conservative in reporting findings. Example of recommendations include that we require larger effect sizes or smaller p-values, that we correct for the “researcher degrees of freedom”, and that we use Bayesian analyses with pessimistic priors. I tend to agree with many of these recommendations but I have yet to see a specific proposal on exactly how conservative we should be. Note that we could easily bring the false positives all the way down to 0 by simply taking this recommendation to its extreme and stop publishing biomedical research results all together. This absurd proposal brings me to receiver operating characteristic (ROC) curves.\nROC curves of scienceROC curves plot true positive rates (TPR) versus false positive rates (FPR) for a given classifying procedure. For example, suppose a regulatory agency that runs randomized trials on drugs (e.g. FDA) classifies a drug as effective when a pre-determined statistical test produces a p-value < 0.05 or a posterior probability > 0.95. This procedure will have a historical false positive rate and true positive rate pair: one point in an ROC curve. We can change the 0.05 to, say, 0.2 (or the 0.95 to 0.80) and we would move up the ROC curve: higher FPR and TPR. Not doing research would put us at the useless bottom left corner. It is important to keep in mind that biomedical science is done by imperfect humans on imperfect and stochastic measurements so to make discoveries the field has to tolerate some false discoveries (ROC curves don’t shoot straight up from 0% to 100%). Also note that it can take years to figure out which publications report important true discoveries.\nI am going to use the concept of ROC curve to distinguish between reducing FPR by being statistically more conservative and reducing FPR via more general improvements. In my ROC curve the y-axis represents the number of important discoveries per decade and the x-axis the number of false positives per decade (to avoid confusion I will continue to use the acronyms TPR and FPR). The current state of biomedical research is represented by one point on the red curve: one TPR,FPR pair. The pessimist argue that the FPR is close to 100% of all results but they rarely comment on the TPR. Being more conservative lowers our FPR, which saves us time and money, but it also lowers our TPR, which could reduce the number of important discoveries that improve human health. So what is the optimal balance and how far are we from it? I don’t think this is an easy question to answer.\nNow, one thing we can all agree on is that moving the ROC curve up is a good thing, since it means that we get a higher TPR for any given FPR. Examples of ways we can achieve this are developing better measurement technologies, statistically improving the quality of these measurements, augmenting the statistical training of researchers, thinking harder about the hypotheses we test, and making less coding or experimental mistakes. However, applying a more conservative procedure does not move the ROC up, it moves our point left on the existing ROC: we reduce our FPR but reduce our TPR as well.\nIn the plot above I draw two imagined ROC curves: one for physics and one for biomedical research. The physicists’ curve looks great. Note that it shoots up really fast which means they can make most available discoveries with very few false positives. Perhaps due to the maturity of the field, physicists can afford and tend to use very stringent criteria. The biomedical research curve does not look as good. This is mainly due to the fact that biology is way more complex and harder to model mathematically than physics. However, because there is a larger uncharted territory and more research funding, I argue that the rate of discoveries is higher in biomedical research than in physics. But, to achieve this higher TPR, biomedical research has to tolerate a higher FPR. According to my imaginary ROC curves, if we become as stringent as physicists our TPR would be five times smaller. It is not obvious to me that this would result in a better situation than the current one. At the same time, note that the red ROC suggests that increasing the FPR, with the hopes of increasing our TPR, is not a good idea because the curve is quite flat beyond our current location on the curve.\nClearly I am oversimplifying a very complicated issue, but I think it is important to point out that there are two discussions to be had: 1) where should we be on the ROC curve (keeping in mind the relationship between FPR and TPR)? and 2) what can we do to improve the ROC curve? My own view is that we can probably move down the ROC curve some and reduce the FPR without much loss in TPR (for example, by raising awareness of the researcher degrees of freedom). But I also think that most our efforts should go to reducing the FPR by improving the ROC. In general, I think statisticians can add to the conversation about 1) while at the same time continue collaborating to move the red ROC curve up.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-31-the-researcher-degrees-of-freedom-recipe-tradeoff-in-data-analysis/",
    "title": "The researcher degrees of freedom - recipe tradeoff in data analysis",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-07-31",
    "categories": [],
    "contents": "\nAn important concept that is only recently gaining the attention it deserves is researcher degrees of freedom. From Simmons et al.:\n\nThe culprit is a construct we refer to as researcher degrees of freedom. In the course of collecting and analyzing data, researchers have many decisions to make: Should more data be collected? Should some observations be excluded? Which conditions should be combined and which ones compared? Which control variables should be considered? Should specific measures be combined or transformed or both?\n\nSo far, researcher degrees of freedom has primarily been used with negative connotations. This probably stems from the original definition of the idea which focused on how analysts could “manufacture” statistical significance by changing the way the data was processed without disclosing those changes. Reproducible research and distributed code would of course address these issues to some extent. But it is still relatively easy to obfuscate dubious analysis by dressing it up in technical language.\nOne interesting point that I think sometimes gets lost in all of this is the researcher degrees of freedom - recipe tradeoff. You could think of this as thebias-variance tradeoff for big data.\nAt one end of the scale you can allow the data analyst full freedom, in which case researcher degrees of freedom may lead to overfitting and open yourself up to the manufacture of statistical results (optimistic significance or point estimates or confidence intervals). Or you can require a recipe for every data analysis which means that it isn’t possible to adapt to the unanticipated quirks (missing data mechanism, outliers, etc.) that may be present in an individual data set.\nAs with the bias-variance tradeoff, the optimal approach probably depends on your optimality criteria. You could imagine fitting a model that minimizes the mean squared error for fitting a linear model where you do not constrain the degrees of freedom in any way (that might represent an analysis where the researcher tries all possible models, including all types of data munging, choices of which observations to drop, how to handle outliers, etc.) to get the absolute best fit. Of course, this would likely be a strongly overfit/biased model. Alternatively you could penalize the flexibility allowed to the analyst. For example, you minimize a weighted criteria like:\n\n\n\nSome examples of the penalties could be:\n\n\n\nYou could also combine all of the penalties together into the “elastic researcher net” type approach. Then as the collective pentalty  you get the DSM, like you have in a clinical trial for example.As  you get fully flexible data analysis, which you might want for discovery.\nOf course if you allow researchers to choose the penalty you are right back to a scenario where you have degrees of freedom in the analysis (the problem you always get with any penalized approach). On the other hand it would make it easier to disclose how those degrees of freedom were applied.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-28-sunday-datastatistics-link-roundup-72813/",
    "title": "Sunday data/statistics link roundup (7/28/13)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-07-28",
    "categories": [],
    "contents": "\nAn article in the Huffpo about a report claiming there is no gender bias in the hiring of physics faculty. I didn’t read the paper carefully but  I definitely agree with the quote from  Prof. Dame Athene Donald that the comparison should be made to the number of faculty candidates on the market. I’d also be a little careful about touting my record of gender equality if only 13% of faculty in my discipline were women (via Alex N.).\nIf you are the only person who hasn’t seen the upwardly mobile by geography article yet, here it is (via Rafa). Also covered over at the great “charts n things” blog.\nFinally some good news on the science funding front; a Senate panel raises NSF’s budget by 8% (the link worked for me earlier but I was having a little trouble today). I think that this is of course a positive development. I think that article pairs very well with this provocative piece suggesting Detroit might have done better if they had a private research school.\nI’m going to probably talk about this more later in the week because it gets my blood pressure up, but I thought I’d just say again that hyperbolic takedowns of the statistical methods in specific papers in the popular press leads only one direction.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-26-statistics-takes-center-stage-in-the-independent/",
    "title": "Statistics takes center stage in the Independent",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-07-26",
    "categories": [],
    "contents": "\nCheck out this really good piece over at the Independent. It talks about the rise of statisticians as rockstars, naming Hans Rosling, Nate Silver, and Chris Volinsky among others. I think that those guys are great and deserve all the attention they get.\nI only hope that more of the superstars that fly under the radar of the general public but have made huge contributions  to science/medicine (like Ross Prentice, Terry Speed, Scott Zeger, or others that were highlighted in the comments here) get the same kind of attention (although I suspect they might not want it).\nI think one of the best parts of the article (which you should read in it’s entirety) is Marie Davidian’s quote:\n\nThere are rock stars, and then there are rock bands: statisticians frequently work in teams\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-22-what-are-the-5-most-influential-statistics-papers-of-2000-2010/",
    "title": "What are the 5 most influential statistics papers of 2000-2010?",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-07-22",
    "categories": [],
    "contents": "\nA few folks here at Hopkins were just reading the comments of our post on  awesome young/senior statisticians. It was cool to see the diversity of opinions and all the impressive people working in our field. We realized that another question we didn’t have a great answer to was:\n\nWhat are the 5 most influential statistics papers of the aughts (2000-2010)?\n\nNow that the auggies or aughts or whatever are a few years behind us, we have the benefit of a little hindsight and can get a reasonable measure of retrospective impact.\nSince this is a pretty broad question I’d thought I’d lay down some generic ground rules for nominations:\nPapers must have been published in 2000-2010.\nPapers must primarily report a statistical method or analysis (the impact shouldn’t be only because of the scientific result).\nPapers may be published in either statistical or applied journals.\nFor extra credit, along with your list give your definition of impact. Mine would be something like:\nHas been cited at a high rate in scientific papers (in other words, it is used by science, not just cited by statisticians trying to beat it)\nHas corresponding software that has been used\nMade simpler/changed the way we did a specific type of analysis\nI don’t have my list yet (I know, a cop-out) but I’m working on it.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-21-sunday-datastatistics-link-roundup-7212013/",
    "title": "Sunday data/statistics link roundup (7/21/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-07-21",
    "categories": [],
    "contents": "\nLet’s shake up the social sciences is a piece in the New York Times by Nicholas Christakis who rose to fame by claiming that obesity is contagious. Gelman responds that he thinks maybe Christakis got a little ahead of himself. I’m going to stay out of this one as it is all pretty far outside my realm - but I will say that I think quantitative social sciences is a hot area and all hot areas bring both interesting new results and hype. You just have to figure out which is which (via Rafa).\nThis is both creepy and proves my point about the ubiquity of data. Basically police departments are storing tons of information about where we drive because, well, it is easy to do so why not?\nI mean, I’m not an actuary and I don’t run cities, but this strikes me as a little insane. How do you not just keep track of all the pensions you owe people and add them up to know your total obligation? Why predict it when you could actually just collect the data? Maybe an economist can explain this one to me.  (via Andrew J.)\n[ 1. Let’s shake up the social sciences is a piece in the New York Times by Nicholas Christakis who rose to fame by claiming that obesity is contagious. Gelman responds that he thinks maybe Christakis got a little ahead of himself. I’m going to stay out of this one as it is all pretty far outside my realm - but I will say that I think quantitative social sciences is a hot area and all hot areas bring both interesting new results and hype. You just have to figure out which is which (via Rafa).\nThis is both creepy and proves my point about the ubiquity of data. Basically police departments are storing tons of information about where we drive because, well, it is easy to do so why not?\nI mean, I’m not an actuary and I don’t run cities, but this strikes me as a little insane. How do you not just keep track of all the pensions you owe people and add them up to know your total obligation? Why predict it when you could actually just collect the data? Maybe an economist can explain this one to me.  (via Andrew J.) 4.](http://www.nytimes.com/2013/07/19/opinion/in-defense-of-clinical-drug-trials.html?src=recg&gwh=9D33ABD1323113EF3AC9C48210900171) reverse scoops our clinical trials post! In all seriousness, there are a lot of nice responses there to the original article.\nJH Hospital back to #1. Order is restored. Read our analysis of Hopkins ignominious drop to #2 last year (via Sherri R.).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-19-defending-clinical-trials/",
    "title": "Defending clinical trials",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-07-19",
    "categories": [],
    "contents": "\nThe New York Times has published some letters to the Editor in response to the piece by Clifton Leaf on clinical trials. You can also see our response here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-19-the-failure-of-moocs-and-the-ecological-fallacy/",
    "title": "The \"failure\" of MOOCs and the ecological fallacy",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-07-19",
    "categories": [],
    "contents": "\nAt first blush the news out of San Jose State that the partnership with Udacity is being temporarily suspended is bad news for MOOCs. It is particularly bad news since the main reason for the suspension is poor student performance on exams. I think in the PR game there is certainly some reason to be disappointed in the failure of this first big experiment, but as someone who loves the idea of high-throughput education, I think that this is primarily a good learning experience.\nThe money quote in my mind is:\n\nOfficials say the data suggests many of the students had little college experience or held jobs while attending classes. Both populations traditionally struggle with college courses.\n“We had this element that we picked, student populations who were not likely to succeed,” Thrun said.\n\nI think it was a really nice idea to try to expand educational opportunities to students who traditionally dont have time for college or have struggled with college. But this represents a pretty major confounder in the analysis comparing educational outcomes between students in the online and in person classes. There is a lot of room for the ecological fallacy to make it look like online classes are failing. They could very easily address this problem by using a subset of students randomized in the right way. There are even really good papers - like this one by Glynn - on the optimal way to do this.\nI think there are some potential lessons learned here from this PR problem:\nWe need good study design in high-throughput education. I don’t know how rigorous the study design was in the case of the San Jose State experiment, but if the comparison is just whoever signed up in class versus whoever signed up online we have a long way to go in evaluating these classes.\nWe need coherent programs online It looks like they offered a scattered collection of mostly lower level courses online (elementary statistics, college algebra, entry level math, introduction to programming and introduction to psychology). These courses are obvious ones for picking off with MOOCs since they are usually large lecture-style courses in person as well. But they are also hard classes to “get motivated for” if there isn’t a clear end goal in mind. If you are learning college algebra online but don’t have a clear path to using that education it might make more sense to start with the Khan Academy\nWe need to parse variation in educational attainment.  It makes sense to evaluate in class and online students with similar instruments. But I wonder if there is a way to estimate the components of variation: motivation, prior skill, time dedicated to the course, learning from course materials, learning from course discussion, and learning for different types of knowledge (e.g. vocational versus theoretical) using statistical models. I think that kind of modeling would offer a much more clear picture of whether these programs are “working”.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-17-name-5-statisticians-now-name-5-young-statisticians/",
    "title": "Name 5 statisticians, now name 5 young statisticians",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-07-17",
    "categories": [],
    "contents": "\nI have been thinking for a while how hard it is to find statisticians to interview for the blog. When I started the interview series, it was targeted at interviewing statisticians at the early stages of their careers. It is relatively easy, if you work in academic statistics, to name 5 famous statisticians. If you asked me to do that, I’d probably say something like: Efron, Tibshirani, Irizarry, Prentice, and Storey. I could also name 5 famous statisticians in industry with relative ease: Mason, Volinsky, Heineike, Patil, Conway.\nMost of that is because of where I went to school (Storey/Prentice), the area I work in (Tibshirani/Irizarry/Storey), my advisor (Storey), or the bootstrap (Efron) and the people I see on Twitter (all the industry folks). I could, of course, name a lot of other famous statisticians. Almost all of them biased by my education or the books I read.\nBut almost surely I will miss people who work outside my area or didn’t go to school where I did. This is particularly true in applied statistics, where people might not even spend most of their time in statistics departments. It is doubly true of people who are young and just getting started, as I haven’t had a chance to hear about them.\nSo if you have a few minutes in the comments name five statisticians you admire. Then name five junior statisticians you think will be awesome. They don’t have to be famous (in fact it is better if they are good but not famous so I can learn something). Plus it will be interesting to see the responses.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-15-yes-clinical-trials-work/",
    "title": "Yes, Clinical Trials Work",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-07-15",
    "categories": [],
    "contents": "\nThis saturday the New York Times published an opinion pieces wondering “do clinical trials work?”. The answer, of course, is: absolutely. For those that don’t know the history, randomized control trials (RCTs) are one of the reasons why life spans skyrocketed in the 20th century. Before RCTs wishful thinking and arrogance lead numerous well-meaning scientist and doctors to incorrectly believe their treatments worked. They are so successful that they have been adopted with much fanfare in far flung arenas like poverty alleviation (see e.g.,this discussion by Esther Duflo); where wishful thinking also lead many to incorrectly believe their interventions helped.\nThe first chapter of this book contains several examples and this is a really nice introduction to clinical studies. A very common problem was that the developers of the treatment would create treatment groups that were healthier to start with. Randomization takes care of this. To understand the importance of controls I quote the opinion piece to demonstrate a common mistake we humans make: “Some patients did do better on the drug, and indeed, doctors and patients insist that some who take Avastin significantly beat the average.” The problem is that the fact that Avastin did not do better on average means that the exact same statement can be made about the control group! It also means that some patient did worse than average too. The use of a control points to the possibility that Avastin has nothing to do with the observed improvements.\nThe opinion piece is very critical of current clinical trials work and complains about the “dismal success rate for drug development”. But what is the author comparing too? Dismal compared to what? We are talking about developing complicated compounds that must be both safe and efficacious in often critically ill populations. It would be surprising if our success rate was incredibly high.  Or is the author comparing the current state of affairs to the pre-clinical-trials days when procedures such as bloodletting were popular.\nA better question might be, “how can we make clinical trials more efficient?” To answer this question there is definitely a lively and ongoing research area. In some cases they can definitely be better by adapting to new developments such as biomarkers and the advent of personalized medicine. This is why there are dozens of statisticians working in this area.\nThe article says that\n\n“[p]art of the novelty lies in a statistical technique called Bayesian analysis that lets doctors quickly glean information about which therapies are working best.”\n\nAs Jeff pointed out this a pretty major oversimplification of all of the hard work that it takes to maintain scientific integrity and patient safety when studying new compounds. The fact that the analysis is Bayesian is ancillary to other issues like adaptive trials (as Julian pointed out in the comments), dynamic treatment regimes, or even more established ideas like group sequential trials. The basic principle underlying these ideas is the same: _can we run a trial more efficiently while achieving reasonable estimates of effect sizes and uncertainties? _You could imagine doing this by focusing on subpopulations that seem to work well for subpopulations with specific biomarkers, or by stopping trials early if drugs are strongly (in)effective, or by picking optimal paths through multiple treatments. That the statistical methodology is Bayesian or Frequentist has little to do with the ways that clinical trials are adapting to be more efficient.\nThis is a wide open area and deserves a much more informed conversation. I’m providing here a list of resources that would be a good place to start:\nAn introduction to clinical trials\nMichael Rosenblum’s adaptive trial design page. \nClinicaltrials.gov - registry of clinical trials\nTest, learn adapt - a white paper on using clinical trials for public policy\nAlltrials - an initiative to make all clinical trial data public\nASCO clinical trials resources - on clinical trials ethics and standards\nDon Berry’s paper on adaptive design.\nFundamentals of clinical trials - a good general book (via David H.)\nClinical trials, a methodological perspective - a more regulatory take (via David H.)\nThis post is by Rafa and Jeff. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-14-sunday-datastatistics-link-roundup-7142013/",
    "title": "Sunday data/statistics link roundup (7/14/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-07-14",
    "categories": [],
    "contents": "\nQuestion: Do clinical trials work?Answer: Yes. Clinical trials are one of the defining success stories in the process of scientific inquiry. Do they work as fast/efficiently as a pharma company with potentially billions on the line would like? That is definitely much more up for debate. Most of the article is a good summary of how drug development works - although I think the statistics reporting is a little prone to hyperbole. I also think this sentence is both misleading, wrong, and way over the top, “Part of the novelty lies in a statistical technique called Bayesian analysis that lets doctors quickly glean information about which therapies are working best. There’s no certainty in the assessment, but doctors get to learn during the process and then incorporate that knowledge into the ongoing trial.” \nThe fun begins in the grim world of patenting genes. Two companies are being sued by Myriad even though they just lost the case on their main patent. Myriad is claiming violation of one of their 500 or so other patents. Can someone with legal expertise give me an idea - is Myriad now a patent troll?\nR spells for data wizards from Thomas Levine. I also link the pink on grey look.\nLarry W. takes on non-informative priors. Worth the read, particularly the discussion of how non-informative priors can be informative in different parameterizations. The problem Larry points out here is one I think that is critical - in big data applications where the goal is often discovery, we rarely have enough prior information to make reasonable informative priors either. Not to say some regularization can’t be helpful, but I think there is danger in putting an even weakly informative prior on a poorly understood, high dimensional space and then claiming victory when we discover something.\nStatistics and actuarial science are jumping into a politically fraught situation by raising the insurance on schools that allow teachers to carry guns. Fiscally, this is clearly going to be the right move. I wonder what the political fallout will be for the insurance company and for the governments that passed these laws (via Rafa via Marginal Revolution).\nTimmy!! Tim Lincecum throws his first no hitter. I know this isn’t strictly data/stats but he went to UW like me!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-10-what-are-the-iconic-data-graphs-of-the-past-10-years/",
    "title": "What are the iconic data graphs of the past 10 years?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-07-10",
    "categories": [],
    "contents": "\nThis article in the New York Times about the supposed death of photography got me thinking about statistics. Apparently, the death of photography has been around the corner for some time now:\n\nFor years, photographers have been bracing for this moment, warned that the last rites will be read for photography when video technology becomes good enough for anyone to record. But as this Fourth of July showed me, I think the reports of the death of photography have been greatly exaggerated.\n\nYet, photography has not died and, says Robin Kelsey, a professor of photography at Harvard,\n\nThe fact that we can commit a single image to memory in a way that we cannot with video is a big reason photography is still used so much today.\n\nThis got me thinking about data graphics. One long-time gripe about data graphics in R has been it’s horrible lack of support for dynamic or interactive graphics. graphics. This is an indisputable fact, especially in the early years. Nowadays there are quite a few extensions and packages that allow R to create dynamic graphics, but it still doesn’t feel like part of the “core”. I still feel like when I talk to people about R, the first criticism they jump to is the poor support for dynamic/interactive graphics.\nBut personally, I’ve never thought it was a big deal. Why? Because I don’t really find such graphics useful for truly thinking about data. I’ve definitely enjoyed viewing some of them (especially some of the D3 stuff), and it’s often fun to move sliders around and see how things change (perhaps my favorite is the Baby Name Voyager or maybe this one showing rapper wealth).\nBut in the end, what are you supposed to walk away with? As a a creator of such a graphic, how are you supposed to communicate the evidence in the data? The key element of dynamic/interactive graphics is that it allows the viewer to explore the data in their own way, not in some prescribed static way that you’ve explicitly set out. Ultimately, I think that aspect makes dynamic graphics useful for presenting data, but not that useful for presenting evidence. If you want to present evidence, you have to tell a story with the data, you can’t just let the viewer tell their own story.\nThis got me thinking about what are the iconic data “photos” of the past 10 years (or so). The NYT article mentions the famous “Raising the Flag on Iwo Jima” by AP photographer Joe Rosenthal as an image that many would recognize (and perhaps remember). What are the data graphics that are burned in your memory?\nI’ll give one example. I remember seeing Richard Peto give a talk here about the benefits of smoking cessation and its effect on life expectancy. He found that according to large population surveys, people who quit smoking by the age of 40 or so had more or less the same life expectancy as those who never smoked at all.  The graph he showed was one very similar to Figure 3 from this article. Although I already knew that smoking was bad for you, this picture really crystalized it for me in a specific way.\nOf course, sometimes data graphics are memorable for other reasons, but I’d like to try and stay positive here. Which data graphics have made a big impression on you?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-09-repost-preventing-errors-through-reproducibility/",
    "title": "Repost: Preventing Errors Through Reproducibility",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-07-09",
    "categories": [],
    "contents": "\nChecklist mania has hit clinical medicine thanks to people like Peter Pronovost and many others. The basic idea is that simple and short checklists along with changes to clinical culture can prevent major errors from occurring in medical practice. One particular success story is Pronovost’s central line checklist which dramatically reduced bloodstream infections in hospital intensive care units.\nThere are three important points about the checklist. First, it neatly summarizes information, bringing the latest evidence directly to clinical practice. It is easy to follow because it is short. Second, it serves to slow you down from whatever you’re doing. Before you cut someone open for surgery, you stop for a second and run the checklist. Third, it is a kind of equalizer that subtly changes the culture: everyone has to follow the checklist, no exceptions. A number of studies have now shown that when clinical units follow checklists, infection rates go down and hospital stays are shorter compared to units using standard procedures.\nHere’s a question: What would it take to convince you that an article’s results were reproducible, short of going in and reproducing the results yourself? I recently raised this question in a talk I gave at the Applied Mathematics Perspectives conference. At the time I didn’t get any responses, but I’ve had some time to think about it since then.\nI think most people are thinking of this issue along the lines of “The only way I can confirm that an analysis is reproducible is to reproduce it myself”. In order for that to work, everyone needs to have the data and code available to them so that they can do their own independent reproduction. Such a scenario would be sufficient (and perhaps ideal) to claim reproducibility, but is it strictly necessary? For example, if I reproduced a published analysis, would that satisfy you that the work was reproducible, or would you have to independently reproduce the results for yourself? If you had to choose someone to reproduce an analysis for you (not including yourself), who would it be?\nThis idea is embedded in the reproducible research policy at _Biostatistics, _but of course we make the data and code available too. There, a (hopefully) trusted third party (the Associate Editor for Reproducibility) reproduces the analysis and confirms that the code was runnable (at least at that moment in time).\nIt’s important to point out that reproducible research is not only about correctness and prevention of errors. It’s also about making research results available to others so that they may more easily build on the work. However, preventing errors is an important part and the question is then what is the best way to do that? Can we generate a reproducibility checklist?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-08-use-r-2014-to-be-at-ucla/",
    "title": "Use R! 2014 to be at UCLA",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-07-08",
    "categories": [],
    "contents": "\nThe 2014 Use R! conference will be in Los Angeles, CA and will be hosted by the UCLA Department of Statistics (an excellent department, I must say) and the newly created Foundation for Open Access Statistics. This is basically the meeting for R users and developers and has grown to be quite an event.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-04-fourth-of-july-datastatistics-link-roundup-742013/",
    "title": "Fourth of July data/statistics link roundup (7/4/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-07-04",
    "categories": [],
    "contents": "\nAn interesting post about how lots of people start out in STEM majors but eventually bail because they are too hard. They recommend either: (1) we better prepare high school students or (2) we make STEM majors easier. I like the idea of making STEM majors more interactive and self-paced. There is a bigger issue here of weed-out classes and barrier classes that deserves a longer discussion (via Alex N.)\nThis is an incredibly interesting FDA proposal to share all clinical data. I didn’t know this, but apparently right now all FDA data is proprietary. That is stunning to me, given the openness that we have say in genomic data - where most data are public. This goes beyond even the alltrials idea of reporting all results. I think we need full open disclosure of data and need to think hard about the privacy/consent implications this may have (via Rima I.).\nThis is a pretty cool data science fellowship program for people who want to transition from academia to industry, post PhD. I have no idea if the program is any good, but certainly the concept is a great one. (via Sherri R.)\nA paper in Nature Methods about data visualization and understanding the levels of uncertainty in data analysis. I love seeing that journals are recognizing the importance of uncertainty in analysis. Sometimes I feel like the “biggies” want perfect answers with no uncertainty - which never happens.\nThat’s it, just a short set of links today. Enjoy your 4th!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:06:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-03-repost-the-5-most-critical-statistical-concepts/",
    "title": "Repost: The 5 Most Critical Statistical Concepts",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-07-03",
    "categories": [],
    "contents": "\n(Editor’s Note: This is an old post but a good one from Jeff.)\nIt seems like everywhere we look, data is being generated - from politics, to biology, to publishing, to social networks. There are also diverse new computational tools, like GPGPU and cloud computing, that expand the statistical toolbox. Statistical theory is more advanced than its ever been, with exciting work in a range of areas.\nWith all the excitement going on around statistics, there is also increasing diversity. It is increasingly hard to define “statistician” since the definition ranges from very mathematical to very applied. An obvious question is: what are the most critical skills needed by statisticians?\nSo just for fun, I made up my list of the top 5 most critical skills for a statistician by my own definition. They are by necessity very general (I only gave myself 5).\nThe ability to manipulate/organize/work with data on computers - whether it is with excel, R, SAS, or Stata, to be a statistician you have to be able to work with data.\nA knowledge of exploratory data analysis - how to make plots, how to discover patterns with visualizations, how to explore assumptions\nScientific/contextual knowledge - at least enough to be able to abstract and formulate problems. This is what separates statisticians from mathematicians.\nSkills to distinguish true from false patterns - whether with p-values, posterior probabilities, meaningful summary statistics, cross-validation or any other means.\nThe ability to communicate results to people without math skills - a key component of being a statistician is knowing how to explain math/plots/analyses.\nWhat are your top 5? What order would you rank them in? Even though these are so general, I almost threw regression in there because of how often it pops up in various forms.\nRelated Posts: Rafa on graduate education and What is a Statistician? Roger on “Do we really need applied statistics journals?”\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-01-measuring-the-importance-of-data-privacy-embarrassment-and-cost/",
    "title": "Measuring the importance of data privacy: embarrassment and cost",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-07-01",
    "categories": [],
    "contents": "\nWe We when it is inexpensive and easy to collect data about ourselves or about other people. These data can take the form of health information - like medical records, or they could be financial data - like your online bank statements, or they could be social data - like your friends on Facebook. We can also easily collect information about our genetic makeup or our fitness (although it can be hard to get).\nAll of these data types are now stored electronically. There are obvious reasons why this is both economical and convenient. The downside, of course, is that the data can be used by the government or other entities in ways that you may not like. Whether it is to track your habits to sell you new products or to use your affiliations to make predictions about your political leanings, these data are not just “numbers”.\nData protection and data privacy are major issues in a variety of fields. In some areas, laws are in place to govern how your data can be shared and used (e.g. HIPAA). In others it is a bit more of a wild west mentality (see this interesting series of posts, “Know your data” by junkcharts talking about some data issues). I think most people have some idea that they would like to keep at least certain parts of their data private (from the government, from companies, or from their friends/family), but I’m not sure how most people think about data privacy.\nFor me there are two scales on which I measure the importance of the privacy of my own data:\nEmbarrassment - Data about my personal habits, whether I let my son watch too much TV, or what kind of underwear I buy could be embarrassing if it was out in public.\nFinancial  - Data about my social security number, my bank account numbers, or my credit card account could be used to cost me either my current money or potential future money.\nMy concerns about data privacy can almost always be measured primarily on these two scales. For example, I don’t want my medical records to be public because: (1) it might be embarrassing for people to know how bad my blood pressure is and (2) insurance companies might charge me more if they knew. On the other hand, I don’t want my bank account to get out primarily because it could cost me financially. So that mostly only registers on one scale.\nOne option, of course, would be to make all of my data totally private. But the problem is I want to share some of it with other people - I want my doctor to know my medical history and my parents to get to see pictures of my son. Usually I just make these choices about data sharing without even thinking about them, but after a little reflection I think these are the main considerations that go into my data sharing choices:\nWhere does it rate on the two scales above?\nHow much do I trust the person I’m sharing with? For example, my wife knows my bank account info, but I wouldn’t give it to a random stranger on the street. Google has my email and uses it to market to me, but that doesn’t bother me too much. But I trust them (I think) not to say - tell people I’m negotiating with my plans based on emails I sent to my wife (this goes with #4 below).\nHow hard would it be to use the information? I give my credit card to waiters at restaurants all the time, but I also monitor my account - so it would be relatively hard to run up a big bill before I (or the bank) noticed. I put my email address online, but it is a couple of steps between that and anything that is embarrassing/financially dubious for me. You’d have to be able to use that to hack some account.\nIs there incentive for someone to use the information? I’m not fabulously wealthy or famous. So most of the time, even if financial/embarrassing stuff is online about me, it probably wouldn’t get used. On the other hand, if I was an actor, a politician, or a billionaire there would be a lot more people incentivized to use my data against me. For example, if Google used my info to blow up a negotiation they would gain very little. I, on the other hand, would lose a lot and would probably sue them.*\nWith these ideas in mind it makes it a little easier for me to (at least personally) classify how much I care about different kinds of privacy breaches.\nFor example, suppose my health information was posted on the web. I would consider this a  problem because of both financial and embarrassment potential. It is also on the web, so I basically don’t trust the vast majority of people that would have access. On the other hand, it would be at least reasonably hard to use this data directly against me unless you were an insurance provider and most people wouldn’t have the incentive.\nTake another example: someone tagging me in Facebook photos (I don’t have my own account). Here the financial considerations are only potential future employment problems, but the embarrassment considerations are quite high. I probably somewhat trust the person tagging me since I at least likely know them. On the other hand it would be super-easy to use the info against me - it is my face in a picture and would just need to be posted on the web. So in this case, it mostly comes down to incentive and I don’t think most people have an incentive to use pictures against me (except in jokes - which I’m mostly cool with).\nI could do more examples, but you get the idea. I do wonder if there is an interesting statistical model to be built here on the basis of these axioms (or other more general ones) about when/how data should be used/shared.\n* An interesting side note is that I did use my gmail account when I was considering a position at Google fresh out of my Ph.D. I sent emails to my wife and my advisor discussing my plans/strategy. I always wondered if they looked at those emails when they were negotiating with me - although I never had any reason to suspect they had. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-06-27-what-is-the-best-way-to-analyze-data/",
    "title": "What is the Best Way to Analyze Data?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-06-27",
    "categories": [],
    "contents": "\nOne topic I’ve been thinking about recently is extent to which data analysis is an art versus a science. In my thinking about art and science, I rely on Don Knuth’s distinction, from his 1974 lecture “Computer Programming as an Art”:\n\nScience is knowledge that we understand so well that we can teach it to a computer; and if we don’t fully understand something, it is an art to deal with it. Since the notion of an algorithm or a computer program provides us with an extremely useful test for the depth of our knowledge about any given subject, the process of going from an art to a science means that we learn how to automate something.\n\nOf course, the phrase “analyze data” is far too general; it needs to be placed in a much more specific context. So choose your favorite specific context and consider this question: Is there a way to teach a computer how to analyze the data generated in that context? Jeff wrote about this a while back and he called this magical program the deterministic statistical machine.\nFor example, one area where I’ve done some work is in estimating short-term/acute population-level effects of ambient air pollution. These are typically done using time series data of ambient pollution from central monitors and community-level counts of some health outcome (e.g. deaths, hospitalizations). The basic question is if pollution goes up on a given day, do we also see health outcomes go up on the same day, or perhaps in the few days afterwards. This is a fairly well-worn question in the air pollution literature and there have been hundreds of time series studies published. Similarly, there has been a lot of research into the statistical methodology for conducting time series studies and I would wager that as a result of that research we actually know something about what to do and what not to do.\nBut is our level of knowledge about the methodology for analyzing air pollution time series data to the point where we could program a computer to do the whole thing? Probably not, but I believe there are aspects of the analysis that we could program.\nHere’s how I might break it down. Assume we basically start with a rectangular dataset with time series data on a health outcome (say, daily mortality counts in a major city), daily air pollution data, and daily data on other relevant variables (e.g. weather). Typically, the target of analysis is the association between the air pollution variable and the outcome, adjusted for everything else.\nExploratory analysis. Not sure this can be fully automated. Need to check for missing data and maybe stop analysis if proportion of missing data is too high? Check for high leverage points as pollution data tends to be skewed. Maybe log-transform if that makes sense in this context. Check for other outliers and note them for later (we may want to do a sensitivity analysis without those observations). \nModel fitting. This is already fully automated. If the outcome is a count, then typically a Poisson regression model is used. We already know that maximum likelihood is an excellent approach and better than most others under reasonable circumstances. There’s plenty of GLM software out there so we don’t even have to program the IRLS algorithm.\nModel building. Since this is not a prediction model, the main concern we have is that we properly adjusted for measured and unmeasured confounding. Francesca Dominici and some of her colleagues have done some interesting work regarding how best to do this via Bayesian model averaging and other approaches. I would say that in principle this can be automated, but the lack of easy-to-use software at the moment makes it a bit complicated. That said, I think simpler versions of the “ideal approach” can be easily implemented.\nSensitivity analysis. There are a number of key sensitivity analyses that need to be done in all time series analyses. If there were outliers during EDA, maybe re-run model fit and see if regression coefficient for pollution changes much. How much is too much? (Not sure.) For time series models, unmeasured temporal confounding is a big issue so this is usually checked using spline smoothers on the time variable with different degrees of freedom. This can be automated by fitting the model many different times with different degrees of freedom in the spline.\nReporting. Typically, some summary statistics for the data are reported along with the estimate + confidence interval for the air pollution association. Estimates from the sensitivity analysis should be reported (probably in an appendix), and perhaps estimates from different lags of exposure, if that’s a question of interest. It’s slightly more complicated if you have a multi-city study.\nSo I’d say that of the five major steps listed above, the one that I find most difficult to automate is EDA. There a lot of choices have to be made that are not easy to program into a computer. But I think the rest of the analysis could be automated. I’ve left out the cleaning and preparation of the data here, which also involves making many choices. But in this case, much of that is often outside the control of the investigator. These analyses typically use publicly available data where the data are available “as-is”. For example, the investigator would likely have no control over how the mortality counts were created.\nWhat’s the point of all this? Well, I would argue that if we cannot completely automate a data analysis for a given context, then either we need to narrow the context, or we have some more statistical research to do. Thinking about how one might automate a data analysis process is a useful way to identify where are the major statistical gaps in a given area. Here, there may be some gaps in how best to automate the exploratory analyses. Whether those gaps can be filled (or more importantly, whether you are interested in filling them) is not clear. But most likely it’s not a good idea to think about better ways to fit Poisson regression models.\nSo what do you do when all of the steps of the analysis have been fully automated? Well, I guess time to move on then….\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-06-26-art-from-data/",
    "title": "Art from Data",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-06-26",
    "categories": [],
    "contents": "\nThere’s a nice piece by Mark Hansen about data-driven aesthetics in the New York Times special section on big data.\n\nFrom a speedometer to a weather map to a stock chart, we routinely interpret and act on data displayed visually. With a few exceptions, data has no natural “look,” no natural “visualization,” and choices have to be made about how it should be displayed. Each choice can reveal certain kinds of patterns in the data while hiding others.\n\nI think drawing a line between a traditional statistical graphic and a pure work of art would be somewhat difficult. You can find examples of both that might fall in the opposite category: traditional graphics that transcend their utilitarian purposes and “pure art” works that tell you something new about your world.\nIndeed, I think Mark Hansen’s own work with Ben Rubin falls into the latter category–art pieces that perhaps had their beginnings as purely works of art but ended up giving you new insight into the world. For example, Listening Post was a highly creative installation that simultaneously gave you an emotional connection to random people chatting on the Internet as well as insight into what the Internet was “saying” at any given time (I wonder if NSA employees took a field trip to the Whitney Museum of American Art!).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-06-25-doing-statistical-research/",
    "title": "Doing Statistical Research",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-06-25",
    "categories": [],
    "contents": "\nThere’s a wonderful article over at the STATtr@k web site by Terry Speed on How to Do Statistical Research. There is a lot of good advice there, but the column is most notable because it’s pretty much the exact opposite of the advice that I got when I first started out.\nTo quote the article:\n\nThe ideal research problem in statistics is “do-able,” interesting, and one for which there is not much competition. My strategy for getting there can be summed up as follows:\nConsulting: Do a very large amount\nCollaborating: Do quite a bit\nResearch: Do some\n\nFor the most part, I was told to flip the research and consulting bits. That is, you want to spend most of your time doing “research” and very little of your time doing “consulting”. Why? Because ostensibly, the consulting work doesn’t involve new problems, only solving old problems with existing techniques. The research work by definition involves addressing new problems.\nBut,\n\nA strategy I discourage is “develop theory/model/method, seek application.” Developing theory, a model, or a method suggests you have done some context-free research; already a bad start. The existence of proof (Is there a problem?) hasn’t been given. If you then seek an application, you don’t ask, “What is a reasonable way to answer this question, given this data, in this context?” Instead, you ask, “Can I answer the question with this data; in this context; with my theory, model, or method?” Who then considers whether a different (perhaps simpler) answer would have been better?\n\nThe truth is, most problems can be solved with an existing method. They may not be 100% solvable with existing tools, but usually 90% is good enough and it’s not worth developing a new statistical method to cover the remaining 10%. What you really want to be doing is working on the problem that is 0% solvable with existing methods. Then there’s a pretty big payback if you develop a new method to address it and it’s more likely that your approach will be adopted by others simply because there’s no alternative. But in order to find these 0% problems, you have to see a lot of problems, and that’s where the consulting and collaboration comes in. Exposure to lots of problems lets you see the universe of possibilities and gives you a sense of where scientists really need help and where they’re more or less doing okay.\nEven if you agree with Terry’s advice, implementing it may not be so straightforward. It may be easier/harder to do consulting and collaboration depending on where you work. Also, finding good collaborators can be tricky and may involve some trial and error.\nBut it’s useful to keep this advice in mind, especially when looking for a job. The places you want to be on the lookout for are places that give you the most exposure to interesting scientific problems, the 0% problems. These places will give you the best opportunities for collaboration and for having a real impact on science.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-06-24-does-fraud-depend-on-my-philosophy/",
    "title": "Does fraud depend on my philosophy?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-06-24",
    "categories": [],
    "contents": "\nEver since my last post on replication and fraud I’ve been doing some more thinking about why people consider some things “scientific fraud”. (First of all, let me just say that I was a bit surprised by the discussion in the comments for that post. Some people apparently thought I was asking about the actual probability that the study was a fraud. This was not the case. I just wanted people to think about how they would react when confronted with the scenario.)\nI often find that when I talk to people about the topic of scientific fraud, especially statisticians, there is a sense that much work that goes on out there is fraudulent, but the precise argument for why is difficult to pin down.\nConsider the following three cases:\nI conduct a randomized clinical trial comparing a new treatment and a control and their effect on outcome Y1. I also collect data on outcomes Y2, Y3, … Y10. After conducting the trial I see that there isn’t a significant difference for Y1 so I test the other 9 outcomes and find a significant effect (defined as p-value equal to 0.04) for Y7. I then publish a paper about outcome Y7 and state that it’s significant with p=0.04. I make no mention of the other outcomes.\nI conduct the same clinical trial with the 10 different outcomes and look at the difference between the treatment groups for all outcomes. I notice that the largest standardized effect size is for Y7 with a standardized effect of 3, suggesting the treatment is highly effective in this trial. I publish a paper about outcome Y7 and state that the standardized effect size was 3 for comparing treatment vs. control. I note that a difference of 3 is highly significant, but I make no mention of statistical significance or p-values. I also make no mention of the other outcomes.\nI conduct the same clinical trial with the 10 outcomes. Now I look at all 10 outcomes and calculate the posterior probability that the effect is greater than zero (favoring the new treatment), given a pre-specified diffuse prior on the effect (assume it’s the same prior for each effect). Of the 10 outcomes I see that Y7 has the largest posterior probability of 0.98. I publish a paper about Y7 stating that my posterior probability for a positive effect is 0.98. I make no mention of the other outcomes.\nWhich one of these cases constitutes scientific fraud?\nI think most people would object to Case 1. This is the classic multiple testing scenario where the end result is that the stated p-value is not correct. Rather than a p-value of 0.04 the real p-value is more like 0.4. A simple Bonferroni correction fixes this but obviously would have resulted in not finding any significant effects based on a 0.05 threshold. The real problem is that in Case 1 you are clearly trying to make an inference about future studies. You’re saying that if there’s truly no difference, then in 100 other studies just like this one, you’d expect only 4 to detect a difference under the same criteria that you used. But it’s incorrect to say this and perhaps fraudulent (or negligent) depending on your underlying intent. In this case a relevant detail that is missing is the number of other outcomes that were tested.\nCase 2 differs from case 1 only in that no p-values are used but rather the measure of significance is the standardized effect size. Therefore, no probability statements are made and no inference is made about future studies. Although the information about the other outcomes is similarly omitted in this case as in case 1, it’s difficult for me to identify what is wrong with this paper.\nCase 3 takes a Bayesian angle and is more or less like case 2 in my opinion. Here, probability is used as a measure of belief about a parameter but no explicit inferential statements are made (i.e. there is no reference to some population of other studies). In this case I just state my belief about whether an effect/parameter is greater than 0. Although I also omit the other 9 outcomes in the paper, revealing that information would not have changed anything about my posterior probability.\nIn each of these three scenarios, the underlying data were generated in the exact same way (let’s assume for the moment that the trial itself was conducted with complete integrity).  In each of the three scenarios, 10 outcomes were examined and outcome Y7 was in some sense the most interesting.\nOf course, the analyses and the interpretation of the data were not the same in each scenario. Case 1 makes an explicit inference whereas Cases 2 and 3 essentially do not. However, I would argue the evidence about the new treatment compared to the control treatment in each scenario was identical.\nI don’t believe that the investigator in Case 1 should be allowed to engage in such shenanigans with p-values, but should he/she be pilloried simply because the p-value was the chosen metric of significance? I guess the answer would be “yes” for many of you, but keep in mind that the investigator in Case 1 still generated the same evidence as the others. Should the investigators in Case 2 and Case 3 be thrown in the slammer? If so, on what basis?\nMy feeling is not that people should be allowed to do whatever they please, but we need a better way to separate the “stuff” from the stuff. This is both a methodological and a communications issue. For example, Case 3 may not be fraud but I’m not necessarily interested in what the investigator’s opinion about a parameter is. I want to know what the data say about that parameter (or treatment difference in this case). Is it fraud to make any inferences in the first place (as in Case 1)? I mean, how could you possible know that your inference is “correct”? If “all models are wrong, but some are useful”, does that mean that everyone is committing fraud?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-06-23-sunday-datastatistics-link-roundup-62313/",
    "title": "Sunday data/statistics link roundup (6/23/13)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-06-23",
    "categories": [],
    "contents": "\nAn interesting study describing the potential benefits of using significance testing may be potentially beneficial and a scenario where the file drawer effect may even be beneficial. Granted this is all simulation so you have to take it with a grain of salt, but I like the pushback against the hypothesis testing haters. In all things moderation, including hypothesis testing.\nVenn Diagrams for the win, bro.\nThe new basketball positions. The idea is to cluster players based on the positions on the floor where they shoot, etc. I like the idea of data driven position definitions; I am a little worried about “reading ideas in” to a network picture.\nA really cool idea about a startup that makes data on healthcare procedures available to patients. I’m all about data transparency, but it makes me wonder, how often do people with health insurance negotiate the prices of procedures (via Leah J.)\nAnother interesting article about using tweets (and other social media) to improve public health. I do wonder about potential sampling issues, like what happened with google flu trends (via Nick C.)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-06-21-interview-with-miriah-meyer-microsoft-faculty-fellow-and-visualization-expert/",
    "title": "Interview with Miriah Meyer - Microsoft Faculty Fellow and Visualization Expert",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-06-21",
    "categories": [],
    "contents": "\n\nMiriah Meyer received her Ph.D. in computer science from the University of Utah, then did a postdoctoral fellowship at Harvard University and was a visiting fellow at MIT and the Broad Institute. Her research focuses on developing visualization tools in close collaboration with biological scientists. She has been recognized as a Microsoft Faculty Fellow, a TED Fellow, and appeared on the TR35. We talked with Miriah about visualization, collaboration, and her influences during her career as part of the Simply Statistics Interview Series.\nSS: Which term applies to you: data scientist, statistician, computer scientist, or something else?\nMM: My training is as a computer scientist and much of the way I problem solve is grounded in computational thinking. I do, however, sometimes think of myself as a data counselor, as a big part of what I do is help my collaborators move towards a deeper and more articulate statement about what they want/need to do with their data.\nSS: Most data analysis is done by scientists, not trained statisticians. How does data visualization help/hurt scientists when looking at piles of complicated data?\nMM: In the sciences, visualization is particularly good for hypothesis generation and early stage exploration. With many fields turning toward data-driven approaches, scientists are often not sure of exactly what they will find in a mound of data. Visualization allows them to look into the data without having to specify a specific question, query, or model. This early, exploratory analysis is very difficult to do strictly computationally. Exploration via interactive visualization can lead a scientist towards establishing a more specific question of the data that could then be addressed algorithmically.\nSS: What are the steps in developing a visualization with a scientific collaborator?\nMM: The first step is finding good collaborators \nThe beginning of a project is spent in discussions with the scientists, trying to understand their needs, data, and mental models of a problem. I find this part to be the most interesting, and also the most challenging. The goal is to develop a clear, abstract understanding of the problem and set of design requirements. We do this through interviews and observations, with a focus on understanding how people currently solve problems and what they want to do but can’t with current tools.\nNext is to take this understanding and prototype ideas for visualization designs. Rapid prototyping on paper is usually first, followed by more sophisticated, software prototypes after getting feedback from the collaborators. Once a design is sufficiently fleshed out and validated, a (relatively) full-featured visualization tool is developed and deployed.\nAt this point, the scientists tend to realize that the problem they initially thought was most interesting isn’t… and the cycle continues.\nFast iteration is really essential in this process. In the past I’ve gone through as many as three cycles of this process before find the right problem abstractions and designs.\nSS: You have tackled some diverse visualizations (from synteny to poems); what are the characteristics of a problem that make it a good candidate for new visualizations?\nMM: For me, the most important thing is to find good collaborators. It is essential to find partners that are willing to give lots of their time up front, are open-minded about research directions, and are working on cutting-edge problems in their field. This latter characteristic helps to ensure that there will be something novel needed from a data analysis and visualization perspective.\nThe other thing is to test whether a problem passes the Tableau/R/Matlab test: if the problem can’t be solved using one of these environments, then that is probably a good start.\nSS: What is the four-level nested model for design validation and how did you extend it?\nMM: This is a design decision model that helps to frame the different kinds of decisions made in the visualization design process, such as decisions about data derivations, visual representations, and algorithms. The model helps to put any one decision in the context of other visualization ideas, methods, and techniques, and also helps a researcher generalize new ideas to a broader class of problems. We recently extended this model to specifically define what a visualization “guideline” is, and how to relate this concept to how we design and evaluate visualizations.\nSS: Who are the key people who have been positive influences on your career and how did they help you?\nMM: One influence that jumps out to me is a collaboration with a designer in Boston named Bang Wong. Working with Bang completely changed my approach to visualization development and got me thinking about iteration, rapid prototyping, and trying out many ideas before committing. Also important were two previous supervisors, Ross Whitaker and Tamara Munzner, who constantly pushed me to be precise and articulate about problems and approaches to them. I believe that precision is a hallmark of good data science, even when characterizing unprecise things \nSS: Do you have any advice for computer scientists/statisticians who want to work on visualization as a research area?\nMM: Do it! Visualization is a really fun, vibrant, growing field. It relies on a broad spectrum of skills, from computer science, to design, to collaboration. I would encourage those interested to not get to infatuated with the engineering or the aesthetics and to instead focus on solving real-world problems. There is an unlimited supply of those!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-06-20-googles-brainteasers-that-dont-work-and-johns-hopkins-biostatistics-data-analysis/",
    "title": "Google's brainteasers (that don't work) and Johns Hopkins Biostatistics Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-06-20",
    "categories": [],
    "contents": "\nThis article is getting some attention, because Google’s VP for people operations at Google has made public a few insights that the Google HR team has come to over the last several years. The most surprising might be:\nThey don’t collect GPAs except for new candidates\nTest scores are worthless\nInterview scores weren’t correlated with success.\nBrainteasers that Google is so famous for are worthless\nBehavioral interviews are the most effective\nThe reason the article is getting so much attention is how surprising these facts may be to people who have little experience hiring/managing in technical fields. But I thought this quote was really telling:\n\n One of my own frustrations when I was in college and grad school is that you knew the professor was looking for a specific answer. You could figure that out, but it’s much more interesting to solve problems where there isn’t an obvious answer.\n\nInterestingly, that is the whole point of my data analysis course here at Hopkins. Over my relatively limited time as a faculty member I realized there were two key qualities that made students in biostatistics stand out: (1) that they were hustlers - willing to just work until the problem is solved even if it was frustrating and (2) that they were willing/able to try new approaches or techniques they weren’t comfortable with. I don’t have the quantitative data that Google does, but I would venture to guess those two traits explain 80%+ of the variation in success rates for graduate students in statistics/computing/data analysis.\nOnce that realization is made, it becomes clear pretty quickly that textbook problems or re-analysis of well known data sets measure something orthogonal to traits (1) and (2). So I went about redesigning the types of problems our students had to tackle. Instead of assigning problems out of a book I redesigned the questions to have the following characteristics:\nThe were based on live data sets. I define a “live” data set as a data set that has not been used to answer the question of interest previously. \nThe questions are problem forward, not solution backward. I would have an idea of what would likely work and what would likely not work. But I defined the question without thinking about what methods the students might use.\nThe answer was open ended (and often not known to me in advance).\nThe problems often had to do with unique scenarios not encountered frequently in statistics (e.g. you have a data census instead of just a sample).\nThe problems involved methods application/development, coding, and writing/communication.\nI have found that problems with these characteristics more precisely measure hustle and flexibility, like Google is looking for in their hiring practices. Of course, there are some down sides to this approach. I think it can be more frustrating for students, who don’t have as clearly defined a path through the homework. It also means dramatically more work for the instructor in terms of analyzing the data to find the quirks, creating personalized feedback for students, and being able to properly estimate the amount of work a project will take.\nWe have started thinking about how to do this same thing at scale on Coursera. In the meantime, Google will just have to send their recruiters to Hopkins Biostats to find students who meet the characteristics they are looking for :-).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-06-16-sunday-datastatistics-link-roundup-61613-fathers-day-edition/",
    "title": "Sunday data/statistics link roundup (6/16/13 - Father's day edition!)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-06-16",
    "categories": [],
    "contents": "\nDatapalooza! I’m wondering where my invite is? I do health data stuff, pick me, pick me! Actually it does sound like a pretty good idea - in general giving a bunch of smart people access to interesting data and real science problems can produce some cool results (link via Dan S.)\nThis report on precision medicine from the Manhattan Institute is related to my post this week on personalized medicine. I like the idea that we should be focusing on developing new ideas for adaptive trials (my buddy Michael is all over that stuff). I did thing that it was a little pie-in-the-sky with plenty of buzzwords like Bayesian causal networks and pattern recognition. I think these ideas are certainly applicable, but the report, I think, overstates the current level of applicability of these methods. We need more funding and way more research to support this area before we should automatically adopt it - big data can be used to confuse when methods aren’t well understood (link via Rafa via Marginal Revolution).\nrOpenSci wins a grant from the Sloan Foundation! Psyched to see this kind of innovative open software development get the support it deserves. My favorite rOpenSci package is rFigshare, what’s yours?\nA k-means approach to detecting what will be trending on Twitter. It always gets me so pumped up to see the creative ways that methods that have been around forever can be adapted to solve real, interesting problems.\nFinally, I thought this link was very appropriate for father’s day. I couldn’t agree more that the best kind of learning happens when you are just so in to something that you forget you are learning. Happy father’s day everyone!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-06-14-the-vast-majority-of-statistical-analysis-is-not-performed-by-statisticians/",
    "title": "The vast majority of statistical analysis is not performed by statisticians",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-06-14",
    "categories": [],
    "contents": "\n\nWhether you know it or not, everything you do produces data - from the websites you read to the rate at which your heart beats. Until pretty recently, most of the data you produced wasn’t collected, it floated off unmeasured. The only data that were collected were painstakingly gathered by scientists one number at a time in small experiments with a few people. This laborious process meant that data were expensive and time-consuming to collect. Yet many of the most amazing scientific discoveries over the last two centuries were squeezed from just a few data points. But over the last two decades, the unit price of data has dramatically dropped. New technologies touching every aspect of our lives from our money, to our health, to our social interactions have made data collection cheap and easy (see e.g. Camp Williams).\n\n\nTo give you an idea of how steep the drop in the price of data has been, in 1967 Stanley Milgram did an experiment to determine the number of degrees of separation between two people in the U.S. In his experiment he sent 296 letters to people in Omaha, Nebraska and Wichita, Kansas. The goal was to get the letters to a specific person in Boston, Massachusetts. The trick was people had to send the letters to someone they knew, and they then sent it to someone they knew and so on. At the end of the experiment, only 64 letters made it to the individual in Boston. On average, the letters had gone through 6 people to get there. This is where the idea of “6-degrees of Kevin Bacon” comes from. Based on 64 data points.  A 2007 study updated that number to “7 degrees of Kevin Bacon”. The study was based on 30 billion instant messaging conversations collected over the course of a month or two with the same amount of effort.\n\n\nOnce data started getting cheaper to collect, it got cheaper fast. Take another example, the human genome. The genome is the unique DNA code in every one of your cells. It consists of a set of 3 billion letters that is unique to you. By many measures, the race to be the first group to collect all 3 billion letters from a single person kicked off the data revolution in biology. The project was completed in 2000 after a decade of work and $3 billion to collect the 3 billion letters in the first human genome. This project was actually a stunning success, most people thought it would be much more expensive. But just over a decade later, new technology means that we can now collect all 3 billion letters from a person’s genome for about $10,000 in about a week.\n\n As the price of data dropped so dramatically over the last two decades, the division of labor between analysts and everyone else became less and less clear. Data became so cheap that it couldn’t be confined to just a few highly trained people. So raw data started to trickle out in a number of different ways. It started with maps of temperatures across the U.S. in newspapers and quickly ramped up to information on how many friends you had on Facebook, the price of tickets on 50 airlines for the same flight, or measurements of your blood pressure, good cholesterol, and bad cholesterol at every doctor’s visit. Arguments about politics started focusing on the results of opinion polls and who was asking the questions. The doctor stopped telling you what to do and started presenting you with options and the risks that went along with each.\n\nThat is when statisticians stopped being the primary data analysts. At some point, the trickle of data about you, your friends, and the world started impacting every component of your life. Now almost every decision you make is based on data you have about the world around you. Let’s take something simple, like where are you going to eat tonight. You might just pick the nearest restaurant to your house. But you could also ask your friends on Facebook where you should eat, or read reviews on Yelp, or check out menus on the restaurants websites. All of these are pieces of data that are collected and presented for you to “analyze”.\n\n\nThis revolution demands a new way of thinking about statistics. It has precipitated explosive growth in data visualization - the most accessible form of data analysis. It has encouraged explosive growth in MOOCs like the ones Roger, Brian and I taught. It has created open data initiatives in government. It has also encouraged more accessible data analysis platforms in the form of startups like StatWing that make it easier for non-statisticians to analyze data.\n\n\nWhat does this mean for statistics as a discipline? Well it is great news in that we have a lot more people to train. It also really drives home the importance of statistical literacy. But it also means we need to adapt our thinking about what it means to teach and perform statistics. We need to focus increasingly on interpretation and critique and away from formulas and memorization (think English composition versus grammar). We also need to realize that the most impactful statistical methods will not be used by statisticians, which means we need more fool proofing, more time automating, and more time creating software. The potential payout is huge for realizing that the tide has turned and most people who analyze data aren’t statisticians.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-06-13-false-discovery-rate-regression-cc-nsas-prism/",
    "title": "False discovery rate regression (cc NSA's PRISM)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-06-13",
    "categories": [],
    "contents": "\nThere is an idea I have been thinking about for a while now. It re-emerged at the top of my list after seeing this really awesome post on using metadata to identify “conspirators” in the American revolution. My first thought was: but how do you know that you aren’t just making lots of false discoveries?\nHypothesis testing and significance analysis were originally developed to make decisions for single hypotheses. In many modern applications, it is more common to test hundreds or thousands of hypotheses. In the standard multiple testing framework, you perform a hypothesis test for each of the “features” you are studying (these are typically genes or voxels in high-dimensional problems in biology, but can be other things as well). Then the following outcomes are possible:\n\n\n\n\nCall Null True\n\n\nCall Null False\n\n\nTotal\n\n\nNull True\n\n\nTrue Negatives\n\n\nFalse Positives\n\n\nTrue Nulls\n\n\nNull False\n\n\nFalse Negatives\n\n\nTrue Positives\n\n\nFalse Nulls\n\n\n\n\nNo Decisions\n\n\nRejections\n\n\nThe reason for “No Decisions” is that the way hypothesis testing is set up, one should technically never accept the null hypothesis. The number of rejections is the total number of times you claim that a particular feature shows a signal of interest.\nA very common measure of embarrassment in multiple hypothesis testing scenarios is the false discovery rate defined as:\n\n\n\n.\nThere are some niceties that have to be dealt with here, like the fact that the  may be equal to zero, inspiring things like the positive false discovery rate, which has some nice Bayesian interpretations.\nThe way that the process usually works is that a test statistic is calculated for each hypothesis test where a larger statistic means more significant and then operations are performed on these ordered statistics. The two most common operations are: (1) pick a cutoff along the ordered list of p-values - call everything less than this threshold significant and estimate the FDR for that cutoff and (2) pick an acceptable FDR level and find an algorithm to pick the threshold that controls the FDR where control is defined usually by saying something like the algorithm produces .\nRegardless of the approach these methods usually make an assumption that the rejection regions should be nested. In other words, if you call statistic \\(T\\_k\\) significant and \\(T\\_j > T\\_k\\) then your method should also call statistic \\(T\\_j\\) significant. In the absence of extra information, this is a very reasonable assumption.\nBut in many situations you might have additional information you would like to use in the decision about whether to reject the null hypothesis for test \\(j\\).\nExample 1 A common example is gene-set analysis. Here you have a group of hypotheses that you have tested individually and you want to say something about the level of noise in the group. In this case, you might want to know something about the level of noise if you call the whole set interesting.\nExample 2 Suppose you are a mysterious government agency and you want to identify potential terrorists. You observe some metadata on people and you want to predict who is a terrorist - say using betweenness centrality. You could calculate a P-value for each individual, say using a randomization test. Then estimate your FDR based on predictions using the metadata.\nExample 3 You are monitoring a system over time where observations are random. Say for example whether there is an outbreak of a particular disease in a particular region at a given time. So, is the rate of disease higher than background. How can you estimate the rate at which you make false claims?\nFor now I’m going to focus on the estimation scenario but you could imagine using these estimates to try to develop controlling procedures as well.\nIn each of these cases you have a scenario where you are interested in something like:\n\n\n\nwhere  is a covariate-specific estimator of the false discovery rate. Returning to our examples you could imagine:\nExample 1\n\n\n\nExample 2\n\n\n\nExample 3\n\n\n\nWhere in the last case, we have parameterized the relationship between FDR and time with a flexible model like cubic splines.\nThe hard problem is fitting the regression models in Examples 1-3. Here I propose a basic estimator of the FDR regression model and leave it to others to be smart about it. Let’s focus on P-values because they are the easiest to deal with. Suppose that we calculate the random variables \\(G(\\lambda)\\) is the empirical distribution function for the P-values under the alternative hypothesis. This may be a mixture distribution. If we assume reasonably powered tests and that \\(\\lambda\\) is large enough, then . So\n\n\n\nOne obvious choice is then to try to model\n\n\n\nWe could, for example use the model:\n\n\n\nwhere  is a linear model or spline, etc. Then we get the fitted values and calculate:\n\n\n\nHere is a little simulated example where the goal is to estimate the probability of being a false positive as a smooth function of time.\n## Load libraries\n\nlibrary(splines)\n## Define the number of tests\nset.seed(1345)\nntest <- 1000\n\n## Set up the time vector and the probability of being null\ntme <- seq(-2,2,length=ntest)\npi0 <- pnorm(tme)\n\n## Calculate a random variable indicating whether to draw\n## the p-values from the null or alternative\nnullI <- rbinom(ntest,prob=pi0,size=1)> 0\n\n## Sample the null P-values from U(0,1) and the alternatives\n## from a beta distribution\n\npValues <- rep(NA,ntest)\npValues[nullI] <- runif(sum(nullI))\npValues[!nullI] <- rbeta(sum(!nullI),1,50)\n\n## Set lambda and calculate the estimate\n\nlambda <- 0.8\ny <- pValues > lambda\nglm1 <- glm(y ~ ns(tme,df=3))\n\n## Get the estimate pi0 values\npi0hat <- glm1$fitted/(1-lambda)\n\n## Plot the real versus fitted probabilities\n\nplot(pi0,pi0hat,col=\"blue\",type=\"l\",lwd=3,xlab=\"Real pi0\",ylab=\"Fitted pi0\")\nabline(c(0,1),col=\"grey\",lwd=3)\n\nThe result is this plot:\n\nReal versus estimated false discovery rate when calling all tests significant.\nThis estimate is obviously not guaranteed to estimate the FDR well, the operating characteristics both theoretically and empirically need to be evaluated and the other examples need to be fleshed out. But isn’t the idea of FDR regression cool?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-06-12-personalized-medicine-is-primarily-a-population-health-intervention/",
    "title": "Personalized medicine is primarily a population-health intervention",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-06-12",
    "categories": [],
    "contents": "\nThere has been a lot of discussion of personalized medicine, individualized health, and precision medicine in the news and in the medical research community. Despite this recent attention, it is clear that healthcare has always been personalized to some extent. For example, men are rarely pregnant and heart attacks occur more often among older patients. In these cases, easily collected variables such as sex and age, can be used to predict health outcomes and therefore used to “personalize” healthcare for those individuals.\nSo why the recent excitement around personalized medicine? The reason is that it is increasingly cheap and easy to collect more precise measurements about patients that might be able to predict their health outcomes. An example that has recently been in the news is the measurement of mutations in the BRCA genes. Angelina Jolie made the decision to undergo a prophylactic double mastectomy based on her family history of breast cancer and measurements of mutations in her BRCA genes. Based on these measurements, previous studies had suggested she might have a lifetime risk as high as 80% of developing breast cancer.\nThis kind of scenario will become increasingly common as newer and more accurate genomic screening and predictive tests are used in medical practice. When I read these stories there are two points I think of that sometimes get obscured by the obviously fraught emotional, physical, and economic considerations involved with making decisions on the basis of new measurement technologies:\nIn individualized health/personalized medicine the “treatment” is information about risk. In some cases treatment will be personalized based on assays. But in many other cases, we still do not (and likely will not) have perfect predictors of therapeutic response. In those cases, the healthcare will be “personalized” in the sense that the patient will get more precise estimates of their likelihood of survival, recurrence etc. This means that patients and physicians will increasingly need to think about/make decisions with/act on information about risks. But communicating and acting on risk is a notoriously challenging problem; personalized medicine will dramatically raise the importance of understanding uncertainty.\nIndividualized health/personalized medicine is a population-level treatment. Assuming that the 80% lifetime risk estimate was correct for Angelina Jolie, it still means there is a 1 in 5 chance she was never going to develop breast cancer. If that had been her case, then the surgery was unnecessary. So while her decision was based on personal information, there is still uncertainty in that decision for her. So the “personal” decision may not always be the “best” decision for any specific individual. It may however, be the best thing to do for everyone in a population with the same characteristics.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-06-11-why-not-have-a-future-of-the-field-session-at-a-conference-with-only-young-speakers/",
    "title": "Why not have a \"future of the field\" session at a conference with only young speakers?",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-06-11",
    "categories": [],
    "contents": "\nI’m in the process of trying to get together a couple of sessions to submit to ENAR 2014. I’m pretty psyched about the topics and am looking forward to hosting the conference in Baltimore. It is pretty awesome to have one of the bigger stats conferences on our home turf and we are going to try to be well represented at the conference.\nWhile putting the sessions together I’ve been thinking about what are my favorite characteristics of sessions at stats conferences. Alyssa has a few suggestions for speakers which I’m completely in agreement with, but I’m talking about whole sessions. Since statistics is often concerned primarily with precision/accuracy the talks tend to be a little bit technical and sometimes dry. Even on topics I really am excited about, people try not to exaggerate. I think overall this is a great quality, but I’d I’m in the process of trying to get together a couple of sessions to submit to ENAR 2014. I’m pretty psyched about the topics and am looking forward to hosting the conference in Baltimore. It is pretty awesome to have one of the bigger stats conferences on our home turf and we are going to try to be well represented at the conference. at a conference. I realized that one of my favorite kind of sessions is the “future of statistics” session.\nMy only problem is that future of the field talks are always given by luminaries who have a lot of experience. This isn’t surprising, since (1) they are famous and their names are a big draw, (2) they have made lots of interesting/unique contributions, and (3) they are established so they don’t have to worry about being a little imprecise.\nBut I’d love to see a “future of the field” session with only people who are students/postdocs/first year assistant professors. These are the people who will really be the future of the field and are often more on top of new trends. It would be so cool to see four or five of the most creative young people in the field making bold predictions about where we will go as a discipline. Then you could have one senior person discuss the talks and give some perspective on how realistic the visions would be in light of past experience.\nTell me that wouldn’t be an awesome conference session.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-06-02-sunday-datastatistics-link-roundup-6213/",
    "title": "Sunday data/statistics link roundup (6/2/13)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-06-02",
    "categories": [],
    "contents": "\nAwesome, a GUI for d3 graphs. Via John M.\nTom L. on why statistics matter, especially at the Census!\nI’ve been spending the last several weeks house hunting like crazy, so the idea of data on schools is high on my mind right now. So this link to data on geography of [ 1. Awesome, a GUI for d3 graphs. Via John M.\nTom L. on why statistics matter, especially at the Census!\nI’ve been spending the last several weeks house hunting like crazy, so the idea of data on schools is high on my mind right now. So this link to data on geography of](http://greatergreatereducation.org/post/18992/osse-releases-more-school-data-on-students-neighborhoods/?utm_source=feedly) seemed particularly interesting (via Rafa).\nA student dramatically reduces the cost of the self-driving car. The big technological breakthrough? Sampling! (via Marginal Revolution).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-05-29-what-statistics-should-do-about-big-data-problem-forward-not-solution-backward/",
    "title": "What statistics should do about big data: problem forward not solution backward",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-05-29",
    "categories": [],
    "contents": "\nThere has been a lot of discussion among statisticians about big data and what statistics should do to get involved. Recently Steve M. and Larry W. took up the same issue on their blog. I have been thinking about this for a while, since I work in genomics, which almost always comes with “big data”. It is also one area of big data where statistics and statisticians have played a huge role.\nA question that naturally arises is, “why have statisticians been so successful in genomics?” I think a major reason is the phrase I borrowed from Brian C. (who may have borrowed it from Ron B.)\n\nproblem first, not solution backward\n\nOne of the reasons that “big data” is even a term is that there is that data are less expensive than they were a few years ago. One example is the dramatic drop in the price of DNA-sequencing. But there are many many more examples. The quantified self movement and Fitbits, Google Books, social network data from Twitter, etc. are all areas where data that cost us a huge amount to collect 10 years ago can now be collected and stored very cheaply.\nAs statisticians we look for generalizable principles; I would say that you have to zoom pretty far out to generalize from social networks to genomics but here are two:\nThe data can’t be easily analyzed in an R session on a simple laptop (say low Gigs to Terabytes)\nThe data are generally quirky and messy (unstructured text, json files with lots of missing data, fastq files with quality metrics, etc.)\nSo how does one end up at the “leading edge” of big data? By being willing to deal with the schlep and work out the knitty gritty of how you apply even standard methods to data sets where taking the mean takes hours. Or taking the time to learn all the kinks that are specific to say, how does one process a microarray, and then taking the time to fix them. This is why statisticians were so successful in genomics, they focused on the practical problems and this gave them access to data no one else had/could use properly.\nDoing these things requires a lot of effort that isn’t elegant. It also isn’t “statistics” by the definition that only mathematical methodology is statistics. Steve alludes to this in his post when he says:\n\nFrankly I am a little disappointed that there does not seem to be any really compelling new idea (e.g. as in neural nets or the kernel embedding idea that drove machine learning).\n\nI think this is a view shared by many statisticians. That since there isn’t a new elegant theory yet, there aren’t “new ideas” in big data. That focus is solution backward. We want an elegant theory that we can then apply to specific problems if they happen to come up.\nThe alternative is problem forward. The fact that we can collect data so cheaply means we can measure and study things we never could before. Computer scientists, physicists, genome biologists, and others are leading in big data precisely because they aren’t thinking about the statistical solution. They are thinking about solving an important scientific problem and are willing to deal with all the dirty details to get there. This allows them to work on data sets and problems that haven’t been considered by other people.\nIn genomics, this has happened before. In that case, the invention of microarrays revolutionized the field and statisticians jumped on board, working closely with scientists, handling the dirty details, and building software so others could too. As a discipline if we want to be part of the “big data” revolution I think we need to focus on the scientific problems and let methodology come second. That requires a rethinking of what it means to be statistics. Things like parallel computing, data munging, reproducibility, and software development have to be accepted as equally important to methods development.\nThe good news is that there is plenty of room for statisticians to bring our unique skills in dealing with uncertainty to these new problems; but we will only get a seat at the table if we are willing to deal with the mess that comes with doing real science.\nI’ll close by listing a few things I’d love to see:\nA Bioconductor-like project for social network data. Tyler M. and Ali S. have a paper that would make for an awesome package for this project. \nStatistical pre-processing for fMRI and other brain imaging data. Keep an eye on our smart group for that.\nData visualization for translational applications, dealing with all the niceties of human-data interfaces. See healthvis or the stuffy Miriah Meyer is doing.\nMost importantly, starting with specific, unsolved scientific problems. Seeking novel ways to collect cheap data, and analyzing them, even with known and straightforward statistical methods to deepen our understanding about ourselves or the universe.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-05-19-sunday-datastatistics-link-roundup-5192013/",
    "title": "Sunday data/statistics link roundup (5/19/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-05-19",
    "categories": [],
    "contents": "\nThis is a 1. This is a on 20th versus 21st century problems and the rise of the importance of empirical science. I particularly like the discussion of what it means to be a “solved” problem and how that has changed.\nA discussion in Science about the (arguably) most important statistics among academics, the impact factor and h-index. This comes on the heels of the San Francisco Declaration of Research Assessment. I like the idea that we should focus on evaluating science for its own merit rather than focusing on summaries like impact factor. But I worry that the “gaming” people are worried about with quantitative numbers like IF will be replaced with “politicking” if it becomes too qualitative. (via Rafa)\nA write-up about a survey  in Britain that suggests people don’t believe statistics (surprise!). I think this is symptomatic of a bigger issue which is being raised over and over. In the era when scientific problems don’t have deterministic solutions how do we determine if a problem has been solved? There is no good answer for this yet and it threatens to undermine a major fraction of the scientific enterprise going forward.\nBusinesses are confusing data analysis and big data. This is so important and true. Big data infrastructure is often critical for creating/running data products. But discovering new ideas from data often happens on much smaller data sets with good intuition and interactive data analysis.\nReally interesting article about how the baseball card numbering system matters and how changing it can upset collectors (via Chris V.).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-05-17-when-does-replication-reveal-fraud/",
    "title": "When does replication reveal fraud?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-05-17",
    "categories": [],
    "contents": "\nHere’s a little thought experiment for your weekend pleasure. Consider the following:\nJoe Scientist decides to conduct a study (call it Study A) to test the hypothesis that a parameter D > 0 vs. the null hypothesis that D = 0. He designs a study, collects some data, conducts an appropriate statistical analysis and concludes that D > 0. This result is published in the Journal of Awesome Results along with all the details of how the study was done.\nJane Scientist finds Joe’s study very interesting and tries to replicate his findings. She conducts a study (call it Study B) that is similar to Study A but completely independent of it (and does not communicate with Joe). In her analysis she does not find strong evidence that D > 0 and concludes that she cannot rule out the possibility that D = 0. She publishes her findings in the Journal of Null Results along with all the details.\nFrom these two studies, which of the following conclusions can we make?\nStudy A is obviously a fraud. If the truth were that D > 0, then Jane should have concluded that D > 0 in her independent replication.\nStudy B is obviously a fraud. If Study A were conducted properly, then Jane should have reached the same conclusion.\nNeither Study A nor Study B was a fraud, but the result for Study A was a Type I error, i.e. a false positive.\nNeither Study A nor Study B was a fraud, but the result for Study B was a Type II error, i.e a false negative.\nI realize that there are a number of subtle details concerning why things might happen but I’ve purposely left them out. My question is, based on the information that you actually have about the two studies, what would you consider to be the most likely case? What further information would you like to know beyond what was given here?_\n_\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-05-15-the-bright-future-of-applied-statistics/",
    "title": "The bright future of applied statistics",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-05-15",
    "categories": [],
    "contents": "\nIn 2013, the Committee of Presidents of Statistical Societies (COPSS) celebrates its 50th Anniversary. As part of its celebration, COPSS will publish a book, with contributions from past recipients of its awards, titled “Past, Present and Future of Statistical Science”. Below is my contribution titled The bright future of applied statistics.\nWhen I was asked to contribute to this issue, titled Past, Present, and Future of Statistical Science, I contemplated my career while deciding what to write about. One aspect that stood out was how much I benefited from the right circumstances. I came to one clear conclusion: it is a great time to be an applied statistician. I decided to describe the aspects of my career that I have thoroughly enjoyed in the past and present and explain why I this has led me to believe that the is bright for applied statisticians.\nI became an applied statistician while working with David Brillinger on my PhD thesis. When searching for an advisor I visited several professors and asked them about their interests. David asked me what I liked and all I came up with was “I don’t know. Music?”, to which he responded “That’s what we will work on”. Apart from the necessary theorems to get a PhD from the Statistics Department at Berkeley, my thesis summarized my collaborative work with researchers at the Center for New Music and Audio Technology. The work involved separating and parameterizing the harmonic and non-harmonic components of musical sound signals [5]. The sounds had been digitized into data. The work was indeed fun, but I also had my first glimpse into the incredible potential of statistics in a world becoming more and more data-driven.\nDespite having expertise only in music, and a thesis that required a CD player to hear the data, fitted models and residuals, I was hired by the Department of Biostatistics at Johns Hopkins School of Public Health. Later I realized what was probably obvious to the School’s leadership: that regardless of the subject matter of my thesis, my time series expertise could be applied to several public health applications [8, 2, 1]. The public health and biomedical challenges surrounding me were simply too hard to resist and my new department knew this. It was inevitable that I would quickly turn into an applied Biostatistician. \nSince the day that I arrived at Hopkins 15 years ago, Scott Zeger, the department chair, fostered and encouraged faculty to leverage their statistical expertise to make a difference and to have an immediate impact in science. At that time, we were in the midst of a measurement revolution that was transforming several scientific fields into data-driven ones. By being located in a School of Public Health and next to a medical school, we were surrounded by collaborators working in such fields. These included environmental science, neuroscience, cancer biology, genetics, and molecular biology. Much of my work was motivated by collaborations with biologists that, for the first time, were collecting large amounts of data. Biology was changing from a data poor discipline to a data intensive ones.\nA specific example came from the measurement of gene expression. Gene expression is the process where DNA, the blueprint for life, is copied into RNA, the templates for the synthesis of proteins, the building blocks for life. Before microarrays were invented in the 1990s, the analysis of gene expression data amounted to spotting black dots on a piece of paper (see Figure 1A below). With microarrays, this suddenly changed to sifting through tens of thousands of numbers (see Figure 1B). Biologists went from using their eyes to categorize results to having thousands (and now millions) of measurements per sample to analyze. Furthermore, unlike genomic DNA, which is static, gene expression is a dynamic quantity: different tissues express different genes at different levels and at different times. The complexity was exacerbated by unpolished technologies that made measurements much noisier than anticipated. This complexity and level of variability made statistical thinking an important aspect of the analysis. The Biologists that used to say, “if I need statistics, the experiment went wrong” were now seeking out our help. The results of these collaborations have led to, among other things, the development of breast cancer recurrence gene expression assays making it possible to identify patients at risk of distant recurrence following surgery [9].\n\n\n\n\n\nFigure 1: Illustration of gene expression data before and after micorarrays.\n\n\nWhen biologists at Hopkins first came to our department for help with their  microarray data, Scott put them in touch with me because I had experience with (what was then) large datasets (digitized music signals are represented by 44,100 points per second). The more I learned about the scientific problems and the more data I explored, the more motivated I became. The potential for statisticians having an impact in this nascent field was clear and my department was encouraging me to take the plunge. This institutional encouragement and support was crucial as successfully working in this field made it harder to publish in the mainstream statistical journals; an accomplishment that had traditionally been heavily weighted in the promotion process. The message was clear: having an immediate impact on specific scientific fields would be rewarded as much as mathematically rigorous methods with general applicability.\nAs with my thesis applications, it was clear that to solve some of the challenges posed by microarray data I would have to learn all about the technology. For this I organized a sabbatical with Terry Speed’s group in Melbourne where they helped me accomplish this goal. During this visit I reaffirmed my preference for attacking applied problems with simple statistical methods, as opposed to overcomplicated ones or developing new techniques. Learning that deciphering clever ways of putting the existing statistical toolbox to work was good enough for an accomplished statistician like Terry gave me the necessary confidence to continue working this way. More than a decade later this continues to be my approach to applied statistics. This approach has been instrumental for some of my current collaborative work. In particular, it led to important new biological discoveries made together with Andy Feinberg’s lab [7].\nDuring my sabbatical we developed preliminary solutions that improved precision and aided in the removal of systematic biases for microarray data [6]. I was aware that hundreds, if not thousands, of other scientists were facing the same problematic data and were searching for solutions. Therefore I was also thinking hard about ways in which I could share whatever solutions I developed with others. During this time I received an email from Robert Gentleman asking if I was interested in joining a new software project for the delivery of statistical methods for genomics data. This new collaboration eventually became the Bioconductor project, which to this day continues to grow its user and developer base [4]. Bioconductor was the perfect vehicle for having the impact that my department had encouraged me to seek. With Ben Bolstad and others we wrote an R package that has been downloaded tens of thousands of times [3]. Without the availability of software, the statistical method would not have received nearly as much attention. This lesson served me well throughout my career, as developing software packages has greatly helped disseminate my statistical ideas. The fact that my department and school rewarded software publications provided important support.\nThe impact statisticians have had in genomics is just one example of our fields accomplishment in the 21st century. In academia, the number of statistician becoming leaders in fields like environmental sciences, human genetics, genomics, and social sciences continues to grow. Outside of academia, Sabermetrics has become a standard approach in several sports (not just baseball) and inspired the Hollywood movie Money Ball. A PhD Statistician led the team that won the Netflix million dollar prize. Nate Silver proved the pundits wrong by once again using statistical models to predict election results almost perfectly. R has become a widely used programming language. It is no surprise that Statistics majors at Harvard have more than quadrupled since 2000 and that statistics MOOCs are among the most popular.\nThe unprecedented advance in digital technology during the second half of the 20th century has produced a measurement revolution that is transforming science. Scientific fields that have traditionally relied upon simple data analysis techniques have been turned on their heads by these technologies. Furthermore, advances such as these have brought about a shift from hypothesis to discovery-driven research. However, interpreting information extracted from these massive and complex datasets requires sophisticated statistical skills as one can easily be fooled by patterns that arise by chance. This has greatly elevated the importance of our discipline in biomedical research. \nI think that the data revolution is just getting started. Datasets are currently being, or have already been, collected that contain, hidden in their complexity, important truths waiting to be discovered. These discoveries will increase the scientific understanding of our world. Statisticians should be excited and ready to play an important role in the new scientific renaissance driven by the measurement revolution.\n\nBibliography\n\n\n\n[1]   NE Crone, L Hao, J Hart, D Boatman, RP Lesser, R Irizarry, and B Gordon. Electrocorticographic gamma activity during word production in spoken and sign language. Neurology, 57(11):2045–2053, 2001.\n\n\n[2]   Janet A DiPietro, Rafael A Irizarry, Melissa Hawkins, Kathleen A Costigan, and Eva K Pressman. Cross-correlation of fetal cardiac and somatic activity as an indicator of antenatal neural development. American journal of obstetrics and gynecology, 185(6):1421–1428, 2001.\n\n\n[3]   Laurent Gautier, Leslie Cope, Benjamin M Bolstad, and Rafael A Irizarry. affyanalysis of affymetrix genechip data at the probe level. Bioinformatics, 20(3):307–315, 2004.\n\n\n[4]   Robert C Gentleman, Vincent J Carey, Douglas M Bates, Ben Bolstad, Marcel Dettling, Sandrine Dudoit, Byron Ellis, Laurent Gautier, Yongchao Ge, Jeff Gentry, et al. Bioconductor: open software development for computational biology and bioinformatics. Genome biology, 5(10):R80, 2004.\n\n\n[5]   Rafael A Irizarry. Local harmonic estimation in musical sound signals. Journal of the American Statistical Association, 96(454):357–367, 2001.\n\n\n[6]   Rafael A Irizarry, Bridget Hobbs, Francois Collin, Yasmin D Beazer-Barclay, Kristen J Antonellis, Uwe Scherf, and Terence P Speed. Exploration, normalization, and summaries of high density oligonucleotide array probe level data. Biostatistics, 4(2):249–264, 2003.\n\n\n[7]   Rafael A Irizarry, Christine Ladd-Acosta, Bo Wen, Zhijin Wu, Carolina Montano, Patrick Onyango, Hengmi Cui, Kevin Gabo, Michael Rongione, Maree Webster, et al. The human colon cancer methylome shows similar hypo-and hypermethylation at conserved tissue-specific cpg island shores. Nature genetics, 41(2):178–186, 2009.\n\n\n[8]   Rafael A Irizarry, Clarke Tankersley, Robert Frank, and Susan Flanders. Assessing homeostasis through circadian patterns. Biometrics, 57(4):1228–1237, 2001.\n\n\n[9]   Laura J van’t Veer, Hongyue Dai, Marc J Van De Vijver, Yudong D He, Augustinus AM Hart, Mao Mao, Hans L Peterse, Karin van der Kooy, Matthew J Marton, Anke T Witteveen, et al. Gene expression profiling predicts clinical outcome of breast cancer. nature, 415(6871):530–536, 2002.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-05-12-sunday-datastatistics-link-roundup-5122013-mothers-day/",
    "title": "Sunday data/statistics link roundup (5/12/2013, Mother's Day!)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-05-12",
    "categories": [],
    "contents": "\nA tutorial on deep-learning, I really enjoyed reading it, but I’m still trying to figure out how this is different than non-linear logistic regression to estimate features then supervised prediction using those features? Or maybe I’m just naive….\nRafa on political autonomy for science for a blog in PR called 80 grados.  He writes about Rep. Lamar Smith and then focuses more closely on issues related to the University of Puerto Rico. A very nice read. (via Rafa)\nHighest paid employees by state. I should have coached football…\nNewton took the mean. It warms my empirical heart to hear about how the theoretical result was backed up by averaging (via David S.)\nReinhart and Rogoff publish a correction but stand by their original claims. I’m not sure whether this is a good or a bad thing. But it definitely is an overall win for reproducibility.\nStatesy folks are getting some much-deserved attention. Terry Speed is a Fellow of the Royal Society, Peter Hall is a foreign associate of the NAS, Gareth Roberts is also a Fellow of the Royal Society (via Peter H.)\nStatisticians go to the movies and the hot hand analysis makes the NY Times (via Dan S.)\nBonus Link!  Karl B.’s Github tutorial is awesome and every statistician should be required to read it. I only ask why he gives all the love to Nacho’s admittedly awesome Clickme package and no love to healthvis, we are on Github too!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-05-08-a-shiny-web-app-to-find-out-how-much-medical-procedures-cost-in-your-state/",
    "title": "A Shiny web app to find out how much medical procedures cost in your state.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-05-08",
    "categories": [],
    "contents": "\nToday the front page of the Huffington Post featured the Today the [front page of the Huffington Post featured](http://www.huffingtonpost.com/2013/05/08/hospital-prices-cost-differences_n_3232678.html) the that shows the cost of many popular procedures broken down by hospital. We here at Simply Statistics think you should be able to explore these data more easily. So we asked Today the [front page of the Huffington Post featured](http://www.huffingtonpost.com/2013/05/08/hospital-prices-cost-differences_n_3232678.html) the [Today the [front page of the Huffington Post featured](http://www.huffingtonpost.com/2013/05/08/hospital-prices-cost-differences_n_3232678.html) the](https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/index.html) that shows the cost of many popular procedures broken down by hospital. We here at Simply Statistics think you should be able to explore these data more easily. So we asked to help us build a Shiny App that allows you to interact with these data. You can choose your state and your procedure and see how much the procedure costs at hospitals in your state. It takes a second to load because it is a lot of data….\nHere is the link the app. \nHere are some screenshots for intracranial hemmhorage for the US and for Idaho.\n\nThe R code is here if you want to tweak/modify.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-05-06-talking-about-moocs-on-mpt-direct-connection/",
    "title": "Talking about MOOCs on MPT Direct Connection",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-05-06",
    "categories": [],
    "contents": "\n\nWatch Monday, April 29, 2013 on PBS. See more from Direct Connection.\n\nI appeared on Maryland Public Television’s Direct Connection with Jeff Salkin last Monday to talk about MOOCs (along with our Dean Mike Klag).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-05-06-why-the-current-over-pessimism-about-science-is-the-perfect-confirmation-bias-vehicle-and-we-should-proceed-rationally/",
    "title": "Why the current over-pessimism about science is the perfect confirmation bias vehicle and we should proceed rationally",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-05-06",
    "categories": [],
    "contents": "\nRecently there have been some high profile flameouts in scientific research. A couple examples include the Duke saga, the replication issues in social sciences, p-value hacking, fabricated data, not enough open-access publication, and on and on.\nSome of these results have had major non-scientific consequences, which is the reason they have drawn so much attention both inside and outside of the academic community. For example, the Duke saga Recently there have been some high profile flameouts in scientific research. A couple examples include [the Duke saga](http://simplystatistics.org/2012/02/27/the-duke-saga-starter-set/), [the replication issues in social sciences](http://simplystatistics.org/2012/07/03/replication-and-validation-in-omics-studies-just-as/), [p-value hacking](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1850704), [fabricated data](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2114571&http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2114571), [not enough open-access publication](http://www.michaeleisen.org/blog/?p=1312), and on and on. , the lack of replication has led to high-profile arguments between scientists in Discover and Nature among other outlets, and the Recently there have been some high profile flameouts in scientific research. A couple examples include [the Duke saga](http://simplystatistics.org/2012/02/27/the-duke-saga-starter-set/), [the replication issues in social sciences](http://simplystatistics.org/2012/07/03/replication-and-validation-in-omics-studies-just-as/), [p-value hacking](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1850704), [fabricated data](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2114571&http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2114571), [not enough open-access publication](http://www.michaeleisen.org/blog/?p=1312), and on and on. (sometimes comically) because of a lack of reproducibility.\nThe result of this high-profile attention is that there is a movement on to “clean up science”. As has been pointed out, there is a group of scientists who are making names for themselves primarily as critics of what is wrong with the scientific process. The good news is that these key players are calling attention to issues: reproducibility, replicability, and open access, among others, that are critically important for the scientific enterprise.\nI too am concerned about these issues and have altered my own research process to try to address them for my own research group.  I also think that the solutions others have proposed on a larger scale like alltrials.net or PLoS are great advances for the scientific community.\nI am also very worried that people are using a few high-profile cases to hyperventilate about the real, solvable, and recognized problems in the scientific process These people get credit and a lot of attention for pointing out how science is “failing”. But they aren’t giving proportional time to all of the incredible success stories we have had, both in performing research and in reforming research with reproducibility, open access, and replication initiatives.\nWe should recognize that science is hard and even dedicated, diligent, and honest scientists will make mistakes , perform irreproducible or irreplicable studies, or publish in closed access journals.  Sometimes this is because of ignorance of good research principles, sometimes it is because people are new to working in a world where data/computation are a major player, and some will be because it is legitimately, really hard to make real advances in science. I think people who participate in real science recognize these problems and are eager to solve them. I also have noticed that real scientists generally try to propose a solution when they complain about these issues.\nBut it seems like sometimes people use these high-profile mistakes out of context to push their own scientific pet peeves. For example:\nI don’t like p-values and there are lots of results that fail to replicate so it must be the fault of p-values.  Many studies fail to replicate not because the researchers used p-values, but because they performed studies that were either weak or had poorly understood scientific mechanisms.\nI don’t like not being able to access people’s code so lack of reproducibility is causing science to fail. Even in the two most infamous cases (Potti and Reinhart - Rogoff) the problem with the science wasn’t reproducibility - it was that the analysis was incorrect/flawed. Reproducibility compounded the problem but wasn’t the root cause of the problem.\nI don’t like not being able to access scientific papers so closed-access journals are evil. For whatever reason (I don’t know if I understand why) it is expensive to publish journals. Clearly, because publishing open access is expensive and closed access journals are expensive. If I’m a junior researcher, I’ll definitely post my preprints online, but I also want papers in “good” journals and don’t have a ton of grant money, so sometimes I’ll choose close access.\nI don’t like these crazy headlines from social psychology (substitute other field here) and there have been some that haven’t replicated, so none must replicate. Of course some papers won’t replicate, including even high profile papers. If you are doing statistics, then by definition some papers won’t replicate since you have to make a decision on noisy data.\nThese are just a few examples where I feel like a basic, fixable flaw in science has been used to justify a hugely pessimistic view of science in general. I’m not saying it is all rainbows and unicorns. Of course we want to improve the process. But I’m worried that the rational reasonable problems we have, with enough hyperbole, will make it look like the scientific process “sky is falling” and will leave the door open for individuals like Rep. Lamar Smith to come in and turn the scientific process into a political one.\nP.S. Andrew Gelman posted on a similar topic yesterday as well.. He argues the case for less optimism and to make sure we don’t stay complacent. He added a P.S. and mentioned two points on which we can agree: (1) science is hard and is a human system and we are working to fix the flaws inherent in such systems and (2) that it is still easier to publish as splashy claim than to publish a correction. I do definitely agree with both. I think Gelman would also likely agree that we need to be careful about reciprocity with these issues. If earnest scientists work hard to address reproducibility, replicability, open access, etc. then people who criticize them should have to work just as hard to justify their critiques. Just because it is a critique doesn’t mean it should automatically get the same treatment as the original paper.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:05:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-05-02-reproducibility-at-nature/",
    "title": "Reproducibility at Nature",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-05-02",
    "categories": [],
    "contents": "\nNature has jumped on to the reproducibility bandwagon and has announced a new approach to improving reproducibility of submitted papers. The new effort is focused primarily and methodology, including statistics, and in making sure that it is clear what an author has done.\n\nTo ease the interpretation and improve the reliability of published results we will more systematically ensure that key methodological details are reported, and we will give more space to methods sections. We will examine statistics more closely and encourage authors to be transparent, for example by including their raw data.\n\nTo this end they have created a checklist for highlighting key aspects that need to be clear in the manuscript. A number of these points are statistical, and two specifically highlight data deposition and computer code availability. I think an important change is the following:\n\nTo allow authors to describe their experimental design and methods in as much detail as necessary, the participating journals, including Nature, will abolish space restrictions on the methods section.\n\nI think this is particularly important because of the message it sends. Most journals have overall space limitations and some journals even have specific limits on the Methods section. This sends a clear message that “methods aren’t important, results are”. Removing space limits on the Methods section will allow people to just say what they actually did, rather than figure out some tortured way to summarize years of work into a smattering of key words.\nI think this is a great step forward by a leading journal. The next step will be for Nature to stick to it and make sure that authors live up to their end of the bargain.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-04-30-reproducibility-and-reciprocity/",
    "title": "Reproducibility and reciprocity",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-04-30",
    "categories": [],
    "contents": "\nOne element about the entire discussion about reproducible research that I haven’t seen talked about very much is the potential for the lack of reciprocity. I think even if scientists were not concerned about the possibility of getting scooped by others by making their data/code available this issue would be sufficient to give people pause about making their work reproducible.\nWhat do I mean by reciprocity? Consider the following (made up) scenario:\nI conduct a study (say, a randomized controlled trial, for concreteness) that I register at clinicaltrials.gov beforehand and specify details about the study like the design, purpose, and primary and secondary outcomes.\nI rigorously conduct the study, ensuring safety and privacy of subjects, collect the data, and analyze the data.\nI publish the results for the primary and secondary outcomes in the peer-reviewed literature where I describe how the study was conducted and the statistical methods that were used. For the sake of concreteness, let’s say the results were “significant” by whatever definition of significant you care to use and that the paper was highly influential.\nAlong with publishing the paper I make the analytic dataset and computer code available so that others can look at what I did and, if they want, reproduce the result.\nSo far so good right? It seems this would be a great result for any study. Now consider the following possible scenarios:\nSomeone obtains the data and the code from the web site where it is hosted, analyzes it, and then publishes a note claiming that the intervention negatively affected a different outcome not described in the original study (i.e. not one of the primary or secondary outcomes).\nA second person obtains the data, analyzes it, and then publishes a note on the web claiming that the intervention was ineffective for the primary outcome in a the subset of participants that were male.\nA third person obtains the data, analyzes the data, and then publishes a note on the web saying that the study is flawed and that the original results of the paper are incorrect. No code, data, or details of their methods are given.\nNow, how should one react to the follow-up note claiming the study was flawed? It’s easy to imagine a spectrum of possible responses ranging from accusations of fraud to staunch defenses of the original study. Because the original study was influential, there is likely to be a kerfuffle either way.\nBut what’s the problem with the three follow-up scenarios described? The one thing that they have in common is that none of the three responding people were subjected to the same standards to which the original investigator (me) was subjected. I was required to register my trial and state the outcomes in advance. In an ideal world you might argue I should have stated my hypotheses in advance too. That’s fine, but the point is that the people analyzing the data subsequently were not required to do any of this. Why should they be held to a lower standard of scrutiny?\nThe first person analyzed a different outcome that was not a primary or secondary outcome. How many outcomes did they test before the came to that one negatively significant one? The second person examined a subset of the participants. Was the study designed (or powered) to look at this subset? Probably not. The third person claims fraud, but does not provide any details of what they did.\nI think it’s easy to take care of the third person–just require that they make their work reproducible too. That way we can all see what they did and verify that there was in fact fraud. But the first two people are a little more difficult. If there are no barriers to obtaining the data, then they can just get the data and run a bunch of analyses. If the results don’t go their way, they can just move on and no one would be the wiser. If they did, they can try to publish something.\nWhat I think a good reproducibility policy should have is a type of “viral” clause. For example, the GNU General Public License (GPL) is an open source software license that requires, among other things, that anyone who writes their own software, but links to or integrates software covered under the GPL, must publish their software under the GPL too. This “viral” requirement ensures that people cannot make use of the efforts of the open source community without also giving back to that community. There have been numerous heated discussions in the software community regarding the pros and cons of such a clause, with (large) commercial software developers often coming down against it. Open source developers have largely beens skeptical of the arguments of large commercial developers, claiming that those companies simply want to “steal” open source software and/or maintain their dominance.\nI think it is important that if we are going to make reproducibility the norm in science, that we have analogous “viral” clauses to ensure that everyone is held to the same standard. This is particularly important in policy-relevant or in politically sensitive subject areas where there are often parties involved who have essentially no interest (and are in fact paid to have no interest) in holding themselves to the same standard of scientific conduct.\nRichard Stallman was right to assume that without the copyleft clause in the GPL that large commercial interests would simply usurp the work of the free software community and essentially crush it before it got started. Reproducibility needs its own version of copyleft or else scientists will be left to defend themselves against unscrupulous individuals who are not held to the same standard.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-04-28-sunday-datastatistics-link-roundup-4282013/",
    "title": "Sunday data/statistics link roundup (4/28/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-04-28",
    "categories": [],
    "contents": "\nWhat it feels like to be bad at math. My personal experience like this culminated in some difficulties with Green’s functions back in my early days at USU. I think almost everybody who does enough math eventually runs into a situation where they don’t understand what is going on and it stresses them out.\nAn article about companies that are using data to try to identify people for jobs (via Rafa).\nGoogle trends for predicting the market. I’m not sure that “predicting” is the right word here. I think a better word might be “explaining/associating”. I also wonder if this could go off the rails.\nThis article [ 1. What it feels like to be bad at math. My personal experience like this culminated in some difficulties with Green’s functions back in my early days at USU. I think almost everybody who does enough math eventually runs into a situation where they don’t understand what is going on and it stresses them out.\nAn article about companies that are using data to try to identify people for jobs (via Rafa).\nGoogle trends for predicting the market. I’m not sure that “predicting” is the right word here. I think a better word might be “explaining/associating”. I also wonder if this could go off the rails.\nThis article](http://www.r-bloggers.com/faster-higher-stonger-a-guide-to-speeding-up-r-code-for-busy-people/?utm_source=feedly&utm_medium=feed&utm_campaign=Feed:+RBloggers+(R+bloggers)) in terms of describing the ways that you can speed up R code. My favorite part of it is that it starts with the “why”. Exactly. Premature optimization is the root of all evil.\nA discussion of data science at Tumblr. The author/speaker also has a great blog.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-04-26-mindlessly-normalizing-genomics-data-is-bad-but-ignoring-unwanted-variability-can-be-worse/",
    "title": "Mindlessly normalizing genomics data is bad - but ignoring unwanted variability can be worse",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-04-26",
    "categories": [],
    "contents": "\nYesterday, and bleeding over into today, quantile normalization (QN) was being discussed on Twitter. This is the Yesterday, and bleeding over into today, [quantile normalization](http://www.ncbi.nlm.nih.gov/pubmed/12538238) (QN) was being discussed on Twitter. This is the that started the whole thing off. The conversation went a bunch of different directions and then this happened:\n\nwell, this happens all over bio-statistics - ie, naive use in seemingly undirected ways until you get a “good” pvalue. And then end\n\nSo Jeff and I felt it was important to respond - since we are biostatisticians that work in genomics. We felt a couple of points were worth making:\nMost statisticians we know, including us, know QN’s limitations and are always nervous about using QN. But with most datasets we see, unwanted variability is overwhelming  and we are left with no choice but to normalize in orde to extract anything useful from the data.  In fact, many times QN is not enough and we have to apply further transformations, e.g., to remove batch effects.\n2. We would be curious to know which biostatisticians were being referred to. We would like some examples, because most of the genomic statisticians we know work very closely with biologists to aid them in cleaning dirty data to help them find real sources of signal. Furthermore, we encourage biologists to validate their results. In many cases, quantile normalization (or other transforms) are critical to finding results that validate and there is a long literature (both biological and statistical) supporting the importance of appropriate normalization.\n3. Assuming the data that you get (sequences, probe intensities, etc.) from high-throughput tech = direct measurement of abundance is incorrect. Before worrying about QN (or other normalization) being an arbitrary transformation that distorts the data, keep in mind that what you want to measure has already been distorted by PCR, the imperfections of the microarray, scanner measurement error, image bleeding, cross hybridization or alignment artifacts, ozone effects, etc…\nTo go into a little more detail about the reasons that normalization may be important in many cases, so I have written a little more detail below with data if you are interested.\n\nMost, if not all, the high throughput data we have analyzed needs some kind of normalization. This applies to both microarrays and next-gen sequencing. To demonstrate why, below I include 5 boxplots of log intensities from 5 microarrays that were hybridized to the same RNA (technical replicates).\n\nSee the problem? If we took the data at face value we would conclude that there is a large (almost 2 fold) global change in expression when comparing, say, samples C and E. But they are technical replicates so the observed difference is not biologically driven. Discrepancies like these are the rule rather than the exception. Biologists seem to underestimate the amount of unwanted variability present in the data they produce. Look at enough data and you will quickly learn that, in most cases, unwanted experimental variability dwarfs the biological differences we are interested in discovering. Normalization is the statistical technique that saves biologists millions of dollars  a year by fixing this problem in silico rather than redoing the experiment.\nFor the data above you might be tempted to simply standardize the data by subtracting the median. But the problem is more complicated than that as shown in the plot below. This plot shows the log ratio (M) versus the average of the logs intensities (A) for two technical replicates in which 16 probes (red dots) have been “spiked-in” to have true fold changes of 2. The other ~20,000 probesets (blue streak) are supposed to be unchanged (M=0). See the curvature of the genes that are supposed to be at 0?  Taken at face value, thousands of the low expressed probes exhibit larger differential expression than the only 16 that are actually different. That’s a problem. And standardizing by the subtracting the median won’t fix it. Non-linear biases such as this one are also quite common.\nQN offers one solution to this problem  if you can assume that the true distribution of what you are measuring is roughly the same across samples. Briefly, QN forces each sample to have the same distribution. The after picture above is the result of QN. It removes the curvature but preserves most of the real differences.\nSo why should we be nervous? QN and other normalization techniques risk throwing the baby out with the bath water. What if there is a real global difference? If there is, and you use QN, you will miss it and you may introduce artifacts. But the assumptions are no secret and it’s up to the biologists to decide if they are reasonable. At the same time, we have to be very careful about interpreting large scale changes given that we see large scale changes when we know there are none. Other than cases were global differences are forced or simulated, I have yet to see a good example in which QN causes more harm than good. I’m sure there are some real data examples out there, so if you have one please share, as I would love to use it as an example in class.\nAlso note that statisticians (including me) are working hard at deciphering ways  to normalize without the need for such strong assumptions. Although in their first incarnation they were useless, current control probes/transcripts techniques are promising. We have used them in the past to normalize methylation data (a similar approach was used here for gene expression data). And then there is subset quantile normalization. I am sure there are others and more to come. So Biologists, don’t worry, we have your backs and serve at your pleasure. In the meantime don’t be so afraid of QN: at least give it a try before you knock it.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-04-23-interview-at-yale-center-for-environmental-law-policy/",
    "title": "Interview at Yale Center for Environmental Law &amp; Policy",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-04-23",
    "categories": [],
    "contents": "\nInterview with Roger Peng from YCELP on Vimeo.\nA few weeks ago I sat down with Angel Hsu of the Yale Center for Environmental Law and Policy to talk about some of their work on air pollution indicators.\n(Note: I haven’t moved–I still work at the John_s_ Hopkins School of Public Health.)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-04-21-nevins-potti-reinhart-rogoff/",
    "title": "Nevins-Potti, Reinhart-Rogoff",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-04-21",
    "categories": [],
    "contents": "\nThere’s an interesting parallel between the Nevins-Potti debacle (a true debacle, in my mind) and the recent Reinhart-Rogoff kerfuffle. Both were exposed via some essentially small detail that had nothing to do with the real problem.\nIn the case of Reinhart-Rogoff, the Excel error was what made them look ridiculous, but it was in fact the “unconventional weighting” of the data that had the most dramatic effect. Furthermore, ever since the paper had come out, academic economists were debating and challenging its conclusions from the get go. Even when legitimate scientific concerns were raised, policy-makers and other academics were not convinced. As soon as the Excel error was revealed, everything needed to be re-examined.\nIn the Nevins-Potti debacle, Baggerly and Coombes wrote article after article pointing out all the problems and, for the most part, no one in a position of power really cared. The Nevins-Potti errors were real zingers too, not some trivial Excel error (i.e. switching the labels between people with disease and people without disease). But in the end, it took Potti’s claim of being a Rhodes Scholar to bring him down. Clearly, the years of academic debate beforehand were meaningless compared to lying on a CV.\nIn the Reinhart-Rogoff case, reproducibility was an issue and if the data had been made available earlier, the problems would have been discovered earlier and perhaps that would have headed off years of academic debate (for better or for worse). In the Nevins-Potti example, reproducibility was not an issue–the original Nature Medicine study was done using public data and so was reproducible (although it would have been easier if code had been made available). The problem there is that no one listened.\nOne has to wonder if the academic system is working in this regard. In both cases, it took a minor, but _personal _failing, to bring down the entire edifice. But the protestations of reputable academics, challenging the research on the merits, were ignored. I’d say in both cases the original research conveniently said what people wanted to hear (debt slows growth, personalized gene signatures can predict response to chemotherapy), and so no amount of research would convince people to question the original findings.\nOne also has to wonder whether reproducibility is of any help here. I certainly don’t think it hurts, but in the case of Nevins-Potti, where the errors were shockingly obvious to anyone paying attention, the problems were deemed merely technical (i.e. statistical). The truth is, reproducibility will be most necessary in highly technical and complex analyses where it’s often not obvious how an analysis is done. If you can show a flaw in an analysis that is complicated, what’s the use if your work will be written off as merely concerned with technical details (as if those weren’t important)? Most of the news articles surrounding Reinhart-Rogoff characterized the problems as complex and statistical (i.e. not important) and not concerned with fundamental questions of interest.\nIn both cases, I think science was used to push an external agenda, and when the science was called into question, it was difficult to back down. I’ll write more in a future post about these kinds of situations and what, if anything, we can do to improve matters.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-04-19-podcast-7-reinhart-rogoff-reproducibility/",
    "title": "Podcast #7: Reinhart, Rogoff, Reproducibility",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-04-19",
    "categories": [],
    "contents": "\nJeff and I talk about the recent Reinhart-Rogoff reproducibility kerfuffle and how it turns out that data analysis is really hard no matter how big the dataset.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-04-16-i-wish-economists-made-better-plots/",
    "title": "I wish economists made better plots",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-04-16",
    "categories": [],
    "contents": "\nI’m seeing lots of traffic on a big-time economics article by that failed to reproduce and here are my quick thoughts. You can read a pretty good summary here by Mike Konczal.\nQuick background: Carmen Reinhart and Kenneth Rogoff wrote an influential paper that was used by many to justify the need for austerity measures taken by governments to reduce debts relative to GDP. Yesterday, Thomas Herndon, Michael Ash, and Robert Pollin (HAP) released a paper where they reproduced the Reinhart-Rogoff (RR) analysis and noted a few irregularities or errors. In their abstract, HAP claim that they “find that coding errors, selective exclusion of available data, and unconventional weighting of summary statistics [in the RR analysis] lead to serious errors that inaccurately represent the relationship between public debt and GDP growth among 20 advanced economies in the post-war period.\nIt appears there were three points made by HAP: (1) RR excluded some important data from their final analysis; (2) RR weighted countries in a manner that was not proportional to the number of years they contributed to the dataset (RR used equal weighting of countries); and (3) there was an error in RR’s Excel formula which resulted in them inadvertently leaving out five countries from their final analysis.\nThe bottom line is shown in HAP’s Figure 1, which I reproduce below (on the basis of fair use):\n\nFrom the plot you can see that the HAP’s adjusted analysis (circles) more or less coincides with RR’s analysis (diamonds) except for the last categories of countries with debt/GDP ratios over 90%. In that category RR’s analysis shows a large drop in growth whereas HAP’s analysis shows a more or less smooth decline (but still positive growth).\nTo me, it seems that the incorrect Excel formula is a real error, but easily fixed. It also seemed to have the least impact on the final analysis. The other two problems, which had far bigger impacts, might have some explanation that I’m not aware of. I am not an economist so I await others to weigh in. RR apparently do not comment on the exclusion of certain data points or on the weighting scheme so it’s difficult to say what the thinking was, whether it was inadvertent or purposeful.\nIn summary, so what? Here’s what I think:\nIs there some fishiness? Sure, but this is not the Potti-Nevins scandal a la economics. I suppose it’s possible RR manipulated the analysis to get the answer austerity hawks were looking for, but we don’t have the evidence yet and this just doesn’t feel like that kind of thing.\nWhat’s the counterfactual? Or, what would have happened if the analysis had been done the way HAP propose? Would the world have embraced pro-growth policies by taking on a greater debt burden? My guess is no. Austerity hawks would have found some other study that supported their claims (and in fact there was at least one other).\nRR’s original analysis did not contain a plot like Figure 1 in HAP’s analysis, which I personally find very illuminating. From HAP’s figure, you can see that there’s quite a bit of variation across countries and perhaps an overall downward trend. I’m not sure I would have dramatically changed my conclusion if I had done the HAP analysis instead of the RR analysis. My point is that plots like this, which show the variability, are very important._\n_ 4. People see what they want to see. I would not be surprised to see some claim that HAP’s analysis supports the austerity conclusion because growth under high debt loads is much lower (almost 50%!) than under low debt loads. 5. If RR’s analysis had been correct, should they have even made the conclusions they made? RR indicated that there was a “threshold” at 90% debt/GDP. My experience is that statements about thresholds, are generally very hard to make, even with good data. I wonder what other more knowledgable people think of the original conclusions. 6. If the data had been made available sooner, this problem would have been fixed sooner. But in my opinion, that’s all that would have happened.\nThe vibe on the Internets seems to be that if only this problem had been identified sooner, the world would be a better place. But my cynical mind says, uh, no. You can toss this incident in the very large bucket of papers with some technical errors that are easily fixed. Thankfully, someone found these errors and fixed them, and that’s a good thing. Science moves on.\nUPDATE: Reinhart-Rogoff respond.\nUPDATE 2: Reinhart-Rogoff more detailed response.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-04-15-data-science-only-poses-a-threat-to-biostatistics-if-we-dont-adapt/",
    "title": "Data science only poses a threat to (bio)statistics if we don't adapt",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-04-15",
    "categories": [],
    "contents": "\nWe have previously mentioned on this blog how statistics needs better marketing. Recently, Karl B. has suggested that “Data science is statistics” and Larry W. has wondered if “Data science is the end of statistics?” I think there are a couple of types of data science and that each has a different relationship to the discipline of academic statistics:\nData science as marketing tool. Data analytics, data science, big data, etc. are terms that companies who already did something (IT infrastructure, consulting, database management, etc.) throw around to make them sound like they are doing the latest and greatest thing. These marketers are dabblers in what I would call the real “science of data” or maybe deal with just one part of the data pipeline. I think they pose no threat to the statistics community other than by generating backlash by over promising on the potential of data science or diluting the term to the point of being almost non-sensical.\nData science as business analytics. Another common use of “data science” is to describe the exact same set of activities that use to be performed by business analytics people, maybe allowing for some growth in the size of the data sets. This might be a threat to folks who do statistics in business schools - although more likely it will be beneficial to those programs as there is growth in the need for business-oriented statisticians.\nData science as big data engineer Sometimes data science refers to people who do stuff with huge amounts of data. Larry refers to this in his post when he talks about people working on billions of data points. Most classically trained statisticians aren’t comfortable with data of this size. But at places like Google - where big data sets are routine - the infrastructure is built so that statisticians can access and compress the parts of the data that they need to do their jobs. I don’t think this is necessarily a threat to statistics; but we should definitely be integrating data access into our curriculum.\nData science as replacement for statistics Some people (and I think it is the minority) are exactly referring to things that statisticians do when they talk about data science. This means manipulating, collecting, and analyzing data, then making inferences to a population or predictions about what will happen next. This is, of course, a threat to statisticians. Some places, like NC State and Columbia, are tackling this by developing centers/institutes/programs with data science in the name. But I think that is a little dangerous. The data don’t matter - it is the problem you can solve with the data. So the key thing is that these institutes need to focus on solving real problems - not just churning out people who know a little R, a little SQL, and a little Python.\nSo why is #4 happening? I think one reason is reputation. Larry mentions that a statistician produces an estimate and a confidence interval and maybe the confidence interval is too wide. I think he is on to something there, but I think it is a bigger problem. As Roger has pointed out - statisticians often see themselves as referees - rather scientists/business people. So a lot of people have the experience of going to a statistician and feel like they have been criticized for bad experimental design, too small a sample size, etc. These issues are hugely important - but sometimes you have to make due with what you have. I think data scientists in category 4 are taking advantage of a cultural tendency of statisticians to avoid making concrete decisions.\nA second reason is that some statisticians have avoided getting their hands dirty. “Hands clean” statisticians don’t  get the data from the database, or worry about the data munging, or match identifiers, etc. They wait until the data are nicely formated in a matrix to apply their methods. To stay competitive, we need to produce more “hands dirty” statisticians who are willing to go beyond schlep blindness and handle all aspects of a data analysis. In academia, we can encourage this by incorporating more of those issues into our curriculum.\nFinally, I think statisticians focus on optimality hurts us. Our field grew up in an era where data was sparse and we had to squeeze every last ounce of information out what little data we had. Those constraints led to a cultural focus on optimality to a degree that is no longer necessary when data are abundant. In fact, an abundance of data is often unreasonably effective even with suboptimal methods. “Data scientists” understand this and shoot for the 80% solution that is good enough in most cases.\nIn summary I don’t think statistics will be killed off by data science. Most of the hype around data science is actually somewhat removed from our field (see above). But I do think that it is worth considering some potential changes that reposition our discipline as the most useful for answering questions with data. Here are some concrete proposals:\nRemove some theoretical requirements and add computing requirements to statistics curricula.\nFocus on statistical writing, presentation, and communication as a main part of the curriculum.\nFocus on positive interactions with collaborators (being a scientist) rather than immediately going to the referee attitude.\nAdd a unit on translating scientific problems to statistical problems.\nAdd a unit on data munging and getting data from databases.\nIntegrating real and live data analyses into our curricula.\nMake all our students create an R package (a data product) before they graduate.\nMost important of all have a “big tent” attitude about what constitutes statistics.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-04-14-sunday-datastatistics-link-roundup-4142013/",
    "title": "Sunday data/statistics link roundup (4/14/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-04-14",
    "categories": [],
    "contents": "\nThe most influential data scientists on Twitter, featuring Amy Heineike, Hilary Mason, and a few other familiar names to readers of this blog. In other news, I love reading list of the “Top K _____” as much as the next person. I love them even more when they are quantitative (the list above isn’t) - even when the quantification is totally bogus. (via John M.)\nRod Little and our own Tom Louis over at the Huffingtonpost talking about the ways in which the U.S. Census supports our democracy. It is a very good piece and I think highlights the critical importance that statistics and data play in keeping government open and honest.\nAn article about the growing number of fake academic journals and their potential predatory practices. I think I’ve been able to filter out the fake journals/conferences pretty well (if they’ve invited 30 Nobel Laureates - probably fake). But this poses big societal problems; how do we tell what is real science from what is fake if you don’t have inside knowledge about which journals are real? (via John H.)\n[ 1. The most influential data scientists on Twitter, featuring Amy Heineike, Hilary Mason, and a few other familiar names to readers of this blog. In other news, I love reading list of the “Top K _____” as much as the next person. I love them even more when they are quantitative (the list above isn’t) - even when the quantification is totally bogus. (via John M.)\nRod Little and our own Tom Louis over at the Huffingtonpost talking about the ways in which the U.S. Census supports our democracy. It is a very good piece and I think highlights the critical importance that statistics and data play in keeping government open and honest.\nAn article about the growing number of fake academic journals and their potential predatory practices. I think I’ve been able to filter out the fake journals/conferences pretty well (if they’ve invited 30 Nobel Laureates - probably fake). But this poses big societal problems; how do we tell what is real science from what is fake if you don’t have inside knowledge about which journals are real? (via John H.) 4.](https://www.capitalbikeshare.com/trip-history-data) on the DC Capitol Bikeshare. One of my favorite things is when a government organization just opens up its data. The best part is that the files are formatted as csv’s. Clearly someone who knows that the best data formats are open, free, and easy to read into statistical software. In other news, I think one of the most important classes that could be taught is “How to share data 101” (via David B.)\nA slightly belated link to a remembrance of George Box. He was the one who said, “All models are wrong, but some are useful.”  An absolute titan of our field.\nCheck out these cool logotypes for famous scientists. I want one! Also, see the article on these awesome minimalist posters celebrating legendary women in science. I want the Sally Ride poster on a t-shirt.\nAs an advisor, I aspire to treat my students/postdocs like this. ([@hunterwalk](https://twitter.com/hunterwalk)). I’m not always so good at it, but those are some good ideals to try to live up to.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-04-12-great-scientist-statistics-lots-of-failed-experiments/",
    "title": "Great scientist - statistics = lots of failed experiments",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-04-12",
    "categories": [],
    "contents": "\nE.O. Wilson is a famous evolutionary biologist. He is currently an emeritus professor at Harvard and just this last week dropped this little gem in the Wall Street Journal. In the piece, he suggests that knowing mathematics is not important for becoming a great scientist. Wilson goes even further, suggesting that you can be mathematically semi-literate and still be an amazing scientist. There are two key quotes in the piece that I think deserve special attention:\n\nFortunately, exceptional mathematical fluency is required in only a few disciplines, such as particle physics, astrophysics and information theory. Far more important throughout the rest of science is the ability to form concepts, during which the researcher conjures images and processes by intuition.\n\nI agree with this quote in general as does Paul Krugman. Many scientific areas don’t require advanced measure theory, differential geometry, or number theory to make big advances. It seems like this is is the kind of mathematics to which E.O. Wilson is referring to and on that point I think there is probably universal agreement that you can have a hugely successful scientific career without knowing about measurable spaces.\nWilson doesn’t stop there, however. He goes on to paint a much broader picture about how one can pursue science without the aid of even basic mathematics or statistics_ _and this is where I think he goes off the rails a bit:\n\nIdeas in science emerge most readily when some part of the world is studied for its own sake. They follow from thorough, well-organized knowledge of all that is known or can be imagined of real entities and processes within that fragment of existence. When something new is encountered, the follow-up steps usually require mathematical and statistical methods to move the analysis forward. If that step proves too technically difficult for the person who made the discovery, a mathematician or statistician can be added as a collaborator.\n\nI see two huge problems with this statement:\nPoor design of experiments is one of, if not the most, common reason for an experiment to fail. It is so important that Fisher said, “To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination.  He can perhaps say what the experiment died of.” Wilson is suggesting that with careful conceptual thought and some hard work you can do good science, but without a fundamental understanding of basic math, statistics, and study design even the best conceived experiments are likely to fail.\nWhile armchair science was likely the norm when Wilson was in his prime, huge advances have been made in both science and technology. Scientifically, it is difficult to synthesize and understand everything that has been done without some basic understanding of the statistical quality of previous experiments. Similarly, as data collection has evolved statistics and computation are playing a more and more central role. As Rafa has pointed out, people in positions of power who don’t understand statistics are a big problem for science.\nMore importantly, as we live in an increasingly data rich environment both in the sciences and in the broader community - basic statistical and numerical literacy are becoming more and more important. While I agree with Wilson that we should try not to discourage people who have a difficult first encounter with math from pursuing careers in science, I think it is both disingenuous and potentially disastrous to downplay the importance of quantitative skill at the exact moment in history that those skills are most desperately needed.\nAs a counter proposal to Wilson’s idea that we should encourage people to disregard quantitative sciences I propose that we build a better infrastructure for ensuring all people interested in the sciences are able to improve their quantitative skills and literacy. Here at Simply Stats we are all about putting our money where our mouth is and we have already started by creating free, online versions of our quantitative courses. Maybe Wilson should take one….\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-04-10-climate-science-day-on-capitol-hill/",
    "title": "Climate Science Day on Capitol Hill",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-04-10",
    "categories": [],
    "contents": "\nA few weeks ago I participated in the fourth annual Climate Science Day organized by the ASA and a host of other professional and scientific societies. There’s a nice write up of the event written by Steve Pierson over at Amstat News. There were a number of statisticians there besides me, but the vast majority of people were climate modelers, atmospheric scientists, agronomists, and the like. Below is our crack team of scientists outside the office of (Dr.) Andy Harris. Might be the only time you see me wearing a suit.\n\nThe basic idea behind the day is to get scientists who do climate-related research into the halls of Congress to introduce themselves to members of Congress and make themselves available for scientific consultations. I was there (with Brooke Anderson, the other JHU rep) because of some of my work on the health effects of heat. I was paired up with Tony Broccoli, a climate modeler at Rutgers, as we visited the various offices of New Jersey and Maryland legislators. We also talked to staff from the Senate Health, Education, Labor, and Pensions (HELP) committee.\nHere are a few things I learned:\nIt was fun. I’d never been to Congress before so it was interesting for me to walk around and see how people work. Everyone (regardless of party) was super friendly and happy to talk to us.\nThe legislature appears to be run by women. Seriously, I think every staffer we met with (but one) was a woman. Might have been a coincidence, but I was not expecting that. We only met with one actual member of Congress, and that was (Dr.) Andy Harris from Maryland’s first district.\nClimate change is not really on anyone’s radar. Oh well, we were there 3 days before the sequester hit so there were understandably other things on their minds. Waxman-Markey was the most recent legislation taken up by the House and it went nowhere in the Senate.\nThe Senate HELP committee has PhDs working on its staff. Didn’t know that.\nStaffers are working on like 90 things at once, probably none of which are related to each other. That’s got to be a tough job.\nI used more business cards on this one day than in my entire life.\nSenate offices are way nicer than House offices.\nThe people who write our laws are around 22 years old. Maybe 25 if they went to law school. I’m cool with that, I think.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-04-08-nih-is-looking-for-an-associate-director-for-data-science-statisticians-should-consider-applying/",
    "title": "NIH is looking for an Associate Director for Data Science: Statisticians should consider applying",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-04-08",
    "categories": [],
    "contents": "\nNIH understands the importance of data and several months ago they announced this new position. Here is an excerpt from the add:\n\nThe ADDS will focus on the urgent need and increased opportunities for capitalizing on the expanding collections of biomedical data to advance NIH’s mission. In doing so, the incumbent will provide programmatic NIH-wide leadership for areas of data science that relate to data emanating from many areas of study (e.g., genomics, imaging, and electronic heath records). This will require knowledge about multiple domains of study as well as familiarity with approaches for integrating data from these various domains.\n\nIn my opinion, the person holding this job should have hands-on experience with data analysis and programming. The nuisances nuances of what a data analyst needs to successfully do his/her job can’t be underestimated. This knowledge will help this director make the right decisions when it comes to choosing what data to make available and how to make it available.  When it comes to creating data resources, good intentions don’t always translate into usable products.\nIn this new era of data driven science this position will be highly influential making this job quite attractive. If you know of a Statistician that you think is interested please pass along the information.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-04-02-introducing-the-healthvis-r-package-one-line-d3-graphics-with-r/",
    "title": "Introducing the healthvis R package  - one line D3 graphics with R",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-04-02",
    "categories": [],
    "contents": "\n\nWe have been a little slow on the posting for the last couple of months here at Simply Stats. That’s bad news for the blog, but good news for our research programs!\n\nToday I’m announcing the new healthvis R package that is being developed by my student Prasad Patil (who needs a website like yesterday), Hector Corrada Bravo, and myself*. The basic idea is that I have loved D3 interactive graphics for a while. But they are hard to create from scratch, since they require knowledge of both Javascript and the D3 library.\nEven with those skills, it can take a while to develop a new graphic. On the other hand, I know a lot about R and am often analyzing biomedical data where interactive graphics could be hugely useful. There are a couple of really useful tools for creating interactive graphics in R, most notably Shiny, which is awesome. But these tools still require a bit of development to get right and are designed for “stand alone” tools.\nSo we created an R package that builds specific graphs that come up commonly in the analysis of health data like survival curves, heatmaps, and icon arrays. For example, here is how you make an interactive survival plot comparing treated to untreated individuals with healthvis:\n# Load libraries\n\nlibrary(healthvis)\nlibrary(survival)\n\n# Run a cox proportional hazards regression\n\ncobj <- coxph(Surv(time, status)~trt+age+celltype+prior, data=veteran)\n\n# Plot using healthvis - one line!\n\nsurvivalVis(cobj, data=veteran, plot.title=\"Veteran Survival Data\", group=\"trt\", group.names=c(\"Treatment\", \"No Treatment\"), line.col=c(\"#E495A5\",\"#39BEB1\"))\n\n\nThe “survivalVis” command above  produces an interactive graphic like this. Here it is embedded (you may have to scroll to see the dropdowns on the right - we are working on resizing)\n`\n\nWe have been a little slow on the posting for the last couple of months here at Simply Stats. That’s bad news for the blog, but good news for our research programs!\n\nToday I’m announcing the new healthvis R package that is being developed by my student Prasad Patil (who needs a website like yesterday), Hector Corrada Bravo, and myself*. The basic idea is that I have loved D3 interactive graphics for a while. But they are hard to create from scratch, since they require knowledge of both Javascript and the D3 library.\nEven with those skills, it can take a while to develop a new graphic. On the other hand, I know a lot about R and am often analyzing biomedical data where interactive graphics could be hugely useful. There are a couple of really useful tools for creating interactive graphics in R, most notably Shiny, which is awesome. But these tools still require a bit of development to get right and are designed for “stand alone” tools.\nSo we created an R package that builds specific graphs that come up commonly in the analysis of health data like survival curves, heatmaps, and icon arrays. For example, here is how you make an interactive survival plot comparing treated to untreated individuals with healthvis:\n# Load libraries\n\nlibrary(healthvis)\nlibrary(survival)\n\n# Run a cox proportional hazards regression\n\ncobj <- coxph(Surv(time, status)~trt+age+celltype+prior, data=veteran)\n\n# Plot using healthvis - one line!\n\nsurvivalVis(cobj, data=veteran, plot.title=\"Veteran Survival Data\", group=\"trt\", group.names=c(\"Treatment\", \"No Treatment\"), line.col=c(\"#E495A5\",\"#39BEB1\"))\n\n\nThe “survivalVis” command above  produces an interactive graphic like this. Here it is embedded (you may have to scroll to see the dropdowns on the right - we are working on resizing)\n`\nThe advantage of this approach is that you can make common graphics interactive without a lot of development time. Here are some other unique features:\n\nThe graphics are hosted on Google App Engine. With one click you can get a permanent link and share it with collaborators.\n\n\nWith another click you can get the code to embed the graphics in your website.\n\n\nIf you have already created D3 graphics it only takes a few minutes to develop a healthvis version to let R users create their own - email us and we will make it part of the healthvis package!\n\n\nhealthvis is totally general - you can develop graphics that don’t have anything to do with health with our framework. Just email to get in touch if you want to be a developer at healthvis@gmail.com\n\nWe have started a blog over at healthvis.org where we will be talking about the tricks we learn while developing D3 graphics, updates to the healthvis package, and generally talking about visualization for new technologies like those developed by the CCNE and individualized health. If you are interested in getting involved as a developer, user or tester, drop us a line and let us know. In the meantime, happy visualizing!\n* This project is supported by the JHU CCNE (U54CA151838) and the Johns Hopkins inHealth initiative.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-03-26-an-instructors-thoughts-on-peer-review-for-data-analysis-in-coursera/",
    "title": "An instructor's thoughts on peer-review for data analysis in Coursera",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-03-26",
    "categories": [],
    "contents": "\nI used peer-review for the data analysis course I just finished. As I mentioned in the post-mortem podcast I knew in advance that it was likely to be the most controversial component of the class. So it wasn’t surprising that based on feedback in the discussion boards and on this blog, the peer review process is by far the thing students were most concerned about.\nBut to evaluate complete data analysis projects at scale there is no other alternative that is economically feasible. To give you an idea, I have our local students perform 3 data analyses in an 8 week term here at Johns Hopkins. There are generally 10-15 students in that class and I estimate that I spend around an hour reading each analysis, digesting what was done, and writing up comments. That means I usually spend almost an entire weekend grading just for 10-15 data analyses. If you extrapolate that out to the 5,000 or so people who turned in data analysis assignments, it is clearly not possible for me to do all the grading.\nAnother alternative would be to pay trained data analysts to grade all the assignments. Of course that would be expensive - you couldn’t farm it out to the mechanical turk. If you want to get a better/more consistent grading scheme than peer review you’d need to hire highly trained data analysts to do that and that would be very expensive. While Johns Hopkins has been incredibly supportive in terms of technical support and giving me the flexibility to pursue the class, it is definitely something I did on my own time and with a lot of my own resources. It isn’t clear that it make sense for Hopkins to pour huge resources into really high-quality grading. At the same time, I’m not sure Coursera could afford to do this for all of the classes where peer review is needed, as they are just a startup.\nSo I think that at least for the moment, peer review is the best option for grading. This has  big implications for the value of the Coursera statements of accomplishment in classe where peer review is necessary. I think that it would benefit Coursera hugely to do some research on how to ensure/maintain quality in peer review (Coursera - if you are reading this and you have some $$ you want to send my way to support some students/postdocs I have some ideas on how to do that). The good news is that the amazing Coursera platform collects so much data that it is possible to do that kind of research.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-03-25-podcast-6-data-analysis-mooc-post-mortem/",
    "title": "Podcast #6: Data Analysis MOOC Post-mortem",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-03-25",
    "categories": [],
    "contents": "\nJeff and I talk about Jeff’s recently completed MOOC on Data Analysis.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-03-24-sunday-datastatistics-link-roundup-3242013/",
    "title": "Sunday data/statistics link roundup (3/24/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-03-24",
    "categories": [],
    "contents": "\nMy Coursera Data Analysis class is done for now! All the lecture notes are on Github all the videos are on Youtube. They are tagged by week with tags “Week x”.\nAfter ENAR the comments on how to have better stats conferences started flowing. Check out Frazee, Xie, and Broman. My favorite cherry picked ideas: conference app (frazee), giving the poster session more focus (frazee), free and announced wifi (broman), more social media (i loved following ENAR on twitter but wish there had been more tweeting) (xie), add some jokes to talks (xie).\nA related post is this one from Hilary M. on how a talk should entertain, not teach.\nThis is a [ 1. My Coursera Data Analysis class is done for now! All the lecture notes are on Github all the videos are on Youtube. They are tagged by week with tags “Week x”.\nAfter ENAR the comments on how to have better stats conferences started flowing. Check out Frazee, Xie, and Broman. My favorite cherry picked ideas: conference app (frazee), giving the poster session more focus (frazee), free and announced wifi (broman), more social media (i loved following ENAR on twitter but wish there had been more tweeting) (xie), add some jokes to talks (xie).\nA related post is this one from Hilary M. on how a talk should entertain, not teach.\nThis is a](http://blogs.spectator.co.uk/books/2013/03/interview-with-a-writer-jaron-lanier/) I found via AL Daily. My favorite lines? “You run into this attitude, that if ordinary people cannot set their Facebook privacy settings, then they deserve what is coming to them. There is a hacker superiority complex to this.” I think this is certainly something we have a lot of in statistics as well.\nThe CIA wants to collect all the dataz. Call me when cat videos become important for national security, ok guys?\nGiven I just completed my class, the MOOC completion rates graph is pretty appropriate. I think my #’s are right in line with that other people report. I’m still trying to figure out how to know how many people “completed” the class.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-03-21-youtube-should-check-its-checksums/",
    "title": "Youtube should check its checksums",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-03-21",
    "categories": [],
    "contents": "\nI am in the process of uploading the video lectures for Data Analysis. I am getting ready to send out the course wrap-up email and I wanted to include the link to the Youtube playlist as well.\nUnfortunately, Youtube keeps reporting that a pair of the videos in week 2 are duplicates. This is true despite them being different lengths (12:15 vs. 16:58), having different titles, and having dramatically different content. I I am in the process of uploading the video lectures for [Data Analysis](https://www.coursera.org/course/dataanalysis). I am getting ready to send out the course wrap-up email and I wanted to include the link to the Youtube playlist as well. on the forums:\n\nYouTube uses a checksum to determine duplicates. The chances of having two different files containing different content but have the same checksum would be astronomical.\n\nThat isn’t on the official Google documentation page, which is pretty sparse, but is the only description I can find of how Youtube checks for duplicate content. A checksum is a function you apply to the data from a video that (ideally) with high probability will yield different values when different videos are uploaded and the same value when the same video is uploaded. One possible checksum function could be the length of the video. Obviously that won’t work in general because many videos might be 2 minutes exactly.\nRegardless, it looks like Youtube can’t distinguish my lecture videos. I’m thinking Vimeo or something else if I can’t get this figured out. Of course, if someone has a suggestion (short of re-exporting the videos from Camtasia) that would allow me to circumvent this problem I’d love to hear it!\nUpdate: I ended up fiddling with the videos and got them to upload. Thanks to the helpful comments!\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-03-19-call-for-papers-for-a-special-issue-of-statistical-analysis-and-data-mining/",
    "title": "Call for papers for a special issue of Statistical Analysis and Data Mining",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-03-19",
    "categories": [],
    "contents": "\nDavid Madigan sends the following. It looks like a really interesting place to submit papers for both statisticians and data scientists, so submit away!\n\nStatistical Analysis and Data Mining, An American Statistical Association Journal\n\nCall for Papers\n\n\nSpecial Issue on Observational Healthcare Data\n\n\n\n\n\nGuest Editors: Patrick Ryan, J&J and Marc Suchard, UCLA\n\n\n\n\n\nDue date: July 1, 2013\n\n\n\n\n\nData sciences is the rapidly evolving field that integrates\n\n\nmathematical and statistical knowledge, software engineering and large-scale data management skills, and domain expertise to tackle difficult problems that typically cannot be solved by any one discipline alone.  Some of the most difficult, and arguably most important, problems exist in healthcare.  Knowledge about human biology has exponentially advanced in the past two decades with exciting progress in genetics, biophysics, and pharmacology.  However, substantial opportunities exist to extend the evidence base about human disease, patient health and effects of medical interventions and translate knowledge into actions that can directly impact clinical care.  The emerging availability of ‘big data’ in healthcare, ranging from prospective research with aggregated genomics and clinical trials to observational data from administrative claims and electronic health records through social media, offer unprecedented opportunities for data scientists to contribute to advancing healthcare through the development, evaluation, and application of novel analytical solutions to explore these data to generate evidence at both the patient and population level.  Statistical and computational challenges abound and\n\n\nmethodological progress will draw on fields such as data mining,\n\n\nepidemiology, medical informatics, and biostatistics to name but a\n\n\nfew.  This special issue of Statistical Analysis and Data Mining seeks to capture the current state of the art in healthcare data sciences. We welcome contributions that focus on methodology for healthcare data and original research that demonstrates the application of data sciences to problems in public health.\n\n\n\n\n\nhttp://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1932-1872\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-03-17-sunday-datastatistics-link-roundup-31713/",
    "title": "Sunday data/statistics link roundup (3/17/13)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-03-17",
    "categories": [],
    "contents": "\nA post on the Revolutions blog about an analysis of the worldwide email traffic patterns. The corresponding paper is also pretty interesting. The best part is the whole analysis was done in R. \nA bill in California that would require faculty approved online classes to be given credit. I think this is potentially game changing if it passes - depending on who has to do the approving. If there is local control within departments it could be huge. On the other hand, as I’ll discuss later this week, there is still some ground to be made up before I think MOOCs are ready for prime time credit in areas outside of the very basics.\nA pretty amazing blog post about a survival analysis of RuPaul’s drag race. Via Hadley.\nIf you are a statistician hiding under a rock you missed the NY Times messing up P-values.  The statistical blogosphere came out swinging. Gelman, Wasserman, Parker, etc.\nAs a statistician who is pretty fired up about the tech community, I can get lost a bit in the hype as much as the next guy. I thought this article was pretty sobering. I think the way to make sure we keep innovating is having the will to fund long term companies and long term research. Look at how it paid off with Amazon…\nBroman on interactive graphics is worth a read. I agree that more of our graphics should be interactive, but there is an inherent tension/tradeoff in graphics, similar to the bias variance tradeoff. I’m sure there is a real word for it but it is the flexibility vs. understandability tradeoff. Too much interaction and its hard to see what is going on, not enough and you might as well have made a static graph.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-03-14-postdoctoral-fellow-position-in-reproducible-research/",
    "title": "Postdoctoral fellow position in reproducible research",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-03-14",
    "categories": [],
    "contents": "\nWe are looking to recruit a postdoctoral fellow to work on developing tools to make scientific research more easily reproducible. We’re looking for someone who wants to work on (and solve!) real research problems in the biomedical sciences and address the growing need for reproducible research tools. The position would be in the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health and would be jointly advised by Jeff and myself.\nQualifications: PhD in statistics, biostatistics, computer science, or related field; strong programming skills in R and Perl/Python/C; excellent written and oral communication skills; serious moxie\nAdditional Information: Informal questions about the position can be sent to Dr. Roger Peng at rpeng @ jhsph.edu. Applications will be considered as they arrive.\n\n\nTo apply, send a cover letter describing your research interests and interest in the position, a CV, and the names of three references. In your application, please reference “Reproducible Research postdoctoral fellowship”. Application materials should be emailed to Dr. Roger Peng at rpeng @ jhsph.edu.\n\n\nApplications from minority and female candidates are especially encouraged. Johns Hopkins University is an AA/EOE.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-03-13-heres-my-enar2013-wednesday-schedule/",
    "title": "Here's my #ENAR2013 Wednesday schedule",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-03-13",
    "categories": [],
    "contents": "\nHere are my picks for ENAR sessions today (Wednesday):\n8:30-10:15am: Large Data Visualization and Exploration, Grand Ballroom 4 (make sure you stay till the end to see Karl Broman); Innovative Methods in Causal Inference with Applications to Mediation, Neuroimaging, and Infectious Diseases, Grand Ballroom 8A; Next Generation Sequencing, Grand Ballroom 5\n10:30am-12:15pm: Statistical Information Integration of -Omics Data, Grand Ballrooms 1 & 2\nOkay, so this schedule actually requires me to split myself in to three separate entities. However, if you find a way to do that, the 8:30-10:15am block is full of good stuff.\nHave fun!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-03-12-if-i-were-at-enar2013-today-heres-where-id-go/",
    "title": "If I were at #ENAR2013 today, here's where I'd go",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-03-12",
    "categories": [],
    "contents": "\nThis week is the annual ENAR meeting, the big biostatistics conference, in Orlando, Florida. It actually started on Sunday but I haven’t gotten around to looking at the program (obviously, I’m not there right now). Flipping through the program now, here’s what looks good to me for Tuesday:\n8:30-10:15am: Functional Neuroimaging Decompositions, Grand Ballroom 3 \n10:30am-12:15pm: Hmm…I guess you should go to the Presidential Invited Address, Grand Ballroom 7\n1:45-3:30pm: JABES Showcase, Grand Ballroom 8A; Statistical Body Language: Analytical Methods for Wearable Computing, Grand Ballroom 4\n3:45-5:30pm: Big Data: Wearable Computing, Crowdsourcing, Space Telescopes, and Brain Imaging, Grand Ballroom 8A; Sample Size Planning for Clinical Development, Grand Ballroom 6\nThat’s right, you can pack in two sessions on wearable computing today if you want. I’ll post tomorrow for what looks good on Wednesday.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:04:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-03-10-sunday-datastatistics-link-roundup-31013/",
    "title": "Sunday data/statistics link roundup (3/10/13)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-03-10",
    "categories": [],
    "contents": "\nThis is an outstanding follow up analysis to our paper on the rate of false discoveries in the medical literature. I hope that the author of the blog post will consider submitting it for publication in a journal, I think it is worth having more methodology out there in this area. \nIf you are an academic in statistics and aren’t following Karl and Thomas on Twitter, you should be. Also check out Karl’s (mostly) reproducible paper.\nAn article in the WSJ that I think I received about 40 times this week. The online version has a quote from our own B-Caffo. It is a really good read. If you are into this, it seems like the interviews with Rebecca Nugent (where we discuss growing undergrad programs) and Joe Blitzstein where we discuss stats ed are relevant. I thought this quote was hugely relevant, “The bulk of the people coming out [with statistics degrees] are technically competent but they’re missing the consultative and the soft skills, everything else they need to be successful” We are focusing heavily on both components of these skills in the grad program here at Hopkins - so if people are looking for awesome data people, just let us know!\nA cool discussion of how the A’s look for players with “positive residuals” - positive value missed by the evaluations of other teams. (via Rafa)\nThe physicist and the bikini model. If you haven’t read it, you must be living under a rock. (via Alex N.)\nAn interesting article about how IBM is using Watson to come up with new recipes based on the data from old recipes. I’m a little suspicious of the Spanish crescent though - no butter?!\nYou should vote for Steven Salzberg for the Ben Franklin award. The dude has come up huge for open software and we should come up huge for him. Gotta vote today though.\nThe Harlem Shake has killed more than one of my lunch hours. But this one is the best. By far. How all simulation studies should be done (via StatsChat).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-03-08-send-me-studentpostdoc-blogs-in-statistics-and-computational-biology/",
    "title": "Send me student/postdoc blogs in statistics and computational biology",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-03-08",
    "categories": [],
    "contents": "\nI’ve been writing a blog for a few years now, but it started after I was already comfortably settled in a tenure track job. There have been some huge benefits of writing a scientific blog. It has certainly raised my visibility and given me opportunities to talk about issues that are a little outside of my usual research agenda. It has also inspired more than one research project that has ended up in a full blown peer-reviewed publication. I also frequently look to blogs/twitter accounts to see “what’s happening” in the world of statistics/data science.\nOne thing that gets me incredibly fired up are student blogs. A I’ve been writing a blog for a few years now, but it started after I was already comfortably settled in a tenure track job. There have been some huge benefits of writing a scientific blog. It has certainly raised my visibility and given me opportunities to talk about issues that are a little outside of my usual research agenda. It has also inspired more than one research project that has ended up in a full blown peer-reviewed publication. I also frequently look to blogs/twitter accounts to see “what’s happening” in the world of statistics/data science. of my students have them and I read them whenever they post. But I have found it is hard to discover all of the blogs that might be written by students I’m not directly working with.\nSo this post is designed for two things:\nI’d really like it if you could please send me the links to twitter feeds/blogs/google+ pages etc. of students (undergrad, grad or postdoc) in statistics, computational biology, computational neuroscience, computational social science, etc. Anything that touches statistics and data is fair game.\nI plan to create a regularly-maintained page on the blog with links to student blogs  with some kind of tagging system so other people can find all the cool stuff that students are thinking about/doing.\nPlease feel free to either post links in the comments, send them to us on twitter, or email them to me directly. I’ll follow up in a couple of weeks once I have things organized.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-03-06-the-importance-of-simulating-the-extremes/",
    "title": "The importance of simulating the extremes",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-03-06",
    "categories": [],
    "contents": "\nSimulation is commonly used by statisticians/data analysts to: (1) estimate variability/improve predictors, (2) to evaluate the space of potential outcomes, and (3) to evaluate the properties of new algorithms or procedures. Over the last couple of days, discussions of simulation have popped up in a couple of different places.\nFirst, the reviewers of a paper that my student is working on had asked a question about the behavior of the method in different conditions. I mentioned in passing, that I thought it was a good idea to simulate some cases where our method will definitely break down.\nI also saw this post by John Cook about simple/complex models. He raises the really important point that increasingly complex models built on a canonical, small, data set can fool you. You can make the model more and more complicated - but in other data sets the assumptions might not hold and the model won’t generalize. Of course, simple models can have the same problems, but generally simple models will fail on small data sets in the same way they would fail on larger data sets (in my experience) - either they work or they don’t.\nThese two ideas got me thinking about why I like simulation. Some statisticians, particularly applied statisticians, aren’t fond of simulation for evaluating methods. I think the reason is that you can always simulate a situation that meets all of your assumptions and make your approach look good. Real data rarely conform to model assumptions and so are harder to “trick”. On the other hand, I really like simulation, it can reveal a lot about how and when a method will work well and it allows you to explore scenarios - particularly for new or difficult to obtain data.\nHere are the simulations I like to see:\nSimulation where the assumptions are true There are a surprising number of proposed methods/analysis procedures/analyses that fail or perform poorly even when the model assumptions hold. This could be because the methods overfit, have a bug, are computationally unstable, are on the wrong place on the bias/variance tradeoff curve, etc. etc. etc. I always do at least one simulation for every method where the answer should be easy to get, because I know if I don’t get the right answer, it is back to the drawing board.\nSimulation where things should definitely fail I like to try out a few realistic scenarios where I’m pretty sure my model assumptions won’t hold and the method should fail. This kind of simulation is good for two reasons: (1) sometimes I’m pleasantly surprised and the model will hold up and (2) (the more common scenario) I can find out where the model assumption boundaries are so that I can give concrete guidance to users about when/where the method will work and when/where it will fail.\nThe first type of simulation is easy to come up with - generally you can just simulate from the model. The second type is much harder. You have to creatively think about reasonable ways that your model can fail. I’ve  found that using real data for simulations can be the best way to start coming up with ideas to try - but I usually find that it is worth building on those ideas to imagine even more extreme circumstances. Playing the evil demon for my own methods often leads me to new ideas/improvements I hadn’t thought of before. It also helps me to evaluate the work of other people - since I’ve tried to explore the contexts where methods likely fail.\nIn any case, if you haven’t simulated the extremes I don’t think you really know how your methods/analysis procedures are working.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-03-04-big-data-context-bad/",
    "title": "Big Data - Context = Bad",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-03-04",
    "categories": [],
    "contents": "\nThere’s a nice article by Nick Bilton in the New York Times Bits blog about the need for context when looking at Big Data. Actually, the article starts off by describing how Google’s Flu Trends model overestimated the number of people infected with flue in the U.S. this season, but then veers off into a more general discussion about Big Data.\nMy favorite quote comes from Mark Hansen:\n\n“Data inherently has all of the foibles of being human,” said Mark Hansen, director of the David and Helen Gurley Brown Institute for Media Innovation at Columbia University. “Data is not a magic force in society; it’s an extension of us.”\n\nBilton also talks about a course he taught where students built sensors to install in elevators and stairwells at NYU to see how often they were used. The idea was to explore how often and when the NYU students used the stairs versus the elevator.\n\nAs I left campus that evening, one of the N.Y.U. security guards who had seen students setting up the computers in the elevators asked how our experiment had gone. I explained that we had found that students seemed to use the elevators in the morning, perhaps because they were tired from staying up late, and switch to the stairs at night, when they became energized.\n“Oh, no, they don’t,” the security guard told me, laughing as he assured me that lazy college students used the elevators whenever possible. “One of the elevators broke down a few evenings last week, so they had no choice but to use the stairs.”\n\nI can see at least three problems here, not necessarily mutually exclusive:\nBig Data are often “Wrong” Data. The students used the sensors measure something, but it didn’t give them everything they needed. Part of this is that the sensors were cheap, and budget was likely a big constraint here. But Big Data are often big because they are cheap. But of course, they still couldn’t tell that the elevator was broken.\nA failure of interrogation. With all the data the students collected with their multitude of sensors, they were unable to answer the question “What else could explain what I’m observing?”\nA strong desire to tell a story. Upon looking at the data, they seemed to “make sense” or to at least match a preconceived notion of that they should look like. This is related to #2 above, which is that you have to challenge what you see. It’s very easy and tempting to let the data tell an interesting story rather than the right story.\nI don’t mean to be unduly critical of some students in a class who were just trying to collect some data. I think there should be more of that going on. But my point is that it’s not as easy as it looks. Even trying to answer a seemingly innocuous question of how students use elevators and stairs requires some forethought, study design, and careful analysis.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-03-03-sunday-datastatistics-link-roundup-332013/",
    "title": "Sunday data/statistics link roundup (3/3/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-03-03",
    "categories": [],
    "contents": "\nA really nice example where epidemiological studies are later confirmed by a randomized trial. From a statistician’s point of view, this is the idealized way that science would work. First, data that are relatively cheap (observational/retrospective studies) are used to identify potential associations of interest. After a number of these studies show a similar effect, a randomized study is performed to confirm what we suspected from the cheaper studies.\nJoe Blitzstein talking about the “Soul of Statistics”, we interviewed Joe a while ago. Teaching statistics is critical for modern citizenship. It is not just about learning which formula to plug a number into - it is about critical thinking with data. Joe’s talk nails this issue.\nSignificance magazine has a writing contest. If you are a grad student in statistics/biostatistics this is an awesome way to (a) practice explaining your discipline to people who are not experts - a hugely important skill and (b) get your name out there, which will help when it comes time to look for jobs/apply for awards, etc.\nA great post from David Spiegelhalter about the UK court’s interpretation of probability. It reminds me of the Supreme Court’s recent decision that also hinged on a statistical interpretation.  This post brings up two issues I think are worth a more in-depth discussion. One is that it is pretty clear that many court decisions are going to hinge on statistical arguments. This suggests (among other things) that statistical training should be mandatory in legal education. The second issue is a minor disagreement I have with Spiegelhalter’s characterization that only Bayesians use epistemic uncertainty. I frequently discuss this type of uncertainty in my classes although I take a primarily frequentist/classical approach to teaching these courses.\nThomas Lumley is giving an online course in complex surveys.\nOn the protective value of an umbrella when encountering a lion. Seems like a nice way to wrap up a post that started with the power of epidemiology and clinical trials. (via Karl B.)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-02-27-please-save-the-unsolicited-r01s/",
    "title": "Please save the unsolicited R01s",
    "description": {},
    "author": [
      {
        "name": "Steven Salzberg",
        "url": {}
      }
    ],
    "date": "2013-02-27",
    "categories": [],
    "contents": "\nEditor’s note: With the sequestration deadline hours away, the career of many young US scientists is on the line.  In this guest post, our colleague Steven Salzberg , an avid _Editor’s note: With the sequestration deadline hours away, the career of many young US scientists is on the line.  In this guest post, our colleague Steven Salzberg , an avid  and its peer review process, tells us why now more than ever the NIH should prioritize funding R01s over other project grants .\nFirst let’s get the obvious facts out of the way: the federal budget is a mess, and Congress is completely disfunctional.  When it comes to NIH funding, this is not a good thing.\nHidden within the larger picture, though, is a serious menace to our decades-long record of incredibly successful research in the United States.  The investigator-driven, basic research grant is in even worse shape than the overall NIH budget.  A recent analysis by FASEB, shown in the figure here, reveals that the number of new R01s reached its peak in 2003 - ten years ago! - and has been steadily declining since.  In 2003, 7,430 new R01s were awarded.  In 2012, that number had dropped to 5,437, a 27% decline.\n\n\nFor those who might not be familiar with the NIH system, the R01 grant is the crown jewel of research grants.  R01s are awarded to individual scientists to pursue all varieties of biomedical research, from very basic science to clinical research.  For R01s, NIH doesn’t tell the scientists what to do: we propose the ideas, we write them up, and then NIH organizes a rigorous peer review (which isn’t perfect, but it’s the best system anyone has).  Only the top-scoring proposals get funded.\nThis process has gotten much tougher over the years.  In 1995, the success rate for R01s was 25.9%.  Today it is 18.4% and falling.  This includes applications from everyone, even the most experienced and proven scientists.  Thus no matter who you are, you can expect that there is more than an 80% chance that your grant application will be turned down.  In some areas it is even worse: NIAID’s website announced that it is currently funding only 6% of R01s.\nWhy are R01s declining?  Not for lack of interest: the number of applications last year was 29,627, an all-time high.  Besides the overall budget problem, another problem is growing: the fondness of the NIH administration for big, top-down science projects, many times with the letters “ome” or “omics” attached.\nYes, the human genome was a huge success.  Maybe the human microbiome will be too.  But now NIH is pushing gigantic, top-down projects: ENCODE, 1000 Genomes, the cancer anatomy genome project (CGAP), the cancer genome atlas (TCGA), a new “brain-ome” project, and more. The more money is allocated to these big projects, the less R01s NIH can fund. For example, NIAID, with its 6% R01 success rate, has been spending tens of millions of dollars per year on 3 large Microbial Genome Sequencing Center contracts and tens of millions more on 5 large Bioinformatics Resource Center contracts.  As far as I can tell, no one uses these bioinformatics resource centers for anything - in fact, virtually no one outside the centers even knows they exist. Furthermore, these large, top-down driven sequencing projects don’t address specific scientific hypotheses, but they produce something that the NIH administration seems to love: numbers.  It’s impressive to see how many genomes they’ve sequenced, and it makes for nice press releases.  But very often we simply don’t need these huge, top-down projects to answer scientific questions.  Genome sequencing is cheap enough that we can include it in an R01 grant, if only NIH will stop pouring all its sequencing money into these huge, monolithic projects.\nI’ll be the first person to cheer if Congress gets its act together and fund NIH at a level that allows reasonable growth.  But whether or not that happens, the growth of big science projects, often created and run by administrators at NIH rather than scientists who have successfully competed for R01s, represents a major threat to the scientist-driven research that has served the world so well for the past 50 years.  Many scientists are afraid to speak out against this trend, because by doing so we (yes, this includes me) are criticizing those same NIH administrators who manage our R01s.   But someone has to say something.  A 27% decline in the number of R01s over the past decade is not a good thing.  Maybe it’s time to stop the omics train.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-02-25-big-data-giving-people-what-they-want/",
    "title": "Big data: Giving people what they want",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-02-25",
    "categories": [],
    "contents": "\nNetflix is using data to create original content for its subscribers, the first example of which was House of Cards. Three main data points for this show were that (1) People like David Fincher (because they watch The Social Network, like, all the time); (2) People like Kevin Spacey; and (3) People liked the British version of House of Cards. Netflix obviously has tons of other data, including when you stop, pause, rewind certain scenes in a movie or TV show.\n\nNetflix has always used data to decide which shows to license, and now that expertise is extended to the first-run. And there was not one trailer for “House of Cards,” there were many. Fans of Mr. Spacey saw trailers featuring him, women watching “Thelma and Louise” saw trailers featuring the show’s female characters and serious film buffs saw trailers that reflected Mr. Fincher’s touch.\n\nUsing data to program television content is about as new as Bryl Cream, but Netflix has the Big Data and has direct interaction with its viewers (so does Amazon Prime, which apparently is also looking to create original content). So the question is, does it work? My personal opinion is that it’s probably not any worse than previous methods, but may not be a lot better. But I would be delighted to be proven wrong. From my walks around the hallway here it seems House of Cards is in fact a good show (I haven’t seen it). But one observation probably isn’t enough to draw a conclusion here.\nJohn Landgraf of FX Networks thinks Big Data won’t help:\n\n“Data can only tell you what people have liked before, not what they don’t know they are going to like in the future,” he said. “A good high-end programmer’s job is to find the white spaces in our collective psyche that aren’t filled by an existing television show,” adding, those choices were made “in a black box that data can never penetrate.”\n\nI was a bit confused when I read this but the use of the word “programmer” here I’m pretty sure is in reference to television programmer. This quote is reminiscent of Steve Jobs’ line about how it’s not he consumer’s job to know what he/she wants. It also reminds me of financial markets where all the data it the world can only tell you about the past.\nIn the end, can any of it help you predict the future? Or do some people just get lucky?\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-02-24-sunday-datastatistics-link-roundup-2242013/",
    "title": "Sunday data/statistics link roundup (2/24/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-02-24",
    "categories": [],
    "contents": "\nAn attempt to create a version of knitr for stata (via John M.). I  like the direction that reproducible research is moving - toward easier use and wider spread adoption. The success of iPython notebook is another great sign for the whole research area.\nEmail is always a problem for me. In the last week I’ve been introduced to a couple of really nice apps that give me insight into my email habits (Gmail meter - via John M.) and that help me to send reminders to myself with minimal hassle (Boomerang - via Brian C.)\nAndrew Lo proposes a new model for cancer research funding based on his research in financial engineering. In light of the impending sequester I’m interested in alternative funding models for data science/statistics in biology. But the concerns I have about both crowd-funding and Lo’s idea are whether the basic scientists get hosed and whether sustained funding at a level that will continue to attract top scientists is possible.\nThis is a really nice rundown of why medical costs are so high. They key things in the article to me are that: (1) he chased down the data about actual costs versus charges and (2) he highlights the role of the chargemaster - the price setter in medical centers - and how the prices are often set historically with yearly markups (not based on estimates of costs, etc.), and (3) he discusses key nuances like medical liability if the “best” tests aren’t run on everyone. Overall, it is definitely worth a read and this seems like a hugely important problem a statistician could really help with (if they could get their hands on the data).\nA really cool applied math project where flying robot helicopters toss and catch a stick. Applied math can be super impressive, but they always still need a little boost from statistics, ““This also involved bringing the insights gained from their initial\nand many subsequent experiments to bear on their overall system\n\ndesign. For example, a learning algorithm was added to account for\n\nmodel inaccuracies.\" (via Rafa via MR).\nWe’ve talked about trying to reduce meetings to increase producitivity before. Here is an article in the NYT talking about the same issue (via Rafa via Karl B.). Brian C. made an interesting observation though, that in a soft money research environment there should be evolutionary pressure against anything that doesn’t improve your ability to obtain research funding. Despite this, meetings proliferate in soft-money environments. So there must be some selective advantage to them! Another interesting project for a stats/evolutionary biology student.\nIf you have read all the Simply Statistics interviews and still want more, check out http://www.analyticstory.com/.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-02-18-tesla-vs-nyt-do-the-data-really-tell-all/",
    "title": "Tesla vs. NYT: Do the Data Really Tell All?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-02-18",
    "categories": [],
    "contents": "\nI’ve enjoyed so far the back and forth between Tesla Motors and New York Times reporter John Broder. The short version is\nBroder tested one of Tesla’s new Model S all-electric sedans on a drive from Washington, D.C. to Groton, CT. Part of the reason for this specific trip was to make use of Tesla’s new supercharger stations along the route (one in Delaware and one in Connecticut).\nBroder’s trip appeared to have some bumps, including running out of electricity at one point and requiring a tow.\nAfter the review was published in the New York Times, Elon Musk, the CEO/Founder of Tesla, was apparently livid. He published a detailed response on the Tesla blog explaining that what Broder wrote in his review was not true and that “he simply did not accurately capture what happened and worked very hard to force our car to stop running”.\nBroder has since responded to Musk’s response with further explanation.\nOf course, the most interesting aspect of Musk’s response on the Tesla blog was that he published the data collected by the car during Broder’s test drive. When revelations of this data came about, I thought it was a bit creepy, but Musk makes clear in his post that they require data collection for all reviewers because of a previous bad experience. So, the fact that data were being collected on speed, cabin temperature, battery charge %, and rated range remaining, was presumably known to all, especially Broder. Given that you know Big Brother Musk is watching, it seems odd to deliberately lie in a widely read publication like the Times.\nHaving read the original article, Musk’s response, and Broder’s rebuttal, one things is clear to me–there’s more than one way to see the data. The challenge here is that Broder had the car, but not the data, so had to rely on his personal recollection and notes. Musk has the data, but wasn’t there, and so has to rely on peering at graphs to interpret what happened on the trip.\nOne graph in particular was fascinating. Musk shows a periodic-looking segment of the speed graph and concludes\n\nInstead of plugging in the car, he drove in circles for over half a mile in a tiny, 100-space parking lot. When the Model S valiantly refused to die, he eventually plugged it in.\n\nBroder claims\n\nI drove around the Milford service plaza in the dark looking for the Supercharger, which is not prominently marked. I was not trying to drain the battery. (It was already on reserve power.) As soon as I found the Supercharger, I plugged the car in.\n\nOkay, so who’s right? Isn’t the data supposed to settle this?\nIn a few other cases in this story, the data support both people. In particular, it seems that there was some serious miscommunication between Broder and Tesla’s staff. I’m sure they also have recordings of those telephone calls too but they were not reproduced in Musk’s response.\nThe bottom line here, in my opinion, is that sometimes the data don’t tell all, especially “big data”. In the end, data are one thing, interpretation is another. Tesla had reams of black-box data from the car and yet some of the data still appear to be open to interpretation. My guess is that the data Tesla collects is not collected specifically to root out liars, and so is maybe not optimized for this purpose. Which leads to another key point about big data–they are often used “off-label”, i.e. not for the purpose they were originally designed.\nI read this story with interest because I actually think Tesla is a fascinating company that makes cool products (that sadly, I could never afford). This episode will surely not be the end of Tesla or of the New York Times, but it illustrates to me that simply “having the data” doesn’t necessarily give you what you want.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-02-17-sunday-datastatistics-link-roundup-2172013/",
    "title": "Sunday data/statistics link roundup (2/17/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-02-17",
    "categories": [],
    "contents": "\nThe Why Axis - discussion of important visualizations on the web. This is one I think a lot of people know about, but it is new to me. (via Thomas L. - p.s. I’m @leekgroup on Twitter, not @jtleek). \nThis paper says that people who “engage in outreach” (read: write blogs) tend to have higher academic output (hooray!) but that outreach itself doesn’t help their careers (boo!).\nIt is a little too late for this year, but next year you could make a Valentine with R.\n[ 1. The Why Axis - discussion of important visualizations on the web. This is one I think a lot of people know about, but it is new to me. (via Thomas L. - p.s. I’m @leekgroup on Twitter, not @jtleek). \nThis paper says that people who “engage in outreach” (read: write blogs) tend to have higher academic output (hooray!) but that outreach itself doesn’t help their careers (boo!).\nIt is a little too late for this year, but next year you could make a Valentine with R. 4.](http://emailcharter.org/) (via Rafa). This is pretty similar to my getting email responses from busy people. Not sure who scooped who. I’m still waiting for my to-do list app. Mailbox is close, but I still want actions to be multiple choice or yes/no or delegation rather than just snoozing emails for later.\nTop ten reasons not to share your code, and why you should anyway.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-02-15-interview-with-nick-chamandy-statistician-at-google/",
    "title": "Interview with Nick Chamandy, statistician at Google",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-02-15",
    "categories": [],
    "contents": "\n\n\n<strong>Nick Chamandy<\/strong>\n\n\n\n\n\n<a href=\"http://simplystatistics.org/2013/02/15/interview-with-nick-chamandy-statistician-at-google/person_photo/\" rel=\"attachment wp-att-1029\"><img class=\"alignnone size-full wp-image-1029\" alt=\"person_photo\" src=\"http://simplystatistics.org/wp-content/uploads/2013/02/person_photo.png\" width=\"190\" height=\"235\" /><\/a>\n\n\n\n\n\nNick Chamandy received his M.S. in statistics from the University of Chicago, his Ph.D. in statistics at McGill University and joined Google as a statistician. We talked to him about how he ended up at Google, what software he uses, and how big the Google data sets are. To read more interviews - check out our <a href=\"http://simplystatistics.org/interviews/\">interviews page<\/a>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<strong>SS: Which term applies to you: data scientist, statistician, computer scientist, or something else?<\/strong>\n\n\nNC: I usually use the term Statistician, but at Google we are also known as Data Scientists or Quantitative Analysts. All of these titles apply to some degree. As with many statisticians, my day to day job is a mixture of analyzing data, building models, thinking about experiments, and trying to figure out how to deal with large and complex data structures. When posting job opportunities, we are cognizant that people from different academic fields tend to use different language, and we don’t want to miss out on a great candidate because he or she comes from a non-statistics background and doesn’t search for the right keyword. On my team alone, we have had successful “statisticians” with degrees in statistics, electrical engineering, econometrics, mathematics, computer science, and even physics. All are passionate about data and about tackling challenging inference problems.\n\n\n<p>\n  <strong>SS: How did you end up at Google?<\/strong>\n<\/p>\n\n\nComing out of my PhD program at McGill, I was somewhat on the fence about the academia vs. industry decision. Ideally I wanted an opportunity that combined the intellectual freedom and stimulation of academia with the concreteness and real-world relevance of industrial problems. Google seemed to me at the time (and still does) to be by far the most exciting place to pursue that happy medium. The culture at Google emphasizes independent thought and idea generation, and the data are staggering in both size and complexity. That places us squarely on the “New Frontier” of statistical innovation, which is really motivating. I don’t know of too many other places where you can both solve a research problem and have an impact on a multi-billion dollar business in the same day.\n\n\n<p>\n  <strong>SS: Is your work related to the work you did as a Ph.D. student?<\/strong>\n<\/p>\n\n\nNC: Although I apply many of the skills I learned in grad school on a daily basis, my PhD research was on Gaussian random fields, with particular application to brain imaging data. The bulk of my work at Google is in other areas, since I work for the Ads Quality Team, whose goal is to quantify and improve the experience that users have interacting with text ads on the google.com search results page. Once in a while though, I come across data sets with a spatial or spatio-temporal component and I get the opportunity to leverage my experience in that area. Some examples are eye-tracking studies run by the user research lab (measuring user engagement on different parts of the search page), and click pattern data. These data sets typically violate many of the assumptions made in neuroimaging applications, notably smoothness and isotropy conditions. And they are predominantly 2-D applications, as opposed to 3-D or higher.\n\n\n<p>\n  <strong>What is your programming language of choice, R, Python or something else?  <\/strong>\n<\/p>\n\n\nI use R, and occasionally matlab, for data analysis. There is a large, active and extremely knowledgeable R community at Google. Because of the scale of Google data, however, R is typically only useful after a massive data aggregation step has been accomplished. Before that, the data are not only too large for R to handle, but are stored on many thousands of machines. This step is usually accomplished using the MapReduce parallel computing framework, and there are several Google-developed scripting languages that can be used for this purpose, including Go. We also have an interactive, ad hoc query language which can be applied to massive, “sharded” data sets (even those with a nested structure), and for which there is an R API. The engineers at Google have also developed a truly impressive package for massive parallelization of R computations on hundreds or thousands of machines. I typically use shell or python scripts for chaining together data aggregation and analysis steps into “pipelines”.\n\n\n<p>\n  <strong>SS: How big are the data sets you typically handle? Do you extract them yourself or does someone else extract them for you?<\/strong>\n<\/p>\n\n\nOur data sets contain billions of observations before any aggregation is done. Even after aggregating down to a more manageable size, they can easily consist of 10s of millions of rows, and on the order of 100s of columns. Sometimes they are smaller, depending on the problem of interest. In the vast majority of cases, the statistician pulls his or her own data – this is an important part of the Google statistician culture. It is not purely a question of self-sufficiency. There is a strong belief that without becoming intimate with the raw data structure, and the many considerations involved in filtering, cleaning, and aggregating the data, the statistician can never truly hope to have a complete understanding of the data. For massive and complex data, there are sometimes as many subtleties in whittling down to the right data set as there are in choosing or implementing the right analysis procedure. Also, we want to guard against creating a class system among data analysts – every statistician, whether BS, MS or PhD level, is expected to have competence in data pulling. That way, nobody becomes the designated data puller for a colleague. That said, we always feel comfortable asking an engineer or other statistician for help using a particular language, code library, or tool for the purpose of data-pulling. That is another important value of the Google culture – sharing knowledge and helping others get “unstuck”.\n\n\n<p>\n  <strong>Do you work collaboratively with other statisticians/computer scientists at Google? How do projects you work on get integrated into Google's products, is there a process of approval?<\/strong>\n<\/p>\n\n\nYes, collaboration with both statisticians and engineers is a huge part of working at Google. In the Ads Team we work on a variety of flavours of statistical problems, spanning but not limited to the following categories: (1) Retrospective analysis with the goal of understanding the way users and advertisers interact with our system; (2) Designing and running randomized experiments to measure the impact of changes to our systems; (3) Developing metrics, statistical methods and tools to help evaluate experiment data and inform decision-making; (4) Building models and signals which feed directly into our engineering systems. “Systems” here are things like the algorithms that decide which ads to display for a given query and context.\n\n\nClearly (2) and (4) require deep collaboration with engineers – they can make the changes to our production codebase which deploy a new experiment or launch a new feature in a prediction model. There are multiple engineering and product approval steps involved here, meant to avoid introducing bugs or features which harm the user experience. We work with engineers and computer scientists on (1) and (3) as well, but to a lesser degree. Engineers and computer scientists tend to be extremely bright and mathematically-minded people, so their feedback on our analyses, methodology and evaluation tools is pretty invaluable!\n\n\n<p>\n  <strong>Who have been good mentors to you during your career? Is there something in particular they did to help you?<\/strong>\n<\/p>\n\n\nI’ve had numerous important mentors at Google (in addition, of course, to my thesis advisors and professors at McGill). Largely they are statisticians who have worked in industry for a number of years and have mastered the delicate balance between deep-thinking a problem and producing something quick and dirty that can have an immediate impact. Grad school teaches us to spend weeks thinking about a problem and coming up with an elegant or novel methodology to solve it (sometimes without even looking at data). This process certainly has its place, but in some contexts a better outcome is to produce an unsophisticated but useful and data-driven answer, and then refine it further as needed. Sometimes the simple answer provides 80% of the benefit, and there is no reason to deprive the consumers of your method this short-term win while you optimize for the remaining 20%. By encouraging the “launch and iterate” mentality for which Google is well-known, my mentors have helped me produce analysis, models and methods that have a greater and more immediate impact.\n\n\n<p>\n  <strong>What skills do you think are most important for statisticians/data scientists moving into the tech industry?<\/strong>\n<\/p>\n\n\nBroadly, statisticians entering the tech industry should do so with an open mind. Technically speaking, they should be comfortable with heavy-tailed, poorly-behaved distributions that fail to conform to assumptions or data structures underlying the models taught in most statistics classes. They should not be overly attached to the ways in which they currently interact with data sets, since most of these don’t work for web-scale applications. They should be receptive to statistical techniques that require massive amounts of data or vast computing networks, since many tech companies have these resources at their disposal. That said, a statistician interested in the tech industry should not feel discouraged if he or she has not already mastered large-scale computing or the hottest programming languages. To me, it is less about what skills one must brush up on, and much more about a willingness to adaptively learn new skills and adjust one’s attitude to be in tune with the statistical nuances and tradeoffs relevant to this New Frontier of statistics. Statisticians in the tech industry will be well-served by the classical theory and techniques they have mastered, but at times must be willing to re-learn things that they have come to regard as trivial. Standard procedures and calculations can quickly become formidable when the data are massive and complex.\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-02-13-im-a-young-scientist-and-sequestration-will-hurt-me/",
    "title": "I'm a young scientist and sequestration will hurt me",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-02-13",
    "categories": [],
    "contents": "\nI’m a biostatistician. That means that I help scientists and doctors analyze their medical data to try to figure out new screening tools, new therapies, and new ways to improve patients’ health. I’m also a professor. I  spend a good fraction of my time teaching students about analyzing data in classes here at my university and online. Big data/data analysis is an area of growth for the U.S. economy and some have even suggested that there will be a critical shortage of trained data analysts.\nI have other responsibilities but these are the two biggies - teaching and research. I work really hard to be good at them because I’m passionate about education and I’m passionate about helping people. I’m by no means the only (relatively) young person with this same drive. I would guess this is a big reason why a lot of people become scientists. They want to contribute to both our current knowledge (research) and the future of knowledge (teaching).\nMy salary comes from two places - the students who pay tuition at our school and, to a much larger extent, the federal government’s research funding through the NIH. So you are paying my salary. The way that the NIH distributes that funding is through a serious and very competitive process. I submit proposals of my absolute best ideas, so do all the other scientists in the U.S., and they are evaluated by yet another group of scientists who don’t have a vested interest in our grants. This system is the reason that only the best, most rigorously vetted science is funded by taxpayer money.\nIt is very hard to get a grant. In 2012, between 7% and 16% of new projects were funded. So you have to write a proposal that is better than 84-93% of all other proposals being submitted by other really, really smart and dedicated scientists. The practical result is that it is already very difficult for a good young scientist to get a grant. The NIH recognizes this and implements special measures for new scientists to get grants, but it still isn’t easy by any means.\nSequestration will likely dramatically reduce the fraction of grants that get funded. Already on that website, the “payline” or cutoff for funding, has dropped from 10% of grants in 2012 to 6% in 2013 for some NIH institutes. If sequestration goes through, it will be worse - maybe a lot worse. The result is that it will go from being really hard to get individual grants to nearly impossible. If that happens, many young scientists like me won’t be able to get grants. No matter how passionate we are about helping people or doing the right thing, many of us will have to stop being researchers and scientists and get other jobs to pay the bills - we have to eat.\nSo if sequestration or other draconian cuts to the NIH go through, they will hurt me and other junior scientists like me. It will make it harder - if not impossible - for me to get grants. It will affect whether I can afford to educate the future generation of students who will analyze all the data we are creating. It will create dramatic uncertainty/difficulty in the lives of the young biological scientists I work with who may not be able to rely on funding from collaborative grants to the extent that I can. In the end, this will hurt me, it will hurt my other scientific colleagues, and it could dramatically reduce our competitiveness in science technology and mathematics (STEM) for years to come. Steven wrote this up beautifully on his blog.\nI know that these cuts will also affect the lives of many other people from all walks of life, not just scientists. So I hope that Congress will do the right thing and decide that hurting all these people isn’t worth the political points they will score - on both sides. Sequestration isn’t the right choice - it is the choice that was most politically expedient when people’s backs were against the wall.\nInstead of making dramatic, untested, and possibly disastrous cuts across the board for political reasons, let’s do what scientists and statisticians have been doing for years when deciding which drugs work and don’t. Let’s run controlled studies and evaluate the impact of budget cuts to different programs - as Ben Goldacre and his colleagues of so beautifully laid out in their proposal. That way we can bring our spending into line, but sensibly and based on evidence, rather than the politics of the moment or untested economic models not based on careful experimentation.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-02-10-sunday-datastatistics-link-roundup-2102013/",
    "title": "Sunday data/statistics link roundup (2/10/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-02-10",
    "categories": [],
    "contents": "\nAn article about how NBA teams have installed cameras that allow their analysts to collect information on every movement/pass/play that is performed in a game. I think the most interesting part for me would be how you would define features. They talk about, for example, how many times a  player drives. I wonder if they have an intern in the basement manually annotating those features or if they are using automatic detection algorithms (via Marginal Revolution).\nOur friend Florian jumps into the MIC debate. I haven’t followed the debate very closely, but I agree with Florian that if a theory paper  is published in a top journal, later falling back on heuristics and hand waving seems somewhat unsatisfying.\nAn opinion piece pushing the Journal of Negative Results in Biomedicine. If you can’t get your negative result in there, think about our P > 0.05 journal :-).\nThis has nothing to do with statistics/data but is a bit of nerd greatness. Run these commands from a terminal: traceroute 216.81.59.173.\nA data visualization describing the effectiveness of each state’s election administrations. I think that it is a really cool idea, although I’m not sure I understand the index. A couple of related plots are this one that shows distance to polling place versus election day turnout and this one that shows the same thing for early voting. It’s pretty interesting how dramatically different the plots are.\nPostdoc Sherri Rose writes about big data and junior statisticians at Stattrak. My favorite quote: “ We need to take the time to understand the science behind our projects before applying and developing new methods. The importance of defining our research questions will not change as methods progress and technology advances”.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-02-06-issues-with-reproducibility-at-scale-on-coursera/",
    "title": "Issues with reproducibility at scale on Coursera",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-02-06",
    "categories": [],
    "contents": "\nAs you know, we are big fans of reproducible research here at Simply Statistics. As you know, we are [big fans of reproducible research](http://simplystatistics.org/?s=reproducible+research) here at Simply Statistics. around the lack of reproducibility in the analyses performed by Anil Potti and subsequent fallout drove the importance of this topic home.\nSo when I started teaching a course on Data Analysis for Coursera, of course I wanted to focus on reproducible research. The students in the class will be performing two data analyses during the course. They will be peer evaluated using a rubric specifically designed for evaluating data analyses at scale. One of the components of the rubric was to evaluate whether the code people submitted with their assignments reproduced all the numbers in the assignment.\nUnfortunately, I just had to cancel the reproducibility component of the first data analysis assignment. Here are the things I realized while trying to set up the process that may seem obvious but weren’t to me when I was designing the rubric:\nSecurity I realized (thanks to a very smart subset of the students in the class who posted on the message boards) that there is a major security issue with exchanging R code and data files with each other. Even if they use only the data downloaded from the official course website, it is possible that people could use the code to try to hack/do nefarious things to each other. The students in the class are great and the probability of this happening is small, but with a class this size, it isn’t worth the risk.\nCompatibility I’m requiring that people use R for the course. Even so, people are working on every possible operating system, with many different versions of R . In this scenario, it is entirely conceivable for a person to write totally reproducible code that works on their machine but won’t work on a random peer-reviewers machine\nComputing Resources The range of computing resources used by people in the class is huge. Everyone from people using modern clusters to people running on a single old beat up laptop. Inefficient code on a fast computer is fine, but on a slow computer with little memory it could mean the difference between reproducibility and crashed computers.\nOverall, I think the solution is to run some kind of EC2 instance with a standardized set of software. That is the only thing I can think of that would be scalable to a class this size. On the other hand that would both be expensive, a pain to maintain, and would require everyone to run code on EC2.\nRegardless, it is a super interesting question. How do you do reproducibility at scale?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-02-03-sunday-datastatistics-link-roundup-232013/",
    "title": "Sunday data/statistics link roundup (2/3/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-02-03",
    "categories": [],
    "contents": "\nMy student, Hilary, wrote a post about how her name is the most poisoned in history. A poisoned name is a name that quickly loses popularity year over year. The post is awesome for the following reasons: (1) she is a good/funny writer and has lots of great links in the post, (2) she very clearly explains concepts that are widely used in biostatistics like relative risk, and (3) she took the time to try to really figure out all the trends she saw in the name popularity. I’m not the only one who thinks it is a good post, it was reprinted in New York Magazine and went viral this last week.\nIn honor of it being Super Bowl Sunday (go Ravens!) here is a post about the reasons why it often doesn’t make sense to consider the odds of an event retrospectively due to the Wyatt Earp effect. Another way to think about it is, if you have a big tournament with tons of teams - someone will win. But at the very beginning, any team had a pretty small chance of winning all the games and taking the championship. If we wait until some team wins and calculate their pre-tournament odds of winning, it will probably be small. (via David S.)\nA new article by Ben Goldacre in the NYT about unreported clinical trials. This is a major issue and Ben is all over it with his All Trials project. This is another reason we need a deterministic statistical machine. Don’t worry, we are working on building it.\nEven though it is Super Bowl Sunday, I’m still eagerly looking forward to spring and the real sport of baseball. Rafa sends along this link analyzing the effectiveness of patient hitters when they swing at a first strike. It looks like it is only a big advantage if you are an elite hitter.\nAn article in Wired on the importance of long data. The article talks about how in addition to cross-sectional big data, we might also want to be looking at data over time - possibly large amounts of time. I think the title is maybe a little over the top, but the point is well taken. It turns out this is something a bunch of my colleagues in imaging and environmental health  have been working on/talking about for a while. Longitudinal/time series big data seems like an important and wide-open field (via Nick R.).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-31-paste0-is-statistical-computings-most-influential-contribution-of-the-21st-century/",
    "title": "paste0 is statistical computing's most influential contribution of the 21st century",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-01-31",
    "categories": [],
    "contents": "\nThe day I discovered paste0 I literally cried. No more paste(bla,bla, sep=““). While looking through code written by a student who did not know about paste0 I started pondering about how many person hours it has saved humanity. So typing sep=”” takes about 1 second. We R users use paste about 100 times  a day and there are about 1,000,000 R users in the world. That’s over 3 person years a day! Next up read.table0 (who doesn’t want  as.is to be TRUE?).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-28-data-supports-claim-that-if-kobe-stops-ball-hogging-the-lakers-will-win-more/",
    "title": "Data supports claim that if Kobe stops ball hogging the Lakers will win more",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-01-28",
    "categories": [],
    "contents": "\nThe Lakers recently snapped a four game losing streak. In that game Kobe, the league leader in field goal attempts and missed shots, had a season low of 14 points but a season high of 14 assists. This makes sense to me since Kobe shooting less means more efficient players are shooting more. Kobe has a lower career true shooting % than Gasol, Howard and Nash (ranked 17, 3 and 2 respectively). Despite this he takes more than 1/4 of the shots. Commentators usually praise top scorers no matter what, but recently they The Lakers recently snapped a four game losing streak. In that game Kobe, the league leader in field goal attempts and missed shots, had a season low of 14 points but a season high of 14 assists. This makes sense to me since Kobe shooting less means more efficient players are shooting more. Kobe has a lower career true shooting % than Gasol, Howard and Nash (ranked 17, 3, and 2 respectively). Despite this he takes more than 1/4 of the shots. Commentators usually praise top scorers no matter what, but recently they noticed that the Lakers are 6-22 when Kobe has more than 19 field goal attempts and 12-3 in the rest of the games.\n\nThis graph shows score differential versus % of shots taken by Kobe* . Linear regression suggests that an increase of 1% in % of shots taken by Kobe results in a drop of 1.16 points (+/- 0.22)  in score differential. It also suggests that when Kobe takes 15% of the shots, the Lakers win by an average of about 10 points, when he takes 30% (not a rare occurrence) they lose by an average of about 5. Of course we should not take this regression analysis to seriously but it’s hard to ignore the fact that when Kobe takes less than 23 23.25% of the shots the Lakers are 13-1.\nI suspect that this relationship is not unique to Kobe and the Lakers. In general, teams with  a more balanced attack probably do better. Testing this could be a good project for Jeff’s class.\n* I approximated shots taken as field goal attempts + floor(0.5 x Free Throw Attempts).\nData is here.\nUpdate: Commentator Sidney fixed some entires in the  data file. Data and plot updated.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-27-sunday-datastatistics-link-roundup-1272013/",
    "title": "Sunday data/statistics link roundup (1/27/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-01-27",
    "categories": [],
    "contents": "\nWisconsin is decoupling the education and degree granting components of education. This means if you take a MOOC like mine, Brian’s or Roger’s and there is an equivalent class to pass at Wisconsin, you can take the exam and get credit. This is big. (via Rafa)\n1. Wisconsin is d[ecoupling the education and degree granting components](http://marginalrevolution.com/marginalrevolution/2013/01/the-wisconsin-revolution.html) of education. This means if you take a MOOC like [mine](https://www.coursera.org/course/dataanalysis), [Brian’s](https://www.coursera.org/course/biostats) or [Roger’s](https://www.coursera.org/course/compdata) and there is an equivalent class to pass at Wisconsin, you can take the exam and get credit. This is big. (via Rafa) 2.  is a really cool MLB visualisation done with d3.js and Crossfilter. It was also prototyped in R, which makes it even cooler. (via Rafa via Chris V.)\nHarvard is encouraging their professors to only publish in open access journals and to resign from closed access journals. This is another major change and bodes well for the future of open science (again via Rafa - noticing a theme this week?).\nThis deserves a post all to itself, but Greece is prosecuting a statistician for analyzing data in a way that changed their deficit figure. I wonder what the folks at the International Year of Statistics think about that? (via Alex N.)\nBe on the twitters at 10:30AM Tuesday and follow the hashtag #jhsph753 if you want to hear all the crazy stuff I tell my students when I’m running on no sleep.\nThomas at StatsChat is fed up with Nobel correlations. Although I’m still partial to the length of country name association.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-25-my-advanced-methods-class-is-now-being-live-tweeted/",
    "title": "My advanced methods class is now being live-tweeted",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-01-25",
    "categories": [],
    "contents": "\nA student in my class is going to be live-tweeting my (often silly/controversial) comments in the advanced/Ph.D. data analysis and methods class I’m teaching here at Hopkins. The hashtag is #jhsph753 and the class runs from 10:30am to 12:00PM EST. Check it out here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-24-why-i-disagree-with-andrew-gelmans-critique-of-my-paper-about-the-rate-of-false-discoveries-in-the-medical-literature/",
    "title": "Why I disagree with Andrew Gelman's critique of my paper about the rate of false discoveries in the medical literature",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-01-24",
    "categories": [],
    "contents": "\nWith a colleague, I wrote a paper titled, “Empirical estimates suggest most published medical research is true”  which we quietly posted to ArXiv a few days ago. I posted to the ArXiv in the interest of open science and because we didn’t want to delay the dissemination of our approach during the long review process. I didn’t email anyone about the paper or talk to anyone about it, except my friends here locally.\nI underestimated the internet. Yesterday, the paper was covered in this piece on the MIT Tech review. That exposure was enough for the paper to appear in a few different outlets. I’m totally comfortable with the paper, but was not anticipating all of the attention so quickly.\nIn particular, I was a little surprised to see it appear on Andrew Gelman’s blog with the disheartening title, “I don’t believe the paper, “Empirical estimates suggest most published medical research is true.” That is, most published medical research may well be true, but I’m not at all convinced by the analysis being used to support this claim.” I responded briefly this morning to his post, but then had to run off to teach class. After thinking about it a little more, I realized I have some objections to his critique.\nHis main criticisms of our paper are: (1) with type I/type II errors instead of type S versus type M errors (paragraph 2), (2) that we didn’t look at replication, we performed inference (paragraph 4), (3) that there is p-value hacking going on (paragraph 4), and (4) he thinks that our model does not apply because p-value hacking my change the assumptions underlying this model in genomics.\nI will handle each of these individually:\nThis is primarily semantics. Andrew is concerned with interesting/uninteresting with his Type S and Type M Errors. We are concerned with true/false positives as defined by type I and type II errors (and a null hypothesis). You might believe that the null is never true - but then by the standards of the original paper all published research is true. Or you might say that a non-null result might have an effect size too small to be interesting - but the framework being used here is hypothesis testing and we have stated how we defined a true positive in that framework explicitly.  We define the error rate by the rate of classifying thing as null when they should be classified as alternative and vice versa. We then estimate the false discovery rate, under the framework used to calculate those p-values. So this is not a criticism of our work with evidence, rather it is a stated difference of opinion about the philosophy of statistics not supported by conclusive data.\nGelman says he originally thought we would follow up specific p-values to see if the results replicated and makes that a critique of our paper. That would definitely be another approach to the problem. Instead, we chose to perform statistical inference using justified and widely used statistical techniques. Others have taken the replication route, but of course that approach too would be fraught with difficulty - are the exact conditions replicable (e.g. for a clinical trial), can we sample from the same population (if it has changed or is hard to sample), and what do we mean by replicates (would two p-values less than 0.05 be convincing?). This again is not a criticism of our approach, but a statement of another, different analysis Gelman was wishing to see.\n(3)-(4) Gelman states, “You don’t have to be Uri Simonsohn to know that there’s a lot of p-hacking going on.” Indeed Uri Samuelson wrote a paper where he talks about the potential for p-value hacking. He does not collect data from real experiments/analyses, but uses simulations, theoretical arguments, and prospective experiments designed to show specific problems. While these arguments are useful and informative, it gives no indication of the extent of p-value hacking in the medical literature. So this argument is made on the basis of a supposition by Gelman that this happens broadly, rather than on data.\nMy objection to his criticism is that his critiques are based primarily on philosophy (1), a wish that we had done the study a different way (2), and assumptions about the way science works with only anecdotal evidence (3-4).\nOne thing you could very reasonably argue is how sensitive our approach is to violations of our assumptions (which Gelman implied with criticisms 3-4). To address this,  my co-author and I have now performed a simulation analysis. In the first simulation, we considered a case where every p-value less than 0.05 was reported and the p-values were uniformly distributed, just as our assumptions would state. We then plot our estimates of the swfdr versus the truth. Here our estimator works pretty well.\n \n\nWe also simulate a pretty serious p-value hacking scenario where people report only the minimum p-value they observe out of 20 p-values. Here our assumption of uniformity is strongly violated. But we still get pretty accurate estimates of the swfdr for the range of values (14%) we report in our paper.\n\nSince I recognize this is only a couple of simulations, I have also put the code up on Github with the rest of our code for the paper so other people can test it out.\nWhether you are convinced by Gelman, or convinced by my response, I agree with him that it is pretty unlikely that “most published research is false” so I’m glad our paper is at least bringing that important point up. I also hope that by introducing a new estimator of the science-wise fdr we inspire more methodological development and that philosophical criticisms won’t prevent people from looking at the data in new ways.\n \n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-23-statisticians-and-computer-scientists-if-there-is-no-code-there-is-no-paper/",
    "title": "Statisticians and computer scientists - if there is no code, there is no paper",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-01-23",
    "categories": [],
    "contents": "\nI think it has been beat to death that the incentives in academia lean heavily toward producing papers and less toward producing/maintaining software. There are people that are way, way more knowledgeable than me about building and maintaining software. For example, Titus Brown hit a lot of the key issues in his interview. The open source community is also filled with advocates and researchers who know way more about this than I do.\nThis post is more about my views on changing the perspective of code/software in the data analysis community. I have been frustrated often with statisticians and computer scientists who write papers where they develop new methods and seem to demonstrate that those methods blow away all their competitors. But then no software is available to actually test and see if that is true. Even worse, sometimes I just want to use their method to solve a problem in our pipeline, but I have to code it from scratch!\nI have also had several cases where I emailed the authors for their software and they said it “wasn’t fit for distribution” or they “don’t have code” or the “code can only be run on our machines”. I totally understand the first and last, my code isn’t always pretty (I have zero formal training in computer science so messy code is actually the most likely scenario) but I always say, “I’ll take whatever you got and I’m willing to hack it out to make it work”. I often still am turned down.\nSo I have a new policy when evaluating CV’s of candidates for jobs, or when I’m reading a paper as a referee. If the paper is about a new statistical method or machine learning algorithm and there is no software available for that method - I simply mentally cross it off the CV. If I’m reading a data analysis and there isn’t code that reproduces their analysis - I mentally cross it off. In my mind, new methods/analyses without software are just vapor ware. Now, you’d definitely have to cross a few papers off my CV, based on this principle. I do that. But I’m trying really hard going forward to make sure nothing gets crossed off.\nIn a future post I’ll talk about the new issue I’m struggling with - maintaing all that software I’m creating.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-20-sunday-datastatistics-link-roundup-1202013/",
    "title": "Sunday data/statistics link roundup (1/20/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-01-20",
    "categories": [],
    "contents": "\nThis might be short. I have a couple of classes starting on Monday. The first is our 1. This might be short. I have a couple of classes starting on Monday. The first is our class. This is one of my favorite classes to teach, our Ph.D. students are pretty awesome and they always amaze me with what they can do. The other is my Coursera debut in Data Analysis. We are at about 88,000 enrolled. Tell your friends, maybe we can make it an even 100k! In related news, some California schools are 1. This might be short. I have a couple of classes starting on Monday. The first is our [ 1. This might be short. I have a couple of classes starting on Monday. The first is our](http://www.jhsph.edu/courses/course/140.753/01/2012/16424/) class. This is one of my favorite classes to teach, our Ph.D. students are pretty awesome and they always amaze me with what they can do. The other is my Coursera debut in [Data Analysis](https://www.coursera.org/course/dataanalysis). We are at about 88,000 enrolled. Tell your friends, maybe we can make it an even 100k! In related news, some California schools are with offering credit for online courses. (via Sherri R.)\nSome interesting numbers on why there aren’t as many “gunners” in the NBA - players who score a huge number of points.  I love the talk about hustling, rotating team defense. I have always enjoyed watching good defense more than good offense. It might not be the most popular thing to watch, but seeing the Spurs rotate perfectly to cover the open man is a thing of athletic beauty. My Aggies aren’t too bad at it either…(via Rafa).\nA really interesting article suggesting that nonsense math can make arguments seem more convincing to non-technical audiences. This is tangentially related to a previous study which showed that more equations led to fewer citations in biology articles. Overall, my take home message is that we don’t need less equations necessarily; we need to elevate statistical/quantitative literacy to the importance of reading literacy. (via David S.)\nThis has been posted elsewhere, but a reminder to send in your statistical stories for the 365 stories of statistics.\nAutomatically generate a postmodernism essay. Hit refresh a few times. It’s pretty hilarious. It reminds me a lot of this article about statisticians. Here is the technical paper describing how they simulate the essays. (via Rafa)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-18-comparing-online-and-in-class-outcomes/",
    "title": "Comparing online and in-class outcomes",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-01-18",
    "categories": [],
    "contents": "\nMy colleague John McGready has just published a study he conducted comparing the outcomes of students in the online and in-class versions of his Statistical Reasoning in Public Health class that he teaches here in the fall. In this class the online and in-class portions are taught concurrently, so it’s basically one big class where some people are not in the building. Everything is the same for both groups–quizzes, tests, homework, instructor, lecture notes. From the article:\n\n\nThe on-campus version employs twice-weekly 90 minute live lectures. Online students view pre-recorded narrated versions of the same materials. Narrated lecture slides are made available to on-campus students.\n\nThe on-campus section has 5 weekly office hour sessions. Online students communicate with the course instructor asynchronously via email and a course bulletin board. The instructor communicates with online students in real time via weekly one-hour online sessions. Exams and quizzes are multiple choice. In 2005, on-campus students took timed quizzes and exams on paper in monitored classrooms. Online students took quizzes via a web-based interface with the same time limits. Final exams for the online students were taken on paper with a proctor.\n\nSo how did the two groups fair in their final grades? Pretty much the same. First off, the two groups of students were not the same. Online students were 8 years older on average, more likely to have an MD degree, and more likely to be male. Final exam scores between online and in-class groups differed by -1.2 (out of 100, online group was lower) and after adjusting for student characteristics they differed by -1.5. In both cases, the difference was not statistically significant.\nThis was not a controlled trial and so there are possibly some problems with unmeasured confounding given that the populations appeared fairly different. It would be interesting to think about a study design that might allow a measure of control or perhaps get a better measure of the difference between online and on-campus learning. But the logistics and demographics of the students would seem to make this kind of experiment challenging.\nHere’s the best I can think of right now: Take a large class (where all students are on-campus) and get a classroom that can fit roughly half the number of students in the class. Then randomize half the students to be in-class and the other half to be online up until the midterm. After the midterm cross everyone over so that the online group comes into the classroom and the in-class group goes online to take the final. It’s not perfect–One issue is that course material tends to get harder as the term goes on and it may be that the “easier” material is better learned online and the harder material is better learned on-campus (or vice versa). Any thoughts?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-16-r-package-meme/",
    "title": "R package meme",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-01-16",
    "categories": [],
    "contents": "\nI just got this from a former student who is working on a project with me:\n\nAwesome.\n \n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-16-review-of-r-graphics-cookbook-by-winston-chang/",
    "title": "Review of R Graphics Cookbook by Winston Chang",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-01-16",
    "categories": [],
    "contents": "\nI just got a copy of Winston Chang’s book R Graphics Cookbook, published by O’Reilly Media. This book follows now a series of O’Reilly books on R, including an R Cookbook. Winston Chang is a graduate student at Northwestern University but is probably better known to R users as an active member of the ggplot2 mailing list and an active contributor to the ggplot2 source code.\nThe book has a typical cookbook format. After some preliminaries about how to install R packages and how to read data into R (Chapter 1), he quickly launches into exploratory data analysis and graphing. The basic outline of each section is:\nStatement of problem (“You want to make a histogram”)\nSolution: If you can reasonably do it with base R graphics, here’s how you do it. Oh, and here’s how you do it in ggplot2. Notice how it’s better? (He doesn’t actually say that. He doesn’t have to.)\nDiscussion: This usually revolves around different options that might be set or alternative approaches.\nSee also: Other recipes in the book.\nInterestingly, nowhere in the book is the lattice package mentioned (except in passing). But I suppose that’s because ggplot2 pretty much supersedes anything you might want to do in the lattice package. Recently, I’ve been wondering what the future of the lattice package is given that it doesn’t seem to me to be going under very active development. But I digress….\nOverall, the book is great. I learned quite a few things just in my initial read of the book and as I dug in a bit more there were some functions that I was not familiar with. Much of the material is straight up ggplot2 stuff so if you’re an expert there you probably won’t get a whole lot more. But my guess is that most are not experts and so will be able to get something out of the book.\nThe meat of the book covers a lot of different plotting techniques, enough to make your toolbox quite full. If you pick up this book and think something is missing, my guess is that you’re making some pretty esoteric plots. I enjoyed the few sections on specifying colors as well as the recipes on making maps (one of ggplot2’s strong points). I wish there were more map recipes, but hey, that’s just me.\nTowards the end there’s a nice discussion of graphics file formats (PDF, PNG, WMF, etc.) and the advantages and disadvantages of each (Chapter 14: Output for Presentation). I particularly enjoyed the discussion of fonts in R graphics since I find this to be a fairly confusing aspect of R, even for seasoned users.\nThe book ends with a series of recipes related to data manipulation. It’s funny how many recipes there are about modifying factor variables, but I guess this is just a function of how annoying it is to modify factor variables. There’s also some highlighting of the plyr and reshape2 packages.\nUltimately, I think this is a nice complement to Hadley Wickham’s _ggplot2 _as most of the recipes focus on implementing plots in ggplot2. I don’t think you necessarily need to have a deep understanding of ggplot2 in order to use this book (there are some details in an appendix), but some people might want to grab Hadley’s book for more background. In fact, this may be a better book to use to get started with ggplot2 simply because it focuses on specific applications. I kept thinking that if the book had been written using base graphics only, it’d probably have to be 2 or 3 times longer just to fit all the code in, which is a testament to the power and compactness of the ggplot2 approach.\nOne last note: I got the e-book version of the book, but I would recommend the paper version. With books like these, I like to flip around constantly (since there’s no need to read it in a linear fashion) and I find e-readers like iBooks and Kindle Reader to be not so good at this.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:03:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-14-welcome-to-the-smog-ocalypse/",
    "title": "Welcome to the Smog-ocalypse",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2013-01-14",
    "categories": [],
    "contents": "\n\nRecent reports of air pollution levels out of Beijing are very very disturbing. Levels of fine particulate matter (PM2.5, or PM less than 2.5 microns in diameter) have reached unprecedented levels. So high are the levels that even the official media are allowed to mention it.\nHere is a photograph of downtown Beijing during the day (Thanks to Sarah E. Burton for the photograph). Hourly levels of PM2.5 hit over 900 micrograms per cubic meter in some parts of the city and 24-hour average levels (the basis for most air quality standards) reached over 500 micrograms per cubic meter. Just for reference, the US national ambient air quality standard for the 24-hour average level of PM2.5 is 35 micrograms per cubic meter.\nBelow is a plot of the PM2.5 data taken from the US Embassy’s rooftop monitor.\n\nThe solid circles indicate the 24-hour average for the day. The red line is the median of the daily averages for the time period in the plot (about 6 weeks) and the dotted blue line is the US 24-hour national ambient air quality standard. The median for the period was about 91 micrograms per cubic meter.\nFirst, it should be noted that a “typical” day of 91 micrograms per cubic meter is still crazy. But suppose we take 91 to be a typical day. Then in a city like Beijing, which has about 20 million people, if we assume that about 700 people die on a typical day, then the last 5 days alone would experience about 307 excess deaths from all causes. I get this from using a rough estimate of a 0.3% increase in all-cause mortality per 10 microgram per cubic meter increase in PM2.5 levels (studies from China and the US tend to report risks in roughly this area). The 700 deaths per day number is a fairly back-of-the-envelope number that I got simply using comparisons to other major cities.  Numbers for things like excess hospitalizations will be higher because both the risks and the baselines are higher. For example, in the US, we estimate about a 1.28% increase in heart failure hospitalization for a 10 microgram per cubic meter increase in PM2.5.\nIf you like, you can also translate current levels to numbers of cigarettes smoked. If you assume a typical adult inhales about 18-20 cubic meters of air per day, then in the last 5 days, the average Beijinger smoked about 3 cigarettes just by getting out of bed in the morning.\nLastly, I want to point to a nice series of photos that the Guardian has collected on the (in)famous London Fog of 1952. Although the levels were quite a bit worse back then (about 2-3 times worse, if you can believe it), the photos bear a striking resemblance to today’s Beijing.\nAt least in the US, the infamous smog episodes that occurred regularly only 60 years ago are pretty much non-existent. But in many places around the world, “crazy bad” air pollution is part of everyday life.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-13-sunday-datastatistics-link-roundup-1132012/",
    "title": "Sunday data/statistics link roundup (1/13/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-01-13",
    "categories": [],
    "contents": "\nThese are some great talks. But definitely watch Michael Eisen’s talk on E-biomed and the history of open access publication. This is particularly poigniant in light of Aaron Swartz’s tragic suicide. It’s also worth checking out the twitter hashtag #pdftribute .\nAn awesome flowchart before a talk given by the creator of the R twotuorials. Roger gets a shoutout (via civilstat).\nThis blog selects a position at random on the planet earth every day and posts the picture taken closest to that point. Not much about the methodology on the blog, but totally fascinating and a clever idea.\nA set of data giving a “report card” for each state on how that state does in improving public education for students. I’m not sure I believe the grades, but the underlying reports look interesting.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-11-nsf-should-understand-that-statistics-in-not-mathematics/",
    "title": "NSF should understand that Statistics is not Mathematics",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-01-11",
    "categories": [],
    "contents": "\nNSF has realized that the role of Statistics is growing in all areas of science and engineering and NSF has realized that the role of Statistics is growing in all areas of science and engineering and  to examine the current structure of support of the statistical sciences.  As Roger explained in August, the NSF is divided into directorates composed of divisions. Statistics is in the Division of Mathematical Sciences (DMS) within the Directorate for Mathematical and Physical Sciences. Within this division it is a Disciplinary Research Program along with Topology, Geometric Analysis, etc.. To statisticians this does not make much sense, and my first thought when asked for recommendations was that we need a proper division. But the committee is seeking out recommendations that\n\n[do] not include renaming of the Division of Mathematical Sciences. Particularly desired are recommendations that can be implemented within the current divisional and directorate structure of NSF; Foundation (NSF) and to provide recommendations for NSF to consider.\n\nThis clarification is there because former director NSF has realized that the role of Statistics is growing in all areas of science and engineering and [NSF has realized that the role of Statistics is growing in all areas of science and engineering and ](http://www.nsf.gov/attachments/124926/public/Request_to_form_MPSAC_Subcommittee_StatsNSF_8-15-2012_Final.pdf) to examine the current structure of support of the statistical sciences.  As [Roger explained](http://simplystatistics.org/2012/08/21/nsf-recognizes-math-and-statistics-are-not-the-same/) in August, the NSF is divided into directorates composed of divisions. Statistics is in the Division of Mathematical Sciences (DMS) within the Directorate for Mathematical and Physical Sciences. Within this [division](http://www.nsf.gov/div/index.jsp?div=dms) it is a Disciplinary Research Program along with Topology, Geometric Analysis, etc.. To statisticians this does not make much sense, and my first thought when asked for recommendations was that we need a proper division. But the committee is seeking out recommendations that names to “Division of Mathematical and Statistical Sciences”.  The NSF shot down this idea and listed this as one of the reasons:\n\nIf the name change attracts more proposals to the Division from the statistics community, this could draw funding away from other subfields\n\nSo NSF does not want to take away from the other math programs and this is understandable given the current levels of research funding for Mathematics. But this being the case, I can’t really think of a recommendation other than giving Statistics it’s own division or give data related sciences their own directorate. Increasing support for the statistical sciences means increasing funding. You secure the necessary funding either by asking congress for a bigger budget (good luck with that) or by cutting from other divisions, not just Mathematics. A new division makes sense not only in practice but also in principle because Statistics is not Mathematics.\nStatistics is analogous to other disciplines that use mathematics as a fundamental language, like Physics, Engineering, and Computer Science. But like those disciplines, Statistics contributes separate and fundamental scientific knowledge. While the field of applied mathematics tries to explain the world with deterministic equations, Statistics takes a dramatically different approach. In highly complex systems, such as the weather, Mathematicians battle LaPlace’s demon and struggle to explain nature using mathematics derived from first principles. Statisticians accept  that deterministic approaches are not always useful and instead develop and rely on random models. These two approaches are both important as demonstrated by the improvements in meteorological predictions  achieved once data-driven statistical models were used to compliment deterministic mathematical models.\nAlthough Statisticians rely heavily on theoretical/mathematical thinking, another important distinction from Mathematics is that advances in our field are almost exclusively driven by empirical work. Statistics always starts with a specific, concrete real world problem: we thrive in Pasteur’s quadrant. Important theoretical work that generalizes our solutions always follows. This approach, built mostly by basic researchers, has yielded some of the most useful concepts relied upon by modren science: the p-value, randomization, analysis of variance, regression, the proportional hazards model, causal inference, Bayesian methods, and the Bootstrap, just to name a few examples. These ideas were instrumental in the most important genetic discoveries, improving agriculture, the inception of the empirical social sciences, and revolutionizing medicine via randomized clinical trials. They have also fundamentally changed the way we abstract quantitative problems from real data.\nThe 21st century brings the era of big data, and distinguishing Statistics from Mathematics becomes more important than ever.  Many areas of science are now being driven by new measurement technologies. Insights are being made by discovery-driven, as opposed to hypothesis-driven, experiments. Although testing hypotheses developed theoretically will of course remain important to science, it is inconceivable to think that, just like Leeuwenhoek became the father of microbiology by looking through the microscope without theoretical predictions, the era of big data will enable discoveries that we have not yet even imagined. However, it is naive to think that these new datasets will be free of noise and unwanted variability. Deterministic models alone will almost certainly fail at extracting useful information from these data just like they have failed at predicting complex systems like the weather. The advancement in science during the era of big data that the NSF wants to see will only happen if the field that specializes in making sense of data is properly defined as a separate field from Mathematics and appropriately supported.\nAddendum: On a related note, NIH just announced that they plan to recruit a new senior scientific position: the Associate Director for Data Science\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-10-the-landscape-of-data-analysis/",
    "title": "The landscape of data analysis",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-01-10",
    "categories": [],
    "contents": "\nI have been getting some questions via email, LinkedIn, and Twitter about the content of the Data Analysis class I will be teaching for Coursera. Data Analysis and Data Science mean different things to different people. So I made a video describing how Data Analysis fits into the landscape of other quantitative classes here:\nHere is the corresponding presentation. I also made a tentative list of topics we will cover, subject to change at the instructor’s whim. Here it is:\nThe structure of a data analysis  (steps in the process, knowing when to quit, etc.)\nTypes of data (census, designed studies, randomized trials)\nTypes of data analysis questions (exploratory, inferential, predictive, etc.)\nHow to write up a data analysis (compositional style, reproducibility, etc.)\nObtaining data from the web (through downloads mostly)\nLoading data into R from different file types\nPlotting data for exploratory purposes (boxplots, scatterplots, etc.)\nExploratory statistical models (clustering)\nStatistical models for inference (linear models, basic confidence intervals/hypothesis testing)\nBasic model checking (primarily visually)\nThe prediction process\nStudy design for prediction\nCross-validation\nA couple of simple prediction models\nBasics of simulation for evaluating models\nWays you can fool yourself and how to avoid them (confounding, multiple testing, etc.)\nOf course that is a ton of material for 8 weeks and so obviously we will be covering just the very basics. I think it is really important to remember that being a good Data Analyst is like being a good surgeon or writer. There is no such thing as a prodigy in surgery or writing, because it requires long experience, trying lots of things out, and learning from mistakes. I hope to give people the basic information they need to get started and point to resources where they can learn more. I also hope to give them a chance to practice a couple of times some basics and to learn that in data analysis the first goal is to “do no harm”.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-08-by-introducing-competition-open-online-education-will-improve-teaching-at-top-universities/",
    "title": "By introducing competition open online education will improve teaching at top universities",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-01-08",
    "categories": [],
    "contents": "\nIt is no secret that faculty evaluations at top universities weigh research much more than teaching. This is not surprising given that, among other reasons,  global visibility comes from academic innovation (think Nobel Prizes) not classroom instruction. Come promotion time the peer review system carefully examines your publication record and ability to raise research funds. External experts within your research area are asked if you are a leader in the field. Top universities maintain their status by imposing standards that lead to a highly competitive environment in which only the most talented researchers survive.\nHowever, the assessment of teaching excellence is much less stringent. Unless they reveal utter incompetence, teaching evaluations are practically ignored; especially if you have graduated numerous PhD students. Certainly, outside experts are not asked about your teaching. This imbalance in incentives explains why faculty use research funding to buy-out of teaching and why highly recruited candidates negotiate low teaching loads.\nTop researchers end up at top universities but being good at research does not necessarily mean you are a good teacher. Furthermore,  the effort required to be a competitive researcher leaves limited time for class preparation. To make matters worse, within a university, faculty have a monopoly on the classes they teach. With few incentives and  practically no competition it is hard to believe that top universities are doing the best they can when it comes to classroom instruction. By introducing competition, MOOCs might change this.\nTo illustrate, say you are a chair of a soft money department in 2015. Four of your faculty receive 25% funding to teach the big Stat 101 class and your graduate program’s three main classes. But despite being great researchers these four are mediocre teachers. So why are they teaching if 1) a MOOC exists for each of these classes and 2) these professors can easily cover 100% of their salary with research funds. As chair, not only do you wonder why not let these four profs  focus on what they do best, but also why your department is not creating MOOCs and getting global recognition for it. So instead of hiring 4 great researchers that are mediocre teachers why not hire (for the same cost) 4 great researchers (fully funded by grants) and 1 great teacher (funded with tuition $)? I think in the future tenure track positions will be divided into top researchers doing mostly research and top teachers doing mostly classroom teaching and MOOC development. Because top universities will feel the pressure to compete and develop the courses that educate the world, there will be no room for mediocre teaching.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-06-sunday-datastatistics-link-roundup-162013/",
    "title": "Sunday data/statistics link roundup (1/6/2013)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-01-06",
    "categories": [],
    "contents": "\nNot really statistics, but this is an interesting article about how rational optimization by individual actors does not always lead to an optimal solutiohn. Related, ere is the coolest street sign I think I’ve ever seen, with a heatmap of traffic density to try to influence commuters.\nAn interesting paper that talks about how clustering is only a really hard problem when there aren’t obvious clusters. I was a little disappointed in the paper, because it defines the “obviousness” of clusters only theoretically by a distance metric. There is very little discussion of the practical distance/visual distance metrics people use when looking at clustering dendograms, etc.\nA post about the two cultures of statistical learning and a related post on how data-driven science is a failure of imagination. I think in both cases, it is worth pointing out that the only good data science is good science - i.e. it seeks to answer a real, specific question through the scientific method. However, I think for many modern scientific problems it is pretty naive to think we will be able to come to a full, mechanistic understanding complete with tidy theorems that describe all the properties of the system. I think the real failure of imagination is to think that science/statistics/mathematics won’t change to tackle the realistic challenges posed in solving modern scientific problems.\nA graph that shows the incredibly strong correlation ( > 0.99!) between the growth of autism diagnoses and organic food sales. Another example where even really strong correlation does not imply causation.\nThe Buffalo Bills are going to start an advanced analytics department (via Rafa and Chris V.), maybe they can take advantage of all this free play-by-play data from years of NFL games.\nA prescient interview with Isaac Asimov on learning, predicting the Kahn Academy, MOOCs and other developments in online learning (via Rafa and Marginal Revolution).\nThe statistical software signal - what your choice of software says about you. Just another reason we need a deterministic statistical machine.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-04-does-nih-fund-innovative-work-does-nature-care-about-publishing-accurate-articles/",
    "title": "Does NIH fund innovative work?  Does Nature care about publishing accurate articles?",
    "description": {},
    "author": [
      {
        "name": "Steven Salzberg",
        "url": {}
      }
    ],
    "date": "2013-01-04",
    "categories": [],
    "contents": "\nEditor’s Note: In a recent post we disagreed with a Nature article claiming that NIH doesn’t support innovation. Our colleague Steven Salzberg actually looked at the data and wrote the guest post below. \nNature published an article last month with the provocative title “Research grants: Conform and be funded.”  The authors looked at papers with over 1000 citations to find out whether scientists “who do the most influential scientific work get funded by the NIH.”  Their dramatic conclusion, widely reported, was that only 40% of such influential scientists get funding.\nDramatic, but wrong.  I re-analyzed the authors’ data and wrote a letter to Nature, which was published today along with the authors response, which more or less ignored my points.  Unfortunately, Nature cut my already-short letter in half, so what readers see in the journal omits half my argument.  My entire letter is published here, thanks to my colleagues at Simply Statistics.  I titled it “NIH funds the overwhelming majority of highly influential original science results,” because that’s what the original study should have concluded from their very own data.  Here goes:\n\nTo the Editors:\n\n\nIn their recent commentary, “Conform and be funded,” Joshua Nicholson and John Ioannidis claim that “too many US authors of the most innovative and influential papers in the life sciences do not receive NIH funding.”  They support their thesis with an analysis of 200 papers sampled from 700 life science papers with over 1,000 citations.  Their main finding was that only 40% of “primary authors” on these papers are PIs on NIH grants, from which they argue that the peer review system “encourage[s] conformity if not mediocrity.”\n\n\nWhile this makes for an appealing headline, the authors’ own data does not support their conclusion.  I downloaded the full text for a random sample of 125 of the 700 highly cited papers [data available upon request].  A majority of these papers were either reviews (63), which do not report original findings, or not in the life sciences (17) despite being included in the authors’ database.  For the remaining 45 papers, I looked at each paper to see if the work was supported by NIH.  In a few cases where the paper did not include this information, I used the NIH grants database to determine if the corresponding author has current NIH support.  34 out of 45 (75%) of these highly-cited papers were supported by NIH.  The 11 papers not supported included papers published by other branches of the U.S. government, including the CDC and the U.S. Army, for which NIH support would not be appropriate.  Thus, using the authors’ own data, one would have to conclude that NIH has supported a large majority of highly influential life sciences discoveries in the past twelve years.\n\n\nThe authors – and the editors at Nature, who contributed to the article – suffer from the same biases that Ioannidis himself has often criticized.  Their inclusion of inappropriate articles and especially the choice to require that both the first and last author be PIs on an NIH grant, even when the first author was a student, produced an artificially low number that misrepresents the degree to which NIH supports innovative original research.\n\nIt seems pretty clear that Nature wanted a headline about how NIH doesn’t support innovation, and Ioannidis was happy to give it to them.  Now, I’d love it if NIH had the funds to support more scientists, and I’d also be in favor of funding at least some work retrospectively - based on recent major achievements, for example, rather than proposed future work.  But the evidence doesn’t support the “Conform and be funded” headline, however much Nature might want it to be true.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-03-the-scientific-reasons-it-is-not-helpful-to-study-the-newtown-shooters-dna/",
    "title": "The scientific reasons it is not helpful to study the Newtown shooter's DNA",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-01-03",
    "categories": [],
    "contents": "\nThe Connecticut Medical Examiner has asked to sequence and study the DNA of the recent Newtown shooter. I’ve been seeing this pop up over the last few days on a lot of popular media sites, where they mention some objections scientists (or geneticists) may have to this “scientific” study. But I haven’t seen the objections explicitly laid out anywhere. So here are mine.\nIgnoring the fundamentals of the genetics of complex disease: If the violent behavior of the shooter has any genetic underpinning, it is complex. If you only look at one person’s DNA, without a clear behavior definition (violent? mental disorder? etc.?) it is impossible to assess important complications such as penetrance, epistasis, and gene-environment interactions, to name a few. These make statistical analysis incredibly complicated even in huge, well-designed studies.\nSmall Sample Size:  One person hit on the issue that is maybe the biggest reason this is a waste of time/likely to lead to incorrect results. _You can’t draw a reasonable conclusion about any population by looking at only one individual. _This is actually a fundamental component of statistical inference. The goal of statistical inference is to take a small, representative sample and use data from that sample to say something about the bigger population. In this case, there are two reasons that the usual practice of statistical inference can’t be applied: (1) only one individual is being considered, so we can’t measure anything about how variable (or accurate) the data are, and (2) we’ve picked one, incredibly high-profile, and almost certainly not representative, individual to study.\nMultiple testing/data dredging: The small sample size problem is compounded by the fact that we aren’t looking at just one or two of the shooter’s genes, but rather the whole genome. To see why making statements about violent individuals based on only one person’s DNA is a bad idea, think about the 20,000 genes in a human body. Let’s suppose that only one of the genes causes violent behavior (it is definitely more complicated than that) and that there is no environmental cause to the violent behavior (clearly false). Furthermore, suppose that if you have the bad version of the violent gene you will do something violent in your life (almost definitely not a sure thing).\nNow, even with all these simplifying (and incorrect) assumptions for each gene you flip a coin with a different chance of being heads. The violent gene turned up tails, but so did a large number of other genes. If we compare the set of genes that came up tails to another individual, they will have a huge number in common in addition to the violent gene. So based on this information, you would have no idea which gene causes violence even in this hugely simplified scenario.\nHeavy reliance on prior information/intuition: This is a supposedly scientific study, but the small sample size/multiple testing problems mean any conclusions from the data will be very very weak. The only thing you could do is take the set of genes you found and then rely on previous studies to try to determine which one is the “violence gene”. But now you are being guided by intuition, guesswork, and a bunch of studies that may or may not be relevant. The result is that more than likely you’d end up on the wrong gene.\nThe result is that it is highly likely that no solid statistical information will be derived from this experiment. Sometimes, just because the technology exists to run an experiment, doesn’t mean that experiment will teach us anything.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-02-fitbit-why-cant-i-have-my-data/",
    "title": "Fitbit, why can't I have my data?",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2013-01-02",
    "categories": [],
    "contents": "\nI have a Fitbit. I got it because I wanted to collect some data about myself and I liked the simplicity of the set-up. I also asked around and Fitbit seemed like the most “open” platform for collecting one’s own data. You have to pay $50 for a premium account, but after that, they allow you to download your data.\nOr do they?\nI looked into the details, asked a buddy or two, and found out that you actually can’t get the really interesting minute-by-minute data even with a premium account. You only get the daily summarized totals for steps/calories/stairs climbed. While this data is of some value, the minute-by-minute data are oh so much more interesting. I’d like to use it for personal interest, for teaching, for research, and for sharing interesting new ideas back to other Fitbit developers.\nSince I’m not easily dissuaded, I tried another route. I created an application that accessed the Fitbit API. After fiddling around a bit with a few R packages, I was able to download my daily totals. But again, no minute-by-minute data. I looked into it and only I have a [Fitbit](http://www.fitbit.com/). I got it because I wanted to collect some data about myself and I liked the simplicity of the set-up. I also asked around and Fitbit seemed like the most “open” platform for collecting one’s own data. You have to pay $50 for a premium account, but after that, they allow you to download your data. have access to the intraday data. So I emailed Fitbit to ask if I could be a partner app. So far no word.\nI guess it is true, if you aren’t paying for it, you are the product. But honestly, I’m just not that interested in being a product for Fitbit. So I think I’m bailing until I can download intraday data - I’m even happy to pay for it. If anybody has a suggestion of a more open self-monitoring device, I’d love to hear about it.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-01-01-happy-2013-the-international-year-of-statistics/",
    "title": "Happy 2013: The International Year of Statistics",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2013-01-01",
    "categories": [],
    "contents": "\nThe ASA has declared 2013 to be the International Year of Statistics and I am ready to celebrate it in full force. It is a great time to be a statistician and I am hoping more people will join the fun. In fact, as we like to point out in this blog, Statistics has already been at the center of many exciting accomplishments of the 21st century. Sabermetrics  has become a standard approach and inspired the Hollywood movie Money Ball. Friend of the blog Chris Volinsky, a PhD Statistician, led the team that won the Netflix million dollar prize. Nate Silver et al. proved the pundits wrong by, once again, using statistical models to predict election results almost perfectly. R has become one the most widely used programming languages in the world. Meanwhile, in academia, the number of statisticians becoming leaders in fields like environmental sciences, human genetics, genomics, and social sciences continues to grow. It is no surprise that stats majors at Harvard have more than quadrupled since 2000 and that statistics MOOCs are among the most popular.\n\nThe unprecedented advances in digital technology during the second half of the 20th century has produced a measurement revolution that is transforming the world. Many areas of science are now being driven by new measurement technologies and many insights are being made by discovery-driven, as opposed to hypothesis-driven, experiments. Empiricism is back with a vengeance. The current scientific era is defined by its dependence on data and the statistical methods and concepts developed during the 20th century provide an incomparable toolbox to help tackle current challenges. The toolbox, along with computer science, will also serve as a base for the methods of tomorrow.  So I will gladly join the Year of Statistics’ festivities during 2013 and beyond, during the era of data-driven science.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-31-what-makes-a-good-data-scientist/",
    "title": "What makes a good data scientist?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2012-12-31",
    "categories": [],
    "contents": "\nApparently, New Year’s Eve is not a popular day to come to the office as it seems I’m the only one here. No matter, it just means I can blast Mahler 3 (Bernstein, NY Phil, 1980s recording) louder than I normally would.\nToday’s post is inspired by this latest article in the NYT about big data. The article for the most part describes a conference that happened at MIT recently on the topic of big data. Towards the end of the article, it is noted that one of the participants (Rachel Schutt) was asked what makes a good data scientist.\n\n\nObviously, she replied, the requirements include computer science and math skills, but you also want someone who has a deep, wide-ranging curiosity, is innovative and is guided by experience as well as data.\n\n\n“I don’t worship the machine,” she said.\n\n\n\nI think I agree, but I would have put it a different way. Mostly, I think what makes a good data scientist is the same thing that makes you a good [insert field here] scientist. In other words, a good data scientist is a good scientist.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-30-sunday-datastatistics-link-roundup-123012/",
    "title": "Sunday data/statistics link roundup (12/30/12)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2012-12-30",
    "categories": [],
    "contents": "\nAn interesting new app called 100plus, which looks like it uses public data to help determine how little decisions (walking more, one more glass of wine, etc.) lead to more or less health. Here’s a post describing it on the heathdata.gov blog. As far as I can tell, the app is still in beta, so only the folks who have a code can download it.\nData on mass shootings from the Mother Jones investigation.\nA post by Hilary M. on “Getting Started with Data Science”. I really like the suggestion of just picking a project and doing something, getting it out there. One thing I’d add to the list is that I would spend a little time learning about an area you are interested in. With all the free data out there, it is easy to just “do something”, without putting in the requisite work to know why what you are doing is good/bad. So when you are doing something, make sure you take the time to “know something”.\nAn analysis of various measures of citation impact (also via Hilary M.). I’m not sure I follow the reasoning behind all of the analyses performed (seems a little like throwing everything at the problem and hoping something sticks) but one interesting point is how citation/usage are far apart from each other on the PCA plot. This is likely just because the measures cluster into two big categories, but it makes me wonder. Is it better to have a lot of people read your paper (broad impact?) or cite your paper (deep impact?).\nAn [ 1. An interesting new app called 100plus, which looks like it uses public data to help determine how little decisions (walking more, one more glass of wine, etc.) lead to more or less health. Here’s a post describing it on the heathdata.gov blog. As far as I can tell, the app is still in beta, so only the folks who have a code can download it.\nData on mass shootings from the Mother Jones investigation.\nA post by Hilary M. on “Getting Started with Data Science”. I really like the suggestion of just picking a project and doing something, getting it out there. One thing I’d add to the list is that I would spend a little time learning about an area you are interested in. With all the free data out there, it is easy to just “do something”, without putting in the requisite work to know why what you are doing is good/bad. So when you are doing something, make sure you take the time to “know something”.\nAn analysis of various measures of citation impact (also via Hilary M.). I’m not sure I follow the reasoning behind all of the analyses performed (seems a little like throwing everything at the problem and hoping something sticks) but one interesting point is how citation/usage are far apart from each other on the PCA plot. This is likely just because the measures cluster into two big categories, but it makes me wonder. Is it better to have a lot of people read your paper (broad impact?) or cite your paper (deep impact?).\nAn](https://twitter.com/hmason/status/285163907360899072) on Twitter about how big data does not mean you can ignore the scientific method. We have talked a little bit about this before, in terms of how one should motivate statistical projects.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-24-make-a-christmas-tree-in-r-with-random-ornamentspresents/",
    "title": "Make a Christmas Tree in R with random ornaments/presents",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2012-12-24",
    "categories": [],
    "contents": "\nHappy holidays!\n\n \n \nLink to Gist\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-23-sunday-datastatistics-link-roundup-122312/",
    "title": "Sunday data/statistics link roundup 12/23/12",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2012-12-23",
    "categories": [],
    "contents": "\nA cool data visualization for blood glucose levels for diabetic individuals. This kind of interactive visualization can help people see where/when major health issues arise for chronic diseases. This was a class project by Jeff Heer’s Stanford CS448B students Ben Rudolph and Reno Bowen (twitter @RenoBowen). Speaking of interactive visualizations, I also got this link from Patrick M. It looks like a way to build interactive graphics and my understanding is it is compatible with R data frames, worth checking out (plus, Dex is a good name).\nHere is an interesting review of Nate Silver’s book. The interesting thing about the review is that it doesn’t criticize the statistical content, but criticizes the belief that people only use data analysis for good. This is an interesting theme we’ve seen before. Gelman also reviews the review.\nIt’s a little late now, but this tool seems useful for folks who want to know whatdoineedonmyfinal?\nA list of the best open data releases of 2012. I particularly like the rat sightings in New York and think the Baltimore fixed speed cameras (which I have a habit of running afoul of).\nA map of data scientists on Twitter.  Unfortunately, since we don’t have “data scientist” in our Twitter description, Simply Statistics does not appear. I’m sure we would have been central….\nHere is an interesting paper where some investigators developed a technology that directly reads out a bar chart of the relevant quantities. They mention this means there is no need for statistical analysis. I wonder if the technology also reads out error bars.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-20-the-nih-peer-review-system-is-still-the-best-at-identifying-innovative-biomedical-investigators/",
    "title": "The NIH peer review system is still the best at identifying innovative biomedical investigators",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2012-12-20",
    "categories": [],
    "contents": "\nThis recent Nature paper makes the controversial claim that the most innovative (interpreted as best) scientists are not being funded by NIH. Not surprisingly, it is getting a lot of attention in the popular media. The title and introduction make it sound like there is a pervasive problem biasing the funding enterprise against innovative scientists. To me this appears counterintuitive given how much innovation, relative to other funding agencies around the world, comes out of NIH funded researchers (here is a recent example) and how many of the best biomedical investigators in the world elect to work for NIH funded institutions. The authors use data to justify their conclusions but I do not find it very convincing.\nFirst, the paper defines innovative/non-conformist scientists as those with a first/last/single author paper with 1000+ citations in the years 2002-2012. Obvious problems with this definition are already pointed out in the comments of the original paper but for argument’s sake I will accept it as useful quantification  The key data point the authors use is that only 2/5 of people with a first/last single author 1000+ citation paper are principal investigators on NIH grants. I would need to see the complete 2x2 table for people that actually applied for grants (1000+ citations or not x got NIH grant  or not) to be convinced. The reported ratio is meaningful only if most people with 1000+ papers are applying for grants but the authors doen’t report how many are retired, or are still postdocs, or went into industry, or are one-hit-wonders. Given that the payline is about 8%-15%, the 40% number may actually imply that NIH is in fact funding innovative people at a high rate.\nThe paper also implies that many of the undeserving funding recipients are connected individuals that serve on study sections. The evidence for this is that they are funded at a much higher rate than individuals with 1000+ citation papers. But as the authors themselves point out, study section members are often recruited from the subset of individuals who have NIH grants (it’s a way to give back to NIH).  This does not suggest bias in the process, it just suggests that if you recruit funded people to be on a panel, that panel will have a higher rate of funded people.\nNIH’s peer review system is far from perfect but it somehow manages to produce the best biomedical research in the world. How does this happen? Well, I think it’s because NIH is currently funding some of the most innovative biomedical researchers in the world. The current system can certainly improve, but perhaps we should focus on concrete proposals with hard evidence that they will actually make things better.\nDisclaimers: I am a regular member of an NIH study section. I am PI on NIH grants. I am on several papers with more than 1000 citations.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-19-rafa-interviewed-about-statistical-genomics/",
    "title": "Rafa interviewed about statistical genomics",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2012-12-19",
    "categories": [],
    "contents": "\nHe talks about the problems created by the speed of increase in data sizes in molecular biology, the way that genomics is hugely driven by data analysis/statistics, how Bioconductor is an example of bottom up science, Simply Statistics gets a shout out, how new data are going to lead to new modeling/statistical challenges, and gives an ode to boxplots. It’s worth watching the whole thing…\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-18-the-value-of-re-analysis/",
    "title": "The value of re-analysis",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2012-12-18",
    "categories": [],
    "contents": "\nI just saw this really nice post over on John Cook’s blog. He talks about how it is a valuable exercise to re-type code for examples you find in a book or on a blog. I completely agree that this is a good way to learn through osmosis, learn about debugging, and often pick up the reasons for particular coding tricks (this is how I learned about vectorized calculations in Matlab, by re-typing and running my advisors code back in my youth).\nIn a more statistical version of this idea, Gary King has proposed reproducing the analysis in a published paper as a way to get a paper of your own.  You can figure out the parts that a person did well and the parts that you would do differently, maybe finding enough insight to come up with your own new paper. But I think this level of replication involves actually two levels of thinking:\nCan you actually reproduce the code used to perform the analysis?\nCan you solve the “paper as puzzle” exercise proposed by Ethan Perlstein over at his site. Given the results in the paper, can you come up with the story?\nBoth of these things require a bit more “higher level thinking” than just re-running the analysis if you have the code. But I think even the seemingly “low-level” task of just retyping and running the code that is used to perform a data analysis can be very enlightening. The problem is that this code, in many cases, does not exist. But that is starting to change. If you check out Rpubs or RunMyCode or even the right parts of Figshare you can find data analyses you can run through and reproduce.\nThe only downside is there is currently no measure of quality on these published analyses. It would be great if people could focus their time re-typing only good data analyses, rather than one at random. Or, as a guy once (almost) said, “Data analysis practice doesn’t make perfect, perfect data analysis practice makes perfect.”\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-17-should-the-cox-proportional-hazards-model-get-the-nobel-prize-in-medicine/",
    "title": "Should the Cox Proportional Hazards model get the Nobel Prize in Medicine?",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2012-12-17",
    "categories": [],
    "contents": "\nI’m not the first one to suggest that Biostatistics has been undervalued in the scientific community, and some of the shortcomings of epidemiology and biostatistics have been noted elsewhere. But this previous work focuses primarily on the contributions of statistics/biostatistics at the purely scientific level.\nThe Cox Proportional Hazards model is one of the most widely used statistical models in the analysis of data from clinical trials and other medical studies. The corresponding paper has been cited over 32,000 times; this is a dramatically low estimate of the number of times the model has been used. It is one of “those methods” that doesn’t even require a reference to the original methods paper anymore.\nMany of the most influential medical studies, including major studies like the Women’s Health Initiative have used these methods to answer some of our most pressing medical questions. Despite the incredible impact of this statistical technique on the world of medicine and public health, it has not received the Nobel Prize. This isn’t an aberration, statistical methods are not traditionally considered for Nobel Prizes in Medicine. They primarily focus on biochemical, genetic, or public health discoveries.\nIn contrast, many economics Nobel Prizes have been awarded primarily for the discovery of a new statistical or mathematical concept. One example is the ARCH model. The Nobel Prize in Economics in 2003 was awarded to Robert Engle, the person who proposed the original ARCH model. The model has gone on to have a major impact on financial analysis, much like the Cox model has had a major impact on medicine?\nSo why aren’t Nobel Prizes in medicine awarded to statisticians more often? Other methods such as ANOVA, P-values, etc. have also had an incredibly large impact on the way we measure and evaluate medical procedures. Maybe as medicine becomes increasingly driven by data, we will start to see more statisticians recognized for their incredible discoveries and the huge contributions they make to medical research and practice.\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-16-sunday-datastatistics-link-roundup-121612/",
    "title": "Sunday data/statistics link roundup (12/16/12)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2012-12-16",
    "categories": [],
    "contents": "\nA directory of open access journals. Very cool idea to aggregate them. Here is a blog post from one of my favorite statistics bloggers about why open-access journals are so cool. Just like in a lot of other areas, open access journals can be thought of as an open data initiative.\nHere is a website that displays data on the relative wealth of neighborhoods, broken down by census track. It’s pretty fascinating to take a look and see what the income changes are, even in regions pretty close to each other.\nMore citizen science goodness. Zooniverse has a new project where you can look through a bunch of pictures in the Serengeti and see if you can find animals.\nNate Silver talking about his new book with Hal Varian. (via). I have skimmed the book and found that the parts about baseball/politics are awesome and the other parts seem a little light. But maybe that’s just my pre-conceived bias? I’d love to hear what other people thought…\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-14-computing-for-data-analysis-returns/",
    "title": "Computing for Data Analysis Returns",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2012-12-14",
    "categories": [],
    "contents": "\nI’m happy to announce that my course Computing for Data Analysis will return to Coursera on January 2nd, 2013. While I had previously announced that the course would be presented again right here, it made more sense to do it again on Coursera where it is (still) free and the platform there is much richer. For those of you who missed it the last time around, this is your chance to take it and learn a little R.\nI’ve gotten a number of emails from people who were interested in watching the videos for the course. If you just want to sit around and watch videos of me talking, I’ve created a set of four YouTube playlists based on the four weeks of the course:\nBackground and getting started\nWeek 1: Background on R, data types, reading/writing data\nWeek 2: Control structures, functions, apply functions, debugging tools\nWeek 3: Plotting and simulation\nWeek 4: Regular expressions, classes and methods\nThe content in the YouTube playlists reflect the content from the first iteration of the course and will not reflect any new material I add to the second iteration (at least not for a little while).\nI encourage everyone who is interested to enroll in the course on Coursera because there you’ll have the benefit of in-video quizzes and other forms of assessment and will be able to interact with all of the great students who are also enrolled in the class. Also, if you’re interested in signing up for Jeff Leek’s Data Analysis course (starts on January 22, 2013) and are not very familiar with R, I encourage you to check out Computing for Data Analysis first to get yourself up to speed.\nI look forward to seeing you there!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-10-joe-blitzsteins-free-online-stat-course-helps-put-a-critical-satellite-in-orbit/",
    "title": "Joe Blitzstein's free online stat course helps put a critical satellite in orbit",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2012-12-10",
    "categories": [],
    "contents": "\nAs loyal readers know, we are very enthusiastic about MOOCs. One of the main reasons for this is the potential of teaching Statistics to students from all over the world, in particular those that can’t afford or don’t have acces to college. However, it turns out that rocket scientists can also benefit. Check out the feedback Joe Blitztsein, professor of one of the most popular online stat courses,  received from one of his students:\n\nAs an “old bubba” aerospace engineer I watched your Stat 110 class and enjoyed it very much. It sure blew out a lot of cobwebs that had collected over the past 35 years working as an aerospace engineer. As you might guess, we deal with a lot of probability. Just recently I was involved in a study to see what a blocked Reaction Control System (RCS) might do to a satellite… I am a Spacecraft Attitude Control systems engineer and it was my job to simulate what would happen if a certain RCS engine was plugged. It was a weird problem and it inspired me to watch your class… Fortunately, the statistics showed that the RCS nozzles that could get plugged would have a low probability and would not affect our ability to adjust the vehicle’s orbit. And we launched it this past summer and everything went perfect! So I just wanted to tell you that when you teach your “kiddos” tell them that Stat 110 has real life implications. This satellite is a critical national defense asset that saves the lives of our soldiers on the ground.”\n\nI doubt “Old Bubba” has time to go back to school to refresh his stats knowledge… but thanks to Joe’s online class, he no longer needs to. This is yet another advantage MOOCs offer: giving busy professionals a practical way to learn new skills or brush up on specific topics.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-09-sunday-datastatistics-link-roundup-12912/",
    "title": "Sunday data/statistics link roundup (12/9/12)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2012-12-09",
    "categories": [],
    "contents": "\nSome interesting data/data visualizations about working conditions in the apparel industry. Here is the full report. Whenever I see reports like this, I wish the raw data were more clearly linked. I want to be able to get in, play with the data, and see if I notice something that doesn’t appear in the infographics. \nThis is an awesome plain-language discussion of how a bunch of methods (CS and Stats) with fancy names relate to each other. It shows that CS/Machine Learning/Stats are converging in many ways and there isn’t much new under the sun. On the other hand, I think the really exciting thing here is to use these methods on new questions, once people drop the stick. \nIf you are a reader of this blog and somehow do not read anything else on the internet, you will have missed Hadley Wickham’s Rcpp tutorial. In my mind, this pretty much seals it, Julia isn’t going to overtake R anytime soon. In other news, Hadley is coming to visit JHSPH Biostats this week! I’m psyched to meet him. \nFor those of us that live in Baltimore, this interesting set of data visualizations lets you in on the crime hotspots. This is a much fancier/more thorough analysis than Rafa and I did way back when. \nCheck out the new easy stats tool from the Census (via Hilary M.) and read our interview with Tom Louis who is heading over there to the Census to do cool things. \nWatch out, some Tedx talks may be pseudoscience! More later this week on the politicization/glamourization of science, so stay tuned. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-08-dropping-the-stick-in-data-analysis/",
    "title": "Dropping the Stick in Data Analysis",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2012-12-08",
    "categories": [],
    "contents": "\nWhen I was a kid growing up in rough-and-tumble suburban New York, one of the major summer activities was roller hockey, the kind with roller blades (remember roller blades?). My friends and I would be playing in some random parking lot and undoubtedly one of us would be just blowing it the whole game. This would usually lead to an impromptu intervention where the person screwing up (often me) would be told by everyone else on the team to “drop the stick”. The idea was you should stop playing, clear your head, skate around for a bit, and not try to do 20 things at once.\nI don’t play much hockey now, but I do a bit more data analysis. Strangely, little has changed.\nPeople come to me at various stages of data analysis. Close collaborators usually come to me with no data because they are planning a study and need some help. In those cases, I’m involved in the beginning and know how the data are generated. Usually, in those cases I analyze the data in the end so there’s less confusion.\nOthers usually come to me with data in hand wanting know what they should do now that they’ve got all this data. Often there’s confusion about where to start, what method to use, what program, what procedure, what function, what test, Bayesian or frequentist, mean or median, R or Stata, random effects or fixed effects, cat or dog, mice or men, etc. That’s usually the point where I tell them to “drop the stick”, or the data analysis version of that, which is “What question are you trying to answer?”\nUsually, people know what question they’re trying to answer–they just forgot to tell me. But I’m always amazed at how this question can often be the subject of the entire discussion. We might end up answering a question the investigator hadn’t thought of yet, maybe a question that’s better suited to the data.\nSo, job #1 if you’re a statistician: Get more people to drop the stick.  You’ll make everyone play better in the end.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-05-an-idea-for-killing-email/",
    "title": "Email is a to-do list made by other people - can someone make it more efficient?!",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-12-05",
    "categories": [],
    "contents": "\nThis is a follow-up to one of our most popular posts: getting email responses from busy people. This post had been in the drafts for a few weeks, then this morning I saw this quote in our Twitter feed:\n\nYour email inbox is a to-do list created by other people (via)\n\nThis is 100% true of my work email and I have to say, because of the way those emails are organized - as conversations rather than a prioritized, organized to-do list - I end up missing really important things or getting to them too late. This is happening to me with increasing enough frequency I feel like I’m starting to cause serious problems for people.\nSo I am begging someone with way better skills than me to produce software that replaces gmail in the following ways. It is a to-do list that I can allow people to add tasks too. The software shows me the following types of messages.\nWe have an appointment at x time on y date to discuss z. Next to this message is a checkbox. If I click “ok” it gets added to my calendar, if I click “no” then a message gets sent to the person who scheduled the meeting saying I’m unavailable.\nA multiple choice question where they input the categories of answer I can give and I just pick one, it sends them the response.\nA request to be added as a person who can assign me tasks with a yes/no answer.\nA longer request email - this has three entry fields: (1) what do you want, (2) when do you want it by? and (3) a yes/no checkbox asking if I’m willing to perform the task.  If I say yes, it gets added to my calendar with automated reminders.\nIt should interface with all the systems that send me reminder emails to organize the reminders.\nYou can assign quotas to people, where they can only submit a certain number of tasks per month.\nIt allows you to re-assign tasks to other people so when I am not the right person to ask, I can quickly move the task on to the right person.\nIt would collect data and generate automated reports for me about what kind of tasks I’m usually forgetting/being late on and what times of day I’m bad about responding so that I could improve my response times.\nThe software would automatically reorganize events/to-dos to reflect changing deadlines/priorities, etc. This piece of software would revolutionize my life. Any takers?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:02:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-04-advice-for-students-on-the-academic-job-market-2013-edition/",
    "title": "Advice for students on the academic job market (2013 edition)",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2012-12-04",
    "categories": [],
    "contents": "\nJob hunting season is upon us. Those on the job market should be sending in applications already. Here I provide links to some of the related posts we published last year.\nAdvice for stats students on the academic job market\nAnother academic job market option: liberal arts colleges\nPreparing for tenure track job interviews\nOn hard and soft money\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-03-data-analysis-acquisition-worst-deal-ever/",
    "title": "Data analysis acquisition \"worst deal ever\"?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2012-12-03",
    "categories": [],
    "contents": "\nA little over a year ago I mentioned that data analysis companies were getting gobbled up by larger technology companies. In particular, HP bought Autonomy, a British data analysis company, for about $11 billion. (By the way, can anyone tell me if it’s still called Hewlett-Packard, or is it just “HP”, like “AT&T”?) From an article a year ago\n\nAutonomy, with headquarters in Cambridge, England, helps companies and governments store, process, search and analyze large electronic data sets. Its specialty lies in its sophisticated algorithms, which can make sense of unstructured information.\n\nAt the time, the thinking was HP had overpaid (especially given HP’s recent high price for 3Par) but the deal went through anyway. Now, HP has discovered accounting problems at Autonomy and is writing down $8.8 billion.\nWhoops.\nJames Stewart of the New York Times claims this is worse than the failed AOL-Time Warner merger (although the absolute numbers involved here are smaller). With 3 CEOs in 2 years, it seems HP just can’t get anything right these days. But what intrigues me most is the question of what companies like Autonomy are worth and the possibility that HP made a huge mistake in the valuation of this company. Of course, if there was fraud at Autonomy (as it seems to be alleged), then all bets are off. But if not, then perhaps this is the first bubble popping in the realm of data analysis companies more generally?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-12-02-sunday-datastatistics-link-roundup-12212/",
    "title": "Sunday data/statistics link roundup (12/2/12)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2012-12-02",
    "categories": [],
    "contents": "\nAn interview with Anthony Goldbloom, CEO of Kaggle. I’m not sure I’d agree with the characterization that all data scientists are: creative, curious, and competitive and certainly those characteristics aren’t unique to data scientists. And I didn’t know this: “We have 65,000 data scientists signed up to Kaggle, and just like with golf tournaments, we have them all ranked from 1 to 65,000.” \nCheck it out, art with R! It’s actually pretty interesting to see how they use statistical algorithms to generate different artistic styles. Here are some more. \nNow that Ethan Perlstein’s crowdfunding experiment was successful, other people are getting on the bandwagon. If you want to find out what kind of bacteria you have in your gut, for example, you could check out this. \nI thought I had it rough, but apparently some data analysts spend all their time developing algorithms to detect penis drawings!\nRoger was on Anderson Cooper 360 as part of the Building America segment. We can’t find the video, but here is the transcript. \nAn interesting article on the half-life of facts. I think the analogy is an interesting one and certainly there is research to be done there. But I think it jumps the shark a bit when they start talking about how the moon landing was predictable, etc. I completely believe in the retrospective analysis of knowledge, but predicting things is pretty hard, especially when it is the future.  \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-30-statistical-illiteracy-may-lead-to-parents-panicking-about-autism/",
    "title": "Statistical illiteracy may lead to parents panicking about Autism.",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2012-11-30",
    "categories": [],
    "contents": "\nI just was doing my morning reading of a few news sources and stumbled across this Huffington Post article talking about research correlating babies cries to autism. It suggests that the sound of a babies cries may predict their future risk for autism. As the parent of a young son, this obviously caught my attention in a very lizard-brain, caveman sort of way. I couldn’t find a link to the research paper in the article so I did some searching and found out this result is also being covered by Time, Science Daily, Medical Daily, and a bunch of other news outlets.\nNow thoroughly freaked, I looked online and found the pdf of the original research article. I started looking at the statistics and took a deep breath. Based on the analysis they present in the article there is absolutely no statistical evidence that a babies’ cries can predict autism. Here are the flaws with the study:\nSmall sample size. The authors only recruited 21 at risk infants and 18 healthy infants. Then, because of data processing issues, only ended up analyzing 7 high autistic risk versus 5 low autistic-risk in one analysis and 10 versus 6 in another. That is no where near a representative sample and barely qualifies as a pilot study.\nMajor and unavoidable confounding. The way the authors determined high autistic risk versus low risk was based on whether an older sibling had autism. Leaving aside the quality of this metric for measuring risk of autism, there is a major confounding factor: the families of the high risk children all had an older sibling with autism and the families of the low risk children did not! It would not be surprising at all if children with one autistic older sibling might get a different kind of attention and hence cry differently regardless of their potential future risk of autism.\nNo correction for multiple testing. This is one of the oldest problems in statistical analysis. It is also one that is a consistent culprit of false positives in epidemiology studies. XKCD even did a cartoon about it! They tested 9 variables measuring the way babies cry and tested each one with a statistical hypothesis test. They did not correct for multiple testing. So I gathered resulting p-values and did the correction for them. It turns out that after adjusting for multiple comparisons, nothing is significant at the usual P < 0.05 level, which would probably have prevented publication.\nTaken together, these problems mean that the statistical analysis of these data do not show any connection between crying and autism.\nThe problem here exists on two levels. First, there was a failing in the statistical evaluation of this manuscript at the peer review level. Most statistical referees would have spotted these flaws and pointed them out for such a highly controversial paper. A second problem is that news agencies report on this result and despite paying lip-service to potential limitations, are not statistically literate enough to point out the major flaws in the analysis that reduce the probability of a true positive. Should journalists have some minimal in statistics that allows them to determine whether a result is likely to be a false positive to save us parents a lot of panic?\n \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-27-i-give-up-i-am-embracing-pie-charts/",
    "title": "I give up, I am embracing pie charts",
    "description": {},
    "author": [
      {
        "name": "Rafael Irizarry",
        "url": {}
      }
    ],
    "date": "2012-11-27",
    "categories": [],
    "contents": "\nMost statisticians know that pie charts are a terrible way to plot percentages. You can find explanations here, here, and here as well as the R help file for the pie function which states:\n\nPie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data.\n\nI have only used the pie R function once and it was to make this plot:\n\n\nsaveGIF({\n  N=10\n  for(i in 0:(N-1)){\n    x <- cos(2*pi/N*i)\n    y <- x+1\n    z <- (y-2)*22.5\n    pie(c(y,8-y), col=c(\"white\",\"yellow\"), init.angle=135-z, \n        border=FALSE, labels=NA)\n    }\n  }, \"pacman.gif\", interval = 0.1)\n\n\npacmanSo why are they ubiquitous? The best explanation I’ve heard is that they are easy to make in Microsoft Excel. Regardless, after years of training, lay people are probably better at interpreting pie charts than any other graph. So I’m surrendering and embracing the pie chart. Jeff’s recent post shows we have bigger fish to fry.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-12-27T15:39:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-26-the-statisticians-at-fox-news-use-classic-and-novel-graphical-techniques-to-lead-with-data/",
    "title": "The statisticians at Fox News use classic and novel graphical techniques to lead with data",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2012-11-26",
    "categories": [],
    "contents": "\nDepending on where you land in the political spectrum you may either love or despise Fox News. But regardless of your political affiliation, you have to recognize that their statisticians are well-trained in the art of using graphics to persuade folks of a particular viewpoint. I’m not the first to recognize that the graphics department uses some clever tricks to make certain points. But when flipping through the graphs I thought it was interesting to highlight some of the techniques they use to persuade. Some are clearly classics from the literature, but some are (as far as I can tell) newly developed graphical “persuasion” techniques.\nTruncating the y-axis\n\n(via)\nand\n\n(via)\nThis is a pretty common technique for leading the question in statistical graphics, as discussed here and elsewhere.\nNumbers that don’t add up\nI’m not sure whether this one is intentional or not, but it crops up in several places and I think is a unique approach to leading information, at least I couldn’t find a reference in the literature. Basically the idea is to produce percentages that don’t add to one, allowing multiple choices to have closer percentages than they probably should:\n\n(via)\nor to suggest that multiple options are all equally likely, but also supported by large percentages:\n\n(via)\nChanging the units of comparison\nWhen two things are likely to be very similar, one approach to leading information is to present variables in different units. Here is an example where total spending for 2010-2013 is compared to deficits in 2008. This can also be viewed as an example of not labeling the axes.\n\n**** (via)\nChanging the magnitude of units at different x-values\nHere is a plot where the changes in magnitude at high x-values are higher than changes in magnitude at lower x-values. Again, I think this is actually a novel graphical technique for leading readers in one direction.\n\n(via)\nTo really see the difference, compare to the graph with common changes in magnitude at all x-values.\n\n(via)\nChanging trends by sub-sampling x values (also misleading chart titles)\nHere is a graph that shows unemployment rates over time and the corresponding chart with the x-axis appropriately laid out.\n\n\n(via)\nOne could argue these are mistakes, but based on the consistent displays of data supporting one viewpoint, I think these are likely the result of someone with real statistical training who is using data in a very specific way to make a point. Obviously, Fox News isn’t the only organization that does this sort of thing, but it is interesting to see how much effort they put into statistical graphics.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-25-sunday-datastatistics-link-roundup-11252012/",
    "title": "Sunday data/statistics link roundup (11/25/2012)",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2012-11-25",
    "categories": [],
    "contents": "\nMy wife used to teach at Grinnell College, so we were psyched to see that a Grinnell player set the NCAA record for most points in a game. We used to go to the games, which were amazing to watch, when we lived in Iowa. The system the coach has in place there is a ton of fun to watch and is based on statistics!\nSomeone has to vet the science writers at the Huffpo. This is out of control, basically claiming that open access publishing is harming science. I mean, I’m all about being a curmudgeon and all, but the internet exists now, so we might as well get used to it. \nThis one is probably better for Steven’s blog, but this is a pretty powerful graph about the life-saving potential of vaccines.  \nRoger posted yesterday about the NY Times piece on deep learning. It is one of our most shared posts of all time, you should also check out the comments, which are exceedingly good. Two things I thought I’d point out in response to a lot of the reaction: (1) I think part of Roger’s post was suggesting that the statistics community should adopt some of CS’s culture of solving problems with already existing, really good methods and (2) I tried searching for a really clear example of “deep learning” yesterday so we could try some statistics on it and didn’t find any really clear explanations. Does anyone have a really simple example of deep learning (ideally with code) so we can see how it relates to statistical concepts? \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-24-computer-scientists-discover-statistics-and-find-it-useful/",
    "title": "Computer scientists discover statistics and find it useful",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2012-11-24",
    "categories": [],
    "contents": "\nThis article in the New York Times today describes some of the advances that computer scientists have made in recent years.\n\nThe technology, called deep learning, has already been put to use in services like Apple’s Siri virtual personal assistant, which is based on Nuance Communications’ speech recognition service, and in Google’s Street View, which uses machine vision to identify specific addresses.\nBut what is new in recent months is the growing speed and accuracy of deep-learning programs, often called artificial neural networks or just “neural nets” for their resemblance to the neural connections in the brain.\n\nDeep learning? Really?\nOkay, names aside, there are a few things to say here. First, the advances described in the article are real–I think that’s clear. There’s a lot of pretty cool stuff out there (including Siri, in my opinion) coming from the likes of Google, Microsoft, Apple, and many others and, frankly, I appreciate all of it. I hope to have my own self-driving car one day.\nThe question is how did we get here? What worries me about this article and many others is that you can get the impression that there were tremendous advances in the technology/methods used. But I find that hard to believe given that the methods that are often discussed in these advances are methods that have been around for quite a while (neural networks, anyone?). The real advance has been in the incorporation of data into these technologies and the use of statistical models. The interesting thing is not that the data are big, it’s that we’re using data at all.\nDid Nate Silver produce a better prediction of the election than the pundits because he had better models or better technology? No, it’s because he bothered to use data at all. This is not to downplay the sophistication of Silver’s or others’ approach, but many others did what he did (presumably using different methods–I don’t think there was collaboration) and more or less got the same results. So the variation across different models is small, but the variation between using data vs. not using data is, well, big. Peter Norvig notes this in his talk about how Google uses data for translation. An area that computational linguists had been working on for decades was advanced dramatically by a ton of data and (a variation of) Bayes’ Theorem. I may be going out on a limb here, but I don’t think it was Bayes’ Theorem that did the trick. But there will probably be an article in the New York Times soon about how Bayes’ Theorem is revolutionizing artificial intelligence. Oh wait, there already was one.\nIt may sound like I’m trying to bash the computer scientists here, but I’m not. It would be too too easy for me to write a post complaining about how the computer scientists have stolen the ideas that statisticians have been using for decades and are claiming to have discovered new approaches to everything. But that’s exactly what is happening and good for them.\nI don’t like to frame everything as an us-versus-them scenario, but the truth is the computer scientists are winning and the statisticians are losing. The reason is that they’ve taken our best ideas and used them to solve problems that matter to people. Meanwhile, we should have been stealing the computer scientists’ best ideas and using them to solve problems that matter to people. But we didn’t. And now we’re playing catch-up, and not doing a particularly good job of it.\nThat said, I believe there’s still time for statistics to play a big role in “big data”. We just have to choose to do it. Borrowing ideas from other fields is good–that’s why it’s called “re”search, right? Statisticians shouldn’t be shy about it. Otherwise, all we’ll have left to do is complain about how all those people took what we’d been working on for decades and…made it useful.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-21-developing-the-new-york-times-visual-election-outcome-explorer/",
    "title": "Developing the New York Times Visual Election Outcome Explorer",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2012-11-21",
    "categories": [],
    "contents": "\nMike Bostock talks about the design and construction of the “512 Paths to the White House” visualization for the New York Times. I found this visualization extremely useful on election night as it helped me understand the implications of each of the swing state calls as the night rolled on.\nRegarding the use of outside information to annotate the graphic:\n\nApplying background knowledge to give the data greater context—such as the influence of the auto-industry bailout on Ohio’s economy—makes the visualization that much richer. After all, visualizations aren’t just about numbers, but about understanding the world we live in; qualitative information can add substantially to a quantitative graphic.\n\nWhile the technical details are fascinating, I was equally interested in the editorial decisions they had to make to build a usable visualization.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-20-a-grand-experiment-in-science-funding/",
    "title": "A grand experiment in science funding",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2012-11-20",
    "categories": [],
    "contents": "\nAmong all the young scientists I know, I think Ethan Perlstein is one of the most innovative in the way he has adapted to the internet era. His website is incredibly unique among academic websites, he is all over the social media and his latest experiment in crowd-funding his research is something I’m definitely keeping an eye on.\nThe basic idea is that he has identified a project (giving meth to yeast mouse brains -see the comment by Ethan below-, I think) and put it up on Rockethub, which is a crowd funding platform. The basic idea is he is looking for people to donate to his lab to fund the project. I would love it if this project succeeded, so if you have a few extra dollars lying around I’m sure he’d really appreciate it if you’d donate.\nAt the bigger picture level, I love the idea of crowd-funding for science in principal. But it isn’t clear that it is going to work in practice. Ethan has been tearing it up with this project, even ending up in the Economist, but he has still had trouble getting to his goal for funding. In the grand scheme of things he is asking for a relatively small amount given how much he will do, so it isn’t clear to me that this is a viable option for most scientists.\nThe other key problem, as a statistician, is that many of the projects I work on will not be as easily understandable/cool as giving meth to yeast. So, for example, I’m not sure I’d be able to generate the kind of support I’d need for my group to work on statistical analysis of RNA-seq data or batch effect removal methods.\nStill, I love the idea, and it would be great if there were alternative sources of revenue for the incredibly important work that scientists like Ethan and others are doing.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-19-podcast-5-coursera-debrief-2/",
    "title": "Podcast #5: Coursera Debrief",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2012-11-19",
    "categories": [],
    "contents": "\nJeff and I talk with Brian Caffo about teaching MOOCs on Coursera.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-18-sunday-data-statistics-link-roundup-11-18-12/",
    "title": "Sunday Data/Statistics Link Roundup (11/18/12)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-11-18",
    "categories": [],
    "contents": "\nAn interview with Brad Efron about scientific writing. I haven’t watched the whole interview, but I do know that Efron is one of my favorite writers among statisticians.\nSlidify, another approach for making HTML5 slides directly from R.  I love the idea of making HTML slides, I would definitely do this regularly. But there are a couple of issues I feel still aren’t resolved: (1) It is still just a little too hard to change the theme/feel of the slides in my opinion. It is just CSS, but that’s still just enough of a hurdle that it is keeping me away and (2) I feel that the placement/insertion of images is still a little clunky, Google Docs has figured this out, I’d love it if they integrated the best features of Slidify, Latex, etc. into that system. \nStatistics is still the new hotness. Here is a Business Insider list about 5 statistics problems that will “change the way you think about the world”. \nI love this one in the New Yorker, especially the line,”statisticians are the new sexy vampires, only even more pasty” (via Brooke A.)\nWe’ve hit the big time! We have been linked to by a real (Forbes) blogger. \nIf you haven’t noticed, we have a new logo. We are going to be making a few other platform-related changes over the next week or so. If you have any trouble, let us know!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-18-welcome-to-simply-statistics-2-0/",
    "title": "Welcome to Simply Statistics 2.0",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2012-11-18",
    "categories": [],
    "contents": "\nWelcome to the re-designed, re-hosted and re-platformed Simply Statistics blog. We have moved the blog over to the WordPress platform to give us some newer features that were lacking over at tumblr. So far the transition has gone okay but there may be a few bumps over the next 24 hours or so as we learn the platform. Remember, we’re not the young hackers that we used to be.\nA few things have changed. First off, the search box actually works. Also, in moving the Disqus comments over, we seem to have lost all of the old comments. So unfortunately many of your gems from the past are now gone. If anyone knows how to retain old comments on Disqus, please let us know! I think Jeff’s been banging his head for a while now trying to figure this out.\nWe’re hoping to roll out a few new features over the next few months so keep an eye out and come back often.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-16-logo-contest-winner/",
    "title": "Logo Contest Winner",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-11-16",
    "categories": [],
    "contents": "\nCongratulations to Bradley Saul, the winner of the Simply Statistics Logo contest! We had some great entries which made it difficult to choose between them. You can see the new logo to the right of our home page or the full sized version here:\n\nI made some slight modifications to Bradley’s original code (apologies!). The code for his original version is here:\nHere’s the code:\n#########################################################\n#  Project: Simply Statistics Logo Design\n#  Date: 10/17/12\n#  Version: 0.00001\n#  Author: Bradley Saul\n#  Built in R Version: 2.15.0\n#########################################################\n\n#Set Graphical parameters\npar(mar=c(0, 0, 0, 0), pty='s', cex=3.5, pin=c(6,6))\n#Note: I had to hard code the size, so that the text would scale\n#on resizing the image. Maybe there is another way to get around font\n#scaling issues - I couldn't figure it out.\n\nmake_logo <- function(color){\n  \n  x1 <- seq(0,1,.001)\n  ncps <- seq(0,10,1)\n  shapes <- seq(5,15,1)\n  # Plot Beta distributions to make purty lines.\n  plot(x1, pbeta(x1, shape1=10, shape2=.1, ncp=0), type='l', xlab='', ylab='', \n       frame.plot=FALSE, axes=FALSE)\n  for(i in 1:length(ncps)){\n    lines(x1, pbeta(x1,shape1=.1, shape2=10, ncp=ncps[i]), col=color)\n  }\n\n  #Shade in area under curve.\n  coord.x <- c(0,x1,1)\n  coord.y <- c(0,pbeta(x1,shape1=.1,shape2=10, ncp=10),0)\n  polygon(coord.x, coord.y, col=color, border=\"white\")\n\n  #Lazy way to get area between curves shaded, rather than just area under curve.\n  coord.y2 <- c(0,pbeta(x1,shape1=10,shape2=.1, ncp=0),0)\n  polygon(coord.x, coord.y2, col=\"white\", border=\"white\")\n\n  #Add text\n  text(.98,.4,'Simply', col=\"white\", adj=1,family='HersheySerif')\n  text(.98,.25,'St*atistics', col=\"white\", adj=1, family=\"HersheySerif\")\n}\n\nThanks to Bradley for the great logo and congratulations!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-15-reproducible-research-with-us-or-against-us-3/",
    "title": "Reproducible Research: With Us or Against Us?",
    "description": {},
    "author": [
      {
        "name": "Roger Peng",
        "url": {}
      }
    ],
    "date": "2012-11-15",
    "categories": [],
    "contents": "\nLast night this article by Chris Drummond of the Canadian National Research Council (Conseil national de recherches Canada) popped up in my Google Scholar alert. The title of the article, “Reproducible Research: a Dissenting Opinion” would seem to indicate that he disagrees with much that has been circulating out there about reproducible research.\nDrummond singles out the Declaration published by a Yale Law School Roundtable on Data and Code Sharing (I was not part of the roundtable) as an example of the main arguments in favor of reproducibility and has four main objections. What I found interesting about his piece is that I think I more or less agree with all his objections and yet draw the exact opposite conclusion from him. In his abstract, he concludes that “I would also contend that the effort necessary to meet the [reproducible research] movement’s aims, and the general attitude it engenders, would not serve any of the research disciplines well.”\n\nLet’s take his objections one by one:\n\n\n\nReproducibility, at least in the form proposed, is not now, nor has it ever been, an essential part of science. I would say that with the exception of mathematics, this is true. In math, usually you state a theorem and provide the proof. The proof shows you how to obtain the result, so it is a form of reproducibility. But beyond that I would argue that the need for reproducibility is a more recent phenomenon arising from the great complexity and cost of modern data analyses and the lack of funding for full replication. The rise of “consortium science” (think ENCODE project) diminishes our ability to fully replicate (what he calls “Scientific Replication”) an experiment in any reasonable amount of time.\n\n\nThe idea of a single well defined scientific method resulting in an incremental, and cumulative, scientific process is highly debatable. He argues that the idea of a forward moving process by which science builds on top of previous results in an orderly and incremental fashion is a fiction. In particular, there is no single “scientific method” into which you can drop in reproducibility as a key component. I think most scientists would agree with this. Science not some orderly process—it’s messy and can seem haphazard and discoveries come at unpredictable times. But that doesn’t mean that people shouldn’t provide the details of what they’ve done so that others don’t have to essentially reverse engineer the process. I don’t see how the disorderly reality of science is an argument against reproducibility.\n\n\nRequiring the submission of data and code will encourage a level of distrust among researchers and promote the acceptance of papers based on narrow technical criteria. I don’t agree with this statement at all. First, I don’t think it will happen. If a journal required code/data, it would be burdensome for some, but it would just be one of the many requirements that journals have. Second, I don’t think good science is about “trust”. Sure, it’s important to be civilized but if you claim a finding, I’m not going to just trust it because we’re both scientists. Finally, he says “Submitting code — in whatever language, for whatever system — will simply result in an accumulation of questionable software. There may be a some cases where people would be able to use it but I would doubt that they would be frequent.” I think this is true, but it’s not necessarily an argument against submitting code. Think of the all the open source/free software packages out there. I would bet that most of that code has only been looked at by one person—the developer. But does that mean open source software as a whole is not valuable?\n\n\nMisconduct has always been part of science with surprisingly little consequence. The public’s distrust is likely more to with the apparent variability of scientific conclusions. I agree with the first part and am not sure about the second. I’ve tried to argue previously that reproducible research is not just about preventing fraud/misconduct. If someone wants to commit fraud, it’s easy to make the fraud reproducible.\n\n\nIn the end, I see reproducibility as not necessarily a new concept, but really an adaptation of an old concept, that is describing materials and methods. The problem is that the standard format for publication—journal articles—has simply not caught up with the growing complexity of data analysis. And so we need to update the standards a bit.\n\n\nI think the benefit of reproducibility is that if someone wants to question or challenge the findings of a study, they have the materials with which to do so. Providing people with the means to ask questions is how science moves forward.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-09-interview-with-tom-louis-new-chief-scientist-at-the/",
    "title": "Interview with Tom Louis - New Chief Scientist at the Census Bureau",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-11-09",
    "categories": [],
    "contents": "\n\nTom Louis\n\n\n\n\n\n\n\n\n\n\n\nTom Louis is a professor of Biostatistics at Johns Hopkins and will be joining the Census Bureau through an interagency personnel agreement as the new associate director for research and methodology and chief scientist. Tom has an impressive history of accomplishment in developing statistical methods for everything from environmental science to genomics. We talked to Tom about his new role at the Census, how it relates to his impressive research career, and how young statisticians can get involved in the statistical work at the Census. \n\n\n\n\n\n\n\n\nSS: How did you end up being invited to lead the research branch of the Census?\n\nTL: Last winter, then-director Robert Groves (now Provost at Georgetown University) asked if I would be interested in  the possibility of becoming the next Associate Director of Research and Methodology (R&M) and Chief Scientist, succeeding  Rod Little (Professor of Biostatistics at the University of Michigan) in these roles.  I expressed interest and after several discussions with Bob and Rod, decided that if offered, I would accept.  It was offered and I did accept.  \nAs background,  components of my research, especially Bayesian methods, is Census-relevant.  Furthermore, during my time as a member of the National Academies Committee on National Statistics I served on the panel that recommended improvements in small area income and poverty estimates, chaired the panel that evaluated methods for allocating federal and state program funds by formula, and chaired a workshop on facilitating innovation in the Federal statistical system.\nRod and I noted that it’s interesting and possibly not coincidental that with my appointment the first two associate directors are both former chairs of Biostatistics departments.  It is the case that R&D’s mission is quite similar to that of a Biostatistics department; methods and collaborative research, consultation and education.  And, there are many statisticians at the Census Bureau who are not in the R&D directorship, a sociology quite similar to that in a School of Public Health or a Medical campus. \n\nSS: What made you interested in taking on this major new responsibility?\n\nTL: I became energized by the opportunity for national service, and excited by the scientific, administrative, and sociological responsibilities and challenges.  I’ll be engaged in hiring and staff development, and increasing the visibility of the bureau’s pre- and post-doctoral programs.  The position will provide the impetus to take a deep dive into finite-population statistical approaches, and contribute to the evolving understanding of the strengths and weakness of design-based, model-based and hybrid approaches to inference.  That I could remain a Hopkins employee by working via an Interagency Personnel Agreement, sealed the deal.  I will start in January 2013 and serve through 2015, and will continue to participate in some Hopkins-based activities.\nIn addition to activities within the Census Bureau, I’ll be increasing connections among statisticians in other federal statistical agencies, have a role in relations with researchers funded through the NSF to conduct census-related research.\n\n\nSS: What are the sorts of research projects the Census is involved in? \n\n\nTL: The Census Bureau designs and conducts the decennial Census, the Current Population Survey, the American Community Survey, many, many other surveys for other Federal Statistical Agencies including the Bureau of Labor Statistics, and a quite extraordinary portfolio of others. Each identifies issues in design and analysis that merit attention, many entail “Big Data” and many require combining information from a variety of sources.  I give a few examples, and encourage exploration of www.census.gov/research.\n\n\nYou can get a flavor of the types of research from the titles of the six current centers within R&M: The Center for Adaptive Design, The Center for Administrative Records Research and Acquisition, The Center for Disclosure Avoidance Research, The Center for Economic Studies, The Center for Statistical Research and Methodology and The Center for Survey Measurement.  Projects include multi-mode survey approaches, stopping rules for household visits, methods of combining information from surveys and administrative records, provision of focused estimates while preserving identity protection,  improved small area estimates of income and of limited english skills (used to trigger provision of election ballots in languages other than English), and continuing investigation of issues related to model-based and design-based inferences.\n\n\n  <p>\n     <br /><strong>SS: Are those projects related to your research?<\/strong><\/div> \n    \n    <p>\n      <span>TL: Some are, some will be, some will never be.  Small area estimation, hierarchical modeling with a Bayesian formalism, some aspects of adaptive design, some of combining evidence from a variety of sources, and general statistical modeling are in my power zone.  I look forward to getting involved in these and contributing to other projects.<\/span>\n    <\/p>\n    \n    <div class=\"im\">\n      <p>\n        <strong>SS: How does research performed at the Census help the American Public?<\/strong><\/div> \n        \n        <p>\n          <span>TL: Research innovations enable the bureau to produce more timely and accurate information at lower cost, improve validity (for example, new approaches have at least maintained respondent participation in surveys), enhancing the reputation of the the Census Bureau as a trusted source of information.  Estimates developed by Census are used to allocate billions of dollars in school aid, and the provide key planning information for businesses and governments.<\/span>\n        <\/p>\n        \n        <div class=\"im\">\n          <p>\n            <strong>SS: How can young statisticians get more involved in government statistical research?<\/strong><\/div> \n            \n            <p>\n              <span>TL: The first step is to become aware of the wide variety of activities and their high impact.  Visiting the Census website and those of other federal and state agencies, and the Committee on National Statistics (<\/span><a href=\"http://sites.nationalacademies.org/DBASSE/CNSTAT/\" target=\"_blank\"><a href=\"http://sites.nationalacademies.org/DBASSE/CNSTAT/\" target=\"_blank\">http://sites.nationalacademies.org/DBASSE/CNSTAT/<\/a><\/a><span>) and the National Institute of Statistical Sciences (<\/span><a href=\"http://www.niss.org/\" target=\"_blank\"><a href=\"http://www.niss.org/\" target=\"_blank\">http://www.niss.org/<\/a><\/a><span>) is a good start.   Make contact with researchers at the JSM and other meetings and be on the lookout for pre- and post-doctoral positions at Census and other federal agencies.<\/span>\n            <\/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-08-some-academic-thoughts-on-the-poll-aggregators/",
    "title": "Some academic thoughts on the poll aggregators",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-11-08",
    "categories": [],
    "contents": "\nThe night of the presidential elections I wrote a post celebrating the victory of data over punditry. I was motivated by the personal attacks made against Nate Silver by pundits that do not understand Statistics. The post generated a little bit of (justified) nerdrage (see comment section). So here I clarify a couple of things not as a member of Nate Silver’s fan club (my mancrush started with PECOTA not fivethirtyeight) but as an applied statistician.\nThe main reason fivethrityeight predicts election results so well is mainly due to the idea of averaging polls. This idea was around way before fivethirtyeight started. In fact, it’s a version of meta-analysis which has been around for hundreds of years and is commonly used to improve results of clinical trials. This election cycle several groups,  including Sam Wang (Princeton Election Consortium), Simon Jackman (pollster), and Drew Linzer (VOTAMATIC), predicted the election perfectly using this trick. \nWhile each group adds their own set of bells and whistles, most of the gains come from the aggregation of polls and understanding the concept of a standard error. Note that while each individual poll may be a bit biased, historical data shows that these biases average out to 0. So by taking the average you obtain a close to unbiased estimate. Because there are so many pollsters, each one conducting several polls, you can also estimate the standard error of your estimate pretty well (empirically rather than theoretically).  I include a plot below that provides evidence that bias is not an issue and that standard errors are well estimated. The dash line is at +/- 2 standard erros based on the average (across all states) standard error reported by fivethirtyeight. Note that the variability is smaller for the battleground states where more polls were conducted (this is consistent with state-specific standard error reported by fivethirtyeight).\nFinally, there is the issue of the use of the word “probability”. Obviously one can correctly state that there is a 90% chance of observing event A and then have it not happen: Romney could have won and the aggregators still been “right”. Also frequentists complain when we talk about the probability of something that only will happen once? I actually don’t like getting into this philosophical discussion (Gelman has some thoughts worth reading) and I cut people who write for the masses some slack. If the aggregators consistently outperform the pundits in their predictions I have no problem with them using the word “probability” in their reports. I look forward to some of the post-election analysis of all this.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-07-nate-silver-does-it-again-will-pundits-finally-accept/",
    "title": "Nate Silver does it again! Will pundits finally accept defeat?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-11-07",
    "categories": [],
    "contents": "\nMy favorite statistician did it again! Just like in 2008, he predicted the presidential election results almost perfectly. For those that don’t know, Nate Silver is the statistician that runs the fivethirtyeight blog. He combines data from hundreds of polls, uses historical data to weigh them appropriately and then uses a statistical model to run simulations and predict outcomes.\nWhile the pundits were claiming the race was a “dead heat”, the day before the election Nate gave Obama a 90% chance of winning. Several pundits attacked Nate (some attacks were personal) for his predictions and demonstrated their ignorance of Statistics. Jeff wrote a nice post on this. The plot below demonstrates how great Nate’s prediction was. Note that each of the 45 states (including DC) for which he predicted a 90% probability or higher of winning for candidate A, candidate A won. For the other 6 states the range of percentages was 48-52%. If Florida goes for Obama he will have predicted every single state correctly.\nUpdate: Congratulations also to Sam Wang (Princeton Election Consortium) and Simon Jackman (pollster) that also called the election perfectly. And thanks to the pollsters that provided the unbiased (on average) data used by all these folks. Data analysts won “experts” lost.\nUpdate 2: New plot with data from here. Old graph here.\nObserved versus predicted\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-05-if-we-truly-want-to-foster-collaboration-we-need-to/",
    "title": "If we truly want to foster collaboration, we need to rethink the \"independence\" criteria during promotion",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-11-05",
    "categories": [],
    "contents": "\n\nWhen I talk about collaborative work, I don’t mean spending a day or two helping compute some p-values and end up as middle author in a subject-matter paper. I mean spending months working on a project, from start to finish, with experts from other disciplines to accomplish a goal that can only be accomplished with a diverse team. Many papers in genomics are like this (the ENOCDE  and 1000 genomes papers for example). Investigators A dreams up the biology, B develops the technology, C codes up algorithms to deal with massive data, while D analyzes the data and assess uncertainty, with the results reported in one high profile paper. I illustrate the point with genomics because it’s what I know best, but examples abound in other specialties as well. \n\n\nFostering collaborative research seems to be a priority for most higher education institutions. Both funding agencies and universities are creating initiative after initiative to incentivize team science. But at the same time the appointments and promotions process rewards researchers that have demonstrated “independence”. If we are not careful it may seem like we are sending mixed signals. I know of young investigators that have been advised to set time aside to demonstrate independence by publishing papers without their regular collaborators. This advice assumes that one can easily balance collaborative and independent research. But here is the problem: truly collaborative work can take just as much time and intellectual energy as independent research, perhaps more. Because time is limited, we might inadvertently be hindering the team science we are supposed to be fostering. Time spent demonstrating independence is time not spent working on the next high impact project.\n\n\nI understand the argument for striving to hire and promote scholars that can excel no matter the context. But I also think it is unrealistic to compete in team science if we don’t find a better way to promote those that excel in collaborative research as well. It is a mistake to think that scholars that excel in solo research can easily succeed in team science. In fact, I have seen several examples of specializations, that are important to the university, in which the best work is being produced by a small team.  At the same time, “independent” researchers all over the country are also working in these areas and publishing just as many papers. But the influential work is coming almost exclusively from the team. Whom should your university hire and promote in this particular area? To me it seems clear that it is the team. But for them to succeed we can’t get in their way by requiring each individual member to demonstrate “independence” in the traditional sense.\n\n\n \n\n\n \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-04-elite-education-for-the-masses/",
    "title": "Elite education for the masses",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-11-04",
    "categories": [],
    "contents": "\nElite education for the masses\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-04-sunday-data-statistics-link-roundup-11-4-12/",
    "title": "Sunday Data/Statistics Link Roundup (11/4/12)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-11-04",
    "categories": [],
    "contents": "\nBrian Caffo headlines the WaPo article about massive online open courses. He is the driving force behind our department’s involvement in offering these massive courses. I think this sums it up: `“I can’t use another word than unbelievable,” Caffo said. Then he found some more: “Crazy . . . surreal . . . heartwarming.”’\nA really interesting discussion of why “A Bet is a Tax on B.S.”. It nicely describes why intelligent betters must be disinterested in the outcome, otherwise they will end up losing money. The Nate Silver controversy just doesn’t seem to be going away, good news for his readership numbers, I bet. (via Rafa)\nAn interesting article on how scientists are not claiming global warming is the sole cause of the extreme weather events we are seeing, but that it does contribute to them being more extreme. The key quote: “We can’t say that steroids caused any one home run by Barry Bonds, but steroids sure helped him hit more and hit them farther. Now we have weather on steroids.” —Eric Pooley. (via Roger)\nThe NIGMS is looking for a Biomedical technology, Bioinformatics, and Computational Biology Director. I hope that it is someone who understands statistics! (via Karl B.)\nHere is another article that appears to misunderstand statistical prediction.  This one is about the Italian scientists who were jailed for failing to predict an earthquake. No joke. \nWe talk a lot about how much the data revolution will change industries from social media to healthcare. But here is an important reality check. Patients are not showing an interest in accessing their health care data. I wonder if part of the reason is that we haven’t come up with the right ways to explain, understand, and utilize what is inherently stochastic and uncertain information. \nThe BMJ is now going to require all data from clinical trials published in their journal to be public.  This is a brilliant, forward thinking move. I hope other journals will follow suit. (via Karen B.R.)\nAn interesting article about the impact of retractions on citation rates, suggesting that papers in fields close to those of the retracted paper may show negative impact on their citation rates. I haven’t looked it over carefully, but how they control for confounding seems incredibly important in this case. (via Alex N.). \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-11-03-the-year-of-the-mooc/",
    "title": "The Year of the MOOC",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-11-03",
    "categories": [],
    "contents": "\nThe Year of the MOOC\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-31-microsoft-seeks-an-edge-in-analyzing-big-data/",
    "title": "Microsoft Seeks an Edge in Analyzing Big Data",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-31",
    "categories": [],
    "contents": "\nMicrosoft Seeks an Edge in Analyzing Big Data\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-30-on-weather-forecasts-nate-silver-and-the/",
    "title": "On weather forecasts, Nate Silver, and the politicization of statistical illiteracy",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-30",
    "categories": [],
    "contents": "\nAs you know, we have a thing for statistical literacy here at Simply Stats. So of course this column over at Politico got our attention (via Chris V. and others). The column is an attack on Nate Silver, who has a blog where he tries to predict the outcome of elections in the U.S., you may have heard of it…\nThe argument that Dylan Byers makes in the Politico column is that Nate Silver is likely to be embarrassed by the outcome of the election if Romney wins. The reason is that Silver’s predictions have suggested Obama has a 75% chance to win the election recently and that number has never dropped below 60% or so. \nI don’t know much about Dylan Byers, but from reading this column and a quick scan of his twitter feed, it appears he doesn’t know much about statistics. Some people have gotten pretty upset at him on Twitter and elsewhere about this fact, but I’d like to take a different approach: education. So Dylan, here is a really simple example that explains how Nate Silver comes up with a number like the 75% chance of victory for Obama. \nLet’s pretend, just to make the example really simple, that if Obama gets greater than 50% of the vote, he will win the election. Obviously, Silver doesn’t ignore the electoral college and all the other complications, but it makes our example simpler. Then assume that based on averaging a bunch of polls  we estimate that Obama is likely to get about 50.5% of the vote.\nNow, we want to know what is the “percent chance” Obama will win, taking into account what we know. So let’s run a bunch of “simulated elections” where on average Obama gets 50.5% of the vote, but there is variability because we don’t have the exact number. Since we have a bunch of polls and we averaged them, we can get an estimate for how variable the 50.5% number is. The usual measure of variance is the standard deviation. Say we get a standard deviation of 1% for our estimate. That would be a pretty accurate number, but not totally unreasonable given the amount of polling data out there. \nWe can run 1,000 simulated elections like this in R* (a free software programming language, if you don’t know R, may I suggest Roger’s Computing for Data Analysis class?). Here is the code to do that. The last line of code calculates the percent of times, in our 1,000 simulated elections, that Obama wins. This is the number that Nate would report on his site. When I run the code, I get an Obama win 68% of the time (Obama gets greater than 50% of the vote). But if you run it again that number will vary a little, since we simulated elections. \nThe interesting thing is that even though we only estimate that Obama leads by about 0.5%, he wins 68% of the simulated elections. The reason is that we are pretty confident in that number, with our standard deviation being so low (1%). But that doesn’t mean that Obama will win 68% of the vote in any of the elections! In fact, here is a histogram of the percent of the vote that Obama wins: \n\nHe never gets more than 54% or so and never less than 47% or so. So it is always a reasonably close election. Silver’s calculations are obviously more complicated, but the basic idea of simulating elections is the same. \nNow, this might seem like a goofy way to come up with a “percent chance” with simulated elections and all. But it turns out it is actually a pretty important thing to know and relevant to those of us on the East Coast right now. It turns out weather forecasts (and projected hurricane paths) are based on the same sort of thing - simulated versions of the weather are run and the “percent chance of rain” is the fraction of times it rains in a particular place. \nSo Romney may still win and Obama may lose - and Silver may still get a lot of it right. But regardless, the approach taken by Silver is not based on politics, it is based on statistics. Hopefully we can move away from politicizing statistical illiteracy and toward evaluating the models for the real, underlying assumptions they make. \n* In this case, we could calculate the percent of times Obama would win with a formula (called an analytical calculation) since we have simplified so much. In Nate’s case it is much more complicated, so you have to simulate. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-29-computing-for-data-analysis-simply-statistics-edition/",
    "title": "Computing for Data Analysis (Simply Statistics Edition)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-29",
    "categories": [],
    "contents": "\nAs the entire East Coast gets soaked by Hurricane Sandy, I can’t help but think that this is the perfect time to…take a course online! Well, as long as you have electricity, that is. I live in a heavily tree-lined area and so it’s only a matter of time before the lights cut out on me (I’d better type quickly!). \nI just finished teaching my course Computing for Data Analysis through Coursera. This was my first experience teaching a course online and definitely my first experience teaching a course to > 50,000 people. There were definitely some bumps along the road, but the students who participated were fantastic at helping me smooth the way. In particular, the interaction on the discussion forums was very helpful. I couldn’t have done it without the students’ help. So, if you took my course over the past 4 weeks, thanks for participating!\nHere are a couple quick stats on the course participation (as of today) for the curious:\n50,899: Number of students enrolled\n27,900: Number of users watching lecture videos\n459,927: Total number of streaming views (over 4 weeks)\n414,359: Total number of video downloads (not all courses allow this)\n14,375: Number of users submitting the weekly quizzes (graded)\n6,420: Number of users submitting the bi-weekly R programming assignments (graded)\n6393+3291: Total number of posts+comments to the discussion forum\n314,302: Total number of views in the discussion forum\nI’ve received a number of emails from people who signed up in the middle of the course or after the course finished. Given that it was a 4-week course, signing up in the middle of the course meant you missed quite a bit of material. I will eventually be closing down the Coursera version of the course—at this point it’s not clear when it will be offered again on that platform but I would like to do so—and so access to the course material will be restricted. However, I’d like to make that material more widely available even if it isn’t in the Coursera format.\nSo I’m announcing today that next month I’ll be offering the Simply Statistics Edition of Computing for Data Analysis. This will be a slightly simplified version of the course that was offered on Coursera since I don’t have access to all of the cool platform features that they offer. But all of the original content will be available, including some new material that I hope to add over the coming weeks.\nIf you are interested in taking this course or know of someone who is, please check back here soon for more details on how to sign up and get the course information.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-28-sunday-data-statistics-link-roundup-10-28-12/",
    "title": "Sunday Data/Statistics Link Roundup (10/28/12)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-28",
    "categories": [],
    "contents": "\nAn important article about anti-science sentiment in the U.S. (via David S.). The politicization of scientific issues such as global warming, evolution, and healthcare (think vaccination) makes the U.S. less competitive. I think the lack of statistical literacy and training in the U.S. is one of the sources of the problem. People use/skew/mangle statistical analyses and experiments to support their view and without a statistically well trained public, it all looks “reasonable and scientific”. But when science seems to contradict itself, it loses credibility. Another reason to teach statistics to everyone in high school.\nScientific American was loaded this last week, here is another article on cancer screening.  The article covers several of the issues that make it hard to convince people that screening isn’t always good. The predictive value of the positive confusion is a huge one in cancer screening right now. The author of the piece is someone worth following on Twitter @hildabast.\nA bunch of data on the use of Github. Always cool to see new data sets that are worth playing with for student projects, etc. (via Hilary M.). \nA really interesting post over at Stats Chat about why we study seemingly obvious things. Hint, the reason is that “obvious” things aren’t always true. \nA story on “sentiment analysis” by NPR that suggests that most of the variation in a stock’s price during the day can be explained by the number of Facebook likes. Obviously, this is an interesting correlation. Probably more interesting for hedge funders/stockpickers if the correlation was with the change in stock price the next day. (via Dan S.)\nYihui Xie visited our department this week. We had a great time chatting with him about knitr/animation and all the cool work he is doing. Here are his slides from the talk he gave. Particularly check out his idea for a fast journal. You are seeing the future of publishing.  \nBonus Link: R is a trendy open source technology for big data. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-26-i-love-those-first-discussions-about-a-new-research/",
    "title": "I love those first discussions about a new research project",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-26",
    "categories": [],
    "contents": "\nThat has got to be the best reason to stay in academia. The meetings where it is just you and a bunch of really smart people thinking about tackling a new project, coming up with cool ideas, and dreaming about how you can really change the way the world works are so much fun.\nThere is no part of a research job that is better as far as I’m concerned.  It is always downhill after that, you start running into pebbles, your code doesn’t work, or your paper gets rejected. But that first blissful planning meeting always seems so full of potential.\nJust had a great one like that and am full of optimism.  \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-23-lets-make-the-joint-statistical-mettings-modular/",
    "title": "Let's make the Joint Statistical Mettings modular",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-23",
    "categories": [],
    "contents": "\nHave you ever met a statistician that enjoys the joint statistical meetings (JSM)? I haven’t. With the exception of the one night we catch up with old friends there are few positive things we can say about JSM.They are way too big and the two talks I want to see are always somehow scheduled at the same time as mine.\nBut statisticians actually like conferences. Most of us have a favorite statistics conference, or session within a bigger subject matter conference, that we look forward to going to. But it’s never JSM. So why can’t JSM just be a collection of these conferences? For sure we should drop the current format and come up with something new.\nI propose that we start by giving each ASA section two non-concurrent sessions scheduled on two consecutive days (perhaps more slots for bigger sections) and let them do whatever they want. Hopefully they would turn this into the conference that they want to go to. It’s our meeting, we pay for it, so let’s turn it into something we like.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:01:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-22-a-statistical-project-bleg-urgent-ish/",
    "title": "A statistical project bleg (urgent-ish)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-22",
    "categories": [],
    "contents": "\nWe all know that politicians can play it a little fast and loose with the truth. This is particularly true in debates, where politicians have to think on their feet and respond to questions from the audience or from each other. \nUsually, we find out about how truthful politicians are in the “post-game show”. The discussion of the veracity of the claims is usually based on independent fact checkers such as PolitiFact. Some of these fact checkers (Politifact in particular) live-tweet their reports on many of the issues discussed during the debate. This is possible, since both candidates have a pretty fixed set of talking points they use, so it is near real time fact-checking. \nWhat would be awesome is if someone could write an R script that would scrape the live data off of Politifact’s Twitter account and create a truthfullness meter that looks something like CNN’s instant reaction graph (see #7) for independent voters. The line would show the moving average of how honest each politician was being. How cool would it be to show the two candidates and how truthful they are being? If you did this, tell me it wouldn’t be a feature one of the major news networks would pick up…\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-21-sunday-data-statistics-link-roundup-10-21-12/",
    "title": "Sunday Data/Statistics Link Roundup (10/21/12)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-21",
    "categories": [],
    "contents": "\nThis is scientific variant on the #whatshouldwecallme meme isn’t exclusive to statistics, but it is hilarious. \nThis is a really interesting post that is a follow-up to the XKCD password security comic. The thing I find most interesting about this is that researchers realized the key problem with passwords was that we were looking at them purely from a computer science perspective. But _people _use passwords, so we need a person-focused approach to maximize security. This is a very similar idea to our previous post on an experimental foundation for statistics. Looks like Di Cook and others are already way ahead of us on this idea. It would be interesting to redefine optimality incorporating the knowledge that most of the time it is a person running the statistics. \nThis is another fascinating article about the math education wars. It starts off as the typical dueling schools issue in academia - two different schools of thought who routinely go after the other side. But the interesting thing here is it sounds like one side of this math debate is being waged by a person collecting data and the other is being waged by a side that isn’t. It is interesting how many areas are being touched by data - including what kind of math we should teach. \nI’m going to visit Minnesota in a couple of weeks. I was so pumped up to be an outlaw. Looks like I’m just a regular law abiding citizen though….\nHere are outstanding summaries of what went on at the Carl Morris Big Data conference this last week. Tons of interesting stuff there. Parts one, two, and three. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-20-free-online-education-is-now-illegal-in-minnesota/",
    "title": "Free Online Education Is Now Illegal in Minnesota",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-20",
    "categories": [],
    "contents": "\nFree Online Education Is Now Illegal in Minnesota\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-20-minnesota-clarifies-free-online-ed-is-ok/",
    "title": "Minnesota clarifies: Free online ed is OK",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-20",
    "categories": [],
    "contents": "\nMinnesota clarifies: Free online ed is OK\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-19-interview-with-rebecca-nugent-of-carnegie-mellon/",
    "title": "Simply Statistics Podcast #4: Interview with Rebecca Nugent",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-19",
    "categories": [],
    "contents": "\nInterview with Rebecca Nugent of Carnegie Mellon University.\nIn this episode Jeff and I talk with Rebecca Nugent, Associate Teaching Professor in the Department of Statistics at Carnegie Mellon University. We talk with her about her work with the Census and the growing interest in statistics among undergraduates.\n\n(Source: http://www.youtube.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-18-statistics-isnt-math-but-statistics-can-produce-math/",
    "title": "Statistics isn't math but statistics can produce math",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-18",
    "categories": [],
    "contents": "\nMathgen, the web site that can produce randomly generated mathematics papers has apparently gotten a paper accepted in a peer-reviewed journal (although perhaps not the most reputable one). I am not at all surprised this happened, but it’s fun to read both the paper and the reviewer’s comments. \n(Thanks to Kasper H. for the pointer.)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-17-johns-hopkins-grad-anthony-damico-shows-how-to/",
    "title": "Making Coffee With R",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-17",
    "categories": [],
    "contents": "\n[vimeo 43305640 w=500 h=281]\nJohns Hopkins grad Anthony Damico shows how to make coffee with R (except not really). The BLS mug is what makes it for me.\n\n(Source: http://player.vimeo.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-12T08:01:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-17-there-was-a-story-a-few-weeks-ago-on-npr-about-how/",
    "title": "Comparing Hospitals",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-17",
    "categories": [],
    "contents": "\nThere was a story a few weeks ago on NPR about how Medicare will begin fining hospitals that have 30-day readmission rates that are too high. This process was introduced in the Affordable Care Act and\n\nUnder the health care law, the penalties gradually will rise until 3 percent of Medicare payments to hospitals are at risk. Medicare is considering holding hospitals accountable on four more measures: joint replacements, stenting, heart bypass and treatment of stroke.\n\nThose of you taking my computing course on Coursera have already seen some of the data used to for this assessment, which can be obtained at the hospital compare web site. It’s also worth noting that underlying the analysis for this was a detailed and thoughtful report published by the Committee of Presidents of Statistical Societies (COPSS) which was chaired by Tom Louis, a Professor here at Johns Hopkins.\nThe report, titled “Statistical Issues in Assessing Hospital Performance” covers much of the current methodology and its criticisms and has a number of recommendations. Of particular concern for hospitals is the issue of shrinkage targets—in an hierarchical model the estimate of the readmission rate for a hospital is shrunken towards the mean. But which mean? Hospitals with higher risk or sicker patient populations will look quite a bit worse than hospitals sitting amongst a healthy population if they are both compared to the same mean.\nThe report is worth reading even if you’re just interested in the practical application of hierarchical models. And the web site is fun to explore if you want to know how the hospitals around you are fairing.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-15-a-statistician-loves-the-insurancepoll-now-how-do-we/",
    "title": "A statistician loves the #insurancepoll...now how do we analyze it?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-15",
    "categories": [],
    "contents": "\nAmanda Palmer broke Twitter yesterday with her insurance poll. She started off just talking about how hard it is for musicians who rarely have health insurance, but then wandered into polling territory. She sent out a request for people to respond with the following information:\n\nquick twitter poll. 1) COUNTRY?! 2) profession? 3) insured? 4) if not, why not, if so, at what cost per month (or covered by job)?\n\nThis quick little poll struck a nerve with people and her Twitter feed blew up. Long story short, tons of interesting information was gathered from folks. This information is frequently kept semi-obscured, particularly what is the cost of health insurance for folks in different places. This isn’t the sort of info that insurance companies necessarily publicize widely and isn’t the sort of thing people talk about. \nThe results were really fascinating and its worth reading the above blog post or checking out the hashtag: #insurancepoll. But the most fascinating thing for me as a statistician was thinking about how to analyze these data. @aubreyjaubrey is apparently collecting the data someplace, hopefully she’ll make it public. \nAt least two key issues spring to mind:\nThis is a massive convenience sample. \nIt is being collected through a social network\nAlthough I’m sure there are more. If a student is looking for an amazingly interesting/rich data set and some seriously hard stats problems, they should get in touch with Aubrey and see if they can make something of it!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-14-sunday-data-statistics-link-roundup-10-14-12/",
    "title": "Sunday Data/Statistics Link Roundup (10/14/12)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-14",
    "categories": [],
    "contents": "\nA fascinating article about the debate on whether to regulate sugary beverages. One of the protagonists is David Allison, a statistical geneticist, among other things. It is fascinating to see the interplay of statistical analysis and public policy. Yet another example of how statistics/data will drive some of the most important policy decisions going forward. \nA related article is this one on the way risk is reported in the media. It is becoming more and more clear that to be an educated member of society now means that you absolutely have to have a basic understanding of the concepts of statistics. Both leaders and the general public are responsible for the danger that lies in misinterpreting/misleading with risk. \nA press release from the Census Bureau about how the choice of college major can have a major impact on career earnings. More data breaking the results down by employment characteristics and major are here and here. These data update some of the data we have talked about before in calculating expected salaries by major. (via Scott Z.)\nAn interesting article about Recorded Future that describes how they are using social media data etc. to try to predict events that will happen. I think this isn’t an entirely crazy idea, but the thing that always strikes me about these sorts of project is how hard it is to measure success. It is highly unlikely you will ever exactly predict a future event, so how do you define how close you were? For instance, if you predicted an uprising in Egypt, but missed by a month, is that a good or a bad prediction? \nSeriously guys, this is getting embarrassing. An article appears in the New England Journal “finding” an association between chocolate consumption and Nobel prize winners.  This is, of course, a horrible statistical analysis and unless it was a joke to publish it, it is irresponsible of the NEJM to publish. I’ll bet any student in Stat 101 could find the huge flaws with this analysis. If the editors of the major scientific journals want to continue publishing statistical papers, they should get serious about statistical editing.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-10-whats-wrong-with-the-predicting-h-index-paper/",
    "title": "What's wrong with the predicting h-index paper.",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-10",
    "categories": [],
    "contents": "\nEditor’s Note: I recently posted about a paper in Nature that purported to predict the H-index. The authors contacted me to get my criticisms, then responded to those criticisms. They have requested the opportunity to respond publicly, and I think it is a totally reasonable request. Until there is a better comment generating mechanism at the journal level, this seems like as good a forum as any to discuss statistical papers. I will post an extended version of my criticisms here and give them the opportunity to respond publicly in the comments. \nThe paper in question is a clearly a clever idea and the kind that would get people fired up. Quantifying researchers output is all the rage and being able to predict this quantity in the future would obviously make a lot of evaluators happy. I think it was, in that sense, a really good idea to chase down these data, since it was clear that if they found anything at all it would be very widely covered in the scientific/popular press. \n\nMy original post was inspired out of my frustration with Nature, which has a history of publishing somewhat suspect statistical papers, such as this one. I posted the prediction contest after reading another paper I consider to be a flawed statistical paper, both for statistical reasons and for scientific reasons. I originally commented on the statistics in my post. The authors, being good sports, contacted me for my criticisms. I sent them the following criticisms, which I think are sufficiently major that a statistical referee or statistical journal would have likely rejected the paper:\n\n\n\nLack of reproducibility. The code/data are not made available either through Nature or on your website. This is a critical component of papers based on computation and has led to serious problems before. It is also easily addressable. \n\n\nNo training/test set. You mention cross-validation (and maybe the R^2 is the R^2 using the held out scientists?) but if you use the cross-validation step to optimize the model parameters and to estimate the error rate, you could see some major overfitting. \n\n\nThe R^2 values are pretty low. An R^2 of 0.67 is obviously superior to the h-index alone, but (a) there is concern about overfitting, and (b) even without overfitting, that low of R^2 could lead to substantial variance in predictions. \n\n\nThe prediction error is not reported in the paper (or in the online calculator). How far off could you be at 5 years, at 10? Would the results still be impressive with those errors reported?\n\n\nYou use model selection and show only the optimal model (as described in the last paragraph of the supplementary), but no indication of the potential difficulties with this model selection are made in the text. \n\n\nYou use a single regression model without any time variation in the coefficients and without any potential non-linearity. Clearly when predicting several years into the future there will be variation with time and non-linearity. There is also likely heavy variance in the types of individuals/career trajectories, and outliers may be important, etc. \n\n\nThey carefully responded to these criticisms and hopefully they will post their responses in the comments. My impression based on their responses is that the statistics were not as flawed as I originally thought, but that the data aren&#8217;t sufficient to form a useful prediction. \n\n\n\n\n\nHowever, I think the much bigger flaw is the basic scientific premise. The h-index has been identified as having major flaws, biases (including gender bias), and to be a generally poor summary of a scientist&#8217;s contribution. See <a href=\"http://blogs.nature.com/nautilus/2007/10/the_hindex_has_its_flaws.html\" target=\"_blank\">here<\/a>, the list of criticisms <a href=\"http://en.wikipedia.org/wiki/H-index\" target=\"_blank\">here<\/a>, and the discussion <a href=\"http://scholarlykitchen.sspnet.org/2008/06/30/the-h-index-an-objective-mismeasure/\" target=\"_blank\">here<\/a> for starters. The authors of the Nature paper propose a highly inaccurate predictor of this deeply flawed index. While that alone is sufficient to call into question the results in the paper, the authors also make bold claims about their prediction tool: \n\n\n\n  Our formula is particularly useful for funding agencies, peer reviewers and hir­ing committees who have to deal with vast \n<\/div>\n\n<div>\n  numbers of applications and can give each only a cursory examination. Statistical techniques have the advantage of returning \n<\/div>\n\n<div>\n  results instantaneously and in an unbiased way.\n<\/div>\n\n\nSuggesting that this type of prediction should be used to make important decisions on hiring, promotion, and funding is highly scientifically flawed. Coupled with the online calculator the authors handily provide (which produces no measure of uncertainty) it makes it all too easy for people to miss the real value of scientific publications: the science contained in them. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-08-why-we-should-continue-publishing-peer-reviewed-papers/",
    "title": "Why we should continue publishing peer-reviewed papers",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-08",
    "categories": [],
    "contents": "\nSeveral bloggers are calling for the end of peer-reviewed journals as we know them. Jeff suggest that we replace them with a system in which everyone posts their papers on their blog, pubmed aggregates the feeds, and peer-review happens post publication via, for example, counting up like and dislike votes. In my view, many of these critiques seem to conflate problems from different aspects of the process. Here I try to break down the current system into its key components and defend the one aspect I think we should preserve (at least for now): pre-publication peer-review.\nTo avoid confusion let me start by enumerating some of the components for which I agree change is needed.\nThere is no need to produce paper copies of our publications. Indulging our preference for reading hard copies does not justify keeping the price of disseminating our work twice as high as it should be. \nThere is no reason to be sending the same manuscript (adapted to fit guidelines) to several journals, until it gets accepted. This frustrating and time-consuming process adds very little value (we previously described Nick Jewell’s solution). \nThere is no reason for publications to be static. As Jeff and many others suggest, readers should be able to comment and rate systematically on published papers and authors should be able to update them.\nHowever, all these changes can be implemented without doing away with pre-publication peer-review.\nA key reason American and British universities consistently lead the pack of research institutions is their strict adherence to a peer-review system that minimizes cronyism and tolerance for mediocrity. At the center of this system is a promotion process in which outside experts evaluate a candidate’s ability to produce high quality ideas. Peer-reviewed journal articles are the backbone of this evaluation. When reviewing a candidate I familiarize myself with his or her work by reading 5-10 key papers. It’s true that I read these disregarding the journal and blog posts would serve the same purpose. But I also use the publication section of the CV not only because reading all papers is logistically impossible but because these have already been evaluated by ~ three referees plus an editor and provide an independent assessment to mine. I also use the journal’s prestige because although it is a highly noisy measure of quality, the law of large numbers starts kicking in after 10 papers or so. \nSo are three reviewers better than the entire Internet? Can a reddit-like system provide just as much signal as the current peer-reviewed journal? You can think of the current system as a cooperative in which we all agree to read each other’s papers thoroughly (we evaluate 2-3 for each one we publish) with journals taking care of the logistics. The result of a review is an estimate of quality ranging from highest (Nature, Science) to 0 (not published). This estimate is certainly noisy given the bias and quality variance of referees and editors. But, across all papers on a CV variance is reduced and bias averages out (I note that we complain vociferously when the bias keeps us from publishing in a good journal, but we rarely say a word when the bias helps us get into a better journal than deserved). Jeff’s argument is that post-publication review will result in many more evaluations and therefore a stronger signal to noise ratio. I need to see evidence of this before being convinced. In the current system ~ three referees commit to thoroughly reviewing of the paper. If they do a sloppy job, they will embarrass themselves with an editor or an AE (not a good thing). With the post-publication review system nobody is forced to review. I fear most papers will go without comment or votes, including really good ones. My feeling is that marketing and PR will matter even more than it does now and that’s not a good thing.\nDissemination of ideas is another important role of the literature. Jeff describes a couple of anecdotes to argue it can be sped up by just posting it on your blog.\n\nI posted a quick idea called the Leekasso, which led to some discussion on the blog, has nearly 2,000 page views\n\nBut the typical junior investigator does not have a blog with hundreds of followers. Will their papers ever be read if even more papers are added to the already bloated scientific literature? The current peer-review system provides an important filter. There is an inherent trade-off between speed of dissemination and quality and it’s not clear to me that we should swing the balance all the way over to the speed side. There are other ways to speed up dissemination that we should try first. Also there is nothing stopping us from posting our papers online before publication and promoting them via twitter or an aggregator. In fact, as pointed out by Jan Jensen on Jeff’s post,  arXiv papers are indexed on Google Scholar within a week, which also keeps track of arXiv citations.\nThe Internet is bringing many changes that will improve our peer-review system. But the current pre-publication peer-review process does a decent job of  \nproviding signal for the promotion process and\nreducing noise in the literature to make dissemination possible. \nAny alternative systems should be evaluated carefully before dismantling a system that has helped keep our Universities at the top of the world rankings.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-07-fraud-in-the-scientific-literature/",
    "title": "Fraud in the Scientific Literature",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-07",
    "categories": [],
    "contents": "\nFraud in the Scientific Literature\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-07-sunday-data-statistics-link-roundup-10-7-12/",
    "title": "Sunday Data/Statistics Link Roundup (10/7/12)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-07",
    "categories": [],
    "contents": "\nJack Welch got a little conspiracy-theory crazy with the job numbers. Thomas Lumley over at StatsChat makes a pretty good case for debunking the theory. I think the real take home message of Thomas’ post and one worth celebrating/highlighting is that agencies that produce the jobs report do so based on a fixed and well-defined study design. Careful efforts by government statistics agencies make it hard to fudge/change the numbers. This is an underrated and hugely important component of a well-run democracy. \nOn a similar note Dan Gardner at the Ottawa Citizen points out that evidence-based policy making is actually not enough. He points out the critical problem with evidence: in the era of data what is a fact? “Facts” can come from flawed or biased studies just as easily from strong studies. He suggests that a true “evidence based” administration would invest more money in research/statistical agencies. I think this is a great idea. \nAn interesting article by Ben Bernanke suggesting that an optimal approach (in baseball and in policy) is one based on statistical analysis, coupled with careful thinking about long-term versus short-term strategy. I think one of his arguments about allowing players to play even when they are struggling short term is actually a case for letting the weak law of large numbers play out. If you have a player with skill/talent, they will eventually converge to their “true” numbers. It’s also good for their confidence….(via David Santiago).\nHere is another interesting peer review dust-up. It explains why some journals “reject” papers when they really mean major/minor revision to be able to push down their review times. I think this highlights yet another problem with pre-publication peer review. The evidence is mounting, but I hear we may get a defense of the current system from one of the editors of this blog, so stay tuned…\nSeveral people (Sherri R., Alex N., many folks on Twitter) have pointed me to this article about gender bias in science. I initially was a bit skeptical of such a strong effect across a broad range of demographic variables. After reading the supplemental material carefully, it is clear I was wrong. It is a very well designed/executed study and suggests that there is still a strong gender bias in science, across ages and disciplines. Interestingly both men and women were biased against the female candidates. This is clearly a non-trivial problem to solve and needs a lot more work, maybe one step is to make recruitment packages more flexible (see the comment by Allison T. especially). \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-05-not-just-one-statistics-interview-john-mcgready-is/",
    "title": "Not just one statistics interview...John McGready is the Jon Stewart of statistics",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-05",
    "categories": [],
    "contents": "\nEditor’s Note: We usually reserve Friday’s for posting Simply Statistics Interviews. This week, we have a special guest post by John McGready, a colleague of ours who has been doing interviews with many of us in the department and has some cool ideas about connecting students in their first statistics class with cutting edge researchers wrestling with many of the same concepts applied to modern problems. I’ll let him explain…\nI teach a two quarter course in introductory biostatistics to master’s students in public health at Johns Hopkins.  The majority of the class is composed of MPH students, but there are also students doing professional master’s degrees in environmental health, molecular biology, health policy and mental health. Despite the short length of the course, it covers the “greatest hits” of biostatistics, encompassing everything from exploratory data analysis up through and including multivariable proportional hazards regression.  The course focus is more conceptual and less mathematical/computing centric than the other two introductory sequences taught at Hopkins: as such it has earned the unfortunate nickname “baby biostatistics” from some at the School.  This, in my opinion, is an unfortunate misnomer: statistical reasoning is often the most difficult part of learning statistics.  We spend a lot of time focusing on the current literature, and making sense or critiquing research by considering not only the statistical methods employed and the numerical findings, but also the study design and the logic of the substantive conclusions made by the study authors.\nVia the course, I always hope to demonstrate the importance  biostatistics as a core driver of public health discovery, the importance of statistical reasoning in the research process, and how the fundamentals that are covered are the framework for more advanced methodology. At some point it dawned on me that the best approach for doing this was to have my colleagues speak to my students about these ideas.  Because of timing and scheduling constraints, this proved difficult to do in a live setting.  However, in June of 2012 a video recording studio opened here at the Hopkins Bloomberg School. At this point, I knew that I had to get my colleagues on video so that I could share their wealth of experiences and expertise with my students, and give the students multiple perspectives. To my delight my colleagues are very amenable to being interviewed and have been very generous with their time. I plan to continue doing the interviews so long as my colleagues are willing and the studio is available.\nI have created a Youtube channel for these interviews.  At some point in the future, I plan to invite the biostatistics community as a whole to participate.  This will include interviews with visitors to my department, and submissions by biostatistics faculty and students from other schools. (I realize I am very lucky to have these facilities and video expertise at Hopkins: but many folks are tech savvy enough to film their own videos on their cameras, phones etc… in fact you have seen such creativity by the editors of this here blog). With the help of some colleagues I plan on making a complimentary website that will allow for easy submission of videos for posting, so stay tuned!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-04-should-we-stop-publishing-peer-reviewed-papers/",
    "title": "Should we stop publishing peer-reviewed papers?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-04",
    "categories": [],
    "contents": "\nNate Silver, everyone’s favorite statistician made good, just gave an interview where he said he thinks many journal articles should be blog posts. I have been thinking about this same issue for a while now, and I’m not the only one. This is a really interesting post suggesting that although scientific journals once facilitated dissemination of ideas, they now impede the flow of information and make it more expensive. \nTwo recent examples really drove this message home for me. In the first example, I posted a quick idea called the Leekasso, which led to some discussion on the blog, has nearly 2,000 page views (a pretty recent number of downloads for a paper), and has been implemented in software by someone other than me. If this were one of my papers, it would be one of the more reasonably high impact papers. The second example is a post I put up about a recent Nature paper. The authors (who are really good sports) ended up writing to me to get my critiques. I wrote them out, and they responded. All of this happened after peer review and informally. All of the interaction also occurred in email, where no one can see but us. \nIt wouldn’t take much to go to a blog-based system. What if everyone who was publishing scientific results started a blog (free), then there was a site, run by pubmed, that aggregated the feeds (this would be super cheap to set up/maintain). Then people could comment on blog posts and vote for ones they liked if they had verified accounts. We skipped peer review in favor of just producing results and discussing them. The results that were interesting were shared by email, Twitter, etc. \nWhy would we do this? Well, the current journal system: (1) significantly slows the publication of research, (2) costs thousands of dollars, and (3) costs significant labor that is not scientifically productive (such as resubmitting). \nAlmost every paper I have had published has been rejected at least one place, including the “good” ones. This means that the results of even the good papers have been delayed by months. Or in the case of one paper - a full year and a half of delay. Any time I publish open access, it costs me at minimum around $1,500. I like open access because I think science funded by taxpayers should be free. But it is a significant drain on the resources of my group. Finally, most of the resubmission process is wasted labor. It generally doesn’t produce new science or improve the quality of the science. The effort is just in reformatting and re-inputing information about authors.\nSo why not have everyone just post results on their blog/figshare. They’d have a DOI that could be cited. We’d reduce everyone’s labor in reviewing/editing/resubmitting by an order of magnitude or two and save the taxpapers a few thousand dollars each a year in publication fees. We’d also increase the speed of updating/reacting to new ideas by an order of magnitude. \nI still maintain we should be evaluating people based on reading their actual work, not  highly subjective and error-prone indices. But if the powers that be insisted, it would be easy to evaluate people based on likes/downloads/citations/discussion of papers rather than on the basis of journal titles and the arbitrary decisions of editors. \nSo should we stop publishing peer review papers?\nEdit: Titus points to a couple of good posts with interesting ideas about the peer review process that are worth reading, here and here. Also, Joe Pickrell et al. are already on this for population genetics, having set up the aggregator Haldane’s Sieve. It would be nice if this expanded to other areas (and people got credit for the papers published there, like they do for papers in journals). \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-04-statistics-project-ideas-for-students-part-2/",
    "title": "Statistics project ideas for students (part 2)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-04",
    "categories": [],
    "contents": "\nA little while ago I wrote a post on statistics projects ideas for students. In honor of the first Simply Statistics Coursera offering, Computing for Data Analysis, here is a new list of student projects for folks excited about trying out those new R programming skills. Again we have rated each project with my best guess difficulty and effort required. Happy computing!\nData Analysis\nUse city data to predict areas with the highest risk for parking tickets. Here is the data for Baltimore. (Difficulty: Moderate, Effort: Low/Moderate)\nIf you have a Fitbit with a premium account, download the data into a spreadsheet (or get Chris’s data)  Then build various predictors using the data: (a) are you running or walking, (b) are you having a good day or not, (c) did you eat well that day or not, (d) etc. For special bonus points create a blog with your new discoveries and share your data with the world. (Difficulty: Depends on what you are trying to predict, Effort: Moderate with Fitbit/Jawbone/etc.)\nData Collection/Synthesis\nMake a list of skills associated with each component of the Data Scientist Venn Diagram. Then update the data scientist R function described in this post to ask a set of questions, then plot people on the diagram. Hint, check out the readline() function. (_Difficulty: Moderately low, Effort:__Moderate)_\nHealthData.gov has a ton of data from various sources about public health, medicines, etc. Some of this data is super useful for projects/analysis and some of it is just data dumps. Create an R package that downloads data from healthdata.gov and gives some measures of how useful/interesting it is for projects (e.g. number of samples in the study, number of variables measured, is it summary data or raw data, etc.) (Difficulty: Moderately hard, Effort: High)\nBuild an up-to-date aggregator of R tutorials/how-to videos, summarize/rate each one so that people know which ones to look at for learning which tasks. (Difficulty: Low, Effort: Medium)\nTool building\nBuild software that creates a 2-d author list and averages people’s 2-d author lists. (Difficulty: Medium, Effort: Low)\nCreate an R package that interacts with and downloads data from government websites and processes it in a way that is easy to analyze. (Difficulty: Medium, Effort: High)\n_\n_\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-03-2-d-author-lists/",
    "title": "2-D author lists",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-03",
    "categories": [],
    "contents": "\nThe order of authors on scientific papers matters a lot. The best places to be on a paper vary by field. But typically the first and the corresponding (usually last) authors are the prime real estate. When people are evaluated on the job market, for promotion, or to get grants, the number of first/corresponding author papers can be the difference between success and failure. \nAt the same time, many journals list “authors contributions” at the end of the manuscript, but this is rarely prominently displayed. The result is that regardless of the true distribution of credit in a manuscript, the first and last authors get the bulk of the benefit. \nThis system is antiquated for a few reasons:\nIn multidisciplinary science, there are often equal and very different contributions from people working in different disciplines. \nScience is increasing collaborative, even within a single discipline and papers are rarely the effort of 2 people anymore. \nHow about a 2-D, resortable author list? Each author is a column and each kind of contribution is a row. The contributions are: (1) conceived the idea, (2) collected the data, (3) did the computational analysis, (4) wrote the paper (you could imagine adding others). Each category then gets a quantitative number, fraction of the effort to that component of the paper. Then you build an interactive graphic that allows you to sort the authors by each of the categories. So you could see who did what on the paper. \nTo get an overall impression of which activities an author performs, you could average their contribution across papers in each of the categories. Creating a “heatmap of contributions”. Anyone want to build this? \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-03-this-is-an-awesome-paper-all-students-in-statistics/",
    "title": "This is an awesome paper all students in statistics should read",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-03",
    "categories": [],
    "contents": "\nThe paper is a review of how to do software development for academics. I saw it via C. Titus Brown (who we have interviewed), he is also a co-author. How to write software (particularly for other people) is something that is under emphasized in many curricula. But it turns out this is also one of the more important components of disseminating your work in modern applied statistics. My only wish is that there was an accompanying website with resources/links for people to chase down. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-02-the-more-statistics-blogs-the-better/",
    "title": "The more statistics blogs the better",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-10-02",
    "categories": [],
    "contents": "\nGood friend and friend of the blog Rob Gould has started a statistics blog called Citizen Statistician. What is a citizen statistician, you ask?\n\nWhat is a citizen statistician? A citizen statistician participates in formal and informal data gathering. A citizen statistician is aware of his or her data trail, and is aware of the harm that could be done to themselves or to others through data aggregation. Citizen statisticians recognize opportunities to improve their personal or professional lives through analyzing data, and know how to share data with others. They know that almost any question about the world can be answered using data, how to find relevant data sources on the web, and critically evaluate these sources. A citizen statistician also knows how to bring that data into an analysis package and how to start their investigation.\n\nWhat’s even better than having more statistics blogs? Having more statisticians.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-28-john-mcgready-interviews-the-esteemed-jeff-leek/",
    "title": "John McGready interviews Jeff Leek",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-28",
    "categories": [],
    "contents": "\nJohn McGready interviews the esteemed Jeff Leek. This is bearded Jeff, in case you were wondering.\n\n(Source: http://www.youtube.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-27-john-mcgready-a-fellow-faculty-member-in-the/",
    "title": "John McGready interviews Roger Peng",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-27",
    "categories": [],
    "contents": "\nJohn McGready, a fellow faculty member in the Department of Biostatistics, interviewed me for his Statistical Reasoning class. In the interview we talk about some statistical contributions to air pollution epidemiology.\n\n(Source: http://www.youtube.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-27-simply-statistics-logo-contest/",
    "title": "Simply statistics logo contest",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-27",
    "categories": [],
    "contents": "\nSimply Statistics has had the same logo since Roger grabbed the first picture in his results folder that looked “statistics related”. While we do have some affection for the logo, we would like something a little more catchy.\nSo we would like to announce a contest to create our new logo. Here are the rules:\nAll submissions must be sent to Roger with the email subject, “Simply Statistics Logo Contest”\nThe logo must be generated with reproducible R code. Here is an example of how Simple created their logo for inspiration. \nIdeally the logo will convey the “spirit of the blog”: we like data, we like keeping it simple, we like solving real problems, and we like to stir it up.\nHave at it!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-26-nbc-unpacks-trove-of-data-from-olympics/",
    "title": "NBC Unpacks Trove of Data From Olympics",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-26",
    "categories": [],
    "contents": "\nNBC Unpacks Trove of Data From Olympics\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-26-pro-tips-for-graduate-students-part-3/",
    "title": "Pro-tips for graduate students (Part 3)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-26",
    "categories": [],
    "contents": "\nThis is part of the ongoing series of pro tips for graduate students, check out parts one and two for the original installments. \nLearn how to write papers in a very clear and simple style. Whenever you can write in plain English, skip jargon as much as possible, and make the approach you are using understandable and clear. This can (sometimes) make it harder to get your papers into journals. But simple, clear language leads to much higher use/citation of your work. Examples of great writers are: John Storey, Rob Tibshirani, Robert May, Martin Nowak, etc.\nIt is a great idea to start reviewing papers as a graduate student. Don’t do too many, you should focus on your research, but doing a few will give you a lot of insight into how the peer-review system works. Ask your advisor/research mentor they will generally have a review or two they could use help with. When doing reviews, keep in mind a person spent a large chunk of time working on the paper you are reviewing. Also, don’t forget to use Google.\nTry to write your first paper as soon as you possibly can and try to do as much of it on your own as you can. You don’t have to wait for faculty to give you ideas, read papers and think of what you think would have been better (you might check with a faculty member first so you don’t repeat what’s done, etc.). You will learn more writing your first paper than in almost any/all classes.\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-24-computing-for-data-analysis-starts-today/",
    "title": "Computing for Data Analysis starts today!",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-24",
    "categories": [],
    "contents": "\nToday marks the first Simply Statistics course offering happening over at Coursera. I’ll be teaching Computing for Data Analysis over the next four weeks. There’s still plenty of time to register if you are interested in learning about R and the activity on the discussion forums is already quite vibrant.\nAlso starting today is my colleague Brian Caffo’s Mathematical Biostatistics Bootcamp, which I hear also has had an energetic start. With any luck, the students in that class may get to see Brian dressed in military fatigues.\nThis is my first MOOC so I have no idea how it will go. But I’m excited to start and am looking forward to the next four weeks.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-23-sunday-data-statistics-link-roundup-9-23-12/",
    "title": "Sunday Data/Statistics Link Roundup (9/23/12)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-23",
    "categories": [],
    "contents": "\nHarvard Business school is getting in on the fun, calling the data scientist the sexy profession for the 21st century. Although I am a little worried that by the time it gets into a Harvard Business document, the hype may be outstripping the real promise of the discipline. Still, good news for statisticians! (via Rafa via Francesca D.’s Facebook feed). \nThe counterpoint is this article which suggests that data scientists might be able to be replaced by tools/software. I think this is also a bit too much hype for my tastes. Certain things will definitely be automated and we may even end up with a deterministic statistical machine or two. But there will continually be new problems to solve which require the expertise of people with data analysis skills and good intuition (link via Samara K.)\nA bunch of websites are popping up where you can sign up and have people take your online courses for you. I’m not going to give them the benefit of a link, but they aren’t hard to find these days. The thing I don’t understand is, if it is a free online course, why have someone else take it for you? It’s free, its in your spare time, and the bar for passing is pretty low (links via Sherri R. redacted)….\nMaybe mostly useful for me, but for other people with Tumblr blogs, here is a way to insert Latex.\nBrian Caffo shares his impressions of the SAMSI massive data workshop.  He raises an important issue which definitely deserves more discussion: should we be focusing on specific or general problems? Worth a read. \nFor the people into self-tracking, Chris V. points to an app created by the University of Indiana that lets people track their sexual activity. The most interesting thing about that app is how it highlights a key and I suppose often overlooked issue with analyzing self-tracking data. Despite the size of these data sets, they are still definitely biased samples. It’s only a brave few who will tell the University of Indiana all about their sex life. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-21-prediction-contest/",
    "title": "Prediction contest",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-21",
    "categories": [],
    "contents": "\nI have been seeing this paper all over Twitter/the blogosphere. It’s a sexy idea: can you predict how “high-impact” a scientist will be in the future. It is also a pretty flawed data analysis…so this weeks prediction contest is to identify why the statistics in this paper are so flawed. In my first pass read I noticed about 5 major flaws.\nEditor’s note: I posted the criticisms and the authors respond here: http://disq.us/8bmrhl\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T14:00:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-20-every-professor-is-a-startup/",
    "title": "Every professor is a startup",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-20",
    "categories": [],
    "contents": "\nThere has been a lot of discussion lately about whether to be in academia or industry. Some of it I think is a bit unfair to academia. Then I saw this post on Quora asking what Hilary Mason’s contributions were to machine learning, like she hadn’t done anything. It struck me as a bit of academia hating on industry*. I don’t see why one has to be better/worse than the other, as Roger points out, there is no perfect job and it just depends on what you want to do. \nOne thing that I think gets lost in all of this are the similarities between being an academic researcher and running a small startup. To be a successful professor at a research institution, you have to create a product (papers/software), network (sit on editorial boards/review panels), raise funds (by writing grants), advertise (by giving talks/presentations), identify and recruit talent (students and postdocs), manage people and personalities (students,postdocs, collaborators) and scale (you start as just yourself, and eventually grow to a group with lots of people). \nThe goals are somewhat different. In a startup company, your goal is ultimately to become a profitable business. In academia, the goal is to create an enterprise that produces scientific knowledge. But in either enterprise it takes a huge amount of entrepreneurial spirit, passion, and hustle. It just depends on how you are spending your hustle. \n_*Sidenote: One reason I think she is so famous is that she helps people, even people that can’t necessarily do anything for her. One time I wrote her out of the blue to see if we could get some Bitly data to analyze for a class. She cheerfully helped us get it, even though the immediate payout for her was not obvious. But I tell you what, when people ask me about her, I’ll tell them she is awesome. _\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-20-in-data-science-its-the-problem-stupid/",
    "title": "In data science - it's the problem, stupid!",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-20",
    "categories": [],
    "contents": "\nI just saw this article talking about how in the biotech world, you can’t get caught chasing the latest technology. You have to start with a problem you are solving for people and then work your way back. This reminds me a lot of Type B problems in data science/statistics. We have a pile of data, so we don’t need to have a problem to solve, it will come to us later. I think the answer to the question, “Did you start with a scientific/business problem that needs solving regardless of whether the data was in place?” will end up being a near perfect classifier for separating the “Big Data” projects that are just hype from the ones that will pan out long term. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-18-chinese-company-to-acquire-dna-sequencing-firm/",
    "title": "Chinese Company to Acquire DNA Sequencing Firm",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-18",
    "categories": [],
    "contents": "\nChinese Company to Acquire DNA Sequencing Firm\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-18-online-mentors-to-guide-women-into-the-sciences/",
    "title": "Online Mentors to Guide Women Into the Sciences",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-18",
    "categories": [],
    "contents": "\nOnline Mentors to Guide Women Into the Sciences\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-16-sunday-data-statistics-link-roundup-9-16-12/",
    "title": "Sunday Data/Statistics Link Roundup (9/16/12)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-16",
    "categories": [],
    "contents": "\nThere has been a lot of talk about the Michael Lewis (of Moneyball fame) profile of Obama in Vanity fair. One interesting quote I think deserves a lot more discussion is: “On top of all of this, after you have made your decision, you need to feign total certainty about it. People being led do not want to think probabilistically.” This is a key issue that is only going to get worse going forward. All of public policy is probabilistic - we are even moving to clinical trials to evaluate public policy. \nIt’s sort of amazing to me that I hadn’t heard about this before, but a UC Davis professor was threatened for discussing the reasons PSA screening may be overused. This same issue keeps coming up over and over - screening healthy populations for rare diseases is often not effective (you need a ridiculously high specificity or a treatment with almost no side effects). What we need is John McGready to do a claymation public service video or something explaining the reasons screening might not be a good idea to the general public. \nA bleg - I sometimes have a good week finding links myself and there are a few folks who regularly send links (Andrew J., Alex N., etc.) I’d love it if people would send me cool links when they see them with the email title, “Sunday LInks” - i’m sure there is more cool stuff out there. \nThe ICSB has a competition to improve the coverage of computational biology on Wikipedia. Someone should write a surrogate variable analysis or robust multiarray average article. \nI had not hear of the ASA’s Stattrak until this week, it looks like there are some useful resources there for early career statisticians. With the onset of fall, it is closing in on a new recruiting season. If you are a postdoc/student on the job market and you haven’t read Rafa’s post on soft vs. hard money, now is the time to start brushing up! Stay tuned for more job market posts this fall from Simply Statistics. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-15-statistical-analysis-suggests-the-washington-nationals/",
    "title": "Statistical analysis suggests the Washington Nationals were wrong to shut down Stephen Strasburg",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-15",
    "categories": [],
    "contents": "\nStatistical analysis suggests the Washington Nationals were wrong to shut down Stephen Strasburg\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-14-the-statistical-method-made-me-lie/",
    "title": "The statistical method made me lie",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-14",
    "categories": [],
    "contents": "\nThere’s a hubbub brewing over a recent study published in the Annals of Internal Medicine that compares organic food (as in ‘USDA Organic’) to non-organic food. The study, titled “Are Organic Foods Safer or Healthier Than Conventional Alternatives?: A Systematic Review” is a meta-analysis of about 200 previous studies. Their conclusion, which I have cut-and-pasted below, is\n\nThe published literature lacks strong evidence that organic foods are significantly more nutritious than conventional foods. Consumption of organic foods may reduce exposure to pesticide residues and antibiotic-resistant bacteria.\n\nWhen I first heard about this study on the radio, I thought the conclusion seemed kind of obvious. It’s not clear to me why, for example, an organic carrot would have more calcium than a non-organic carrot. At least, I couldn’t explain the mechanism by which this would happen. However, I would expect that an organic carrot would have less pesticide residue than a non-organic carrot. If not, then the certification isn’t really achieving its goals. Lo and behold, that’s more or less what the study found. I don’t see the controversy.\nBut there’s a petition over at change.org titled “Retract the Flawed ‘Organic Study’ Linked to Big Tobacco and Pro-GMO Corps”. It’s quite an interesting read. First, it’s worth noting that the study itself does not list any funding sources. Given that the authors are from Stanford, one could conclude that therefore Stanford funded the study. The petition claims that Stanford has “deep financial ties to Cargill”, a large agribusiness company, but does not get into specifics.\nMore interesting is that the petition highlights the involvement in the study of Ingram Olkin, a renowned statistician at Stanford. The petition says\n\nThe study was authored by the very many [sic] who invented a method of ‘lying with statistics’. Olkin worked with Stanford University to develop a “multivariate” statistical algorithm, which is essentially a way to lie with statistics.\n\nThat’s right, the statistical method made them lie!\nThe petition is ridiculous. Interestingly, even as the petition claims conflict of interest on the part of the study authors, it seems one of the petition authors, Anthony Gucciardi, is “a natural health advocate, and creator of the health news website NaturalSociety” according to his Twitter page. Go figure. It worries me that people would claim the mere use of statistical methods is sufficient grounds for doubt. It also worries me that 3,386 people (as of this writing) would blindly agree.\nBy the way, can anyone propose an alternative to “multivariate statistics”? I need stop all this lying….\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-13-an-experimental-foundation-for-statistics/",
    "title": "An experimental foundation for statistics",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-13",
    "categories": [],
    "contents": "\nIn a recent conversation with Brian (of abstraction fame) about the relationship between mathematics and statistics. Statistics, for historical reasons, has been treated as a mathematical sub-discipline (this is the NSF’s view).\nOne reason statistics is viewed as a sub-discipline of math is because the foundations of statistics are built on the basis of deductive reasoning, where you start with a few general propositions or foundations that you assume to be true and then systematically prove more specific results. A similar approach is taken in most mathematical disciplines. \nIn contrast, scientific disciplines like biology are largely built on the basis of inductive reasoning and the scientific method. Specific individual discoveries are described and used as a framework for building up more general theories and principles. \nSo the question Brian and I had was: what if you started over and built statistics from the ground up on the basis of inductive reasoning and experimentation? Instead of making mathematical assumptions and then proving statistical results, you would use experiments to identify core principals. This actually isn’t without precedent in the statistics community. Bill Cleveland and Robert McGill studied how people perceive graphical information and produced some general recommendations about the use of area/linear contrasts, common axes, etc. There has also been a lot of work on experimental understanding of how humans understand uncertainty. \nSo what if we put statistics on an experimental, rather than on a mathematical foundation. We performed experiments to see what kind of regression models people were able to interpret most clearly, what were the best ways to evaluate confounding/outliers, or what measure of statistical significance people understood best? Basically, what if the “quality” of a statistical method did not rest on the mathematics behind the method, but on the basis of experimental results demonstrating how people used the methods? So, instead of justifying lowess mathematically, we justified it on the basis of its practical usefulness through specific, controlled experiments. Some of this is already happening when people do surveys of the most successful methods in Kaggle contests or with the MAQC.\nI wonder what methods would survive the change in paradigm?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-13-coursera-introduces-three-courses-in-statistics/",
    "title": "Coursera introduces three courses in statistics",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-13",
    "categories": [],
    "contents": "\nCoursera introduces three courses in statistics\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-10-the-pebbles-of-academia/",
    "title": "The pebbles of academia",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-10",
    "categories": [],
    "contents": "\nI have just been awarded a certificate for successful completion of the Conflict of Interest Commitment training (I barely passed). Lately, I have been totally swamped by administrative duties and have had little time for actual research. The experience reminded me of something I read in this NYTimes article by Tyler Cowen\n\nMichael Mandel, an economist with the Progressive Policy Institute, compares government regulation of innovation to the accumulation of pebbles in a stream. At some point too many pebbles block off the water flow, yet no single pebble is to blame for the slowdown. Right now the pebbles are limiting investment in future innovation.\n\nHere are some of the pebbles of my academic career (past and present): financial conflict of interest training , human subjects training, HIPAA training, safety training, ethics training, submitting papers online, filling out copyright forms, faculty meetings, center grant quarterly meetings, 2 hour oral exams, 2 hour thesis committee meetings, big project conference calls, retreats, JSM, anything with “strategic” in the title, admissions committee, affirmative action committee, faculty senate meetings, brown bag lunches, orientations, effort reporting, conflict of interest reporting, progress reports (can’t I just point to pubmed?), dbgap progress reports, people who ramble at study section, rambling at study section, buying airplane tickets for invited talks, filling out travel expense sheets, and organizing and turning in travel receipts. I know that some of these are somewhat important or take minimal time, but read the quote again.\nI also acknowledge that I actually have it real easy compared to others so I am interested in hearing about other people’s pebbles? \nUpdate: add changing my eRA commons password to list!\n\n\n\n\n",
    "preview": "http://rafalab.jhsph.edu/simplystats/pebles4.jpg",
    "last_modified": "2021-11-11T13:59:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-09-sunday-data-statistics-link-roundup-9-9-12/",
    "title": "Sunday Data/Statistics Link Roundup (9/9/12)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-09",
    "categories": [],
    "contents": "\nNot necessarily statistics related, but pretty appropriate now that the school year is starting. Here is a little introduction to “how to google” (via Andrew J.). Being able to “just google it” and find answers for oneself without having to resort to asking folks is maybe the #1 most useful skill as a statistician. \nA really nice presentation on interactive graphics with the googleVis package. I think one of the most interesting things about the presentation is that it was built with markdown/knitr/slidy (see slide 53). I am seeing more and more of these web-based presentations. I like them for a lot of reasons (ability to incorporate interactive graphics, easy sharing, etc.), although it is still harder than building a Powerpoint. I also wonder, what happens when you are trying to present somewhere that doesn’t have a good internet connection?\nWe talked a lot about the ENCODE project this week. We had an interview with Steven Salzberg, then Rafa followed it up with a discussion of top-down vs. bottom-up science. Tons of data from the ENCODE project is now available, there is even a virtual machine with all the software used in the main analysis of the data that was just published. But my favorite quote/tweet/comment this week came from Leonid K. about a flawed/over the top piece trying to make a little too much of the ENCODE discoveries: “that’s a clown post, bro”.\nAnother breathless post from the Chronicle about how there are “dozens of plagiarism cases being reported on Coursera”. Given that tens of thousands of people are taking the course, it would be shocking if there wasn’t plagiarism, but my guess is it is about the same rate you see in in-person classes. I will be using peer grading in my course, hopefully plagiarism software will be in place by then. \nA New York Times article about a new book on visualizing data for scientists/engineers. I love all the attention data visualization is getting. I’ll take a look at the book for sure. I bet it says a lot of the same things Tufte said and a lot of the things Nathan Yau says in his book. This one may just be targeted at scientists/engineers. (link via Dan S.)\nEdo and co. are putting together a workshop on the analysis of social network data for NIPS in December. If you do this kind of stuff, it should be a pretty awesome crowd, so get your paper in by the Oct. 15th deadline!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-08-big-data-in-your-blood/",
    "title": "Big Data in Your Blood",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-08",
    "categories": [],
    "contents": "\nBig Data in Your Blood\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-08-the-weatherman-is-not-a-moron/",
    "title": "The Weatherman Is Not a Moron",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-08",
    "categories": [],
    "contents": "\nThe Weatherman Is Not a Moron\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-07-interview-with-steven-salzberg-about-the-encode/",
    "title": "Simply Statistics Podcast #3: Interview with Steven Salzberg",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-07",
    "categories": [],
    "contents": "\nInterview with Steven Salzberg about the ENCODE Project.\nIn this episode Jeff and I have a discussion with Steven Salzberg, Professor of Medicine and Biostatistics at Johns Hopkins University, about the recent findings from the ENCODE Project where he helps us separate fact from fiction. You’re going to want to watch to the end with this one.\nHere are some excerpts from the interview.\nRegarding why the data should have been released immediately without restriction:\n\nIf this [ENCODE] were funded by a regular investigator-initiated grant, then I would say you have your own grant, you’ve got some hypotheses you’re pursuing, you’re collecting data, you’ve already demonstrated that…you have some special ability to do this work and you should get some time to look at your data that you just generated to publish it. This was not that kind of a project. These are not hypothesis-driven projects. They are data collection projects. The whole model is…they’re creating a resource and it’s more efficient to create the resource in one place…. So we all get this data that’s being made available for less money…. I think if you’re going to be funded that way, you should release the data right away, no restrictions, because you’re funded because you’re good at generating this data cheaply….But you may not be the best person to do the analysis.\n\nRegarding the problem with large-scale top-down funding approaches versus the individual investigator approach:\n\nWell, it’s inefficient because it’s anti-competitive. They have a huge amount of money going to a few centers, they’ll do tons of experiments of the same type—may not be the best place to do that. They could instead give that money to 20 times as many investigators who would be refining the techniques and developing better ones. And a few years from now, instead of having another set of ENCODE papers—which we’re probably going to have—we might have much better methods and I think we’d have just as much in terms of discovery, probably more.\n\nRegarding best way to make discoveries:\n\nI think a problem I have with it…is that the top-down approach to science isn’t the way you make discoveries. And NIH has sort of said we’re going to fund these data generation and data analysis groups—they’re doing both…and by golly we’re going to discover some things. Well, it doesn’t always work if you do that. You can’t just say…so the Human Genome [Project], even though, of course there were lots of promises about curing cancer, we didn’t say we were going to discover how a particular gene works, we said we’re going to discover what the sequence is. And we did! Really well. With these [ENCODE] projects they said we’re going to figure out the function of all the elements, and they haven’t figured that out, at all.\n\n[HD video RSS feed]\n[Audio-only RSS feed]\n[NOTE: Due to clumsy camera operator (who forgot to turn the camera on), we lost one of our three camera angles and so the there’s no front-facing view. Sorry!]\n\n(Source: http://www.youtube.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-07-top-down-versus-bottom-up-science-data-analysis/",
    "title": "Top-down versus bottom-up science: data analysis edition",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-07",
    "categories": [],
    "contents": "\nIn our most recent video, Steven Salzberg discusses the ENCODE project. Some of the advantages and disadvantages of top-down science are described.  Here, top-down refers to big coordinated projects like the Human Genome Project (HGP). In contrast, the approach of funding many small independent projects, via the R01 mechanism, is referred to as bottom-up. Note that for the cost of HGP we could have funded thousands of R01s. However it is not clear that without the HGP we would have had public sequence data as early as we did. As Steven points out, when it comes to data generation the economies of scale make big projects more efficient. But the same is not necessarily true for data analysis.\nBig projects like ENCODE and 1000 genomes include data analysis teams that work in coordination with the data producers.  It is true that very good teams are assembled and very good tools developed. But what if instead of holding the data under embargo until the first analysis is done and a paper (or 30) is published, the data was made publicly available with no restrictions and the scientific community was challenged to compete for data analysis and biological discovery R01s? I have no evidence that this would produce better science, but my intuition is that, at least in the case of data analysis, better methods would be developed. Here is my reasoning. Think of the best 100 data analysts in academia and consider the following two approaches:\n1- Pick the best among the 100 and have a small group carefully coordinate with the data producers to develop data analysis methods.\n2- Let all 100 take a whack at it and see what falls out.\nIn scenario 1 the selected group has artificial protection from competing approaches and there are less brains generating novel ideas. In scenario 2 the competition would be fierce and after several rounds of sharing ideas (via publications and conferences), groups would borrow from others and generate even better methods.\nNote that the big projects do make the data available and R01s are awarded to develop analysis tools for these data. But this only happens after giving the consortium’s group a substantial head start. \nI have not participated in any of these consortia and perhaps I am being naive. So I am very interested to hear the opinions of others.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-06-in-this-episode-of-the-simply-statistics-podcast/",
    "title": "Simply Statistics Podcast #2",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-06",
    "categories": [],
    "contents": "\nIn this episode of the Simply Statistics podcast Jeff and I discuss the deterministic statistical machine and increasing the cost of data analysis. We decided to eschew the studio setup this time and attempt a more guerilla style of podcasting. Also, Rafa was nowhere to be found when we recorded so you’ll have to catch his melodious singing voice in the next episode.\nAnd in case you’re wondering, Jeff’s office is in fact that clean.\nAs always, we welcome your feedback!\n[HD video RSS feed]\n[Audio-only RSS feed]\n\n(Source: http://www.youtube.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-05-how-long-should-the-next-podcast-be/",
    "title": "How long should the next podcast be?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-05",
    "categories": [],
    "contents": "\nHere’s the survival curve for audience retention from the Youtube version of our first podcast.\n\nSo the question is: How long should our next podcast be?\nBy the way, Rafa, Jeff, and I all appreciate the little bump over at the 15 minute mark. However, you’re only encouraging us there!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-03-online-universities-blossom-in-asia/",
    "title": "Online universities blossom in Asia",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-03",
    "categories": [],
    "contents": "\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-02-sunday-data-statistics-link-roundup-9-2-2012/",
    "title": "Sunday Data/Statistics Link Roundup (9/2/2012)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-02",
    "categories": [],
    "contents": "\nJust got back from IBC 2012 in Kobe Japan. I was in an awesome session (organized by the inimitable Lieven Clement) with great talks by Matt McCall, Djork-Arne Clevert, Adetayo Kasim, and Willem Talloen. Willem’s talk nicely tied in our work and how it plays into the pharmaceutical development process and the bigger theme of big data. On the way home through SFO I saw this hanging in the airport. A fitting welcome back to the states. Although, as we talked about in our first podcast, I wonder how long the Big Data hype will last…\nSimina B. sent this link along for a masters program in analytics at NC State. Interesting because it looks a lot like a masters in statistics program, but with a heavier emphasis on data collection/data management. I wonder what role the stat department down there is playing in this program and if we will see more like it pop up? Or if programs like this with more data management will be run by stats departments other places. Maybe our friends down in Raleigh have some thoughts for us. \nIf one set of weekly links isn’t enough to fill your procrastination quota, go check out NextGenSeek’s weekly stories. A bit genomics focused, but lots of cool data/statistics links in there too. Love the “extreme Venn diagrams”. \nThis seems almost like the fast statistics journal I proposed earlier. Can’t seem to access the first issue/editorial board either. Doesn’t look like it is open access, so it’s still not perfect. But I love the sentiment of fast/single round review. We can do better though. I think Yihue X. has some really interesting ideas on how. \nMy wife taught for a year at Grinnell in Iowa and loved it there. They just released this cool data set with a bunch of information about the college. If all colleges did this, we could really dig in and learn a lot about the American secondary education system (link via Hilary M.). \nFrom the way-back machine, a rant from Rafa about meetings. Stayed tuned this week for some Simply Statistics data about our first year on the series of tubes. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-09-01-drought-extends-crops-wither/",
    "title": "Drought Extends, Crops Wither",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-09-01",
    "categories": [],
    "contents": "\nDrought Extends, Crops Wither\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-31-court-blocks-e-p-a-rule-on-cross-state-pollution/",
    "title": "Court Blocks E.P.A. Rule on Cross-State Pollution",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-31",
    "categories": [],
    "contents": "\nCourt Blocks E.P.A. Rule on Cross-State Pollution\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-31-most-americans-confused-by-cloud-computing-according-to/",
    "title": "Most Americans Confused By Cloud Computing According to National Survey",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-31",
    "categories": [],
    "contents": "\nMost Americans Confused By Cloud Computing According to National Survey\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-30-court-upholds-rule-on-nitrogen-dioxide-emissions/",
    "title": "Court Upholds Rule on Nitrogen Dioxide Emissions",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-30",
    "categories": [],
    "contents": "\nCourt Upholds Rule on Nitrogen Dioxide Emissions\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-30-green-will-emissions-disclosure-mean-investor-pressure/",
    "title": "Green: Will Emissions Disclosure Mean Investor Pressure on Polluters?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-30",
    "categories": [],
    "contents": "\nGreen: Will Emissions Disclosure Mean Investor Pressure on Polluters?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-29-i-b-m-mainframe-evolves-to-serve-the-digital-world/",
    "title": "I.B.M. Mainframe Evolves to Serve the Digital World",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-29",
    "categories": [],
    "contents": "\nI.B.M. Mainframe Evolves to Serve the Digital World\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-29-increasing-the-cost-of-data-analysis/",
    "title": "Increasing the cost of data analysis",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-29",
    "categories": [],
    "contents": "\nJeff’s post about the deterministic statistical machine got me thinking a bit about the cost of data analysis. The cost of data analysis these day is in many ways going up. The data being collected are getting bigger and more complex. Analyzing these data require more expertise, more storage hardware, and more computing power. In fact the analysis in some fields like genomics is now more expensive than the collection of the data [There’s a graph that shows this but I can’t seem to find it anywhere; I’ll keep looking and post later. For now see here.].\nHowever, that’s really about the dollars and cents kind of cost. The cost of data analysis has gone very far down in a different sense. For the vast majority of applications that look at moderate to large datasets, many many statistical analyses can be conducted essentially at the push of a button. And so there’s not cost in continuing to analyze data until a desirable result is obtained. Correcting for multiple testing is one way to “fix” this problem. But I personally don’t find multiple testing corrections to be all that helpful because ultimately they still try to boil down a complex analysis into a simple yes/no answer.\nIn the old days (for example when Rafa was in grad school), computing time was precious and things had to be planned out carefully, starting with the planning of the experiment and continuing with the data collection and the analysis. In fact, much of current statistical education is still geared around the idea that computing is expensive, which is why we use things like asymptotic theorems and approximations even when we don’t really have to. Nowadays, there’s a bit of a “we’ll fix it in post” mentality, which values collecting as much data as possible when given the chance and figuring out what to do with it later. This kind of thinking can lead to (1) small big data problems; (2) poorly designed studies; (3) data that don’t really address the question of interest to everyone.\nWhat if the cost of data analysis were not paid in dollars but were paid in some general unit of credibility. For example, Jeff’s hypothetical machine would do some of this.\n\nBy publishing all reports to figshare, it makes it even harder to fudge the data. If you fiddle with the data to try to get a result you want, there will be a “multiple testing paper trail” following you around. \n\nSo with each additional analysis of the data, you get an additional piece of paper added to your analysis paper trail. People can look at the analysis paper trail and make of it what they will. Maybe they don’t care. Maybe having a ton of analyses discredits the final results. The point is that it’s there for all to see.\nI do not think what we need is better methods to deal with multiple testing. This is simply not a statistical issue. What we need is a way to increase the cost of data analysis by preserving the paper trail. So that people hesitate before they run all pairwise combinations of whatever. Reproducible research doesn’t really deal with this problem because reproducibility only really requires that the final analysis is documented.\nIn other words, let the paper trail be the price of pushing the button.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-28-active-in-cloud-amazon-reshapes-computing/",
    "title": "Active in Cloud, Amazon Reshapes Computing",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-28",
    "categories": [],
    "contents": "\nActive in Cloud, Amazon Reshapes Computing\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-28-genes-now-tell-doctors-secrets-they-cant-utter/",
    "title": "Genes Now Tell Doctors Secrets They Can’t Utter",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-28",
    "categories": [],
    "contents": "\nGenes Now Tell Doctors Secrets They Can’t Utter\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:59:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-27-a-deterministic-statistical-machine/",
    "title": "A deterministic statistical machine",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-27",
    "categories": [],
    "contents": "\nAs Roger pointed out the most recent batch of Y Combinator startups included a bunch of data-focused companies. One of these companies, StatWing, is a web-based tool for data analysis that looks like an improvement on SPSS with more plain text, more visualization, and a lot of the technical statistical details “under the hood”. I first read about StatWing on TechCrunch, where the title, “How Statwing Makes It Easier To Ask Questions About Data So You Don’t Have To Hire a Statistical Wizard”.\nStatWing looks super user-friendly and the idea of democratizing statistical analysis so more people can access these ideas is something that appeals to me. But, as one of the aforementioned statistical wizards, this had me freaked out for a minute. Once I looked at the software though, I realized it suffers from the same problem that most “user-friendly” statistical software suffers from. It makes it really easy to screw up a data analysis. It will tell you when something is significant and if you don’t like that it isn’t, you can keep slicing and dicing the data until it is. The key issue behind getting insight from data is knowing when you are fooling yourself with confounders, or small effect sizes, or overfitting. StatWing looks like an improvement on the UI experience of data analysis, but it won’t prevent false positives that plague science and cost business big $$. \nSo I started thinking about what kind of software would prevent these sort of problems while still being accessible to a big audience. My idea is a “deterministic statistical machine”. Here is how it works, you input a data set and then specify the question you are asking (is variable Y related to variable X? can i predict Z from W?) then, depending on your question, it uses a deterministic set of methods to analyze the data. Say regression for inference, linear discriminant analysis for prediction, etc. But the method is fixed and deterministic for each question. It also performs a pre-specified set of checks for outliers, confounders, missing data, maybe even data fudging. It generates a report with a markdown tool and then immediately publishes the result to figshare. \nThe advantage is that people can get their data-related questions answered using a standard tool. It does a lot of the “heavy lifting” in checking for potential problems and produces nice reports. But it is a deterministic algorithm for analysis so overfitting, fudging the analysis, etc. are harder. By publishing all reports to figshare, it makes it even harder to fudge the data. If you fiddle with the data to try to get a result you want, there will be a “multiple testing paper trail” following you around. \nThe DSM should be a web service that is easy to use. Anybody want to build it? Any suggestions for how to do it better? \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-26-sunday-data-statistics-link-roundup-8-26-12/",
    "title": "Sunday data/statistics link roundup (8/26/12)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-26",
    "categories": [],
    "contents": "\nFirst off, a quick apology for missing last week, and thanks to Augusto for noticing! On to the links:\nUnbelievably the BRCA gene patents were upheld by the lower court despite the Supreme Court coming down pretty unequivocally against patenting correlations between metabolites and health outcomes. I wonder if this one will be overturned if it makes it back up to the Supreme Court. \nA really nice interview with David Spiegelhalter on Statistics and Risk. David runs the Understanding Uncertainty blog and published a recent paper on visualizing uncertainty. My favorite line from the interview might be: “There is a nice quote from Joel Best that “all statistics are social products, the results of people’s efforts”. He says you should always ask, “Why was this statistic created?” Certainly statistics are constructed from things that people have chosen to measure and define, and the numbers that come out of those studies often take on a life of their own.”\nFor those of you who use Tumblr like we do, here is a cool post on how to put technical content into your blog. My favorite thing I learned about is the Github Gist that can be used to embed syntax-highlighted code.\nA few interesting and relatively simple stats for projecting the success of NFL teams.  One thing I love about sports statistics is that they are totally willing to be super ad-hoc and to be super simple. Sometimes this is all you need to be highly predictive (see for example, the results of Football’s Pythagorean Theorem). I’m sure there are tons of more sophisticated analyses out there, but if it ain’t broke… (via Rafa). \nMy student Hilary has a new blog that’s worth checking out. Here is a nice review of ProjectTemplate she did. I think the idea of having an organizing principle behind your code is a great one. Hilary likes ProjectTemplate, I think there are a few others out there that might be useful. If you know about them, you should leave a comment on her blog!\nThis is ridiculously cool. Man City has opened up their data/statistics to the data analytics community. After registering, you have access to many of the statistics the club uses to analyze their players. This is yet another example of open data taking over the world. It’s clear that data generators can create way more value for themselves by releasing cool data, rather than holding it all in house. \nThe Portland Public Library has created a website called Book Psychic, basically a recommender system for books. I love this idea. It would be great to have a recommender system for scientific papers. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-25-judge-rules-poker-is-a-game-of-skill-not-luck/",
    "title": "Judge Rules Poker Is A Game Of Skill, Not Luck",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-25",
    "categories": [],
    "contents": "\nJudge Rules Poker Is A Game Of Skill, Not Luck\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-24-simply-statistics-podcast-1-to-mark-the/",
    "title": "Simply Statistics Podcast #1",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-24",
    "categories": [],
    "contents": "\nSimply Statistics Podcast #1.\nTo mark the occasion of our 1-year anniversary of starting the blog, Jeff, Rafa, and I have recorded our first podcast. You can tell that it’s our very first podcast because we don’t appear to have any idea what we’re doing. However, we decided to throw caution to the wind.\nIn this episode we talk about why we started the blog and discuss our thoughts on statistics and big data. Be sure to watch to the end as Rafa provides a special treat.\nUPDATE: For those of you who can’t bear the sight of us, there is an audio only version.\nUPDATE 2: I have setup an RSS feed for the audio-only version of the podcast.\nUPDATE 3: Here is the RSS feed for HD video version of the podcast.\n\n(Source: http://www.youtube.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-23-science-exchange-starts-reproducibility-initiative/",
    "title": "Science Exchange starts Reproducibility Initiative",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-23",
    "categories": [],
    "contents": "\nI’ve fallen behind and so haven’t had a chance to mention this, but Science Exchange has started its Reproducibility Initiative. The idea is that authors can submit their study to be reproduced and Science Exchange will match the study with a validator who will attempt to reproduce the results (for a fee).\n\nValidated studies will receive a Certificate of Reproducibility acknowledging that their results have been independently reproduced as part of the Reproducibility Initiative. Researchers have the opportunity to publish the replicated results as an independent publication in the PLOS Reproducibility Collection, and can share their data via the figshare Reproducibility Collection repository. The original study will also be acknowledged as independently reproduced if published in a supporting journal.\n\nThis is a very interesting initiative and it’s one I and a number of others have been talking about doing. They have an excellent advisory board and seem to have all the right partners/infrastructure lined up. \nThe obvious question to me is if you’re going to submit your study to this service and get it reproduced, why would you ever want to submit it to a journal? The level of review you’d get here is quite a bit more rigorous than you’d receive at a journal and the submission process essentially involves writing a paper without the Introduction and the Discussion (usually the hardest and most annoying parts). At the moment, it seems the service is set up to work in parallel with standard publication or perhaps after the fact. But I could see it eventually replacing standard publication altogether.\nThe timing, of course, could be an issue. It’s not clear how long one should expect it to take to reproduce a study. But it’s probably not much longer than a review you’d get at a statistics journal.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-22-data-startups-from-y-combinator-demo-day/",
    "title": "Data Startups from Y Combinator Demo Day",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-22",
    "categories": [],
    "contents": "\nY Combinator, the tech startup incubator, had its 15th demo day. Here are some of the data/statistics-related highlights (thanks to TechCrunch for doing the hard work):\nEVERYDAY.ME — A PRIVATE, ONLINE RECORD OF YOUR LIFE. \n\nThis company seems to me like a meta-data company. It compiles your data from other sites.\nMTH SENSE: IMPROVING MOBILE AD TARGETING “Most [mobile] ads served are blind. Mth sense’s solution adds demographic data to ads through predictive modeling based on app and device usage. For example, if you have the Pinterest, and Vogue apps, you’re more likely to be a soccer mom.” Hmm, I guess I’d better delete those apps from my phone….\nSURVATA: REPLACING PAYWALLS WITH SURVEYWALLS Survata’s product replaces paywalls on premium content from online publishers with surveys that conduct market research.\nRENT.IO — RENT PRICE PREDICTION Rent.io says it wants to “optimize pricing of the single biggest recurring expense in lives of 100 million Americans.&rdquo\nBIGCALC: FAST NUMBER-CRUNCHING FOR MAKING FINANCIAL TRADING DECISIONS BigCalc says its platform for financial modeling scales to enormous datasets, and purportedly does simulations that typically take 22 hours in 24 minutes.\nDATANITRO — A BACKBONE FOR FINANCE-RELATED DATA DataNitro’s founders have both worked in finance, and they say they know from experience that financial industry software is basically “held together with duct tape.” A big problem with the status quo is how data is exported from Excel.\nSTATWING: EASY TO USE DATA ANALYSIS Most existing data analysis tools (in particular SPSS) are built for statisticians. Statwing has created tools that make it easier for marketers and analysts to interact with data without dealing with arcane technical terminology. Those users only need a few core functions, Statwing says, so that’s what the company provides. With just a few clicks, users can get the graphs that they want. And the data is summarized in a single sentence of conversational English.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-22-harvard-chooses-statistician-to-lead-graduate-school-of/",
    "title": "Harvard chooses statistician to lead Graduate School of Arts and Sciences",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-22",
    "categories": [],
    "contents": "\nHarvard chooses statistician to lead Graduate School of Arts and Sciences\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-21-nsf-recognizes-math-and-statistics-are-not-the-same/",
    "title": "NSF recognizes math and statistics are not the same thing...kind of",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-21",
    "categories": [],
    "contents": "\nThere’s controversy brewing over at the National Science Foundation over names. Back in October 2011, Sastry Pantula, the Director of the Division of Mathematical Sciences at NSF (formerly the Chair of NC State Statistics Department and President of the ASA), proposed that the name of the Division be changed to the “Division of Mathematical and Statistical Sciences”. Excerpting from his original proposal, Pantula says\n\nExtracting useful knowledge from the deluge of data is critical to the scientific successes of the future. Data-intensive research will drive many of the major scientific breakthroughs in the coming decades. There is a long-term need for research and workforce development in computational and data-enabled sciences. Statistics is broadly recognized as a data-centric discipline, thus having it in the Division’s name as proposed would be advantageous whenever “Big Data” and data-sciences investments are discussed internally and externally.\n\nThis bureaucratic move by Pantula created quite a reaction. A sub-committee of the Math and Physical Sciences Advisory Committee (MPSAC) was formed to investigate the name change and to solicit feedback from the relevant communities. The sub-committee was chaired by Fred Roberts (Rutgers) and also included James Berger (Duke), Emery Brown (MIT), Kevin Corlette (U. of Chicago), Irene Fonseca (CMU), and Juan Meza (UC Merced). A number of organizations provided feedback to the sub-committee, including the American Statistical Association and the American Mathematical Society.\nThere was intense feedback both for and against the name change. Somewhat predictably, mathematicians were adamantly opposed to the name change and statisticians were for it. The final report of the sub-committee is both interesting and enlightening for those not familiar with the arguments involved.\nFirst a little background for people (like me) who are not familiar with NSF’s organizational structure. NSF has a number of Directorates, of which Mathematical and Physical Sciences (MPS) is one, and within MPS is the Division of Mathematical Sciences (DMS). DMS includes 11 program areas ranging from algebra and number theory to topology. Statistics is one of those program areas. \nThis should already give one pause. How exactly do statistics and topology end up in the same basket? I’m not exactly sure but I’m guessing it’s the result of bureaucratic inertia. Statistics came later and it had to be stuck somewhere. DMS is not the only place at NSF to get funding for statistics, but a quick search through the currently active grants shows that the vast majority of statistics-related grants go through DMS, with a smattering coming through other Divisions.\nThe primary issue here, and the only reason it’s an issue at all, is money. Statistics is one of 11 program areas in DMS, which means that it roughly gets 9% of the funding allocated to DMS. This is worth noting—the entire field of statistics gets roughly as much funding as, say, topology. For example, one of the arguments against the name change in the sub-committee’s report is\n\n3). Statistics constitutes a small (although significant) proportion of the DMS portfolio in terms of number of programs, number of grant applications, number of grants funded.\n\nWell, yes, but I would argue that the reason for this is the historically (low) prioritization of statistics in the Division. This is a choice, not a fact. I believe statistics could play a much bigger role in the Division and perhaps within NSF more generally if there were an agreement on its importance. A key argument comes next, which is\n\nIf the name change attracts more proposals to the Division from the statistics community, this could draw funding away from other subfields and it could also increase the workload of the Division’s program officers.\n\nOkay, so money’s important too, but let’s get to the main attraction, which comes in comment number 5:\n\n5). Statistics is funded throughout the federal government. The traditional funding of statistics by DMS is appropriate: fund fundamental research in statistics. Broadening the mission of DMS to include more applied statistics would not benefit the overall funding of the mathematical sciences.\n\nThe first sentence is a fact: Many government agencies fund statistics research. For example, the National Institutes of Health funds many statisticians who develop and apply methods to problems in the health sciences. The EPA will occasionally fund statisticians to develop methods for environmental health applications.\nBut who is charged with funding the development and application of statistical methods to every other scientific field? The problem now is that you essentially have a group of NIH-funded (bio)statisticians doing biomedical research and a group of NSF-funded statisticians doing “fundamental” research in statistics (note that “fundamental” equals “mathematical” here). But that hardly represents all of the statisticians out there. So for the rest of the statisticians who are not doing biomedical research and are not doing “fundamental” research, where do they go for funding?\nThese days, statistics is “applied” to everything. NSF itself has acknowledged that we are in an era of big data—it’s clear that statistics will play a big role whether we call it “statistics” or not. If NSF decided to fund research into the application of statistics to all areas, it would likely overwhelm the funding of every other program area in DMS. This is why the “solution” is to resort to what is informally understood as the mission of NSF, which is to fund “fundamental” research.\nBut it’s not clear to me that NSF should limit itself in this manner. In particular, if NSF got serious about funding the application of statistics to all scientific areas (either through DMS or some other Division), it would incentivize statisticians to build stronger and closer collaborations with scientists all over. I see this as a win-win for everyone involved. \nAs a statistician, I’m willing to admit I’m biased, but I think NSF should play a much bigger role in advancing statistics as one of the critical tools of the future. Perhaps the solution is not to rename the Division, but to create a separate division for statistical sciences independent of mathematics, one of the suggestions in the sub-committee report. This separation would mirror what has occurred in many universities over the past 50 years or so with the creation of independent departments of statistics and biostatistics.  \nUltimately, the name of the Division was not changed. Here’s the release from last week:\n\nNSF is committed to supporting the research necessary to maximize the benefits to be derived from the age of data, and to promoting and funding research related to data-centric scientific discovery and innovation, and in particular, the growing role of the statistical sciences in all research areas. Recognizing both the complex composition of the various communities and the support of statistical sciences throughout NSF, and taking into account the various community views described in the very thoughtful report of the MPSAC, I have decided to maintain the name “Division of Mathematical Sciences (DMS)” within MPS, but to affirm strong commitment to the statistical sciences.\nTo demonstrate this commitment, (a) whenever appropriate, we will specifically mention “statistics” alongside “mathematics” in budget requests and in solicitations in order to recognize the unique and pervasive role of statistical sciences, and to ensure that relevant solicitations reach the statistical sciences community….\n\nWell, I feel better already. I suppose this is progress of some sort.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-21-recommended-updates-from-google-scholar/",
    "title": "Recommended updates from Google Scholar",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-21",
    "categories": [],
    "contents": "\nRecommended updates from Google Scholar\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-17-interview-with-c-titus-brown-computational-biologist/",
    "title": "Interview with C. Titus Brown - Computational biologist and open access champion",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-17",
    "categories": [],
    "contents": "\n\nC. Titus Brown \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC. Titus Brown is an assistant professor in the Department of Computer Science and Engineering at Michigan State University. He develops computational software for next generation sequencing and the author of the blog, “Living in an Ivory Basement”. We talked to Titus about open access (he publishes his unfunded grants online!), improving the reputation of PLoS One, his research in computational software development, and work-life balance in academics. \n\n\n\n\n\n\n\n\n\nDo you consider yourself a statistician, data scientist, computer scientist, or something else?\n\n\nGood question.  Short answer: apparently somewhere along the way Ibecame a biologist, but with a heavy dose of “computational scientist”in there.\nThe longer answer?  Well, it’s a really long answer…\nMy first research was on Avida, a bottom-up model for evolution thatChris Adami, Charles Ofria and I wrote together at Caltech in 1993:http://en.wikipedia.org/wiki/Avida.  (Fun fact: Chris, Charles and Iare now all faculty at Michigan State!  Chris and I have offices onedoor apart, and Charles has an office one floor down.)  Avida got mevery interested in biology, but not in the undergrad “memorize stuff”biology — more in research.  This was computational science: usingsimple models to study biological phenomena.\nWhile continuing evolution research, I did my undergrad in pure math at ReedCollege, which was pretty intense; I worked in the Software Developmentlab there, which connected me to a bunch of reasonably well known hackersincluding Keith Packard, Mark Galassi, and Nelson Minar.\nI also took a year off and worked on Earthshine:\nhttp://en.wikipedia.org/wiki/Planetshine#Earthshine\nand then rebooted the project as an RA in 1997, the summer aftergraduation.  This was mostly data analysis, although it included afair amount of hanging off of telescopes adjusting things as thefreezing winter wind howled through the Big Bear Solar Observatory’sobserving room, aka “data acquisition”.\nAfter Reed, I applied to a bunch of grad schools, including Princetonand Caltech in bio, UW in Math, and UT Austin and Ohio State inphysics.  I ended up at Caltech, where I switched over todevelopmental biology and (eventually) regulatory genomics and genomebiology in Eric Davidson’s lab.  My work there included quite a bitof wet bench biology, which is not something many people associate with me,but was nonetheless something I did!\nGenomics was really starting to hit the fan in the early 2000s, and Iwas appalled by how biologists were handling the data — as oneexample, we had about $500k worth of sequences sitting on a sharedWindows server, with no metadata or anything — just the filenames.As another example, I watched a postdoc manually BLAST a few thousandESTs against the NCBI nr database; he sat there and did them three bythree, having figured out that he could concatenate three sequencestogether and then manually deconvolve the results.  As probably themost computationally experienced person in the lab, I quickly gotinvolved in data analysis and Web site stuff, and ended up writingsome comparative sequence analysis software that was mildly popularfor a while.\nAs part of the sequence analysis Web site I wrote, I became aware thatmaintaining software was a really hard problem.  So, towards the endof my 9 year stint in grad school, I spent a few years getting intotesting, both Web testing and more generally automated softwaretesting.  This led to perhaps my most used piece of software, twill, ascripting language for Web testing.  It also ended up being one of thethings that got me elected into the Python Software Foundation,because I was doing everything in Python (which is a really greatlanguage, incidentally).\nI also did some microbial genome analysis (which led to my firstcompletely reproducible paper (Brown and Callan, 2004;http://www.ncbi.nlm.nih.gov/pubmed/14983022) and collaborated with theOrphan lab on some metagenomics:http://www.ncbi.nlm.nih.gov/pubmed?term=18467493.  This led to afascination with the biological “dark matter” in nature that is thesubject of some of my current work on metagenomics.\nI landed my faculty position at MSU right out of grad school, becausebioinformatics is sexy and CS departments are OK with hiring gradstudents as faculty.  However, I deferred for two years to do apostdoc in Marianne Bronner-Fraser’s lab because I wanted to switch tothe chick as a model organism, and so I ended up arriving at MSU in2009.  I had planned to focus a lot on development gene regulatorynetworks, but 2009 was when Illumina sequencing hit, and as one of thefew people around who wasn’t visibly frightened by the term “gigabyte”I got inextricably involved in a lot of different sequence analysisprojects.  These all converged on assembly, and, well, that seems tobe what I work on now :).\nThe two strongest threads that run through my research are these:\n1. “better science through superior software” — so much of sciencedepends on computational inference these days, and so little of theunderlying software is “good”.  Scientists really suck at softwaredevelopment (for both good and bad reasons) and I worry that a lot ofour current science is on a really shaky foundation.  This is onereason I’m invested in Software Carpentry(http://software-carpentry.org), a training program that Greg Wilsonhas been developing — he and I agree that science is our best hopefor a positive future, and good software skills are going to beessential for a lot of that science.  More generally I hope to turngood software development into a competitive advantage for my laband my students.\n2. “better hypothesis generation is needed” — biologists, inparticular, tend to leap towards the first testable hypothesis theyfind.  This is a cultural thing stemming (I think) from a lot ofreally bad interactions with theory: the way physicists andmathematicians think about the world simply doesn’t fit with the RubeGoldberg-esque features of biology (seehttp://ivory.idyll.org/blog/is-discovery-science-really-bogus.html).\nSo getting back to the question, uh, yeah, I think I’m a computationalscientist who is working on biology?  And if I need to write a little(or a lot) of software to solve my problems, I’ll do that, and I’lltry to do it with some attention to good software developmentpractice — not just out of ethical concern for correctness, butbecause it makes our research move faster.\nOne thing I’m definitely not is a statistician.  I have friends whoare statisticians, though, and they seem like perfectly nice people.\n\nYou have a pretty radical approach to open access, can you tell us a little bit about that?\n\n\nEver since Mark Galassi introduced me to open source, I thought itmade sense.  So I’ve been an open source-nik since … 1988?\nFrom there it’s just a short step to thinking that open science makesa lot of sense, too.  When you’re a grad student or a postdoc, youdon’t get to make those decisions, though; it took until I was a PIfor me to start thinking about how to do it.  I’m still conflictedabout how open to be, but I’ve come to the conclusion that postingpreprints is obvious(http://ivory.idyll.org/blog/blog-practicing-open-science.html).\nThe “radical” aspect that you’re referring to is probably my postingof grants (http://ivory.idyll.org/blog/grants-posted.html).  There aretwo reasons I ended up posting all of my single-PI grants.  Both havetheir genesis in this past summer, when I spent about 5 months writing6 different grants — 4 of which were written entirely by me.  Ugh.\nFirst, I was really miserable one day and joked on Twitter that “allthis grant writing is really cutting into my blogging” — a mockingreference to the fact that grant writing (to get $$) is consideredacademically worthwhile, while blogging (which communicates with thepublic and is objectively quite valuable) counts for naught with myemployer.  Jonathan Eisen responded by suggesting that I post all ofthe grants and I thought, what a great idea!\nSecond, I’m sure it’s escaped most people (hah!), but grant fundingrates are in the toilet — I spent all summer writing grants whileexpecting most of them to be rejected.  That’s just flat-outdepressing!  So it behooves me to figure out how to make them servemultiple duties.  One way to do that is to attract collaborators;another is to serve as google bait for my lab; a third is to providemy grad students with well-laid-out PhD projects.  A fourth duty theyserve (and I swear this was unintentional) is to point out to peoplethat this is MY turf and I’m already solving these problems, so maybethey should go play in less occupied territory.  I know, very passiveaggressive…\nSo I posted the grants, and unknowingly joined a really awesome cadreof folk who had already done the same(http://jabberwocky.weecology.org/2012/08/10/a-list-of-publicly-available-grant-proposals-in-the-biological-sciences/).Most feedback I’ve gotten has been from grad students and undergradswho really appreciate the chance to look at grants; some people toldme that they’d been refused the chance to look at grants from theirown PIs!\nAt the end of the day, I’d be lucky to be relevant enough that peoplewant to steal my grants or my software (which, by the way, is under aBSD license — free for the taking, no “theft” required…).  Myobservation over the years is that most people will do just aboutanything to avoid using other people’s software.\n\nIn theoretical statistics, there is a tradition of publishing pre-prints while papers are submitted. Why do you think biology is lagging behind?\n\n\nI wish I knew!  There’s clearly a tradition of secrecy in biology;just look at the Cold Spring Harbor rules re tweeting and blogging(http://meetings.cshl.edu/report.html) - this is a conference, forchrissakes, where you go to present and communicate!  I think it’sself-destructive and leads to an insider culture where only those whoattend meetings and chat informally get to be members of the club,which frankly slows down research. Given the societal and medicalchallenges we face, this seems like a really bad way to continue doingresearch.\nOne of the things I’m proudest of is our effort on the cephalopodgenome consortium’s white paper,http://ivory.idyll.org/blog/cephseq-cephalopod-genomics.html, where agroup of bioinformaticians at the meeting pushed really hard to walkthe line between secrecy and openness.  I came away from that effortthinking two things: first, that biologists were erring on the side ofrisk aversity; and second, that genome database folk were smokingcrack when they pushed for complete openness of data.  (I have a blogpost on that last statement coming up at some point.)\nThe bottom line is that the incentives in academic biology are alignedagainst openness.  In particular, you are often rewarded for the firstobservation, not for the most useful one; if your data is used to docool stuff, you don’t get much if any credit; and it’s all aboutfirst/last authorship and who is PI on the grants.  All too often thismeans that people sit on their data endlessly.\nThis is getting particularly bad with next-gen data sets, becauseanyone can generate them but most people have no idea how to analyzetheir data, and so they just sit on it forever…\n\nDo you think the ArXiv model will catch on in biology or just within the bioinformatics community?\n\n\nOne of my favorite quotes is: “Making predictions is hard, especiallywhen they’re about the future.” I attribute it to Niels Bohr.\nIt’ll take a bunch of big, important scientists to lead the way. Weneed key members of each subcommunity of biology to decide to do it ona regular basis. (At this point I will take the obligatory cheap shotand point out that Jonathan Eisen, noted open access fan, doesn’t posthis stuff to preprint servers very often.  What’s up with that?)  It’sgoing to be a long road.\n\nWhat is the reaction you most commonly get when you tell people you have posted your un-funded grants online?\n\n\n“Ohmigod what if someone steals them?”\nNobody has come up with a really convincing model for why postinggrants is a bad thing.  They’re just worried that it might be.  Iget the vague concerns about theft, but I have a hard time figuringout exactly how it would work out well for the thief — reputation isa big deal in science, and gossip would inevitably happen.  And atleast in bioinformatics I’m aiming to be well enough known thatstraight up ripping me off would be suicidal.  Plus, if reviewersdo/did google searches on key concepts then my grants would pop up,right?  I just don’t see it being a path to fame and glory for anyone.\nRevisiting the passive-aggressive nature of my grant posting, I’d liketo point out that most of my grants depend on preliminary results fromour own algorithms.  So even if they want to compete on my turf, it’llbe on a foundation I laid.  I’m fine with that — more citations forme, either way :).\nMore optimistically, I really hope that people read my grants and thenfind new (and better!) ways of solving the problems posed in them.  Mygoal is to enable better science, not to hunker down in a tenured joband engage in irrelevant science; if someone else can use my grants asa positive or negative signpost to make progress, then broadlyspeaking, my job is done.\nOr, to look at it another way: I don’t have a good model for eitherthe possible risks OR the possible rewards of posting the grants, andmy inclinations are towards openness, so I thought I’d see whathappens.\n\nHow can junior researchers correct misunderstandings about open access/journals like PLoS One that separate correctness from impact? Do you have any concrete ideas for changing minds of senior folks who aren’t convinced?\n\n\nRender them irrelevant by becoming senior researchers who supplant themwhen they retire.  It’s the academic tradition, after all!  And it’sreally the only way within the current academic system, which — forbetter or for worse — isn’t going anywhere.\nHonestly, we need fewer people yammering on about open access and morepeople simply doing awesome science and submitting it to OA journals.Conveniently, many of the high impact journals are shooting themselvesin the foot and encouraging this by rejecting good science that thenends up in an OA journal; that wonderful ecology oped on PLoS Onecitation rates shows this well(http://library.queensu.ca/ojs/index.php/IEE/article/view/4351).\n\nDo you have any advice on what computing skills/courses statistics students interested in next generation sequencing should take?\n\n\nFor courses, no — in my opinion 80% of what any good researcherlearns is self-motivated and often self-taught, and so it’s almostsilly to pretend that any particular course or set of skills issufficient or even useful enough to warrant a whole course.  I’m not abig fan of our current undergrad educational system \nFor skills?  You need critical thinking coupled with an awareness thata lot of smart people have worked in science, and odds are that thereare useful tricks and approaches that you can use.  So talk to otherpeople, a lot!  My lab has a mix of biologists, computer scientists,graph theorists, bioinformaticians, and physicists; more labs shouldbe like that.\nGood programming skills are going to serve you well no matter what, ofcourse.  But I know plenty of good programmers who aren’t veryknowledgeable about biology, and who run into problems doing actualscience.  So it’s not a panacea.\nHow does replicable or reproducible research fit into your interests?\nI’ve wasted so much time reproducing other people’s work that whenthe opportunity came up to put down a marker, I took it.\nhttp://ivory.idyll.org/blog/replication-i.html\nThe digital normalization paper shouldn’t have been particularlyradical; that it is tells you all you need to know about replicationin computational biology.\nThis is actually something I first did a long time ago, with what wasperhaps my favorite pre-faculty-job paper: if you look at the methodsfor Brown & Callan (2004) you’ll find a downloadable package thatcontains all of the source code for the paper itself and the analysisscripts.  But back then I didn’t blog :).\nLack of reproducibility and openness in methods has seriousconsequences — how much of cancer research has been useless, forexample?  See this horrific report<\/span>   <span><<\/span><a href=\"http://online.wsj.com/article/SB10001424052970203764804577059841672541590.html\" target=\"_blank\"><a href=\"http://online.wsj.com/article/SB10001424052970203764804577059841672541590.html\" target=\"_blank\">http://online.wsj.com/article/SB10001424052970203764804577059841672541590.html<\/a><\/a><span>>__.)Again, the incentives are all wrong: you get grant money forpublishing, not for being useful.  The two are not necessarily thesame…\nDo you have a family, and how do you balance work life and home life?\nWhy, thank you for asking!  I do have a family — my wife, Tracy Teal,is a bioinformatician and microbial ecologist, and we have twowonderful daughters, Amarie (4) and Jessie (1).  It’s not easy being ajunior professor and a parent at the same time, and I keep on tryingto figure out how to balance the needs of travel with the need to be aparent (hint: I’m not good at it).  I’m increasingly leaning towardsblogging as being a good way to have an impact while being aroundmore; we’ll see how that goes.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-14-statistics-statisticians-need-better-marketing/",
    "title": "Statistics/statisticians need better marketing",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-14",
    "categories": [],
    "contents": "\nStatisticians have not always been great self-promoters. I think in part this comes from our tendency to be arbiters rather than being involved in the scientific process. In some ways, I think this is a good thing. Self-promotion can quickly become really annoying. On the other hand, I think our advertising shortcomings are hurting our field in a number of different ways. \nHere are a few:\nAs Rafa points out even though statisticians are ridiculously employable right now it seems like statistics M.S. and Ph.D. programs are flying under the radar in all the hype about data/data science (here is an awesome one if you are looking). Computer Science and Engineering, even the social sciences, are cornering the market on “big data”. This potentially huge and influential source of students may pass us by if we don’t advertise better. \nA corollary to this is lack of funding. When the Big Data event happened at the White House with all the major funders in attendance to announce $200 million in new funding for big data, none of the invited panelists were statisticians. \nOur top awards don’t get the press they do in other fields. The Nobel Prize announcements are an international event. There is always speculation/intense interest in who will win. There is similar interest around the Fields medal in mathematics. But the top award in statistics, the COPSS award doesn’t get nearly the attention it should. Part of the reason is lack of funding (the Fields is $15k, the COPSS is $1k). But part of the reason is that we, as statisticians, don’t announce it, share it, speculate about it, tell our friends about it, etc. The prestige of these awards can have a big impact on the visibility of a field. \n A major component of visibility of a scientific discipline, for better or worse, is the popular press. The most recent article in a long list of articles at the New York Times about the data revolution does not mention statistics/statisticians. Neither do the other articles. We need to cultivate relationships with the media. \nWe are all busy solving real/hard scientific and statistical problems, so we don’t have a lot of time to devote to publicity. But here are a couple of easy ways we could rapidly increase the visibility of our field, ordered roughly by the degree of time commitment. \nAll statisticians should have Twitter accounts and we should share/discuss our work and ideas online. The more we help each other share, the more visibility our ideas will get. \nWe should make sure we let the ASA know about cool things that are happening with data/statistics in our organizations and they should spread the word through their Twitter account and other social media. \nWe should start a conversation about who we think will win the next COPSS award in advance of the next JSM and try to get local media outlets to pick up our ideas and talk about the award. \nWe should be more “big tent” about statistics. ASA President Robert Rodriguez nailed this in his speech at JSM. Whenever someone does something with data, we should claim them as a statistician. Sometimes this will lead to claiming people we don’t necessarily agree with. But the big tent approach is what is allowing CS and other disciplines to overtake us in the data era. \nWe should consider setting up a place for statisticians to donate money to build up the award fund for the COPSS/other statistics prizes. \nWe should try to forge relationships with start-up companies and encourage our students to pursue industry/start-up opportunities if they have interest. The less we are insular within the academic community, the more high-profile we will be. \nIt would be awesome if we started a statistical literacy outreach program in communities around the U.S. We could offer free courses in community centers to teach people how to understand polling data/the census/weather reports/anything touching data. \nThose are just a few of my ideas, but I have a ton more. I’m sure other people do too and I’d love to hear them. Let’s raise the tide and lift all of our boats!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-13-big-data-investing-gets-its-own-supergroup/",
    "title": "Big-Data Investing Gets Its Own Supergroup",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-13",
    "categories": [],
    "contents": "\nBig-Data Investing Gets Its Own Supergroup\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-13-johns-hopkins-university-professor-louis-named-to-lead/",
    "title": "Johns Hopkins University Professor Louis Named to Lead Census Bureau Research Directorate",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-13",
    "categories": [],
    "contents": "\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-12-how-big-data-became-so-big/",
    "title": "How Big Data Became So Big",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-12",
    "categories": [],
    "contents": "\nHow Big Data Became So Big\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-12-sunday-data-statistics-link-roundup-8-12-12/",
    "title": "Sunday data/statistics link roundup (8/12/12)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-12",
    "categories": [],
    "contents": "\nAn interesting blog post about the top N reasons to do a Ph.D. in bioinformatics or computational biology. A couple of things that I find interesting and could actually be said of any program in biostatistics as well are: computing is the key skill of the 21st century and computational skills are highly transferrable. Via Andrew J. \nHere is an interesting auto-complete map of the United States where the prompt was, “Why is [state] so”. It seems like using the Google auto-complete functions can lead to all sorts of humorous data, xkcd has used it as a data source a couple of times in the past. By the way, the person(s) who think Idaho is boring haven’t been to the right parts of Idaho. (via Rafa). \nOne of my all-time favorite statistics quotes appears in this column by David Brooks: “…what God hath woven together, even multiple regression analysis cannot tear asunder.” It seems like the perfect quote for any study that attempts to build a predictive model for a complicated phenomenon where only limited knowledge of the underlying mechanisms are known. \nI’ve been reading up a lot on how to summarize and communicate risk. At the moment, I’ve been following a lot of David Spiegelhalter’s stuff, and really liked this 30,000 foot view summary.\nIt is interesting how often you see R popping up in random places these days. Here is a blog post with some clearly R-created plots that appeared on Business Insider about predicting the stock-market. \nRoger and I had a post on MOOC’s this week from the perspective of faculty teaching the courses. For a more departmental/administrative level view, be sure to re-read Rafa’s post on the future of graduate education. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-11-when-dealing-with-poop-its-best-to-just-get-your/",
    "title": "When dealing with poop, it's best to just get your hands dirty",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-11",
    "categories": [],
    "contents": "\nI’m a relatively new dad. Before the kid we affectionately call the “tiny tornado” (TT) came into my life, I had relatively little experience dealing with babies and all the fluids they emit. So admittedly, I was a little squeamish dealing with the poopy explosions the TT would create. Inevitably, things would get much more messy than they had to be while I was being too delicate with the issue. It took me an embarrassingly long time for an educated man, but I finally realized you just have to get in there and change the thing even if it is messy, then wash your hands after. It comes off. \nIt is a similar situation in my professional life, but I’m having a harder time learning the lesson. There are frequently things that I’m not really excited to do: review a lot of papers, go to long meetings, revise a draft of that paper that has just been sitting around forever. Inevitably, once I get going they usually aren’t as difficult or as arduous as I thought. Even better, once they are done I feel a huge sense of accomplishment and relief. I used to have a metaphor for this, I’d tell myself, “Jeff, just rip off the band-aid”. Now, I think “Jeff, just get your hands dirty”. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-10-why-we-are-teaching-massive-open-online-courses-moocs/",
    "title": "Why we are teaching massive open online courses (MOOCs) in R/statistics for Coursera",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-10",
    "categories": [],
    "contents": "\n\nEditor’s Note: This post written by Roger Peng and Jeff Leek. \n\n\nA couple of weeks ago, we announced that we would be teaching free courses in Computing for Data Analysis and Data Analysis on the Coursera platform. At the same time, a number of other universities also announced partnerships with Coursera leading to a large number of new offerings. That, coupled with a new round of funding for Coursera, led to press coverage in the New York Times, the Atlantic, and other media outlets.\n\n\nThere was an ensuing explosion of blog posts and commentaries from academics. The opinions ranged from dramatic, to negative, to critical, to um…hilariously angry. Rafa posted a few days ago that many of the folks freaking out are missing the point - the opportunity to reach a much broader audience of folks with our course content. \n\n\n[Before continuing, we’d like to make clear that at this point no money has been exchanged between Coursera and Johns Hopkins. Coursera has not given us anything and Johns Hopkins hasn’t given them anything. For now, it’s just a mutually beneficial partnership — we get their platform and they get to use our content. In the future, Coursera will need to figure out a way to make money, and they are currently considering a number of options.] \n\n\nNow that the initial wave of hype has died down, we thought we’d outline why we are excited about participating in Coursera. We think it is only fair to start by saying this is definitely an experiment. Coursera is a newish startup and as such is still figuring out its plan/business model. Similarly, our involvement so far has been a little whirlwind and we haven’t actually taught courses yet, and we are happy to collect data and see how things turn out. So ask us again in 6 months when we are both done teaching.\n\n\nBut for now, this is why we are excited.\n\nOpen Access. As Rafa alluded to in his post, this is an opportunity to reach a broad and diverse audience. As academics devoted to open science, we also think that opening up our courses to the biggest possible audience is, in principle, a good thing. That is why we are both basing our courses on free software and teaching the courses for free to anyone with an internet connection. \nExcitement about statistics. The data revolution means that there is a really intense interest in statistics right now. It’s so exciting that Joe Blitzstein’s stat class on iTunes U has been one of the top courses on that platform. Our local superstar John McGready has also put his statistical reasoning course up on iTunes U to a similar explosion of interest. Rafa recently put his statistics for genomics lectures up on Youtube and they have already been viewed thousands of times. As people who are super pumped about the power and importance of statistics, we want to get in on the game. \nWe work hard to develop good materials. We put effort into building materials that our students will find useful. We want to maximize the impact of these efforts. We have over 30,000 students enrolled in our two courses so far. \nIt is an exciting experiment. Online teaching, including very very good online teaching, has been around for a long time. But the model of free courses at incredibly large scale is actually really new. Whether you think it is a gimmick or something here to stay, it is exciting to be part of the first experimental efforts to build courses at scale. Of course, this could flame out. We don’t know, but that is the fun of any new experiment. \nGood advertising. Every professor at a research school is a start-up of one. This idea deserves it’s own blog post. But if you accept that premise, to keep the operation going you need good advertising. One way to do that is writing good research papers, another is having awesome students, a third is giving talks at statistical and scientific conferences. This is an amazing new opportunity to showcase the cool things that we are doing. \nCoursera built some cool toys. As statisticians, we love new types of data. It’s like candy. Coursera has all sorts of cool toys for collecting data about drop out rates, participation, discussion board answers, peer review of assignments, etc. We are pretty psyched to take these out for a spin and see how we can use them to improve our teaching.\nInnovation is going to happen in education. The music industry spent years fighting a losing battle over music sharing. Mostly, this damaged their reputation and stopped them from developing new technology like iTunes/Spotify that became hugely influential/profitable. Education has been done the same way for hundreds (or thousands) of years. As new educational technologies develop, we’d rather be on the front lines figuring out the best new model than fighting to hold on to the old model. \nFinally, we’d like to say a word about why we think in-person education isn’t really threatened by MOOCs, at least for our courses. If you take one of our courses through Coursera you will get to see the lectures and do a few assignments. We will interact with students through message boards, videos, and tutorials. But there are only 2 of us and 30,000 people registered. So you won’t get much one on one interaction. On the other hand, if you come to the top Ph.D. program in biostatistics and take Data Analysis, you will now get 16 weeks of one-on-one interaction with Jeff in a classroom, working on tons of problems together. In other words, putting our lectures online now means at Johns Hopkins you get the most qualified TA you have ever had. Your professor. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-09-a-non-exhaustive-list-of-things-i-have-failed-to/",
    "title": "A non-exhaustive list of things I have failed to accomplish",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-09",
    "categories": [],
    "contents": "\nA few years ago I stumbled across a blog post that described a person’s complete cv. The idea was that the cv listed both the things they had accomplished and the things they had failed to accomplish. At the time, it really helped me to see that to be successful you have to be willing to fail over and over. \nI use my website to show the things I have accomplished career-wise. But I have also failed to achieve a lot of the things I set out to do. The reason was that there was strong competition for the awards/positions I was up for and other deserving people got them.   \nApplied to MIT undergrad in 1999 - rejected\nDonovan J. Thompson Award 2001 - did not receive\nApplied for Barry Goldwater scholarship 2002 - rejected\nApplied for NSF Pre-Doctoral Fellowship 2003 - rejected\nApplied for graduate school in math at MIT 2003, rejected\nOne of my first 3 papers rejected at PLoS Biology 2005\nMany subsequent rejections of papers - too many to list exhaustively but here is one example\nApplied for Youden Award 2010 - rejected\nApplied for Microsoft Faculty Fellowship 2012 - rejected\nApplied for Sloan Fellowship 2012 - rejected\nMany grants have been rejected, again too long to list exhaustively \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-08-on-the-relative-importance-of-mathematical-abstraction/",
    "title": "On the relative importance of mathematical abstraction in graduate statistical education",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-08",
    "categories": [],
    "contents": "\n_Editor’s Note: This is the counterpoint in our series of posts on the value of abstraction in graduate education. See Brian’s defense of abstraction on Monday and the comments on his post, as well as the comments on our original teaser post for more. See below for a full description of the T-bone inside joke*._**\n\nBrian did a good job at defining abstraction. In a cagey debater’s move, he provided an incredibly broad definition of abstraction that includes the reason we call a a smiley face, the reason why we can apply least squares to a variety of data types, and the reason we write functions when programming. At this very broad level, it is clear that abstract thinking is necessary for graduate students or any other data professional.\nBut our debate was inspired by a discussion of whether measure-theoretic probability was a key component of our graduate program. There was some agreement that for many biostatistics Ph.D. students, this exact topic may not be necessary for their research or careers. Brian suggested that measure-theoretic probability was a surrogate marker for something more important - abstract thinking and the ability to generalize ideas. This is a very specific form of generalization and abstraction that is used most commonly by statisticians: the ability that permits one to prove theorems and develop statistical models that can be applied to a variety of data types. I will therefore refocus the debate on the original topic. I have three main points:\n**\n**\nThere is an over emphasis in statistical graduate programs on abstraction defined as the ability to prove mathematical theorems and develop general statistical methods. \nIt is possible to create incredible statistical value without developing generalizable statistical methods\nWhile abstraction as defined generally is good, overemphasis on this specific type of abstraction limits our ability to include computing and real data analysis in our curriculum. It also takes away from the most important learning experience of graduate school: performing independent research.\nThere is an over emphasis in statistical graduate programs on abstraction defined as the ability to prove mathematical theorems and develop general statistical methods. \n\n\nAt a top program, you can expect to take courses in very theoretical statistics, measure theoretic probability, and an applied (or methods) sequence. The first two courses are exclusively mathematical. The third (at the programs I have visited, graduated from, taught in), despite its name, is most generally focused on mathematical details underlying statistical methods. The result is that most Ph.D. students are heavily trained in the mathematical theory behind statistics.\n\n\nAt the same time, there are a long list of skills necessary to develop a successful Ph.D. statistician. These include creativity in applications, statistical programming skills, grit to power through the boring/hard parts of research, interpretation of statistical results on real data, ability to identify the most important scientific problems, and a deep understanding of the scientific problems you are working on. Abstraction is on that list, but it is just one of many skills on that list. Graduate education is a zero-sum game over a finite period of time. Our strong focus on mathematical abstraction means there is less time for everything else.\n\n\nAny hard quantitative course will measure the ability of a student to abstract in the general sense Brian defined. One of these courses would be very useful for our students. But it is not clear that we should focus on mathematical abstraction to the exclusion of other important characteristics of graduate students. It is possible to create incredible statistical value without developing generalizable statistical methods\n\n\nA major standard for success in academia is the ability to generate solutions to problems that are widely read, cited, and used. A graduate student who produces these types of solutions is likely to have a high-impact and well-respected career. In general, it is not necessary to be able to prove theorems, understand measure theory, or develop generalizable statistical models to have this type of success.\n\n\nOne example is one of the co-authors of our blog, best known for his work in genomics. In this field, data is noisy and full of systematic errors, and for several technologies, he invented methods to correct them. For example, he developed the most popular method for making measurements from different experiments comparable, for removing the dependence of measurements on the letters in a gene, and for reducing variability due to operators who run the machine or the ozone levels. Each of these discoveries involved: (1) deep understanding of the specific technology used, (2) a good intuition of what signals were due to biology and which were due to technology, (3) application/development of specific, somewhat ad-hoc, statistical procedures to correct the mistakes, and (4) the development and distribution of good software. His work has been hugely influential on genomics, has been cited thousands of times, and has substantially improved the quality of both biological and statistical results.\n\n\nBut the work did not result in knowledge that was generalizable to other areas of application, it deals with problems that are highly specialized to genomics. If these were his only contributions (they are not), he’d be a hugely successful Ph.D. statistician. But had he focused on general solutions he would have never solved the problems at hand, since the problems were highly specific to a single application. And this is just one example I know well because I work in the area. There are a ton more just like it. While abstraction as defined generally is good, overemphasis on a specific type of abstraction limits our ability to include computing and real data analysis in our curriculum. It also takes away from the most important learning experience of graduate school: performing independent research.\n\n<p>\n  <\/strong>One could argue that the choice of statistical techniques during data analysis is abstraction, or that one needs to abstract to develop efficient software. But the ability to abstract needed for these tasks can be measured by a wide range of classes, not just measure theoretic probability. Some of these classes might teach practically applicable skills like writing fast and efficient algorithms. Many results of high statistical value do not require mathematical proofs, abstract inductive reasoning, or asymptotic theory. It is a good idea to have a some people who can abstract away the science behind statistical methods to the core mathematical philosophy. But our current curriculum is too heavily weighted in this direction. In some cases, statisticians are even being left behind because they do not have sufficient time in their curriculum to develop the computational skills and amass the necessary subject matter knowledge needed to compete with the increasingly diverse set of engineers, computer scientists, data scientists, and computational biologists tackling the same scientific problems.\n<\/p>\n\n<p>\n  We need to reserve a larger portion of graduate education for diving deeply into specific scientific problems, even if it means they spend less time developing generalizable/abstract statistical ideas. <br /><strong id=\"internal-source-marker_0.49558418267406523\"><br /><\/strong><em>* Inside joke explanation: Two years ago at JSM I ran a footrace with <a href=\"http://www.biostat.jhsph.edu/~jgoldsmi/\" target=\"_blank\">this guy<\/a> for the rights to the name “Jeff” in the department of Biostatistics at Hopkins for the rest of 2011. Unfortunately, we did not pro-rate for age and he nipped me by about a half-yard. True to my word, I went by Tullis (my middle name) for a few months, including on the <a href=\"http://biostat.jhsph.edu/~jleek/jsm-2011-title-slide.pdf\" target=\"_blank\">title slide<\/a> of my JSM talk. This was, of course, immediately subjected to all sorts of nicknaming and B-Caffo loves to use “T-bone”. I apologize on behalf of those that brought it up.<\/em>\n<\/p>\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-07-my-worst-nightmare/",
    "title": "My worst nightmare...",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-07",
    "categories": [],
    "contents": "\nI don’t know if you have seen this about a person who’s iCloud account was hacked. But man does it freak me out. As a person who relies pretty heavily on cloud-based storage devices and does some cloud-computing based research as well, this is a pretty freaky scenario. Time to go back everything up again…\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-06-in-which-brian-debates-abstraction-with-t-bone/",
    "title": "In which Brian debates abstraction with T-Bone",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-06",
    "categories": [],
    "contents": "\nEditor’s Note: This is the first in a set of point-counterpoint posts related to the value of abstract thinking in graduate education that we teased a few days ago. Brian Caffo, recently installed Graduate Program Director at the best Biostat department in the country, has kindly agreed to lead off with the case for abstraction. We’ll follow up later in the week with my counterpoint. In the meantime, there have already been a number of really interesting and insightful comments inspired by our teaser post that are well worth reading. See the comments here. \nThe impetus for writing this blog post came out of a particularly heady lunchroom discussion on the role of measure theoretic probability in our curriculum. We have a very mathematically rigorous program at Hopkins Biostatistics that includes a full academic year of measure theoretic probability.  Similar to elsewhere, many faculty dispute the necessity of this course. I am in favor of it. My principal reason being that I believe it is useful for building up and evaluating a student’s abilities in abstraction and generalization.\nIn our discussion, abstraction was the real point of contention. Emphasizing abstraction versus more immediately practical tools is an age-old argument of ivory tower stereotypes (the philosopher archetype) versus  equally stereotypically scientific pragmatists (the engineering archetype).\nSo, let’s begin picking this scab. For your sake and mine, I’ll try to be brief.\n\n\nMy definitions:\n\n\nAbstraction - reducing a technique, idea or concept to its essence or core.\n\n\nGeneralization -  extending a technique, idea or concept to areas for which it was not originally intended.\n\n\nPhD - a post baccalaureate degree  that requires substantial new contributions to knowledge.\n\n\nThe term “substantial new contributions” in my definition of a PhD is admittedly fuzzy. To tie it down, examples that I think do create new knowledge in the field of statistics include: \n\n\napplying existing techniques to data where they have not been used before (generalization of the application of the techniques),\n\n\ndeveloping statistical software (abstraction of statistical and mathematical thoughts into code),\n\n\ndeveloping new statistical methods from existing ones (generalization), \n\n\nproving new theory (both abstraction and generalization) and\n\n\ncreating new data analysis pipelines (both abstraction and generalization). \n\n\nIn every one of these examples, generalization or abstraction is what differentiates it from a purely technical accomplishment.\n\n\nTo give a contrary activity, consider statistical technical specialization. That is, the application an existing method to data where the method is already known to be effective and no new statistical thought is required. Regardless of how necessary, difficult or important applying that method is, such activity does not constitute the creation of new statistical knowledge, even if it is a necessary schlep in the creation of new knowledge of another sort. Though many statistics graduate level activities require substantial technical specialization, to be doctoral statistical research in a way that satisfies my definition, generalization and abstraction are necessary components.\n\n\nI further contend that abstraction is a key tool for obtaining meaningful generalization. A method, theory, analysis, etcetera can not be retooled to non-intended use without stripping away some of its specialization and abstracting it to its core utility.\n\n\nAbstraction is constantly necessary when applying statistical methods. For example, whenever a statistician says “Method A really was designed for a different kind of data than mine. But at its core it’s really useful for finding out B, which I need to know. So I’ll use it anyway until (if ever) I come up with something better.”  \n\n\nAs examples: A = CLT, B = distribution for normalized means, A =  principal components, B = directions of variation, A = bootstrap, B = sampling distributions, A = linear models, B = mean relationships with covariates.\n\n\nAbstraction and generalization facilitates learning new areas. Knowledge of the abstract core of a discipline makes that knowledge much more portable. This is seen across every discipline. Musicians who know music theory can use their knowledge for any instrument;  computer scientists who understand data structures and algorithms can switch languages easily; electrical engineers who understand signal processing can switch between technologies easily. Abstraction is what allows them to see past the concrete (instrument, syntax, technology) to the essence (music, algorithm, signal).\n\n\nAnd statisticians learn statistical and probability theory. However, in statistics, abstraction is not represented only by mathematics and theory. As pointed out by the absolutely unimpeachable source, Simply Statistics, software is exactly an abstraction.\n\n\n\nI think abstraction is important and we need to continue publishing those kinds of ideas. However, I think there is one key point that the statistics community has had difficulty grasping, which is that software represents an important form of abstraction, if not the most important form …\n\n\n\n(A QED is in order, I believe.)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-08-06-samuel-kou-wins-copss-award/",
    "title": "Samuel Kou wins COPSS Award",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-08-06",
    "categories": [],
    "contents": "\nAt JSM this year we learned that Samuel Kou of Harvard’s Department of Statistics won the Committee of Presidents of Statistical Societies (COPSS) President’s award. The award is given annually to\n\na young member of the statistical community in recognition of an outstanding contribution to the profession of statistics. The recipient of the Presidents’ Award must be a member of at least one of the participating societies. The candidate may be chosen for a single contribution of extraordinary merit, or an outstanding aggregate of contributions, to the profession of statistics. \n\nSamuel’s work spans a wide range of areas from biophysics to MCMC to model selection with contributions in the top journals in statistics and elsewhere. He is also a member of a highly selective group of people who have been promoted to full Professor at Harvard’s Department of Statistics. (Bonus points to those who can name the last person to achieve such a distinction.)\nThis is a well-deserved honor to an exemplary member of our field.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-31-if-i-were-at-jsm2012-today-heres-where-id-go/",
    "title": "If I were at #JSM2012 today, here's where I'd go.",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-31",
    "categories": [],
    "contents": "\nObviously, there are tons of sessions everyday at JSM this week and it’s physically impossible to go to everything that looks interesting. Alas, I am but one man, so choices had to be made. Here’s what looks good to me from the JSM program:\n8:30-10:20am: Contemporary Software Design Strategies for Statistical Methodologists, HQ-Sapphire B\n10:30am-12:20pm: Stat-Us Update from Facebook, HQ-Sapphire EF\n2:00-3:50pm: Astrostatistics, CC-Room 29A or Results from the 2010 Census Experimental Program, CC-Room 30A (perhaps you can run back and forth?)\nLots of other good stuff out there, of course. I wouldn’t mind hearing some feedback on how these go.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-31-nyc-and-columbia-to-create-institute-for-data-sciences/",
    "title": "NYC and Columbia to Create Institute for Data Sciences &#038; Engineering",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-31",
    "categories": [],
    "contents": "\nNYC and Columbia to Create Institute for Data Sciences & Engineering\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-30-why-im-staying-in-academia/",
    "title": "Why I'm Staying in Academia",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-30",
    "categories": [],
    "contents": "\nRecently, I’ve seen a few blog posts/articles about professors leaving academia for industry or some other non-academic position. By my last count I think I’ve seen three from computer science professors leaving academia for Google. The most recent one being from Terran Lane at University of New Mexico. At this point, Google should just start a recruiting office in middle of all the CS departments around the country. I think they’d get some good people.\nEach of the “fairwell” blog posts cover many of the same points—difficulty with having an impact, increasing specialization of academic research, difficult funding climate, increasing workloads—and, frankly, all of this is true to varying degrees. Beki Grinter has already written a pretty good response. One topic, massive open online courses (MOOCs), is something on which I’ll comment at a later date. For now, I thought I would add a few of my thoughts.\nThere’s no perfect job. Many of the problems affecting academia—difficult funding, increased workloads—are affecting other industries too. Right now we’re in the worst economic recession in decades and money is tight everywhere. I find it difficult to imagine that there’s a job out there that doesn’t suffer from some form of economic or other constraint. Academia needs to find some solutions, for sure, but times are tough everywhere unfortunately.\nThis is about as close as it gets to the perfect job. Really, it’s a pretty good gig. Everyday I come into work and I sit down and work on whatever I want. I’m surrounded by fantastic students and postdocs and when I walk the halls I can talk to great people who are smarter than I (even if they don’t necessarily appreciate me barging in). But that said, it’s not an easy job. The reality is that every professor is a like 1-person startup company, and you need to work pretty hard to stay afloat. (Okay, I’ve never worked at a startup, but I imagine they work pretty hard there.) They don’t tell you that in grad school but, then again, there’s a lot they don’t tell you in grad school.\nIt helps to work at a medical institution where tenure is meaningless. Okay, I’m being a bit facetious here…but not really. Much of academic anxiety comes from the need to “get tenure”. At most medical institutions, while tenure exists, having it is fairly meaningless (getting it, of course, is still very tough). The reason is because most medical researchers are funded on soft money, so somewhere between 60% to 100% of their salary is paid from grants. Whether this is a good way or a terrible way to do things is worth discussing at a later date, but the end result is if you can’t fund your salary, getting tenure isn’t going to magically come up with the missing dollars. Universities can’t afford it using the current model. So while tenure is a tremendous privilege and honor and will secure your position at the University, it can’t secure your salary. In the end, what I really need to be focusing on is doing the best research. There’s really no “game” to play here.\nThe best way to have an impact is to do it. Every University is different, for sure, and some put many more constraints on their professors than others. I consider myself lucky to be working at an institution that has substantial resources and is in relatively good financial condition. So in the end, if I want to have an impact on statistics or science, I just need to decide to do it. If one day someone comes to me and says “stop what you’re doing, you need to be doing something else”, then I might need to reconsider things. But until that day comes, I’m staying put. It might turn out I’m not good enough to have an impact, but we can’t all be above average.\nUltimately, I don’t want the many grad students out there who may be considering a career in academia to feel discouraged by what they might be reading on the Internets these days. There’s good and bad with every job, but I think with academia the balance is fairly positive, and you get to hang out with cool people. \nOf course, if you’re in computer science, you should just go to Google like everyone else.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-29-in-sliding-internet-stocks-some-hear-echo-of-2000/",
    "title": "In Sliding Internet Stocks, Some Hear Echo of 2000",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-29",
    "categories": [],
    "contents": "\nIn Sliding Internet Stocks, Some Hear Echo of 2000\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-29-statistician-cocteau-to-show-journalists-how-its/",
    "title": "Statistician (@cocteau) to show journalists how it's done",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-29",
    "categories": [],
    "contents": "\nMark Hansen, a Professor at UCLA’s Departments of Statistics and Media Arts, has been appointed as the inaugural Director of the David and Helen Gurley Brown Institute for Media Innovation. The Institute is a joint venture between Columbia University’s Graduate School of Journalism and Stanford’s School of Engineering.\n\nThe Institute and the collaboration between the two schools is groundbreaking in that it is designed to encourage and support new endeavors with the potential to inform and entertain in transformative ways. It will recognize the increasingly important connection between journalism and technology, bringing the best from the East and West Coasts.\n\nCongratulations to Mark for this fantastic opportunity!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-28-predictive-analytics-might-not-have-predicted-the/",
    "title": "Predictive analytics might not have predicted the Aurora shooter",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-28",
    "categories": [],
    "contents": "\nPredictive analytics might not have predicted the Aurora shooter\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-28-tweet-up-jsm2012/",
    "title": "Tweet up #JSM2012",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-28",
    "categories": [],
    "contents": "\nIf only because I won’t be there this year and I need to know what’s going on! Where’s the action?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-28-when-picking-a-c-e-o-is-more-random-than-wise/",
    "title": "When Picking a C.E.O. Is More Random Than Wise",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-28",
    "categories": [],
    "contents": "\nWhen Picking a C.E.O. Is More Random Than Wise\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-27-congress-to-examine-data-sellers/",
    "title": "Congress to Examine Data Sellers",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-27",
    "categories": [],
    "contents": "\nCongress to Examine Data Sellers\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-27-how-important-is-abstract-thinking-for-graduate/",
    "title": "How important is abstract thinking for graduate students in statistics?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-27",
    "categories": [],
    "contents": "\nA recent lunchtime discussion here at Hopkins brought up the somewhat-controversial topic of abstract thinking in our graduate program. We, like a lot of other biostatistics/statistics programs, require our students to take measure theoretic probability as part of the curriculum. The discussion started as a conversation about whether we should require measure theoretic probability for our students. It evolved into a discussion of the value of abstract thinking (and whether measure theoretic probability was a good tool to measure abstract thinking).\nBrian Caffo and I decided an interesting idea would be a point-counterpoint with the prompt, “How important is abstract thinking for the education of statistics graduate students?” Next week Brian and I will provide a point-counterpoint response based on our discussion.\nIn the meantime we’d love to hear your opinions!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:58:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-26-online-education-many-academics-are-missing-the-point/",
    "title": "Online education: many academics are missing the point",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-26",
    "categories": [],
    "contents": "\nMany academics are complaining about online education and warning us about how it can lead to a lower quality product. For example, the New York Times recently published this op-ed piece wondering if “online education [will] ever be education of the very best sort?”. Although pretty much every controlled experiment comparing online and in-class education finds that students learn just about the same under both approaches, I do agree that in-person lectures are more enjoyable to both faculty and students. But who cares? My enjoyment and the enjoyment of the 30 privileged students that physically sit in my classes seems negligible compared to the potential of reaching and educating thousands of students all over the world.  Also, using recorded lectures will free up time that I can spend on one-on-one interactions with tuition paying students.  But what most excites me about online education is the possibility of being part of the movement that redefines existing disciplines as the number of people learning grows by orders of magnitude. How many Ramanujans are out there eager to learn Statistics? I would love it if they learned it from me. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-26-smartphones-big-data-help-fix-bostons-potholes/",
    "title": "Smartphones, Big Data Help Fix Boston's Potholes",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-26",
    "categories": [],
    "contents": "\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-26-voters-say-they-are-wary-of-ads-made-just-for-them/",
    "title": "Voters Say They Are Wary of Ads Made Just for Them",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-26",
    "categories": [],
    "contents": "\nVoters Say They Are Wary of Ads Made Just for Them\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-25-buy-your-own-analytics-startup-for-15-000-at-least-as/",
    "title": "Buy your own analytics startup for $15,000 (at least as of now)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-25",
    "categories": [],
    "contents": "\nBuy your own analytics startup for $15,000 (at least as of now)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-25-really-big-objects-coming-to-r/",
    "title": "Really Big Objects Coming to R",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-25",
    "categories": [],
    "contents": "\nI noticed in the development version of R the following note in the NEWS file:\n\nThere is a subtle change in behaviour for numeric index values 2^31 and larger.  These used never to be legitimate and so were treated as NA, sometimes with a warning.  They are now legal for long vectors so there is no longer a warning, and x[2^31] <- y will now extend the vector on a 64-bit platform and give an error on a 32-bit one.\n\nThis is significant news indeed!\nSome background: In the old days, when most us worked on 32-bit machines, objects in R were limited to be about 4GB in size (and practically a lot less) because memory addresses were indexed using 32 bit numbers. When 64-bit machines became more common in the early 2000s, that limit was removed. Objects could theoretically take up more memory because of the dramatically larger address space. For the most part, this turned out to be true, although there were some growing pains as R was transitioned to be runnable on 64-bit systems (I remember many of those pains).\nHowever, even with the 64-bit systems, there was a key limitation, which is that vectors, one of the fundamental objects in R, could only have a maximum of 2^31-1 elements, or roughly 2.1 billion elements. This was because array indices in R were stored internally as signed integers (specifically as ‘R_len_t’), which are 32 bits on most modern systems (take a look at .Machine$integer.max in R).\nYou might think that 2.1 billion elements is a lot, and for a single vector it still is. But you have to consider the fact that internally R stores all arrays, no matter how many dimensions there are, as just long vectors. So that would limit you, for example, to a square a matrix that was no bigger than roughly 46,000 by 46,000. That might have seemed like a large matrix back in 2000 but it seems downright quaint now. And if you had a 3-way array, the limit gets even smaller. \nNow it appears that change is a comin’. The details can be found in the R source starting at revision 59005 if you follow on subversion. \nA new type called ‘R_xlen_t’ has been introduced with a maximum value of 4,503,599,627,370,496, which is 2^52. As they say where I grew up, that’s a lot of McNuggets. So if your computer has enough physical memory, you will soon be able to index vectors (and matrices) that are significantly longer than before.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-24-a-contest-for-sequencing-genomes-has-its-first-entry-in/",
    "title": "A Contest for Sequencing Genomes Has Its First Entry in Ion Torrent",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-24",
    "categories": [],
    "contents": "\nA Contest for Sequencing Genomes Has Its First Entry in Ion Torrent\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-24-i-b-m-is-no-longer-a-tech-bellwether-its-too-busy/",
    "title": "I.B.M. Is No Longer a Tech Bellwether (It's too busy doing statistics)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-24",
    "categories": [],
    "contents": "\nI.B.M. Is No Longer a Tech Bellwether (It’s too busy doing statistics)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-24-proof-by-example-and-letters-of-recommendation/",
    "title": "Proof by example and letters of recommendation",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-24",
    "categories": [],
    "contents": "\nIn math or statistics, proof by example does not work. One example of a phenomenon does not prove anything. For example, because 2 is prime doesn’t mean that all even numbers are prime. In fact, no even numbers other than 2 are prime. \nBut in other areas proof by example is the best way to demonstrate something. One example is writing letters of recommendation. It is way more convincing when I get one example of something a person has achieved:\n\nKyle created the first R package that can be used to analyze terabytes of sequencing data in under an hour.\n\nThan something much more general but with no details:\n\nBryan is an excellent programmer with a mastery of six different programming languages. \n\nIn mathematics it makes sense why proof by example does not work. There is a concrete result and even one example violating that result means it isn’t true. On the other hand, if most of the time Kyle crushes his work, but every once in a while he has an off day and doesn’t get it done, I can live with that. That’s true of a lot of applied statistical methods too. If it works 99% of the time and 1% of the time fails but you can discover how it failed, that is still a pretty good statistical method…\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-23-facebooks-real-big-data-problem/",
    "title": "Facebook's Real Big Data Problem",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-23",
    "categories": [],
    "contents": "\nFacebook’s first quarterly earnings report as a public company is coming out this Thursday and everyone’s wondering what will be in it. One question is whether advertisers are going to Facebook over other sites like Google.\n\n“Advertisers need more proof that actual advertising on Facebook offers a return on investment,” said Debra Aho Williamson, an analyst with the market research firm eMarketer. “There is such disagreement over whether Facebook is the next big thing on the Internet or whether it’s going to fail miserably.”\nFacebook’s unique asset is the pile of personal data it collects from 900 million users. But using that data to serve up effective, profitable advertisements is a daunting task. Google has been in the advertising game longer and has roughly $40 billion in annual revenue from advertising — 10 times that of Facebook. Since the public offering, Wall Street has tempered its expectations for Facebook’s advertising revenue, and shares closed Friday at $28.76, down from their initial price of $38.\n\nThere’s a pretty fundamental question here: Does it work?\nWith all the data Facebook has at its fingertips, it would be a shame if they couldn’t answer that question.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-23-medalball-moneyball-for-the-olympics/",
    "title": "Medalball: Moneyball for the olympics",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-23",
    "categories": [],
    "contents": "\nMedalball: Moneyball for the olympics\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-23-we-used-you-know-that-statistics-thingy/",
    "title": "We used, you know, that statistics thingy",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-23",
    "categories": [],
    "contents": "\nWe used, you know, that statistics thingy\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-22-sunday-data-statistics-link-roundup-7-22-12/",
    "title": "Sunday Data/Statistics Link Roundup (7/22/12)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-22",
    "categories": [],
    "contents": "\nThis paper is the paper describing how Uri Simonsohn identified academic misconduct using statistical analyses. This approach has received a huge amount of press in the scientific literature. The basic approach is that he calculates the standard deviations of mean/standard deviation estimates across groups being compared. Then he simulates from a Normal distribution and shows that under the Normal model, it is unlikely that the means/standard deviations are so similar. I think the idea is clever, but I wonder if the Normal model is the best choice here…could the estimates be similar because it was the same experimenter, etc.? I suppose the proof is in the pudding though, several of the papers he identifies have been retracted. \nThis is an amazing rant by a history professor at Swarthmore over the development of massive online courses, like the ones Roger, Brian and I are teaching. I think he makes some important points (especially about how we could do the same thing with open access in a heart beat if universities/academics through serious muscle behind it), but I have to say, I’m personally very psyched to be involved in teaching one of these big classes. I think that statistics is a field that a lot of people would like to learn something about and I’d like to make it easier for them to do that because I love statistics. I also see the strong advantage of in-person education. The folks who enroll at Hopkins and take our courses will obviously get way more one-on-one interaction, which is clearly valuable. I don’t see why it has to be one or the other…\nAn interesting discussion with Facebook’s former head of big data. I think the first point is key. A lot of the “big data” hype has just had to do with the infrastructure needed to deal with all the data we are collecting. The bigger issue (and where statisticians will lead) is figuring out what to do with the data. \nThis is a great post about data smuggling. The two key points that I think are raised are: (1) how when the data get big enough, they have their own mass and aren’t going to be moved, and (2) how physically mailing harddrives is still the fastest way of transferring big data sets. That is certainly true in genomics where it is called “sneaker net” when a collaborator walks a hard drive over to our office. Hopefully putting data in physical terms will drive home the point that the new scientists are folks that deal with/manipulate/analyze data. \nNot statistics related, but here is a high-bar to hold your work to: the bus-crash test. If you died in a bus-crash tomorrow, would your discipline notice? Yikes. Via C.T. Brown. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-21-big-data-on-campus/",
    "title": "Big Data on Campus",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-21",
    "categories": [],
    "contents": "\nBig Data on Campus\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-21-risks-in-big-data-attract-big-law-firms/",
    "title": "Risks in Big Data Attract Big Law Firms",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-21",
    "categories": [],
    "contents": "\nRisks in Big Data Attract Big Law Firms\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-20-interview-with-lauren-talbot-quantitative-analyst-for/",
    "title": "Interview with Lauren Talbot - Quantitative analyst for the NYC Financial Crime Task Force",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-20",
    "categories": [],
    "contents": "\n\n\nLauren Talbot\n\n\n\n\nLauren Talbot is a quantitative analyst for the New York City Financial Crime Task Force. Before working for NYC she was an analyst at Acumen LLC and got her degree in economics from Stanford University. She is a key player turning spatial data in NYC into new tools for government management. We talked to Lauren about her work, how she is using open data to do things like predict where fires might occur, and how she got started in the Financial Crime Task Force. \n\n\nSS: Do you consider yourself a statistician, computer scientist, or something else?\n\n\nLT: A lot of us can’t call ourselves statisticians or computer scientists, even if that is a large part of what we do, because we never studied those fields formally. Quantitative or Data Analyst are popular job titles, but don’t really do justice to all the code infrastructure/systems you have to build and cultivate — you aren’t simply analyzing, you are matching and automating and illustrating, too. There is also a large creative aspect, because you have to figure out how to present the data in a way that is useful and compelling to people, many of whom have no prior experience working with data. So I am glad people have started using the term “Data Scientist,” even if makes me chuckle a little. Ideally I would call myself “Data Artist,” or “Data Whisperer,” but I don’t think people would take me seriously.\nSS: How did you end up in the NYC Mayor’s Financial Crimes Task Force?\nLT: I actually responded to a Craigslist posting. While I was still in the Bay Area (where I went to college), I was looking for jobs in NYC because I wanted to relocate back here, where I am originally from. I was searching for SAS programmer jobs, and finding a lot of stuff in healthcare that made me yawn a little. And then I had the idea to try the government jobs section. The Financial Crimes Task Force (now part of a broader citywide analytics effort under the Office of Policy and Strategic Planning) was one of two listings that popped up, and I read the description and immediately thought “dream job!” It has turned out to be even better than I imagined, because there is such a huge opportunity to make a difference — the Bloomberg administration is actually very interested in operationalizing insights from city data, so they are listening to the data people and using their work to inform agency resource allocation and even sometimes policy. My fellow are also just really fun and intelligent. I’m constantly impressed by how quickly they pick up new skills, get to the bottom of things, and jump through hoops to get things done. We also amuse and entertain each other throughout the day, which is awesome. \n\n\nSS: Can you tell us about one of the more interesting cases you have tackled and how data analysis/statistics played into the case?\n\n\nLT: Since this is the NYC Mayor’s Office, dealing with city data, almost of our analyses are in some way location-based. We are trying to answer questions like, “what locations are most likely to have a catastrophic event (e.g. fire) in the near future?” This involves combining many disparate datasets such as fire data, buildings data, emergency calls data, city planning data, even garbage data. We use the tax lot ID as a common identifier, but many of the datasets do not come with this variable - they only have a text address or intersection. In many cases, the address is entered manually and has spelling mistakes. In the beginning, we were using a point-and-click geocoding tool that the city provides that reads the text field and assigns the tax lot ID. However, it was taking a long time to prepare the data so it could be used by the program, and the program was returning many errors. When we visually inspected the errors, we saw that they were caused by minor spelling differences and naming conventions. Now, almost every week we get new datasets in different structures, and we need to geocode them immediately before we can really work with them. So we needed a geocoding program that was automated and flexible, as well as capable of geocoding addresses and intersections with spelling errors and different conventions. Over the past few months, using publicly available city planning datasets and regular expressions, my side project has been creating such a program in SAS. My first test case was self-reported data created solely through user entry. This dataset, which could only be 40% geocoded using the original tool, is now 93% geocoded using the program we developed. The program is constantly evolving and improving. Now it is assigning block faces, spellchecking street and city names, and accounting for the occasional gaps in the data. We use it for everything.\n\n\nSS: What are the computational tools and ideas you use most frequently in your day to day work (R, databases, regression analysis, etc.)?\n\n\nLT: In the beginning, all of the data was sent to us in SQL or Excel, which was not very efficient. Now we are building a multi-agency SAS platform that can be used by programmers and non-programmers. Since there are so many data sources that can work together, having a unified platform creates new discoveries that agencies can use to be more efficient or effective. For example, a building investigator can use 311 noise complaints to uncover vacated properties that are being illegally occupied. The platform employs Palantir, which is an excellent front-end tool for playing around with the data and exploring many-to-many relationships.  Internally, my team has also used R, Python, Java, even VBA. Whatever gets the job done. We use a good mix of statistical tools. The bread and butter is usually manipulating and understanding new data sources, which is necessary before we can start trying to do something like run a multiple regression, for example. In the end, it’s really a mashup: text parsing, name matching, summarizing/describing/reporting using comparative statistics, geomapping, graphing, logistic regression, even kernel density, can all be part of the mix. Our guiding principle is to use the tool/analysis/strategy that has the highest return on investment of time and analyst resources for the city.\n\n\nSS: What are the challenges of working as a quantitative analyst in a regulatory role? Is it hard to make your analyses/discoveries understandable?\n\n\nLT: A lot of data analysts working in government have a difficult time getting agencies and policymakers to take action based on their work due to political priorities and organizational structures. We circumvent that issue by operating based on the needs and requests of the agencies, as well as paying attention to current events. An agency or official may come to us with a problem, and we figure out what we can deliver that will be of use to them. This starts a dialogue. It becomes an iterative process, and projects can grow and morph once we have feedback. Oftentimes, it is better to use a data-mining approach, which is more understandable to non-statisticians, rather than a regression, which can seem like a black box. For example, my colleague came up with an algorithm to target properties that were a high fire risk based on the presence of illegal conversion complaints and evidence that the property owner was under financial distress. He began with a simple list of properties for the Department of Buildings to focus on, and now they go out to inspect a list of places selected by his algorithm weekly. This video of the fire chief speaking about the project illustrates the challenges encountered and why the simpler approach was ultimately successful:http://www.youtube.com/watch?v=425QSx0U8lU&feature=youtube_gdata_player\n\n\nSS: Do you have any advice for statisticians/data scientists who want to get involved with open government or government data analysis?\n\n\nLT: I’ve found that people in government are actually very open to and interested in using data. The first challenge is that they don’t know that the data they have is of value. To be the most effective, you should get in touch with the people who have subject matter expertise (usually employees who have been working on the ground for some time), interview them, check your assumptions, and share whatever you’re seeing in the data on an ongoing basis. Not only will both parties learn faster, but it helps build a culture of interest in the data. Once people see what is possible, they will become more creative and start requesting deliverables that are increasingly actionable. The second challenge is getting data, and the legal and social/political issues surrounding that. The big secret is that so much useful data is actually publicly available. Do your research — you may find what you need without having to fight for it. If what you need is protected, however, consider whether the data would still be useful to you if scrubbed of personally identifiable information. Location-based data is a good example of this. If so, see whether you can negotiate with the data owner to obtain only the parts needed to do your analysis. Finally, you may find that the cohort of data scientists in government is all too sparse, and too few people “speak your language.” Reach out and align yourself with people in other agencies who are also working with data. This is a great way to gain new insight into the goals and issues of your administration, as well as friends to support and advise you as you navigate “the system.”\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-19-big-data-is-worth-nothing-without-big-science/",
    "title": "Big data is worth nothing without big science",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-19",
    "categories": [],
    "contents": "\nBig data is worth nothing without big science\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-19-help-me-find-the-good-jsm-talks/",
    "title": "Help me find the good JSM talks",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-19",
    "categories": [],
    "contents": "\nI’m about to head out for JSM in a couple of weeks. The sheer magnitude of the conference means it is pretty hard to figure out what talks I should attend. One approach I’ve used in the past is to identify people who I know give good talks and go to their talks. But that isn’t a very good talk-discovery mechanism. So this year I’m trying a crowd-sourcing experiment. \nFirst, some background on what kind of talks I like.\nI strongly prefer talks where someone is tackling a problem presented by a new kind of data, whether they got that data from a collaborator, they scraped it off the web, or they generated it themselves.\n I am 100% ok if they only used linear regression to analyze the data if it led to interesting exploratory analysis, surprising results, or a cool conclusion. \nMajor bonus points if the method is being used to solve a real-world problem.\nI also really like creative and informative plots.\nI prefer pictures to text/equations in general\nOn the other hand, I really am not a fan of talks where someone developed a method, no matter how cool, then started looking around for a data set to apply it to. \nIf you know of anyone who is going to give a talk like that can you post it in the comments or tweet it to @simplystats with the hashtag #goodJSMtalks?\nAlso, if you know anyone who gives posters like this, lemme know so I can drop by.  \nThanks!!!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-18-a-closer-look-at-data-suggests-johns-hopkins-is-still/",
    "title": "A closer look at data suggests Johns Hopkins is still the #1 US hospital",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-18",
    "categories": [],
    "contents": "\nThe US News best hospital 2012-20132 rankings are out. The big news is that Johns Hopkins has lost its throne. For 21 consecutive years Hopkins was ranked #1, but this year Mass General Hospital (MGH) took the top spot displacing Hopkins to #2. However, Elisabet Pujadas, an MD-PhD student here at Hopkins, took a close look at the data used for the rankings and made this plot (by hand!). The plot shows histograms of the rankings by speciality and shows Hopkins outperforming MGH.\n\nI reproduced Elisabet’s figure using R (see plot on the left above… hers is way cooler). A quick look at the histograms shows that Hopkins has many more highly ranked specialities. For example, Hopkins has 5 specialities ranked as #1 while MGH has none. Hopkins has 2 specialities ranked #2 while MGH has none. The median rank for Hopkins is 3 while for MGH it’s 5. The plot on the right plots ranks, Hopkins’ versus MGH’s, and shows that Hopkins has a better ranking for 13 out of 16 specialities considered.\nSo how does MGH get ranked higher than Hopkins? Here U.S. News’ explanation of how they rank: \n\nTo make the Honor Roll, a hospital had to earn at least one point in each of six specialties. A hospital earned two points if it ranked among the top 10 hospitals in America in any of the 12 specialties in which the US News rankings are driven mostly by objective data, such as survival rates and patient safety. Being ranked in the next 10 in those specialties earned a hospital one point. In the other four specialties, where ranking is based on each hospital’s reputation among doctors who practice that specialty, the top five hospitals in the country received two Honor Roll points and the next five got one point.\n\nThis actually results in a tie of 30 points, but according to the table here, Hopkins was ranked in 15 specialities to MGH’s 16. This was the tiebreaker. But, the data they put up shows Hopkins ranked in all 16 specialities. Did the specialty ranked 17th do Hopkins in? In any case, a closer look at the data does suggest Hopkins is still #1.\nDisclaimer: I am a professor at Johns Hopkins University ___________________\nThe data for Hopkins is here and I cleaned it up and put it here. For MGH it’s here and here. The script used to make the plots is here. Thanks to Elisabet for the pointer and data.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-18-johns-hopkins-coursera-statistics-courses/",
    "title": "Johns Hopkins Coursera Statistics Courses",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-18",
    "categories": [],
    "contents": "\nComputing for Data Analysis\n[youtube http://www.youtube.com/watch?v=gk6E57H6mTs]\nData Analysis\n[youtube http://www.youtube.com/watch?v=-lutj1vrPwQ]\nMathematical Biostatistics Bootcamp\n[youtube http://www.youtube.com/watch?v=ekdpaf\\_WT\\_8]\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-18-top-universities-test-the-online-appeal-of-free/",
    "title": "Top Universities Test the Online Appeal of Free",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-18",
    "categories": [],
    "contents": "\nTop Universities Test the Online Appeal of Free\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-17-free-statistics-courses-on-coursera/",
    "title": "Free Statistics Courses on Coursera",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-17",
    "categories": [],
    "contents": "\nToday, we’re very excited to announce that the Biostatistics Department at Johns Hopkins is offering three new online courses through Coursera. These courses are\nData Analysis: Data have never been easier or cheaper to come by. This course will cover how to collect, clean, interpret and analyze data, then communicate your results for maximum impact.Instructor: Jeff Leek\nComputing for Data Analysis: This course is about learning the fundamental computing skills necessary for effective data analysis. You will learn to program in R and to use R for reading data, writing functions, making informative graphs, and applying modern statistical methods.Instructor: Roger Peng\nMathematical Biostatistics Bootcamp: This course presents fundamental probability and statistical concepts used in biostatistical data analysis. It is taught at an introductory level for students with junior- or senior-college level mathematical training.Instructor: Brian Caffo\nThese courses will be offered free of charge through Coursera to anyone interested in signing up. Those who complete the course and meet a passing grade will get a certificate of completion from Coursera.\nComputing for Data Analysis and Mathematical Biostatistics Bootcamp will start in the fall on September 24. Data Analysis will start in the spring on January 22, 2013.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-17-universities-reshaping-education-on-the-web/",
    "title": "Universities Reshaping Education on the Web",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-17",
    "categories": [],
    "contents": "\nUniversities Reshaping Education on the Web\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-15-bits-betaworks-buys-whats-left-of-social-news-site/",
    "title": "Bits: Betaworks Buys What's Left of Social News Site Digg",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-15",
    "categories": [],
    "contents": "\nBits: Betaworks Buys What’s Left of Social News Site Digg\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-15-sunday-data-statistics-link-roundup-7-15-12/",
    "title": "Sunday Data/Statistics Link Roundup (7/15/12)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-15",
    "categories": [],
    "contents": "\nA really nice list of journals software/data release policies from Titus’ blog. Interesting that he couldn’t find a data/release policy for the New England Journal of Medicine. I wonder if that is because it publishes mostly clinical studies, where the data are often protected for privacy reasons? It seems like there is going to eventually be a big discussion of the relative importance of privacy and open data in the clinical world. \nSome interesting software that can be used to build virtual workflows for computational science. It seems like a lot of data analysis is still done via “drag and drop” programs. I can’t help but wonder if our effort should be focused on developing drag and drop or educating the next generation of scientists to have minimum scripting capabilities. \nWe added StatsChat by Thomas L. and company to our blogroll. Lots of good stuff there, for example, this recent post on when randomized trials don’t help. You can also follow them on twitter.  \nA really nice post on processing public data with R. As more and more public data becomes available, from governments, companies, APIs, etc. the ability to quickly obtain, process, and visualize public data is going to be hugely valuable. \nSpeaking of public data, you could get it from APIs or from government websites. But beware those category 2 problems! \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-14-bits-mobile-app-developers-scoop-up-vast-amounts-of/",
    "title": "Bits: Mobile App Developers Scoop Up Vast Amounts of Data, Reports Say",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-14",
    "categories": [],
    "contents": "\nBits: Mobile App Developers Scoop Up Vast Amounts of Data, Reports Say\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-13-gdp-figures-in-china-are-for-reference-only/",
    "title": "GDP Figures in China are for \"reference\" only",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-13",
    "categories": [],
    "contents": "\nGDP Figures in China are for “reference” only\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-13-this-is-not-about-statistics-but-its-about/",
    "title": "This is Not About Statistics But It is About Emacs",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-13",
    "categories": [],
    "contents": "\n[youtube http://www.youtube.com/watch?v=p3Te\\_a-AGqM?wmode=transparent&autohide=1&egm=0&hd=1&iv\\_load_policy=3&modestbranding=1&rel=0&showinfo=0&showsearch=0&w=500&h=375]\nThis is not about statistics, but it’s about Emacs, which I’ve been using for a long time. This guy is an Emacs virtuoso, and the crazy thing is that he’s only been using it for 8 months!\nBest line: “Should I wait for the next version of Emacs? Hell no!”\n(Thanks to Brian C. and Kasper H. for the pointer.)\n\n(Source: http://www.youtube.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-12T08:05:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-12-statistical-reasoning-on-itunes-u/",
    "title": "Statistical Reasoning on iTunes U",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-12",
    "categories": [],
    "contents": "\nOur colleague, the legendary John McGready has just put his Statistical Reasoning I and Statistical Reasoning II courses on iTunes U. This course sequence is extremely popular here at Johns Hopkins and now the entire world can experience the joy.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-12-what-is-the-most-important-code-you-write/",
    "title": "What is the most important code you write?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-12",
    "categories": [],
    "contents": "\nThese days, like most people, the research I do involves writing a lot of code. A lot of it. Usually, you need some code to\nProcess the data to take it from its original format to the format that’s convenient for you\nRun exploratory data analyses creating plots, calculating summary statistics, etc.\nTry statistical model 1\nTry statistical model 2\nTry statistical model 3\n…\nFit final statistical model; if this involves MCMC then there’s usually a ton of code to do this\nMake some more plots of results, make tables, more summary statistics of output\nMy question is, of all this code, which is the most important? The code that fits the final model? The code that does that summarizes results? Often you just see the code that fit the final statistical model and maybe some of the code that summarizes the results. The code for fitting all of the previous models and doing the exploratory analysis is lost in the ether (or at least the version control ether). Now, I’m not saying I always want to see all that other code. Usually, I am just interested in the final model.\nThe point is that the code for the final model only represents a small fraction of the work that was done to get there. This work is the bread and butter of applied statistics and it is essentially thrown out. Of course, life would be much easier if someone would just tell me what the final model would be every time. Then I would just fit it! But nooooo, hundreds or thousands of lines of code and numerous judgment calls go into figuring out what that last model is going to be. \nYet when you read a paper, it more or less looks like the final model appeared out of thin air because there’s no space/time to tell the story about everything that came before. I would say the same is true for theoretical statistics too. It’s not as if theorems/proofs appear out of nowhere. Hard work goes into figuring out both the right theorem to prove and the right way to prove it.\nBut I would argue that there’s one key difference between theoretical and applied statistics in this regard: Everyone seems to accept that theoretical statistics is hard. So when you see a theorem/proof in a paper you consciously or unconsciously realize that it must have been hard work to arrive at that point. But in a great applied statistics paper, all you get is an interesting scientific question and some graphs/tables that provide an answer. Who cares about that?\nSeriously though, even for a seasoned applied statistician, it’s sometimes easy to forget that everything looks easy once someone else has done all the work. It’s not clear to me whether we just need to change expectations or if we need a different method for communicating the effort involved (or both). Making research reproducible would be one approach as it would require all the code for the work be available. But that’s mostly just “final model” stuff plus some data processing code. Going one step further might require that a git repository be made available. That way you could see all the history in addition to the final stuff. I’m guessing there would be some resistance to universally adopting that approach!\nAnother approach might be to allow applied stat papers to go into more of the details about the process. With strict space limitations these days, it’s often hard enough to talk about the final model. But in some cases I think I would enjoy reading the story behind the story. Some of that “backstory” would make for good instructional material for applied stat classes.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-11-how-does-the-film-industry-actually-make-money/",
    "title": "How Does the Film Industry Actually Make Money?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-11",
    "categories": [],
    "contents": "\nHow Does the Film Industry Actually Make Money?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-11-my-worst-recent-experience-with-peer-review/",
    "title": "My worst (recent) experience with peer review",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-11",
    "categories": [],
    "contents": "\nMy colleagues and I just published a paper on validation of genomic results in BMC Bioinformatics. It is “highly accessed” and we are really happy with how it turned out. \nBut it was brutal getting it published. Here is the line-up of places I sent the paper. \nScience: Submitted 10/6/10, rejected 10/18/10 without review. I know this seems like a long shot, but this paper on validation was published in Science not too long after. \nNature Methods: Submitted 10/20/10, rejected 10/28/10 without review. Not much to say here, moving on…\nGenome Biology: Submitted 11/1/10, rejected 1/5/11. 2/3 referees thought the paper was interesting, few specific concerns raised. I felt they could be addressed so appealed on 1/10/11, appeal accepted 1/20/11, paper resubmitted 1/21/11. Paper rejected 2/25/11. 2/3 referees were happy with the revisions. One still didn’t like it. \nBioinformatics: Submitted 3/3/11, rejected 3/1311 without review. I appealed again, it turns out “I have checked with the editors about this for you and their opinion was that there was already substantial work in validating gene lists based on random sampling.” If anyone knows about one of those papers let me know :-). \nNucleic Acids Research: Submitted 3/18/11, rejected with invitation for revision 3/22/11. Resubmitted 12/15/11 (got delayed by a few projects here) rejected 1/25/12. Reason for rejection seemed to be one referee had major “philosophical issues” with the paper.\nBMC Bioinformatics: Submitted 1/31/12, first review 3/23/12, resubmitted 4/27/12, second revision requested 5/23/12, revised version submitted 5/25/12, accepted 6/14/12. \n\nAn interesting side note is the really brief reviews from the Genome Biology submission inspired me to do this paper. I had time to conceive the study, get IRB approval, build a web game for peer review, recruit subjects, collect the data, analyze the data, write the paper, submit the paper to 3 journals and have it come out 6 months before the paper that inspired it was published! \n\n\n\n\n\nOk, glad I got that off my chest.\n\n\n\n\n\nWhat is your worst peer-review story?\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:57:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-09-a-northwest-pipeline-to-silicon-valley/",
    "title": "A Northwest Pipeline to Silicon Valley",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-09",
    "categories": [],
    "contents": "\nA Northwest Pipeline to Silicon Valley\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-09-skepticism-ideas-grit/",
    "title": "Skepticism+Ideas+Grit",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-09",
    "categories": [],
    "contents": "\nA number of people seem to have objected to my post quoting Carl Sagan about skepticism (hi Paramita!) and I appreciate the comments. However, I wanted to clarify why I liked the quotation. I think in order to be successful in science three things are necessary:\nA healthy skepticism\nAn original idea\nQuite a bit of grit and moxie\nI find that too often, people consciously or unconsciously stop at (A). In fact some people make an entire career doing (A) but it’s not one that I can personally appreciate.\nWhat we need more of is skepticism coupled with new ideas, not pure skepticism. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-04-the-power-of-power/",
    "title": "The power of power",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-04",
    "categories": [],
    "contents": "\nThose of you living in the mid-Atlantic region are probably not reading this right now because you don’t have power. I’ve been out of power in my house since last Friday and projections are it won’t come back until the end of the week. I am lucky because my family and I have some backup options, but not everyone has those options.\nSo that leads me to this question—do power outages affect health? There have been a number of papers examining this question, mostly looking at one-off episodes, as you might expect. One paper, written by Brooke Anderson (postdoctoral fellow here) and Michelle Bell at Yale University examined the effect of the massive 2003 power outage in New York City on all-cause mortality. This was the first city-wide blackout since 1977 and the data from the time period are striking.\nA key point with this paper is that often mortality is under-estimated in these kinds of situations because deaths are only counted if they are identified as “disaster-related” (there may be other reasons, but I won’t get into that here). The NYC Department of Health and Mental Hygiene reported the total number of deaths to be 6 over the 2-day period of the blackout, mostly from carbon monoxide poisoning. However, the paper estimated a 28% increase in all-cause mortality which, in New York, translates to an excess mortality from of 90 deaths, an order of magnitude higher than official results.\nThe power outage in the mid-Atlantic is ongoing but things appear to be improving by the day. According to BGE, the primary electricity provider in Baltimore City, over half of its customers in the city were without power. On top of that the region is in the middle of a heat wave that has been going on for roughly the same amount of time as the power outage. If you figure the worst of it was in the first 3 days, and if New York’s relative risk could be applied here in Baltimore (a BIG if), then given a typical daily mortality of 17 deaths in the summer months, we would expect an excess mortality for the 3-day period of about 14 deaths from all causes.\nUnfortunately, it seems power outages are likely to become more frequent because of increasing stress on an aging power grids and climate change causing more extreme weather (this outage was caused by a severe thunderstorm). It seems to me that the contribution of such infrastructure failures to health problems will be an interesting problem to study for the future.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-03-replication-and-validation-in-omics-studies-just-as/",
    "title": "Replication and validation in -omics studies - just as important as reproducibility",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-03",
    "categories": [],
    "contents": "\nThe psychology/social psychology community has made replication a huge focus over the last year. One reason is the recent, public blow-up over a famous study that did not replicate. There are also concerns about the experimental and conceptual design of these studies that go beyond simple lack of replication. In genomics, a similar scandal occurred due to what amounted to “data fudging”. Although, in the genomics case, much of the blame and focus has been on lack of reproducibility or data availability. \nI think one of the reasons that the field of genomics has focused more on reproducibility is that replication is already more consistently performed in genomics. There are two forms for this replication: validation and independent replication. Validation generally refers to a replication experiment performed by the same research lab or group - with a different technology or a different data set. On the other hand, independent replication of results is usually performed by an outside laboratory. \nValidation is by far the more common form of replication in genomics. In this article in Science, Ioannidis and Khoury point out that validation has different meaning depending on the subfield of genomics. In GWAS studies, it is now expected that every significant result will be validated in a second large cohort with genome-wide significance for the identified variants.\nIn gene expression/protein expression/systems biology analyses, there has been no similar definition of the “criteria for validation”. Generally the experiments are performed and if a few/a majority/most of the results are confirmed, the approach is considered validated. My colleagues and I just published a paper where we define a new statistical sampling approach for validating lists of features in genomics studies that is somewhat less ambiguous. But I think this is only a starting point. Just like in psychology, we need to focus not just on reproducibility, but also replicability of our results, and we need new statistical approaches for evaluating whether validation/replication have actually occurred. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-02-computing-and-sustainability-what-can-be-done/",
    "title": "Computing and Sustainability: What Can Be Done?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-02",
    "categories": [],
    "contents": "\nLast Friday, the National Research Council released a report titled Computing Research for Sustainability, written by the NRC’s Committee on Computing Research for Environmental and Societal Sustainability, on which I served (press release). This was a novel experience for me given that I was the only non-computer scientist on the committee. That said, I think the report is quite interesting for a number of reasons. As a statistician, I took away a few lessons.\nSustainability presents many opportunities for CS. One of the first things the committee did was hold a workshop where researchers from all over presented their work on CS and sustainability—-and it was impressive. Everything from Shwetak Patel’s clever use of data analysis to monitor home power usage to Bill Tomlinson’s work in human computer interaction. Very educational for me. One thing I remember is that towards the end of the workshop John Doyle made some comment about IPv6 and everyone laughed and…I didn’t get it. I still don’t get it.\nCS faces a number of statistical challenges. Many of the interesting areas posed by sustainability research come across, in my mind, as statistical problems. In particular, there is a need to develop better statistical models for understanding uncertainty in a variety of systems (e.g. electrical power grids, climate models, ecological dynamics). These are CS problems because they are “big data” systems but the underlying issues are largely statistical. Overall, it seems a lot of money has been put into collecting data but relatively little investment has been made (so far) in figuring out what to do with it.\nStatistics and CS will be crashing into each other at a theater near you. In many discussions the Committee had, I couldn’t help thinking that a lot of the challenges in CS are exactly the same as in statistics. Specifically, how integrated should computer scientists be in the other sciences? Being an outsider to that area, it seems there is a debate going on between those who do “pure” computer science, like compilers and programming languages, and those who do “applied” computer science, like computational biology. This debate sounds eerily familiar.\nIt was fun to hang out with the computer scientists for a while, and this group was really exceptional. But now, back to my day job.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-02-meet-the-skeptics-why-some-doubt-biomedical-models/",
    "title": "Meet the Skeptics: Why Some Doubt Biomedical Models - and What it Takes to Win Them Over",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-02",
    "categories": [],
    "contents": "\nMeet the Skeptics: Why Some Doubt Biomedical Models - and What it Takes to Win Them Over\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-01-sunday-data-statistics-link-roundup-7-1/",
    "title": "Sunday data/statistics link roundup (7/1)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-07-01",
    "categories": [],
    "contents": "\nA really nice explanation of the elements of Obamacare. Rafa’s post on the new inHealth initiative Scott is leading got a lot of comments on Reddit. Some of them are funny (Rafa’s spelling got rocked) and if you get past the usual level of internet-commentary politeness, some of them seem to be really relevant - especially the comments about generalizability and the economics of health care. \nFrom Andrew J. a cool visualization of the human genome, they are showing every base of the human genome over the course of a year. That turns out to be about 100 bases per second. I think this is a great way to show how much information is in just one human genome. It also puts the sequencing data deluge in perspective. We are now sequencing thousands of these genomes a year and its only going to get faster. \nCosma Shalizi has a nice list of unsolved problems in statistics on his blog (via Edo A.). These problems primarily fall into what I call Category 1 problems in my post on motivating statistical projects. I think he has some really nice insight though and some of these problems sound like a big deal if one was able to solve them.\nA really provocative talk on why consumers are the job creators. The issue of who are the job creators seems absolutely ripe for a thorough statistical analysis. There are a thousand confounders here and my guess is that most of the work so far has been Category 2 - let’s use convenient data to make a stab at this. But a thorough and legitimate data analysis would be hugely impactful. \nYour eReader is collecting data about you.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-29-obamacare-is-not-going-to-solve-the-health-care-crisis/",
    "title": "Obamacare is not going to solve the health care crisis, but a new initiative, led by a statistician, may help",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-29",
    "categories": [],
    "contents": "\nObamacare may help protect a vulnerable section of our population, but it does nothing to solve the real problem with health care in the US: it is unsustainably expensive and getting worst worse. In the graph below (left) per capita medical expenditures for several countries are plotted against time. The US is the black curve, other countries are in grey. On the right we see life expectancy plotted against per capita medical expenditure. Note that the US spends $8,000 per person on healthcare, more than any other country and about 40% more than Norway, the runner up. If the US spent the same as Norway per person, as a country we would save ~ 1 trillion $ per year. Despite the massive investment, life expectancy in the US is comparable to Chile’s, a country that spends about $1,500 per person. To make matters worse, politicians and pundits greatly oversimply this problem by trying to blame their favorite villains while experts agree: no obvious solution exists.\n\nThis past Tuesday Johns Hopkins announced the launching of the Individualized Health Initiative. This effort will be led by Scott Zeger, a statistician and former chair of our department. The graphs and analysis shown above are from a presentation Scott has  shared on the web. The initiative’s goal is to “discover, test, and implement health information tools that allow the individual to understand, track, and guide his or her unique health state and its trajectory over time”. In other words, by tailoring treatments and prevention schemes for individuals we can improve their health more effectively.\n\n1\n49\n284\nJohns Hopkins University\n2\n1\n332\n14.0\n96\n800x600\nSo how is this going to help solve the health care crisis? Scott explains that when it comes to health care, Hopkins is a self-contained microcosm: we are the patients (all employees), the providers (hospital and health system), and the insurer (Hopkins is self-insured, we are not insured by for-profit companies). And just like the rest of the country, we spend way too much per person on health care. Now, because we are self-contained, it is much easier for us to try out and evaluate alternative strategies than it is for, say, a state or the federal government. Because we are large, we can gather enough data to learn about relatively small strata. And with a statistician in charge, we will evaluate strategies empirically as opposed to ideologically.  \nFurthermore, because we are a University, we also employ Economists, Public Health Specialists, Ethicists, Basic Biologists, Engineers, Biomedical Researchers, and other scientists with expertise that seem indispensable to solve this problem. Under Scott’s leadership, I expect Hopkins to collect data more systematically, run well thought-out experiments to test novel ideas, leverage technology to improve diagnostics, and use existing data to create knowledge. Successful strategies may then be exported to the rest of the country. Part of the new institute’s mission is to incentivize our very creative community of academics to participate in this endeavor. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-28-motivating-statistical-projects/",
    "title": "Motivating statistical projects",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-28",
    "categories": [],
    "contents": "\nIt seems like half of the battle in statistics is identifying an important/unsolved problem. In math, this is easy, they have a list. So why is it harder for statistics? Since I have to think up projects to work on for my research group, for classes I teach, and for exams we give, I have spent some time thinking about ways that research problems in statistics arise.\n\nI borrowed a page out of Roger’s book and made a little diagram to illustrate my ideas (actually I can’t even claim credit, it was Roger’s idea to make the diagram). The diagram shows the rough relationship of science, data, applied statistics, and theoretical statistics. Science produces data (although there are other sources), the data are analyzed using applied statistical methods, and theoretical statistics concerns the math behind statistical methods. The dotted line indicates that theoretical statistics ostensibly generalizes applied statistical methods so they can be applied in other disciplines. I do think that this type of generalization is becoming harder and harder as theoretical statistics becomes farther and farther removed from the underlying science.\nBased on this diagram I see three major sources for statistical problems: \nTheoretical statistical problems One component of statistics is developing the mathematical and foundational theory that proves we are doing sensible things. This type of problem often seems to be inspired by popular methods that exists/are developed but lack mathematical detail. Not surprisingly, much of the work in this area is motivated by what is mathematically possible or convenient, rather than by concrete questions that are of concern to the scientific community. This work is important, but the current distance between theoretical statistics and science suggests that the impact will be limited primarily to the theoretical statistics community. \nApplied statistics motivated by convenient sources of data. The best example of this type of problem are the analyses in Freakonomics.  Since both big data and small big data are now abundant, anyone with a laptop and an internet connection can download the Google n-gram data, a microarray from GEO , data about your city, or really data about anything and perform an applied analysis. These analyses may not be straightforward for computational/statistical reasons and may even require the development of new methods. These problems are often very interesting/clever and so are often the types of analyses you hear about in newspaper articles about “Big Data”. But they may often be misleading or incorrect, since the underlying questions are not necessarily well founded in scientific questions. \nApplied statistics problems motivated by scientific problems. The final category of statistics problems are those that are motivated by concrete scientific questions. The new sources of big data don’t necessarily make these problems any easier. They still start with a specific question for which the data may not be convenient and the math is often intractable. But the potential impact of solving a concrete scientific problem is huge, especially if many people who are generating data have a similar problem. Some examples of problems like this are: can we tell if one batch of beer is better than another, how are quantitative characteristics inherited from parent to child, which treatment is better when some people are censored, how do we estimate variance when we don’t know the distribution of the data, or how do we know which variable is important when we have millions? \nSo this leads back to the question, what are the biggest open problems in statistics? I would define these problems as the “high potential impact” problems from category 3. To answer this question, I think we need to ask ourselves, what are the most common problems people are trying to solve with data but can’t with what is available right now? Roger nailed this when he talked about the role of statisticians in the science club. \nHere are a few ideas that could potentially turn into high-impact statistical problems, maybe our readers can think of better ones?\nHow do we credential students taking online courses at a huge scale?\nHow do we communicate risk about personalized medicine (or anything else) to a general population without statistical training? \nCan you use social media as a preventative health tool?\nCan we perform randomized trials to improve public policy?\n\nImage Credits: The Science Logo is the old logo for the USU College of Science, the R is the logo for the R statistical programming language, the data image is a screenshot of Gapminder, and the theoretical statistics image comes from the Wikipedia page on the law of large numbers.\n\n\n\n\n\nEdit: I just noticed this paper, which seems to support some of the discussion above. On the other hand, I think just saying lots of equations = less citations falls into category 2 and doesn’t get at the heart of the problem. \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-27-follow-up-on-statistics-and-the-science-club/",
    "title": "Follow up on \"Statistics and the Science Club\"",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-27",
    "categories": [],
    "contents": "\nI agree with Roger’s latest post: “we need to expand the tent of statistics and include people who are using their statistical training to lead the new science”. I am perhaps a bit more worried than Roger. Specifically, I worry that talented go-getters interested in leading science via data analysis will achieve this without engaging our research community. \nA  quantitatively trained person (engineers , computer scientists, physicists, etc..) with strong computing skills (knows python, C, and shell scripting), that reads, for example, “Elements of Statistical Learning” and learns R, is well on their way. Eventually, many of these users of Statistics will become developers and if we don’t keep up then what do they need from us? Our already-written books may be enough. In fact, in genomics, I know several people like this that are already developing novel statistical methods. I want these researchers to be part of our academic departments. Otherwise, I fear we will not be in touch with the problems and data that lead to, quoting Roger, “the most exciting developments of our lifetime.” \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-27-the-price-of-skepticism/",
    "title": "The price of skepticism",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-27",
    "categories": [],
    "contents": "\nThanks to John Cook for posting this:\n\n“If you’re only skeptical, then no new ideas make it through to you. You never can learn anything. You become a crotchety misanthrope convinced that nonsense is ruling the world.” – Carl Sagan\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-26-hilary-mason-from-tiny-links-big-insights/",
    "title": "Hilary Mason: From Tiny Links, Big Insights",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-26",
    "categories": [],
    "contents": "\nHilary Mason: From Tiny Links, Big Insights\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-26-the-problem-with-small-big-data/",
    "title": "The problem with small big data",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-26",
    "categories": [],
    "contents": "\nThere’s lots of talk about “big data” these days and I think that’s great. I think it’s bringing statistics out into the mainstream (even if they don’t call it statistics) and it creating lots of opportunities for people with statistics training. It’s one of the reasons we created this blog.\nOne thing that I think gets missed in much of the mainstream reporting is that, in my opinion, the biggest problems aren’t with the truly massive datasets out there that need to be mined for important information. Sure, those types of problems pose interesting challenges with respect to hardware infrastructure and algorithm design.\nI think a bigger problem is what I call “small big data”. Small big data is the dataset that is collected by an individual whose data collection skills are far superior to his/her data analysis skills. You can think of the size of the problem as being measured by the ratio of the dataset size to the investigator’s statistical skill level. For someone with no statistical skills, any dataset represents “big data”.\nThese days, any individual can create a massive dataset with relatively few resources. In some of the work I do, we send people out with portable air pollution monitors that record pollution levels every 5 minutes over a 1-week period. People with fitbits can get highly time-resolved data about their daily movements. A single MRI can produce millions of voxels of data.\nOne challenge here is that these examples all represent datasets that are large “on paper”. That is, there are a lot of bits to store, but that doesn’t mean there’s a lot of useful information there. For example, I find people are often impressed by data that are collected with very high temporal or spatial resolution. But often, you don’t need that level of detail and can get away with coarser resolution over a wider range of scenarios. For example, if you’re interested in changes in air pollution exposure across seasons but you only measure people in the summer, then it doesn’t matter if you measure levels down to the microsecond and produce terabytes of data. Another example might be the idea the sequencing technology doesn’t in fact remove biological variability, no matter how large a dataset it produces.\nAnother challenge is that the person who collected the data is often not qualified/prepared to analyze it. If the data collector didn’t arrange beforehand to have someone analyze the data, then they’re often stuck. Furthermore, usually the grant that paid for the data collection didn’t budget (enough) for the analysis of the data. The result is that there’s a lot of “small big data” that just sits around unanalyzed. This is an unfortunate circumstance, but in my experience quite common.\nOne conclusion we can draw is that we need to get more statisticians out into the field both helping to analyze the data; and perhaps more importantly, designing good studies so that useful data are collected in the first place (as opposed to merely “big” data). But the sad truth is that there aren’t enough of us on the planet to fill the demand. So we need to come up with more creative ways to get the skills out there without requiring our physical presence.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-25-a-specific-suggestion-to-help-recruit-retain-women/",
    "title": "A specific suggestion to help recruit/retain women faculty at Hopkins",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-25",
    "categories": [],
    "contents": "\nA recent article by a former Obama administration official has stirred up debate over the obstacles women face in balancing work/life. This reminded me of this report written by  a committee here at Hopkins to help resolve the current gender-based career obstacles for women faculty. The report is great, but in practice we have a long way to go. For example, my department has not hired a woman at the tenure track level in 15 years. This drought has not been for lack of trying as we have made several offers, but none have been accepted. One issue that has come up multiple times is “spousal hires”. Anecdotal evidence strongly suggests that in academia the “two body” problem is more common with women than men. As hard as my department has tried to find jobs for spouses, efforts are ad-hoc and we get close to no institutional support. As far as I know, as an institution, Hopkins allocates no resources to spousal hires. So, a tangible improvement we could make is changing this. Another specific improvement that many agree will help women is subsidized day care. The waiting list here is very long (as a result few of my colleagues use it) and one still has to pay more than $1,600 a month for infants.\nThese two suggestions are of course easier said than done as they both require $. Quite of bit actually, and Hopkins is not rich compared to other well-known universities. My suggestion is to get rid of the college tuition remission benefit for faculty. Hopkins covers half the college tuition for the children of all their employees. This perk helps male faculty in their 50s much more than it helps potential female recruits. So I say get rid of this benefit and use the $ for spousal hires and to further subsidize childcare.\nIt might be argued the tuition remission perk helps retain faculty, but the institution can invest in that retention on a case-by-case basis as opposed to giving the subsidy to everybody independent of merit. I suspect spousal hires and subsidized day care will be more attractive at the time of recruitment. \nAlthough this post is Hopkins-specific I am sure similar reallocation of funds is possible in other universities.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-25-the-evolution-of-music/",
    "title": "The Evolution of Music",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-25",
    "categories": [],
    "contents": "\nThe Evolution of Music\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-24-sunday-data-statistics-link-roundup-6-24/",
    "title": "Sunday data/statistics link roundup (6/24)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-24",
    "categories": [],
    "contents": "\nWe’ve got a new domain! You can still follow us on tumblr or here: http://simplystatistics.org/. \nA cool article on MIT’s annual sports statistics conference (via @storeylab). I love how the guy they chose to highlight created what I would consider a pretty simple visualization with known tools - but it turns out it is potentially a really new way of evaluating the shooting range of basketball players. This is my favorite kind of creativity in statistics.\nThis is an interesting article calling higher education a “credentials cartel”. I don’t know if I’d go quite that far; there are a lot of really good reasons for higher education institutions beyond credentialing like research, putting smart students together in classes and dorms, broadening experiences etc. But I still think there is room for a smart group of statisticians/computer scientists to solve the credentialing problem on a big scale and have a huge impact on the education industry. \nCheck out John Cook’s conjecture on statistical methods that get used: “The probability of a method being used drops by at least a factor of 2 for every parameter that has to be determined by trial-and-error.” I’m with you. I wonder if there is a corollary related to how easy the documentation is to read? \nIf you haven’t read Roger’s post on Statistics and the Science Club, I consider it a must-read for anyone who is affiliated with a statistics/biostatistics department. We’ve had feedback by email/on twitter from other folks who are moving toward a more science oriented statistical culture. We’d love to hear from more folks with this same attitude/inclination/approach. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-22-statistics-and-the-science-club/",
    "title": "Statistics and the Science Club",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-22",
    "categories": [],
    "contents": "\nOne of my favorite movies is Woody Allen’s Annie Hall. If you’re my age and you haven’t seen it, I usually tell people it’s like When Harry Met Sally, except really good. The movie opens with Woody Allen’s character Alvy Singer explaining that he would “never want to belong to any club that would have someone like me for a member”, a quotation he attributes to Groucho Marx (or Freud).\nLast week I posted a link to ASA President Robert Rodriguez’s column in Amstat News about big data. In the post I asked what was wrong with the column and there were a few good comments from readers. In particular, Alex wrote:\n\nWhen discussing what statisticians need to learn, he focuses on technological changes (distributed computing, Hadoop, etc.) and the use of unstructured text data. However, Big Data requires a change in perspective for many statisticians. Models must expand to address the levels of complexity that massive datasets can reveal, and many standard techniques are limited in utility.\n\nI agree with this, but I don’t think it goes nearly far enough. \nThe key element missing from the column was the notion that statistics should take a  leadership role in this area. I was disappointed by the lack of a more expansive vision displayed by the ASA President and the ASA’s unwillingness to claim a leadership position for the field. Despite the name “big data”, big data is really about statistics and statisticians should really be out in front of the field. We should not be observing what is going on and adapting to it by learning some new technologies or computing techniques. If we do that, then as a field we are just leading from behind. Rather, we should be defining what is important and should be driving the field from both an educational and research standpoint. \nHowever, the new era of big data poses a serious dilemma for the statistics community that needs to be addressed before real progress can be made, and that’s what brings me to Alvy Singer’s conundrum.\nThere’s a strong tradition in statistics of being the “outsiders” to whatever field we’re applying our methods to. In many cases, we are the outsiders to scientific investigation. Even if we are neck deep in collaborating with scientists and being involved in scientific work, we still maintain our ability to criticize and judge scientists because we are “outsiders” trained in a different set of (important) skills. In many ways, this is a Good Thing. The outsider status is important because it gives us the freedom to be “arbiters” and to ensure that scientists are doing the “right” things. It’s our job to keep people honest. However, being an arbiter by definition means that you are merely observing what is going on. You cannot be leading what is going on without losing your ability to arbitrate in an unbiased manner.\nBig data poses a challenge to this long-standing tradition because all of the sudden statistics and science are more intertwined then ever before and statistical methodology is absolutely critical to making inferences or gaining insight from data. Because now there are data in more places than ever before, the demand for statistics is in more places than ever before. We are discovering that we can either teach people to apply the statistical methods to their data, or we can just do it ourselves!\nThis development presents an enormous opportunity for statisticians to play a new leadership role in scientific investigations because we have the skills to extract information from the data that no one else has (at least for the moment). But now we have to choose between being “in the club” by leading the science or remaining outside the club to be unbiased arbiters. I think as an individual it’s very difficult to be both simply because there are only 24 hours in the day. It takes an enormous amount of time to learn the scientific background required to lead scientific investigations and this is piled on top of whatever statistical training you receive. \nHowever, I think as a field, we desperately need to promote both kinds of people, if only because we are the best people for the job. We need to expand the tent of statistics and include people who are using their statistical training to lead the new science. They may not be publishing papers in the Annals of Statistics or in JASA, but they are statisticians. If we do not move more in this direction, we risk missing out on one of the most exciting developments of our lifetime.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-20-pro-tips-for-grad-students-in-statistics-biostatistics/",
    "title": "Pro Tips for Grad Students in Statistics/Biostatistics (Part 2)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-20",
    "categories": [],
    "contents": "\nThis is the second in my series on pro tips for graduate students in statistics/biostatistics. For more tips, see part 1. \nMeet with seminar speakers. When you go on the job market face recognition is priceless. I met Scott Zeger at UW when I was a student. When I came for an interview I already knew him (and Ingo, and Rafa, and ….). An even better idea…ask a question during the seminar.\nBe a finisher. The key to getting a Ph.D. (other than passing your quals) is the ability to sit down and just power through and get it done. This means sometimes you will have to work late or on a weekend. The people who are the most successful in grad school are the people that just nd a way to get it done. If it was easy…anyone would do it.\nWork on problems you genuinely enjoy thinking about/are\npassionate about. A lot of statistics (and science) is long periods of concentrated effort with no guarantee of success at the end. To be a really good statistician requires a lot of patience and effort. It is a lot easier to work hard on something you like or feel strongly about.\n\nMore to come soon. \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-19-e-p-a-casts-new-soot-standard-as-easily-met/",
    "title": "E.P.A. Casts New Soot Standard as Easily Met",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-19",
    "categories": [],
    "contents": "\nE.P.A. Casts New Soot Standard as Easily Met\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-18-pro-tips-for-grad-students-in-statistics-biostatistics-2/",
    "title": "Pro Tips for Grad Students in Statistics/Biostatistics (Part 1)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-18",
    "categories": [],
    "contents": "\n\nI just finished teaching a Ph.D. level applied statistical methods course here at Hopkins. As part of the course, I gave one “pro-tip” a day; something I wish I had learned in graduate school that has helped me in becoming a practicing applied statistician. Here are the first three, more to come soon. \n\nA major component of being a researcher is knowing what’s going on in the research community. Set up an RSS feed with journal articles. Google Reader is a good one, but there are others. Here are some good applied stat journals: Biostatistics, Biometrics, Annals of Applied Statistics…\nReproducible research is a hot topic, in part because a couple of high-profile papers that were disastrously non-reproducible (see “Deriving chemosensitivity from cell lines: Forensic bioinformatics and reproducible research in high-throughput biology”). When you write code for statistical analysis try to make sure that: (a) It is neat and well-commented - liberal and specific comments are your friend. (b)That it can be run by someone other than you, to produce the same results that you report.\nIn data analysis - particularly for complex high-dimensional\ndata - it is frequently better to choose simple models for clearly defined parameters. With a lot of data, there is a strong temptation to go overboard with statistically complicated models; the danger of overfitting/ over-interpreting is extreme. The most reproducible results are often produced by sensible and statistically “simple” analyses (Note: being sensible and simple does not always lead to higher prole results).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-17-sunday-data-statistics-link-roundup-6-17/",
    "title": "Sunday data/statistics link roundup (6/17)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-17",
    "categories": [],
    "contents": "\nHappy Father’s Day!\nA really interesting read on randomized controlled trials (RCTs) and public policy. The examples in the boxes are fantastic. This seems to be one of the cases where the public policy folks are borrowing ideas from Biostatistics, which has been involved in randomized controlled trials for a long time. It’s a cool example of adapting good ideas in one discipline to the specific challenges of another. \nRoger points to this link in the NY Times about the “Consumer Genome”, which basically is a collection of information about your purchases and consumer history. On Twitter, Leonid K. asks: ‘Since when has “genome” becaome a generic term for “a bunch of information”?’. I completely understand the reaction against the “genome of x”, which is an over-used analogy. I actually think the analogy isn’t that unreasonable; like a genome, the information contained in your purchase/consumer history says something about you, but doesn’t tell the whole picture. I wonder how this information could be used for public health, since it is already being used for advertising….\nThis PeerJ journal looks like it has the potential to be good.  They even encourage open peer review, which has some benefits. Not sure if it is sustainable, see for example, this breakdown of the costs. I still think we can do better.  \nElon Musk is one of my favorite entrepreneurs. He tackles what I consider to be some of the most awe-inspiring and important problems around. This article about the Tesla S got me all fired up about how a person with vision can literally change the fuel we run on. Nothing to do with statistics, other than I think now is a similarly revolutionary time for our discipline. \nThere was some interesting discussion on Twitter of the usefulness of the Yelp dataset I posted for academic research. Not sure if this ever got resolved, but I think more and more as data sets from companies/startups become available, the terms of use for these data will be critical. \nI’m still working on Roger’s puzzle from earlier this week. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-15-statisticians-asa-and-big-data/",
    "title": "Statisticians, ASA, and Big Data",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-15",
    "categories": [],
    "contents": "\nToday I got my copy of Amstat News and eagerly opened it before I realized it was not the issue with the salary survey….\nBut the President’s Corner section had the following column on big data by ASA president Robert Rodriguez.\n\nBig Data is big news. It is the focus of stories in The New York Times and the subject of technology blogs, business forums, and economic studies. This column describes how statisticians can prepare for opportunities in Big Data and explains the distinctive value our profession can provide.\n\nHere’s a homework assignment for you all: Please read the column and explain what’s wrong with it. I’ll post the answer in a (near) future post.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-12-big-data-needs-may-create-thousands-of-tech-jobs/",
    "title": "Big Data Needs May Create Thousands Of Tech Jobs",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-12",
    "categories": [],
    "contents": "\nBig Data Needs May Create Thousands Of Tech Jobs\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-12-green-e-p-a-soot-rules-expected-this-week/",
    "title": "Green: E.P.A. Soot Rules Expected This Week",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-12",
    "categories": [],
    "contents": "\nGreen: E.P.A. Soot Rules Expected This Week\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-12-poison-gas-or-air-pollution/",
    "title": "Poison gas or...air pollution?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-12",
    "categories": [],
    "contents": "\nFrom our Beijing bureau, we have the following message from the U.S. embassy that was recently issued to U.S. citizens in China:\n\nThe Embassy has received reports from U.S. citizens living and traveling in Wuhan that the air quality in the city has been particularly poor since yesterday morning.  On June 11 at 16:20, the Wuhan Environmental Protection Administrative Bureau posted information about this on its website.  Below is a translation of that information:\n“Beginning on June 11, 2012 around 08:00 AM, the air quality inside Wuhan appeared to worsen, with low visibility and burning smells. According to city air data, starting at 07:00 AM this morning, the density of the respiratory particulate matter increased in the air downtown; it increased quickly after 08:00 AM.  The density at 14:00 approached 0.574mg/m3, a level that is deemed “serious” by national standards.  An analysis of the air indicates the pollution is caused from burning of plant material northeast of Wuhan.\n\nIt’s not immediately clear which pollutant they’re talking about, but it’s probably PM10 (particulate matter less than 10 microns in aerodynamic diameter). If so, that level is quite high—U.S. 24-hour average standards are at 0.15 mg/m3 (note that the reported level was an hourly level). \n\nOur investigation of downtown’s districts, and based on reports from all of Wuhan’s large industrial enterprises, have determined that that there has not been any explosion, sewage release, leakage of any poisoning gas, or any other type of urgent environmental accident from large industrial enterprises.  Nor is there burning of crops in the new city area.  News spread online of a chlorine leak from Qingshan or a boiler explosion at Wuhan Iron and Steel Plant are rumors.\n\nSo, this is not some terrible incident, it’s just the usual smell. Good to know.\n\nAccording to our investigation, the abnormal air quality in our city is mainly caused by the burning of the crops northeast of Wuhan towards Hubei province.  Similar air quality is occurring in Jiangsu, Henan and Anhui provinces, as well as in Xiaogan, Jingzhou, Jingmen and Xiantao, cities nearby Wuhan.\n\n\nThe weather forecast authority of the city has advised that recent weather conditions have not been good for the dispersion of pollutants.”\n\nThe embassy goes on to warn:\n\nU.S. citizens are reminded that air pollution is a significant problem in many cities and regions in China.  Health effects are likely to be more severe for sensitive populations, including children and older adults.  While the quality of air can differ greatly between cities or between urban and rural areas, U.S. citizens living in or traveling to China may wish to consult their doctor when living in or prior to traveling to areas with significant air pollution.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-11-chris-volinsky-knows-where-you-are/",
    "title": "Chris Volinsky knows where you are",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-11",
    "categories": [],
    "contents": "\nChris Volinsky knows where you are\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-11-getting-a-grant-or-a-startup/",
    "title": "Getting a grant...or a startup",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-11",
    "categories": [],
    "contents": "\nY Combinator is company that invests in startups and brings them to the San Francisco area to get them ready for prime time. One of the co-founders is Paul Graham, whose essays we’ve featured on this blog.\nThe Y Combinator web site itself is quite interesting and in particular, the section on how to apply to Y Combinator caught my eye. Now, I don’t know the first thing about starting a startup (nor do I have any current interest in doing so), but I do know a little bit about applying for NIH grants and it struck me that the advice for the startups seemed very useful for writing grants. It surprised me because I always thought that the process of “marketing” a startup to someone would be quite different from applying for a grant—-startups are supposed to be cool and innovative and futuristic while grants are more about doing the usual thing. Just shows you how much I know about the startup world.\nI thought I’d pluck out a few good parts from Graham’s long list of advice that I found useful. The full essay is definitely worth reading.\nHere’s one that struck me immediately:\n\nIf we get 1000 applications and have 10 days to read them, we have to read about 100 a day. That means a YC partner who reads your application will on average have already read 50 that day and have 50 more to go. Yours has to stand out. So you have to be exceptionally clear and concise. Whatever you have to say, give it to us right in the first sentence, in the simplest possible terms.\n\nIn that past, I always thought that grant reviewers had all the time in the world to read my grant and probably dedicated a week of their life to reading it. Hah! Having served on study sections now, I realize there’s precious little time to dedicate to the tall pile of grants that need to be read. Grants that are well written are a pleasure to read. Ones that are poorly written (or take forever to get to the point) just make me angry.\n\nIt’s a mistake to use marketing-speak to make your idea sound more exciting. We’re immune to marketing-speak; to us it’s just noise. So don’t begin…with something like\n\nWe are going to transform the relationship between individuals and information.\n\nThat sounds impressive, but it conveys nothing. It could be a description of any technology company. Are you going to build a search engine? Database software? A router? I have no idea.\nOne test of whether you’re explaining your idea effectively is to ask how close the reader is to reproducing it. After reading that sentence I’m no closer than I was before, so its content is effectively zero.\n\nI usually tell people if at any stage of writing a grant you have a choice between being more general and more specific, always be more specific. That way people can judge you based on the facts, not based on their imagination of the facts. This doesn’t always lead to success, of course, but it can remove an element of chance. If a reviewer has to fill in the details of your idea, who knows what they’ll think of?\n\nOne reason [company] founders resist giving matter-of-fact descriptions [of their company] is that they seem to constrain your potential. “But [my product] is so much more than a database with a wiki UI!” The problem is, the less constraining your description, the less you’re saying. So it’s better to err on the side of matter-of-factness.\n\nOf course, there are some applications that specifically ask you to “think big” and there the rules may be a bit different. But still, I think it’s better to avoid broad and sweeping generalities. These days, given the relatively tight page limits, you need to convey the maximum amount of information possible.\n\nOne good trick for describing a project concisely is to explain it as a variant of something the audience already knows. It’s like Wikipedia, but within an organization. It’s like an answering service, but for email. It’s eBay for jobs. This form of description is wonderfully efficient. Don’t worry that it will make your idea seem “derivative.” Some of the best ideas in history began by sticking together two existing ideas no one realized could be combined.\n\nNot sure this is so relevant to writing grants, but I thought was interesting. My instinct was to think that this would make your idea seem derivative also, but maybe not.\n\n…if we can see obstacles to your idea that you don’t seem to have considered, that’s a bad sign. This is your idea. You’ve had days, at least, to think about it, and we’ve only had a couple minutes. We shouldn’t be able to come up with objections you haven’t thought of.\nParadoxically, it is for this reason better to disclose all the flaws in your idea than to try to conceal them. If we think of a problem you don’t mention, we’ll assume it’s because you haven’t thought of it. \n\nThis is one definitely true—better to reveal limitations/weaknesses than to look like you haven’t thought of them. Because if a reviewer finds one, then it’s all they’ll talk about. Often times, a big problem is lack of space to fit this in, but if you can do it I think it’s always a good idea to include it.\nFinally,\n\nYou don’t have to sell us on you. We’ll sell ourselves, if we can just understand you. But every unnecessary word in your application subtracts from the effect of the necessary ones. So before submitting your application, print it out and take a red pen and cross out every word you don’t need. And in what’s left be as specific and as matter-of-fact as you can.\n\nI think there are quite a few differences between scientists reviewing grants and startup investors and we probably shouldn’t take the parallels too seriously. In particular, investors I think are going to be more optimistic because, as Graham says, “they get equity”. Scientists are trained to be skeptical and so will be looking at applications with a slightly different eye. \nHowever, I think the general advice to be specific and concise about what you’re doing is good. If anything, it may help you realize that you have no idea what you’re doing.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-10-sunday-data-statistics-link-roundup-6-10/",
    "title": "Sunday data/statistics link roundup (6/10)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-10",
    "categories": [],
    "contents": "\n Yelp put a data set online for people to play with, including reviews, star ratings, etc. This could be a really neat data set for a student project. The data they have made available focuses on the area around 30 universities. My alma mater is one of them. \nA sort of goofy talk about how to choose the optimal marriage partner when viewing the problem as an optimal stopping problem. The author suggests that you need to date around 196,132 partners to make sure you have made the optimal decision. Fortunately for the Simply Statistics authors, it took many fewer for us all to end up with our optimal matches. Via @fhuszar.\nAn interesting article on the recent Kaggle contest that sought to identify statistical algorithms that could accurately match human scoring of written essays. Several students in my advanced biostatistics course competed in this competition and did quite well. I understand the need for these kinds of algorithms, since it takes a huge amount of human labor to score these essays well. But it also makes me a bit sad since it still seems even the best algorithms will have a hard time scoring creativity. For example, this phrase from my favorite president, doesn’t use big words, but it sure is clever, “I think there is only one quality worse than hardness of heart and that is softness of head.”\nA really good article by friend of the blog, Steven, on the perils of gene patents. This part sums it up perfectly, “Genes are not inventions. This simple fact, which no serious scientist would dispute, should be enough to rule them out as the subject of patents.” Simply Statistics has weighed in on this issue a couple of times before. But I think in light of 23andMe’s recent Parkinson’s patent it bears repeating. Here is an awesome summary of the issue from Genomics Lawyer.\nA proposal for a really fast statistics journal I wrote about a month or two ago. Expect more on this topic from me this week. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-05-china-asks-embassies-to-stop-measuring-air-pollution/",
    "title": "China Asks Embassies to Stop Measuring Air Pollution",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-05",
    "categories": [],
    "contents": "\nChina Asks Embassies to Stop Measuring Air Pollution\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-04-how-big-data-gets-real/",
    "title": "How Big Data Gets Real",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-04",
    "categories": [],
    "contents": "\nHow Big Data Gets Real\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-06-01-interview-with-amanda-cox-graphics-editor-at-the-new/",
    "title": "Interview with Amanda Cox - Graphics Editor at the New York Times",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\nAmanda Cox \n\n\n\n\n\n\n\n\n\n\n\nAmanda Cox received her M.S. in statistics from the University of Washington in 2005. She then moved to the New York Times, where she is a graphics editor. She, and the graphics team at the New York Times, are responsible for many of the cool, informative, and interactive graphics produced by the Times. For example, this, this and this (the last one, Olympic Symphony, is one of my all time favorites). \n\n\n\n\n\nYou have a background in statistics, do you consider yourself a statistician? Do you consider what you do statistics?\n\n\n\n\n\nI don’t deal with uncertainty in a formal enough way to call what I do statistics, or myself a statistician. (My technical title is “graphics editor,” but no one knows what this means. On the good days, what we do is “journalism.”) Mark Hansen, a statistician at UCLA, has possibly changed my thinking on this a little bit though, by asking who I want to be the best at visualizing data, if not statisticians.\n\n\n\n\n\nHow did you end up at the NY Times?\n\nIn the middle of my first year of grad school (in statistics at the University of Washington), I started applying for random things. One of them was to be a summer intern in the graphics department at the Times.\nHow are the graphics and charts you develop different than producing graphs for a quantitative/scientific audience?\n\n\n\n\n“Feels like homework” is a really negative reaction to a graphic or a story here. In practice, that means a few things: we don’t necessarily assume our audience already cares about a topic. We try to get rid of jargon, which can be useful shorthand for technical audiences, but doesn’t belong in a newspaper. Most of our graphics can stand on their own, meaning you shouldn’t need to read any accompanying text to understand the basic point. Finally, we probably pay more attention to things like typography and design, which, done properly, are really about hierarchy and clarity, and not just about making things cute. \n\n\nHow do you use R to prototype graphics? \nI sketch in R, which mostly just means reading data, and trying on different forms or subsets or levels of aggregation. It’s nothing fancy: usually just points and lines and text from base graphics. For print, I will sometimes clean up a pdf of R output in Illustrator. You can see some of that in practice at chartsnthings.tumblr.com, which where one of my colleagues, Kevin Quealy, posts some of the department’s sketches. (Kevin and I are the only regular R users here, so the amount of R used on chartsnthings is not at all representative of NYT graphics as a whole.)\nDo you have any examples where the R version and the eventual final web version are nearly identical?\nReal interactivity changes things, so my use of R for web graphics is mostly just a proof-of-concept thing. (Sometimes I will also generate “poor-man’s interactivity,” which means hitting the pagedown key on a pdf of charts made in a for loop.) But here are a couple of proof-of-concept sketches, where the initial R output doesn’t look so different from the final web version.\nThe Jobless Rate for People Like You\n\nHow Different Groups Spend Their Day\n\nYou consistently produce arresting and informative graphics about a range of topics. How do you decide on which topics to tackle?\nNews value and interestingness are probably the two most important criteria for deciding what to work on. In an ideal world, you get both, but sometimes, one is enough (or the best you can do).\nAre your project choices motivated by availability of data?\nSure. The availability of data also affects the scope of many projects. For example, the guys who work on our live election results will probably map them by county, even though precinct-level results are so much better. But precinct-level data isn’t generally available in real time.\nWhat is the typical turn-around time from idea to completed project?\nThe department is most proud of some of its one-day, breaking news work, but very little of that is what I would think of as data-heavy.  The real answer to “how long does it take?” is “how long do we have?” Projects always find ways to expand to fill the available space, which often ranges from a couple of days to a couple of weeks.\n\nDo you have any general principles for how you make complicated data understandable to the general public?\n\n\n\nI’m a big believer in learning by example. If you annotate three points in a scatterplot, I’m probably good, even if I’m not super comfortable reading scatterplots. I also think the words in a graphic should highlight the relevant pattern, or an expert’s interpretation, and not merely say “Here is some data.” The annotation layer is critical, even in a newspaper (where the data is not usually super complicated).\nWhat do you consider to be the most informative graphical elements or interactive features that you consistently use?\nI like sliders, because there’s something about them that suggests story (beginning-middle-end), even if the thing you’re changing isn’t time. Using movement in a way that means something, like this or this, is still also fun, because it takes advantage of one of the ways the web is different from print.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-31-writing-software-for-someone-else/",
    "title": "Writing software for someone else",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-31",
    "categories": [],
    "contents": "\nWriting software for someone else\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-30-why-no-one-reads-the-statistics-literature-anymore/",
    "title": "Why \"no one reads the statistics literature anymore\"",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-30",
    "categories": [],
    "contents": "\nSpurred by Rafa’s post on evaluating statisticians working in genomics, there’s an interesting discussion going on at the Scientists for Reproducible Research group on statistics journals. Evan Johnson kicks it off:\n\n…our statistics journals have little impact on how genomic data are analyzed. My group rarely looks to publish in statistics journals anymore because even IF we can get it published quickly, NO ONE will read it, so the only things we send  there anymore are things that we don’t care if anyone ever uses.\n\nEvan continues:\n\nIt’s crazy to me that all of our statistical journals are barely even noticed by bioinformaticians, computational biologists, and by people in genomics. Even worse, very few non-statisticians in genomics ever try to publish in our journals. Ultimately, this represents a major failure in the statistical discipline to be collectively influential on how genomic data are analyzed. \n\nI may agree with the first point but I’m not sure I agree with second. Regarding the first, I think Karl put it best in that really the problem is that “the bulk of the people who might benefit from my method do not read the statistical literature”. For the second point, I think the issue is that the way science works is changing. Here’s my cartoon of how science worked in the “old days”, say, pre-computer era:\n\nThe idea here is that scientists worked with statisticians (they may have been one and the same) to publish stat papers and scientific papers. If Scientist A saw a paper in a domain journal written by Scientist B using a method developed by Statistician C, how could Scientist A apply that method? He had to talk to Statistician D, who would read that statistics literature and find Statistician C’s paper to learn about the method. The point is that there is no direct link from Scientist A to Statistician C except through statistics journals. Therefore, it was critical for Statistician C to publish in the stat journals to ensure that there would be an impact on scientists.\nMy cartoon of the “new way” of doing things is below.\n\nNow, if Scientist wants to use a method developed by Statistician C (and used by Scientist B), he simply finds the software developed by Statistician C and applies it to his data. Here, there is a direct connection between A and C through software. If Statistician C wants his method to have an impact on scientists, there are two options: publish in stat journals and hope that the method filters through other statisticians, or publish in domain journals with software so that other scientists may apply the method directly. It seems the latter approach is more popular in some areas.\nPeter Diggle makes an important point about generalized linear models and the seminal book written by McCullagh and Nelder:\n\nthe book [by McCullagh and Nelder] would have been read by many fewer people if Nelder and colleague had not embedded the idea in software that (for the time) was innovative in being interactive rather than batch-oriented.\n\nFor better or for worse (and probably very often for worse), the software allowed many many people access to the methods.\nThe supposed attraction of publishing a statistical method in a statistics journal like JASA or JRSS-B is that the methods are published in a more abstract manner (usually using mathematical symbols) in the hopes that the methods will be applicable to a wide array of problems, not just the problem for which it was developed. Of course, the flip side of this argument is, as Karl says, again eloquently, “if you don’t get down to the specifics of a particular data set, then you haven’t really solved any problem”.\nI think abstraction is important and we need to continue publishing those kinds of ideas. However, I think there is one key point that the statistics community has had difficulty grasping, which is that software represents an important form of abstraction, if not the most important form. Anyone who has written software knows that there are many approaches to implementing your method in software and various levels of abstraction one can use. The variety of problems to which the software can be applied depends on how general the interface to your software is. This is why I always encourage people to write R packages because it often forces them to think a bit more abstractly about who might be using the software.\nWhither the statistics journals? It’s hard to say. Having them publish more software probably won’t help as the audience remains the same. I’m a bit stumped here but I look forward to continued discussion!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-29-view-my-statistics-for-genomics-lectures-on-youtube-and/",
    "title": "View my Statistics for Genomics lectures on Youtube and ask questions on facebook/twitter",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-29",
    "categories": [],
    "contents": "\nThis year I recorded my lectures during my Statistics for Genomics course. Slowly but surely I am putting all the videos on Youtube. Links will eventually be here (all slides and the first lecture is already up).  As new lectures become available I will post updates on rafalab’s facebook page and twitter feed where I will answer questions posted as comments (time permitting). Guest lecturers include Jeff Leek, Ben Langmead, Kasper Hansen and Hongkai Ji.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:56:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-28-schlep-blindness-in-statistics/",
    "title": "Schlep blindness in statistics",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-28",
    "categories": [],
    "contents": "\nThis is yet another outstanding post by Paul Graham, this time on “Schlep Blindness”. He talks about how there are great startup ideas that no one considers because they are too much of a “schlep” (a tedious unpleasant task). He talks about how most founders of startups want to put up a clever bit of code they wrote and just watch the money flow in. But of course it doesn’t work like that, you need to advertise, interact with customers, raise money, go out and promote your work, fix bugs at 3am, etc. \nIn academia there is a similar tendency to avoid projects that involve a big schlep. For example, it is relatively straightforward to develop a mathematical model, work out the parameter estimates, and write a paper. But it is a big schlep to then write fast code that implements that method, debug the code, dummy proof the code, fix bugs submitted by users, etc. Rafa’s post, Hadley’s interview, and the discussion Rafa linked to all allude to this issue. Particularly the fact that the schlep, the long slow slog of going through a new data type or writing a piece of usable software is somewhat undervalued. \nI think part of the problem is our academic culture and heritage, which has traditionally put a very high premium on being clever and a relatively low premium on being willing to go through the schlep. As applied statistics touches more areas and the number of users of statistical software and ideas grows, the schlep becomes just as important as the clever idea. If you aren’t willing to put in the time to code your methods up and make them accessible to other investigators, then who will be? \nTo bring this back to the discussion inspired by Rafa’s post, I wonder if applied statistics journals could increase their impact, encourage more readership from scientific folks, and support a broader range of applied statisticians if there was a re-weighting of the importance of cleverness and schlep? As Paul points out: \n\n In addition to their intrinsic value, they’re like undervalued stocks in the sense that there’s less demand for them among founders. If you pick an ambitious idea, you’ll have less competition, because everyone else will have been frightened off by the challenges involved.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-27-sunday-data-statistics-link-roundup-5-27/",
    "title": "Sunday data/statistics link roundup (5/27)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-27",
    "categories": [],
    "contents": "\nAmanda Cox on the process they went through to come up with this graphic about the Facebook IPO. So cool to see how R is used in the development process. A favorite quote of mine, “But rather than bringing clarity, it just sort of looked chaotic, even to the seasoned chart freaks of 620 8th Avenue.” One of the more interesting things about posts like this is you get to see how statistics versus a deadline works. This is typically the role of the analyst, since they come in late and there is usually a deadline looming…\nAn interview with Steve Blank about Silicon valley and how venture capitalists (VC’s) are focused on social technologies since they can make a profit quickly. A depressing/fascinating quote from this one is, “If I have a choice of investing in a blockbuster cancer drug that will pay me nothing for ten years,  at best, whereas social media will go big in two years, what do you think I’m going to pick? If you’re a VC firm, you’re tossing out your life science division.” He also goes on to say thank goodness for the NIH, NSF, and Google who are funding interesting “real science” problems. This probably deserves its own post later in the week, the difference between analyzing data because it will make money and analyzing data to solve a hard science problem. The latter usually takes way more patience and the data take much longer to collect. \nAn interesting post on how Obama’s analytics department ran an A/B test which improved the number of people who signed up for his mailing list. I don’t necessarily agree with their claim that they helped raise $60 million, there may be some confounding factors that mean that the individuals who sign up with the best combination of image/button don’t necessarily donate as much. But still, an interesting look into why Obama needs statisticians. \nA cute statistics cartoon from @kristin_linn  via Chris V. Yes, we are now shamelessly reposting cute cartoons for retweets :-). \nRafa’s post inspired some interesting conversation both on our blog and on some statistics mailing lists. It seems to me that everyone is making an effort to understand the increasingly diverse field of statistics, but we still have a ways to go. I’m particularly interested in discussion on how we evaluate the contribution/effort behind making good and usable academic software. I think the strength of the Bioconductor community and the rise of Github among academics are a good start.  For example, it is really useful that Bioconductor now tracks the number of package downloads. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-24-how-do-we-evaluate-statisticians-working-in-genomics/",
    "title": "\"How do we evaluate statisticians working in genomics? Why don't they publish in stats journals?\" Here is my answer\n",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-24",
    "categories": [],
    "contents": "\n\nDuring the past couple of years I have been asked these questions by several department chairs and other senior statisticians interested in hiring or promoting faculty working in genomics. The main difficulty stems from the fact that we (statisticians working in genomics) publish in journals outside the mainstream statistical journals. This can be a problem during evaluation because a quick-and-dirty approach to evaluating an academic statistician is to count papers in the Annals of Statistics, JASA, JRSS and Biometrics. The evaluators feel safe counting these papers because they trust the fellow-statistician editors of these journals. However, statisticians working in genomics tend to publish in journals like Nature Genetics, Genome Research, PNAS, Nature Methods, Nucleic Acids Research, Genome Biology, and Bioinformatics. In general, these journals do not recruit statistical referees and a considerable number of papers with questionable statistics do get published in them. However, when the paper’s main topic is a statistical method or if it heavily relies on statistical methods, statistical referees are used. So, if the statistician is the corresponding or last author and it’s a stats paper, it is OK to assume the statistics are fine and you should go ahead and be impressed by the impact factor of the journal… it’s not east getting statistics papers in these journals. \n\n\nBut we really should not be counting papers blindly. Instead we should be reading at least some of them. But here again the evaluators get stuck as we tend to publish papers with application/technology specific jargon and show-off by presenting results that are of interest to our potential users (biologists) and not necessarily to our fellow statisticians. Here all I can recommend is that you seek help. There are now a handful of us that are full professors and most of us are more than willing to help out with, for example, promotion letters.\n\n\nSo why don’t we publish in statistical journals? The fear of getting scooped due to the slow turnaround of stats journals is only one reason. New technologies that quickly became widely used (microarrays in 2000 and nextgen sequencing today) created a need for data analysis methods among large groups of biologists. Journals with large readerships and high impact factors, typically not interested in straight statistical methodology work, suddenly became amenable to publishing our papers, especially if they solved a data analytic problem faced by many biologists. The possibility of publishing in widely read journals is certainly seductive. \n\n\nWhile in several other fields, data analysis methodology development is restricted to the statistics discipline, in genomics we compete with other quantitative scientists capable of developing useful solutions: computer scientists, physicists, and engineers were also seduced by the possibility of gaining notoriety with publications in high impact journals. Thus, in genomics, the competition for funding, citation and publication in the top scientific journals is fierce. \n\n\nThen there is funding. Note that while most biostatistics methodology NIH proposals go to the Biostatistical Methods and Research Design (BMRD) study section, many of the genomics related grants get sent to other sections such as the Genomics Computational Biology and Technology (GCAT) and Biodata Management and Anlayis (BDMA) study sections. BDMA and GCAT are much more impressed by Nature Genetics and Genome Research than JASA and Biometrics. They also look for citations and software downloads. \n\n\nTo be considered successful by our peers in genomics, those who referee our papers and review our grant applications, our statistical methods need to be delivered as software and garner a user base. Publications in statistical journals, especially those not appearing in PubMed, are not rewarded. This lack of incentive combined with how time consuming it is to produce and maintain usable software, has led many statisticians working in genomics to focus solely on the development of practical methods rather than generalizable mathematical theory. As a result, statisticians working in genomics do not publish much in the traditional statistical journals. You should not hold this against them, especially if they are developers and maintainers of widely used software.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-20-sunday-data-statistics-link-roundup-5-20/",
    "title": "Sunday data/statistics link roundup (5/20)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-20",
    "categories": [],
    "contents": "\n\nIt’s grant season around here so I’ll be brief:\n\nI love this article in the WSJ about the crisis at JP Morgan. The key point it highlights is that looking only at the high-level analysis and summaries can be misleading, you have to look at the raw data to see the potential problems. As data become more complex, I think its critical we stay in touch with the raw data, regardless of discipline. At least if I miss something in the raw data I don’t lose a couple billion. Spotted by Leonid K. \nOn the other hand, this article in the Times drives me a little bonkers. It makes it sound like there is one mathematical model that will solve the obesity epidemic. Lines like this are ridiculous: “Because to do this experimentally would take years. You could find out much more quickly if you did the math.” The obesity epidemic is due to a complex interplay of cultural, sociological, economic, and policy factors. The idea you could “figure it out” with a set of simple equations is laughable. If you check out their model this is clearly not the answer to the obesity epidemic. Just another example of why statistics is not math. If you don’t want to hopelessly oversimplify the problem, you need careful data collection, analysis, and interpretation. For a broader look at this problem, check out this article on Science vs. PR. Via Andrew J. \nSome cool applications of the raster package in R. This kind of thing is fun for student projects because analyzing images leads to results that are easy to interpret/visualize.\nCheck out John C.’s really fascinating post on determining when a white-collar worker is great. Inspired by Roger’s post on knowing when someone is good at data analysis. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-16-health-by-numbers-a-statisticians-challenge/",
    "title": "Health by numbers: A statistician's challenge",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-16",
    "categories": [],
    "contents": "\nHealth by numbers: A statistician’s challenge\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-16-the-west-wing-was-always-a-favorite-show-of-mine/",
    "title": "The Numbers Don't Lie",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-16",
    "categories": [],
    "contents": "\n[youtube http://www.youtube.com/watch?v=t7FJFuuvxpI?wmode=transparent&autohide=1&egm=0&hd=1&iv\\_load\\_policy=3&modestbranding=1&rel=0&showinfo=0&showsearch=0&w=500&h=375]\nThe West Wing was always a favorite show of mine (at least, seasons 1-4, the Sorkin years) and I think this is a great scene which talks about the difference between evidence and interpretation. The topic is a 5-day waiting period for gun purchases and they’ve just received a poll in a few specific congressional districts showing weak support for this proposed policy.\n\n(Source: http://www.youtube.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-12T08:06:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-14-computational-biologist-blogger-saves-computer-science/",
    "title": "Computational biologist blogger saves computer science department",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-14",
    "categories": [],
    "contents": "\nPeople who read the news should be aware by now that we are in the midst of a big data era. The New York Times, for example, has been writing about this frequently. One of their most recent articles describes how UC Berkeley is getting $60 million dollars for a new computer science center. Meanwhile, at University of Florida the administration seems to be oblivious to all this and about a month ago announced it was dropping its computer science department to save $. Blogger Steven Salzberg, a computational biologists known for his work in genomics, wrote a post titled “University of Florida eliminates Computer Science Department. At least they still have football” ridiculing UF for their decisions. Here are my favorite quotes:\n\n in the midst of a technology revolution, with a shortage of engineers and computer scientists, UF decides to cut computer science completely? \n\n\nComputer scientist Carl de Boor, a member of the National Academy of Sciences and winner of the 2003 National Medal of Science, asked the UF president “What were you thinking?”\n\nWell, his post went viral and days later UF reversed it’s decision! So my point is this: statistics departments, be nice to bloggers that work in genomics… one of them might save your butt some day.\nDisclaimer: Steven Salzberg has a joint appointment in my department and we have joint lab meetings.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-14-facebook-needs-to-turn-data-into-investor-gold/",
    "title": "Facebook Needs to Turn Data Into Investor Gold",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-14",
    "categories": [],
    "contents": "\nFacebook Needs to Turn Data Into Investor Gold\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-13-sunday-data-statistics-link-roundup-5-13/",
    "title": "Sunday data/statistics link roundup (5/13)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-13",
    "categories": [],
    "contents": "\nPatenting statistical sampling? I’m pretty sure the Supreme Court who threw out the Mayo Patent wouldn’t have much trouble tossing this patent either. The properties of sampling are a “law of nature” right? via Leonid K.\nThis video has me all fired up, its called 23 1/2 hours and talks about how the best preventative health measure is getting 30 minutes of exercise - just walking - every day. He shows how in some cases this beats doing much more high-tech interventions. My favorite part of this video is how he uses a ton of statistical/epidemiological terms like “effect sizes”, “meta-analysis”, “longitudinal study”, “attributable fractions”, but makes them understandable to a broad audience. This is a great example of “statistics for good”.\nA very nice collection of 2-minute tutorials in R. This is a great way to teach the concepts, most of which don’t need more than 2 minutes, and it covers a lot of ground. One thing that drives me crazy is when I go into Rafa’s office with a hairy computational problem and he says, “Oh you didn’t know about function x?”. Of course this only happens after I’ve wasted an hour re-inventing the wheel. If more people put up 2 minute tutorials on all the cool tricks they know, the better we’d all be.\nA plot using ggplot2, developed by this week’s interviewee Hadley Wickham appears in the Atlantic! Via David S.\nI’m refusing to buy into Apple’s hegemony, so I’m still running OS 10.5. I’m having trouble getting github up and running. Anyone have this same problem/know a solution? I know, I know, I’m way behind the times on this…\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-11-ha/",
    "title": "Interview with Hadley Wickham - Developer of ggplot2",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-11",
    "categories": [],
    "contents": "\n\nHadley Wickham\n\n\n\n\n\n\n\n\n\n\n\nHadley Wickham is the Dobelman Family Junior Chair of Statistics at Rice University. Prior to moving to Rice, he completed his Ph.D. in Statistics from Iowa State University. He is the developer of the wildly popular ggplot2 software for data visualization and a contributor to the Ggobi project. He has developed a number of really useful R packages touching everything from data processing, to data modeling, to visualization. \n\n\n\n\n\nWhich term applies to you: data scientist, statistician, computerscientist, or something else?\n\n\nI’m an assistant professor of statistics, so I at least partlyassociate with statistics :).  But the idea of data science reallyresonates with me: I like the combination of tools from statistics andcomputer science, data analysis and hacking, with the core goal ofdeveloping a better understanding of data. Sometimes it seems like notmuch statistics research is actually about gaining insight into data.\n\nYou have created/maintain several widely used R packages. Can youdescribe the unique challenges to writing and maintaining packagesabove and beyond developing the methods themselves?\n\n\nI think there are two main challenges: turning ideas into code, and\ndocumentation and community building.\nCompared to other languages, the software development infrastructure\nin R is weak, which sometimes makes it harder than necessary to turn\nmy ideas into code. Additionally, I get less and less time to do\nsoftware development, so I can’t afford to waste time recreating old\nbugs, or releasing packages that don’t work. Recently, I’ve been\ninvesting time in helping build better dev infrastructure; better\ntools for documentation [roxygen2], unit testing [testthat], package development [devtools], and creating package website [staticdocs]. Generally, I’ve\nfound unit tests to be a worthwhile investment: they ensure you never\naccidentally recreate an old bug, and give you more confidence when\nradically changing the implementation of a function.\nDocumenting code is hard work, and it’s certainly something I haven’t\nmastered. But documentation is absolutely crucial if you want people\nto use your work. I find the main challenge is putting yourself in the\nmind of the new user: what do they need to know to use the package\neffectively. This is really hard to do as a package author because\nyou’ve internalised both the motivating problem and many of the common\nsolutions.\nConnected to documentation is building up a community around your\nwork. This is important to get feedback on your package, and can be\nhelpful for reducing the support burden. One of the things I’m most\nproud of about ggplot2 is something that I’m barely responsible for:\nthe ggplot2 mailing list. There are now ggplot2 experts who answer far\nmore questions on the list than I do. I’ve also found github to be\ngreat: there’s an increasing community of users proficient in both R\nand git who produce pull requests that fix bugs and add new features.\nThe flip side of building a community is that as your work becomes\nmore popular you need to be more careful when releasing new versions.\nThe last major release of ggplot2 (0.9.0) broke over 40 (!!) CRAN\npackages, and forced me to rethink my release process. Now I advertise\nreleases a month in advance, and run `R CMD check` on all downstream\ndependencies (`devtools::revdep_check` in the development version), so\nI can pick up potential problems and give other maintainers time to\nfix any issues.\n\nDo you feel that the academic culture has caught up with and supportsnon-traditional academic contributions (e.g. R packages instead ofpapers)?\n\n\nIt’s hard to tell. I think it’s getting better, but it’s still hard toget recognition that software development is an intellectual activityin the same way that developing a new mathematical theorem is. I tryto hedge my bets by publishing papers to accompany my major packages:I’ve also found the peer-review process very useful for improving thequality of my software. Reviewers from both the R journal and theJournal of Statistical Software have provided excellent suggestionsfor enhancements to my code.\n\nYou have given presentations at several start-up and tech companies.Do the corporate users of your software have different interests thanthe academic users?\n\n\nBy and large, no. Everyone, regardless of domain, is struggling tounderstand ever larger datasets. Across both industry and academia,practitioners are worried about reproducible research and thinkingabout how to apply the principles of software engineering to dataanalysis.\n\nYou gave one of my favorite presentations called Tidy Data/Tidy Toolsat the NYC Open Statistical Computing Meetup. What are the keyelements of tidy data that all applied statisticians should know?\n\n\nThanks! Basically, make sure you store your data in a consistent\nformat, and pick (or develop) tools that work with that data format.\nThe more time you spend munging data in the middle of an analysis, the\nless time you have to discover interesting things in your data. I’ve\ntried to develop a consistent philosophy of data that means when you\nuse my packages (particularly plyr and ggplot2), you can focus on the\ndata analysis, not on the details of the data format. The principles\nof tidy data that I adhere to are that every column should be a\nvariable, every row an observation, and different types of data should\nlive in different data frames. (If you’re familiar with database\nnormalisation this should sound pretty familiar!). I expound these\nprinciples in depth in my in-progress [paper on thetopic]. \n\nHow do you decide what project to work on next? Is your work inspiredby a particular application or more general problems you are trying totackle?\n\n\nVery broadly, I’m interested in the whole process of data analysis:\nthe process that takes raw data and converts it into understanding,\nknowledge and insight. I’ve identified three families of tools\n(manipulation, modelling and visualisation) that are used in every\ndata analysis, and I’m interested both in developing better individual\ntools, but also smoothing the transition between them. In every good\ndata analysis, you must iterate multiple times between manipulation,\nmodelling and visualisation, and anything you can do to make that\niteration faster yields qualitative improvements to the final analysis\n(that was one of the driving reasons I’ve been working on tidy data).\nAnother factor that motivates a lot of my work is teaching. I hate\nhaving to teach a topic that’s just a collection of special cases,\nwith no underlying theme or theory. That drive lead to [stringr] (for\nstring manipulation) and [lubridate] (with Garrett Grolemund for working\nwith dates). I recently released the [httr] package which aims to do a similar thing for http requests - I think this is particularly important as more and more data starts living on the web and must be accessed through an API.\n\nWhat do you see as the biggest open challenges in data visualizationright now? Do you see interactive graphics becoming more commonplace?\n\n\nI think one of the biggest challenges for data visualisation is just\ncommunicating what we know about good graphics. The first article\ndecrying 3d bar charts was published in 1951! Many plots still use\nrainbow scales or red-green colour contrasts, even though we’ve known\nfor decades that those are bad. How can we ensure that people\nproducing graphics know enough to do a good job, without making them\nread hundreds of papers? It’s a really hard problem.\nAnother big challenge is balancing the tension between exploration and\npresentation. For explotary graphics, you want to spend five seconds\n(or less) to create a plot that helps you understand the data, while you might spend\nfive hours on a plot that’s persuasive to an audience who\nisn’t as intimately familiar with the data as you. To date, we have\ngreat interactive graphics solutions at either end of the spectrum\n(e.g. ggobi/iplots/manet vs d3) but not much that transitions from one\nend of the spectrum to the other. This summer I’ll be spending some\ntime thinking about what ggplot2 + [d3], might\nequal, and how we can design something like an interactive grammar of\ngraphics that lets you explore data in R, while making it easy to\npublish interaction presentation graphics on the web.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-10-what-are-the-products-of-data-analysis/",
    "title": "What are the products of data analysis?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-10",
    "categories": [],
    "contents": "\nThanks to everyone for the feedback on my post on knowing when someone is good at data analysis. A couple people suggested I take a look here for a few people who have proven they’re good at data analysis. I think that’s a great idea and a good place to start.\nBut I also think that while demonstrating an ability to build good prediction models is impressive and definitely shows an understanding of the data, not all important problems can be easily posed as prediction problems. Most of my work does not involve prediction at all and the problems I face (i.e., estimating very small effects in the presence of large unmeasured confounding factors) would be difficult to formulate as a prediction challenge (at least, I can’t think of an easy way). In fact, part of my and my colleagues’ research involves showing how statistical methods designed for prediction problems can fail miserably when applied to other non-prediction settings.\nThe general question I have is what is a useful product that you can produce from a data analysis that demonstrates the quality of that analysis? So, a very small mean squared error from a prediction model would be one product (especially if it were smaller than everyone else’s). Maybe a cool graph with a story behind it? \nIf I were hiring a musician for an orchestra, I wouldn’t have to meet that person to have strong evidence that he/she were good. I could just listen to some recordings of that person playing and that would be a pretty good predictor of how that person would perform in the orchestra. In fact, some major orchestras do completely blind auditions so that although the person is present in the room, all you hear is the sound of the playing.\nWhat seems to be true with music at least, is that even though the final performance doesn’t specifically reveal the important decisions that were made along the way to craft the interpretation of the music, somehow one is still able to appreciate the fact that all those decisions were made and they benefitted the performance. To me, it seems unlikely to arrive at a sublime performance either by chance or by some route that didn’t involve talent and hard work. Maybe it could happen once, but to produce a great performance over and over requires more than just luck.\nWhat products could you send to someone to convince them you were good at data analysis? I raise this question primarily because when I look around at the products that I make (research papers, software, books, blogs), even if they are very good, I don’t think they necessarily convey any useful information about my ability to analyze data.\nWhat’s the data analysis equivalent of a musician’s performance?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-09-dealbook-glaxo-to-make-hostile-bid-for-human-genome/",
    "title": "DealBook: Glaxo to Make Hostile Bid for Human Genome Sciences",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-09",
    "categories": [],
    "contents": "\nDealBook: Glaxo to Make Hostile Bid for Human Genome Sciences\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-08-data-analysis-competition-combines-statistics-with/",
    "title": "Data analysis competition combines statistics with speed",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-08",
    "categories": [],
    "contents": "\nData analysis competition combines statistics with speed\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-07-how-do-you-know-if-someone-is-great-at-data-analysis/",
    "title": "How do you know if someone is great at data analysis?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-07",
    "categories": [],
    "contents": "\nConsider this exercise. Come up with a list of the top 5 people that you think are really good at data analysis.\nThere’s one catch: They have to be people that you’ve never met nor have had any sort of personal interaction with (e.g. email, chat, etc.). So basically people who have written papers/books you’ve read or have given talks you’ve seen or that you know through other publicly available information. Who comes to mind? It’s okay to include people who are no longer living.\nThe other day I was thinking about the people who I think are really good at data analysis and it occurred to me that they were all people I knew. So I started thinking about people that I don’t know (and there are many) but are equally good at data analysis. This turned out to be much harder than I thought. And I’m sure it’s not because they don’t exist, it’s just because I think good data analysis chops are hard to evaluate from afar using the standard methods by which we evaluate people.\nI think there are a few reasons. First, people who are great at data analysis are likely not publishing papers or being productive in a manner that I, an outsider, would be able to observe. If they’re working at a pharmaceutical company working on a new drug or at some fancy new startup company, there’s no way I’m ever going to know about it unless I’m directly involved.\nAnother reason is that even for people who are well-known scientists or statisticians, the products they produce don’t really highlight the difficulties overcome in data analysis. For example, many good papers in the statistics literature will describe a new method with brief reference to the data that inspired the method’s development. In those cases, the data analysis usually appears obvious, as most things do after they’ve been done. Furthermore, papers usually exclude all the painful details about merging, cleaning, and inspecting the data as well as all the other things you tried that didn’t work. Papers in the substantive literature have a similar problem, which is that they focus on a scientific problem of interest and the analysis of the data is secondary.\nAs skills in data analysis become more important, it seems odd to me that we don’t have a great way to evaluate a person’s ability to do it as we do in other areas.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-06-illumina-stays-independent-for-now/",
    "title": "Illumina stays independent, for now",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-06",
    "categories": [],
    "contents": "\nIllumina stays independent, for now\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-05-ucla-data-fest-2012/",
    "title": "UCLA Data Fest 2012",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-05",
    "categories": [],
    "contents": "\nThe very very cool UCLA Data Fest is going on as we speak. This is a statistical analysis marathon where teams of undergrads work through the night (and day) to address an important problem through data analysis. Last year they looked at crime data from the Los Angeles Police Department. I’m looking forward to seeing how this year goes.\nGreat work by Rob Gould and the Department of Statistics there.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-04-hammer-on-the-importance-of-statistics-or-as-i/",
    "title": "Untitled",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-04",
    "categories": [],
    "contents": "\n[youtube http://www.youtube.com/watch?v=k6aBITJuSQA?wmode=transparent&autohide=1&egm=0&hd=1&iv\\_load\\_policy=3&modestbranding=1&rel=0&showinfo=0&showsearch=0&w=500&h=375]\nHammer on the importance of statistics (or, as I used to know him, MC Hammer). The overlay of the video for “Can’t Touch This” really helps me understand what he’s talking about. (Thanks to Chris V. for the link.)\n\n(Source: http://www.youtube.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-04-new-national-academy-of-sciences-members/",
    "title": "New National Academy of Sciences Members",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-04",
    "categories": [],
    "contents": "\nThe National Academy of Sciences elected new members a few days ago. Among them are statisticians Robert Tibshirani and sociologist Stephen Raudenbush. Obviously well-deserved!\n(Thanks to Karl Broman.)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-02-ges-billion-dollar-bet-on-big-data/",
    "title": "GE's Billion-Dollar Bet on Big Data",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-02",
    "categories": [],
    "contents": "\nGE’s Billion-Dollar Bet on Big Data\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-02-just-like-regular-communism-dongle-communism-has/",
    "title": "Just like regular communism, dongle communism has failed",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-02",
    "categories": [],
    "contents": "\nBad news comrades. Dongle communism in under attack. Check out how this poor dongle has been subjugated. This is in our lab meeting room. To add insult to injury, this happened on May 1st! \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-05-01-sample-mix-ups-in-datasets-from-large-studies-are-more/",
    "title": "Sample mix-ups in datasets from large studies are more common than you think",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-05-01",
    "categories": [],
    "contents": "\nIf you have analyzed enough high throughput data you have seen it before: a male sample that is really a female, a liver that is a kidney, etc… As the datasets I analyze get bigger I see more and more sample mix-ups. When I find a couple of  samples for which sex is incorrectly annotated (one can easily see this from examining data from X and Y chromosomes) I can’t help but wonder if there are more that are undetectable (e.g. swapping samples of same sex). Datasets that include two types of measurements, for example genotypes and gene expression, make it possible to detect sample swaps more generally. I recently attended a talk by Karl Broman on this topic (one of best talks I’ve seen.. check out the slides here). Karl reports an example in which it looks as if whoever was pipetting skipped a sample and kept on going, introducing an off-by-one error for over 50 samples. As I sat through the talk, I wondered how many of the large GWAS studies have mix-ups like this?\nA recent paper (gated) published by Lude Franke and colleagues describes MixupMapper: a method for detecting and correcting mix-ups. They examined several public datasets and discovered mix-ups in all of them. The worst performing study, published in PLoS Genetics, was reported to have 23% of the samples swapped. I was surprised that the MixupMapper paper was not published in a higher impact journal.  Turns out PLoS Genetics rejected the paper. I think this was a big mistake on their part: the paper is clear and well written, reports a problem with a PLoS Genetics papers, and describes a solution to a problem that should have us all quite worried. I think it’s important that everybody learn about this problem so I was happy to see that, eight months later, Nature Genetics published a paper reporting mix-ups (gated)… but they didn’t cite the MixupMapper paper! Sorry Lude, welcome to the reverse scooped club. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-30-a-disappointing-response-from-naturemagazine-about/",
    "title": "A disappointing response from @NatureMagazine about folks with statistical skills",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-30",
    "categories": [],
    "contents": "\nLast week I linked to an ad for a Data Editor position at Nature Magazine. I was super excited that Nature was recognizing data as an important growth area. But the ad doesn’t mention anything about statistical analysis skills; it focuses exclusively on data management expertise. As I pointed out in the earlier post, managing data is only half the equation - figuring out what to do with the data is the other half. The second half requires knowledge of statistics.\nThe folks over at Nature responded to our post on Twitter:\n\n it’s unrealistic to think this editor (or anyone) could do what you suggest. Curation & accessibility are key. ^ng\n\nI disagree with this statement for the following reasons:\nIs it really unrealistic to think someone could have data management and statistical expertise? Pick your favorite data scientist and you would have someone with those skills. Most students coming out of computer science, computational biology, bioinformatics, or statistical genomics programs would have a blend of those two skills in some proportion. \nBut maybe the problem is this:\n\nApplicants must have a PhD in the biological sciences\n\nIt is possible that there are few PhDs in the biological sciences who know both statistics and data management (although that is probably changing). But most computational biologists have a pretty good knowledge of biology and a very good knowledge of data - both managing and analyzing. If you are hiring a data editor, this might be the target audience. I’d replace PhD in the biological science in the ad with, knowledge of biology,statistics, data analysis, and data visualization. There would be plenty of folks with those qualifications.\nThe response mentions curation, which is a critical issue. But good curation requires knowledge of two things: (i) the biological or scientific problem and (ii) how and in what way the data will be analyzed and used by researchers. As the Duke scandal made clear, a statistician with technological and biological knowledge running through a data analysis will identify many critical issues in data curation that would be missed by someone who doesn’t actually analyze data. \nThe response says that “Curation and accessibility” are key. I agree that they are part of the key. It is critical that data can be properly accessed by researchers to perform new analyses, verify results in papers, and discover new results. But if the goal is to ensure the quality of science being published in Nature (the role of an editor) curation and accessibility are not enough. The editor should be able to evaluate statistical methods described in papers to identify potential flaws, or to rerun code and make sure that it performs the same/sensible analyses. A bad analysis that is reproducible will be discovered more quickly, but it is still a bad analysis. \nTo be fair, I don’t think that Nature is the only organization that is missing the value of statistical skill in hiring data positions. It seems like many organizations are still just searching for folks who can handle/process the massive data sets being generated. But if they want to make accurate and informed decisions, statistical knowledge needs to be at the top of their list of qualifications.  \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-29-sunday-data-statistics-link-roundup-4-29/",
    "title": "Sunday data/statistics link roundup (4/29)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-29",
    "categories": [],
    "contents": "\nNature genetics has an editorial on the Mayo and Myriad cases. I agree with this bit: “In our opinion, it is not new judgments or legislation that are needed but more innovation. In the era of whole-genome sequencing of highly variable genomes, it is increasingly hard to justify exclusive ownership of particularly useful parts of the genome, and method claims must be more carefully described.” Via Andrew J.\nOne of Tech Review’s 10 emerging technologies from a February 2003 article? Data mining. I think doing interesting things with data has probably always been a hot topic, it just gets press in cycles. Via Aleks J. \nAn infographic in the New York Times compares the profits and taxes of Apple over time, here is an explanation of how they do it. (Via Tim O.)\nSaw this tweet via Joe B. I’m not sure if the frequentists or the Bayesians are winning, but it seems to me that the battle no longer matters to my generation of statisticians - there are too many data sets to analyze, better to just use what works!\nStatistical and computational algorithms that write news stories. Simply Statistics remains 100% human written (for now). \nThe 5 most critical statistical concepts. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-27-people-in-positions-of-power-that-dont-understand/",
    "title": "People in positions of power that don't understand statistics are a big problem for genomics",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-27",
    "categories": [],
    "contents": "\n\nI finally got around to reading the IOM report on translational omics and it is very good. The report lays out problems with current practices and how these led to undesired results such as the now infamous Duke trials and the growth in retractions in the scientific literature. Specific recommendations are provided related to reproducibility and validation. I expect the report will improve things. Although I think bigger improvements will come as a result of retirements.\n\n\nIn general, I think the field of genomics  (a label that is used quite broadly) is producing great discoveries and I strongly believe we are just getting started. But we can’t help but notice that retraction and questionable findings are particularly high in this field. In my view most of the problems we are currently suffering stem from the fact that a substantial number of the people with positions of power do not understand statistics and have no experience with computing. Nevin’s biggest mistake was not admitting to himself that he did not understand what Baggerly and Coombes were saying. The lack of reproducibility just exacerbated the problem. The same is true for the editors that rejected the letters written by this pair in their effort to expose a serious problem - a problem that was obvious to all the statistics savvy biologists I talked to.\n\n\nUnfortunately Nevins is not the only head of a large genomics lab that does not understand basic statistical principles and has no programming/data-management experience. So how do people without the necessary statistical and computing skills to be considered experts in genomics become leaders of the field?  I think this is due to the speed at which Biology changed from a data poor discipline to a data intensive ones. For example, before microarrays, the analysis of gene expression data amounted to spotting black dots on a piece of paper (see Figure A below). In the mid 90s this suddenly changed to sifting through tens of thousands of numbers (see Figure B).\n\ngene expression\nNote that typically, statistics is not a requirement of the Biology graduate programs associated with genomics. At Hopkins neither of the two major programs (CMM and BCMB) require it. And this is expected, since outside of genomics one can do great  Biology without quantitative skills and for most of the 20th century most Biology was like this. So when the genomics revolution first arrived, the great majority of powerful Biology lab heads had no statistical training whatsoever. Nonetheless, a few of these decided to delve into this “sexy” new field and using their copious resources were able to perform some of the first big experiments. Similarly, Biology journals that were not equipped to judge the data analytic component of genomics papers were eager to publish papers in this field, a fact that further compounded the problem.\n\n\nBut I as I mentioned above, in general, the field of genomics is producing wonderful results. Several lab heads did have statistics and computational expertise, while others formed strong partnerships with quantitative types. Here I should mentioned that for these partnerships to be successful the statisticians also needed to expand their knowledge base. The quantitative half of the partnership needs to be biology and technology savvy or they too can make mistakes that lead to retractions. \n\n\nNevertheless, the field is riddled with problems; enough to prompt an IOM report. But although the present is somewhat grim, I am optimistic about the future. The new generation of biologists leading the genomics field are clearly more knowledgeable and appreciative about statistics and computing than the previous ones. Natural selection helps, as these new investigators can’t rely on pre-genomics-revolution accomplishments and those that do not posses these skills are simply outperformed by those that do. I am also optimistic because biology graduate programs are starting to incorporate statistics and computation into their curricula. For example, as of last year, our Human Genetics program requires our Biostats 615-616 course. \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-26-nature-is-hiring-a-data-editor-how-will-they-make/",
    "title": "Nature is hiring a data editor...how will they make sense of the data?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-26",
    "categories": [],
    "contents": "\nIt looks like the journal Nature is hiring a Chief Data Editor (link via Hilary M.). It looks like the primary purpose of this editor is to develop tools for collecting, curating, and distributing data with the goal of improving reproducible research.\nThe main duties of the editor, as described by the ad are: \n\nNature Publishing Group is looking for a Chief Editor to develop a product aimed at making research data more available, discoverable and interpretable.\n\nThe ad also mentions having an eye for commercial potential; I wonder if this move was motivated by companies like figshare who are already providing a reproducible data service. I haven’t used figshare, but the early reports from friends who have are that it is great. \nThe thing that bothered me about the ad is that there is a strong focus on data collection/storage/management but absolutely no mention of the second component of the data science problem: making sense of the data. To make sense of piles of data requires training in applied statistics (called by whatever name you like best). The ad doesn’t mention any such qualifications. \nEven if the goal of the position is just to build a competitor to figshare, it seems like a good idea for the person collecting the data to have some idea of what researchers are going to do with it. When dealing with data, those researchers will frequently be statisticians by one name or another. \nBottom line: I’m stoked Nature is recognizing the importance of data in this very prominent way. But I wish they’d realize that a data revolution also requires a revolution in statistics. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-25-how-do-i-know-if-my-figure-is-too-complicated/",
    "title": "How do I know if my figure is too complicated?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-25",
    "categories": [],
    "contents": "\nOne of the key things every statistician needs to learn is how to create informative figures and graphs. Sometimes, it is easy to use off-the-shelf plots like barplots, histograms, or if one is truly desperate a pie-chart. \nBut sometimes the information you are trying to communicate requires the development of a new graphic. I am currently working on a project with a graduate student where the standard illustration are Venn Diagrams - including complicated Venn Diagrams with 5 or 10 circles. \nAs we were thinking about different ways of illustrating our data, I started thinking about what are the key qualities of a graphic and how do I know if it is too complicated. I realized that:\nIdeally just looking at the graphic one can intuitively understand what is going on, but sometimes for more technical/involved displays this isn’t possible\nAlternatively, I think a good plot should be able to be explained in 2 sentences or less. I think that is true for pretty much every plot I use regularly. \nThat isn’t including describing what different colors/sizes/shapes specifically represent in any particular version of the graphic. \nI feel like there is probably something to this in the Grammar of Graphics or in some of William Cleveland’s work. But this is one of the first times I’ve come up with a case where a new, generalizable, type of graph needs to be developed. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-24-on-the-future-of-personalized-medicine/",
    "title": "On the future of personalized medicine",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-24",
    "categories": [],
    "contents": "\nJeff Leek, Reeves Anderson, and I recently wrote a correspondence to Nature (subscription req.) regarding the Supreme Court decision in Mayo v. Prometheus and the recent Institute of Medicine report related to the Duke Clinical Trials Saga. \nThe basic gist of the correspondence is that the IOM report stresses the need for openness in the process of developing ‘omics based tests, but the Court decision suggests that patent protection will not be available to protect those details. So how will the future of personalized medicine look? There is a much larger, more general, discussion that could be had about patents in this arena and we do not get into that here (hey, we had to squeeze it into 300 words). But it seems that if biotech companies cannot make money from patented algorithms, then they will have to find a new avenue. \nHere are some slides from a recent lecture I gave outlining some of the ideas and providing some background.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-22-sunday-data-statistics-link-roundup-4-22/",
    "title": "Sunday data/statistics link roundup (4/22)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-22",
    "categories": [],
    "contents": "\nNow we know who is to blame for the pie chart. I had no idea it had been around, straining our ability to compare relative areas, since 1801. However, the same guy (William Playfair) apparently also invented the bar chart. So he wouldn’t be totally shunned by statisticians. (via Leonid K.)\nA nice article in the Guardian about the current group of scientists that are boycotting Elsevier. I have to agree with the quote that leads the article, “All professions are conspiracies against the laity.” On the other hand, I agree with Rafa that academics are partially to blame for buying into the closed access hegemony. I think more than a boycott of a single publisher is needed; we need a change in culture. (first link also via Leonid K)\nA blog post on how to add a transparent image layer to a plot. For some reason, I have wanted to do this several times over the last couple of weeks, so the serendipity of seeing it on R Bloggers merited a mention. \nI agree the Earth Institute needs a better graphics advisor. (via Andrew G.)\nA great article on why multiple choice tests are used - they are an easy way to collect data on education. But that doesn’t mean they are the right data. This reminds me of the Tukey quote: “The data may not contain the answer. The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data”. It seems to me if you wanted to have a major positive impact on education right now, the best way would be to develop a new experimental design that collects the kind of data that really demonstrates mastery of reading/math/critical thinking. \nFinally, a bit of a bleg…what is the best way to do the SVD of a huge (think 1e6 x 1e6), sparse matrix in R? Preferably without loading the whole thing into memory…\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-18-replication-psychology-and-big-science/",
    "title": "Replication, psychology, and big science",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-18",
    "categories": [],
    "contents": "\nReproducibility has been a hot topic for the last several years among computational scientists. A study is reproducible if there is a specific set of computational functions/analyses (usually specified in terms of code) that exactly reproduce all of the numbers in a published paper from raw data. It is now recognized that a critical component of the scientific process is that data analyses can be reproduced. This point has been driven home particularly for personalized medicine applications, where irreproducible results can lead to delays in evaluating new procedures that affect patients’ health. \nBut just because a study is reproducible does not mean that it is replicable. Replicability is stronger than reproducibility. A study is only replicable if you perform the exact same experiment (at least) twice, collect data in the same way both times, perform the same data analysis, and arrive at the same conclusions. The difference with reproducibility is that to achieve replicability, you have to perform the experiment and collect the data again. This of course introduces all sorts of new potential sources of error in your experiment (new scientists, new materials, new lab, new thinking, different settings on the machines, etc.)\nReplicability is getting a lot of attention recently in psychology due to some high-profile studies that did not replicate. First, there was the highly-cited experiment that failed to replicate, leading to a show down between the author of the original experiment and the replicators. Now there is a psychology project that allows researchers to post the results of replications of experiments - whether they succeeded or failed. Finally, the Reproducibility Project, probably better termed the Replicability Project, seeks to replicate the results of every experiment in the journals _Psychological Science, the Journal of Personality and Social Psychology,or the Journal of Experimental Psychology: Learning, Memory, and Cognition _in the year 2008.\nReplicability raises important issues for “big science” projects, ranging from genomics (The Thousand Genomes Project) to physics (The Large Hadron Collider). These experiments are too big and costly to actually replicate. So how do we know the results of these experiments aren’t just errors, that upon replication (if we could do it) would not show up again? Maybe smaller scale replications of sub-projects could be used to help convince us of discoveries in these big projects?\nIn the meantime, I love the idea that replication is getting the credit it deserves (at least in psychology). The incentives in science often only credit the first person to an idea, not the long tail of folks who replicate the results. For example, replications of experiments are often not considered interesting enough to publish. Maybe these new projects will start to change some of the perverse academic incentives.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-16-roche-illumina-is-no-apple/",
    "title": "Roche: Illumina Is No Apple",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-16",
    "categories": [],
    "contents": "\nRoche: Illumina Is No Apple\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-15-sunday-data-statistics-link-roundup-4-15/",
    "title": "Sunday data/statistics link roundup (4/15)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-15",
    "categories": [],
    "contents": "\nIncredibly cook, dynamic real-time maps of wind patterns in the United States. (Via Flowing Data)\nA d3.js coding tool that updates automatically as you update the code. This is going to be really useful for beginners trying to learn about D3. Real time coding (Via Flowing Data)\nAn interesting blog post describing why the winning algorithm in the Netflix prize hasn’t actually been implemented! It looks like it was too much of an engineering hassle. I wonder if this will make others think twice before offering big sums for prizes like this. Unless the real value is advertising…(via Chris V.)\nAn article about a group at USC that plans to collect all the information from apps that measure heart beats. Their project is called everyheartbeat. I think this is a little bit pre-mature, given the technology, but certainly the quantified self field is heating up. I wonder how long until the target audience for these sorts of projects isn’t just wealthy young technofiles? \nA really good deconstruction of a recent paper suggesting that the mood on Twitter could be used to game the stock market. The author illustrates several major statistical flaws, including not correcting for multiple testing, an implausible statistical model, and not using a big enough training set. The scary thing is apparently a hedge fund is teaming up with this group of academics to try to implement their approach. I wouldn’t put my money anywhere they can get their hands on it. This is just one more in the accelerating line of results that illustrate the critical need for statistical literacy both among scientists and in the general public.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-13-interview-with-drew-conway-author-of-machine/",
    "title": "Interview with Drew Conway - Author of \"Machine Learning for Hackers\"",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-13",
    "categories": [],
    "contents": "\nDrew Conway\n\nDrew Conway is a Ph.D. student in Politics at New York University and the co-ordinator of the New York Open Statistical Programming Meetup. He is the creator of the famous (or infamous) data science Venn diagram, the basis for our R function to determine if your a data scientist. He is also the co-author of Machine Learning for Hackers, a book of case studies that illustrates data science from a hacker’s perspective. \n\nWhich term applies to you: data scientist, statistician, computerscientist, or something else?\n\n\n\n\n\nTechnically, my undergraduate degree is in computer science, so that term can be applied.  I was actually double-major in CS and political science, however, so it wouldn’t tell the whole story.  I have always been most interested in answering social science problems with the tools of computer science, math and statistics.\n\n\n\n\n\nI have struggled a bit with the term “data scientist.”  About a year ago, when it seemed to be gaining a lot of popularity, I bristled at it.  Like many others, I complained that it was simply a corporate rebranding of other skills, and that the term “science” was appended to give some veil of legitimacy.  Since then, I have warmed to the term, but—-as is often the case—-only when I can define what data science is in my own terms.  Now, I do think of what I do as being data science, that is, the blending of technical skills and tools from computer science, with the methodological training of math and statistics, and my own substantive interest in questions about collective action and political ideology.\n\n\n\n\n\nI think the term is very loaded, however, and when many people invoke it they often do so as a catch-all for talking about working with a certain a set of tools: R, map-reduce, data visualization, etc.  I think this actually hurts the discipline a great deal, because if it is meant to actually be a science the majority of our focus should be on questions, not tools.\n\n\n\n \n\n\nYou are in the department of politics? How is it being a “dataperson” in a non-computational department?\n\n\n\n\n\n\nData has always been an integral part of the discipline, so in that sense many of my colleagues are data people.  I think the difference between my work and the work that many other political scientist do is simply a matter of where and how I get my data.  \n\n\n\n\n\nFor example, a traditional political science experiment might involve a small set of undergraduates taking a survey or playing a simple game on a closed network.  That data would then be collected and analyzed as a controlled experiment.  Alternatively, I am currently running an experiment wherein my co-authors and I are attempting to code text documents (political party manifestos) with ideological scores (very liberal to very conservative).  To do this we have broken down the documents into small chunks of text and are having workers on Mechanical Turk code single chunks—rather than the whole document at once.  In this case the data scale up very quickly, but by aggregating the results we are able to have a very different kind of experiment with much richer data.\n\n\n\n\n\nAt the same time, I think political science—-and perhaps the social sciences more generally—suffer from a tradition of undervaluing technical expertise. In that sense, it is difficult to convince colleagues that developing software tools is important. \n\n\n\n \n\n\nIs that what inspired you to create the New York Open Statistical Meetup?\n\n\n\n\n\n\n\nI actually didn&#8217;t create the New York Open Statistical Meetup (formerly the R meetup).  Joshua Reich was the original founder, back in 2008, and shortly after the first meeting we partnered and ran the Meetup together.  Once Josh became fully consumed by starting / running BankSimple I took it over by myself.  I think the best part about the Meetup is how it brings people together from a wide range of academic and industry backgrounds, and we can all talk to each other in a common language of computational programming.  The cross-pollination of ideas and talents is inspiring.\n\n\n\n\n\nWe are also very fortunate in that the community here is so strong, and that New York City is a well traveled place, so there is never a shortage of great speakers.\n\n\n\n\n \n\n\nYou created the data science Venn diagram. Where do you fall on the diagram?\n\n\n\n\n\n\nRight at the center, of course! Actually, before I entered graduate school, which is long before I drew the Venn diagram, I fell squarely in the danger zone.  I had a lot of hacking skills, and my work (as an analyst in the U.S. intelligence community) afforded me a lot of substantive expertise, but I had little to no formal training in statistics.  If you could describe my journey through graduate school within the framework of the data science Venn diagram, it would be about me trying to pull myself out of the danger zone by gaining as much math and statistics knowledge as I can.  \n\n\n\n \n\n\nI see that a lot of your software (including R packages) are on Github. Do you post them on CRAN as well? Do you think R developers will eventually move to Github from CRAN?\n\n\n\n\n\n\n\nI am a big proponent of open source development, especially in the context of sharing data and analyses; and creating reproducible results.  I love Github because it creates a great environment for following the work of other coders, and participating in the development process.  For data analysis, it is also a great place to upload data and R scripts and allow the community to see how you did things and comment.  I also think, however, that there is a big opportunity for a new site&#8212;-like Github&#8212;-to be created that is more tailored for data analysis, and storing and disseminating data and visualizations.\n\n\n\n\n\nI do post my R packages to CRAN, and I think that CRAN is one of the biggest strengths of the R language and community.  I think ideally more package developers would open their development process, on Github or some other social coding platform, and then push their well-vetted packages to CRAN.  This would allow for more people to participate, but maintain the great community resource that CRAN provides. \n\n\n\n\n \n\n\nWhat inspired you to write, “Machine Learning for Hackers”? Whowas your target audience?\n\n\n\n\n\n\n\nA little over a year ago John Myles White (my co-author) and I were having a lot of conversations with other members of the data community in New York City about what a data science curriculum would look like.  During these conversations people would always cite the classic text; Elements of Statistical Learning, Pattern Recognition and Machine Learning, etc., which are excellent and deep treatments of the foundational theories of machine learning.  From these conversations it occurred to us that there was not a good text on machine learning for people who thought more algorithmically.  That is, there was not a text for &#8220;hackers,&#8221; people who enjoy learning about computation by opening up black-boxes and getting their hands dirty with code.\n\n\n\n\n\nIt was from this idea that the book, and eventually the title, were borne.  We think the audience for the book is anyone who wants to get a relatively broad introduction to some of the basic tools of machine learning, and do so through code&#8212;-not math.  This can be someone working at a company with data that wants to add some of these tools to their belt, or it can be an undergraduate in a computer science or statistics program that can relate to the material more easily through this presentation than the more theoretically heavy texts they&#8217;re probably already reading for class. \n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-12-the-problem-with-universities/",
    "title": "The Problem with Universities",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-12",
    "categories": [],
    "contents": "\nI have had the following conversation a number of times recently:\nI want to do X. X is a lot of fun and is really interesting. Doing X involves a little of A and a little of B.\nWe should get some students to do X also.\nOkay, but from where should we get the students? Students in Department of A don’t know B. Students from Department of B don’t know A.\nFine, maybe we could start a program that specifically trains people in X. In this program we’ll teach them A and B. It’ll be the first program of it’s kind! Woohoo!\nSure that’s great, but because there aren’t any other departments of X, the graduates of our program now have to get jobs in departments of A or B. Those departments complain that students from Department of X only know a little of A (or B).\nGrrr. Go away.\nHas anyone figured out a solution to this problem? Specifically, how do you train students to do something for which there’s no formal department/program without jeopardizing their career prospects?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-11-statistics-is-not-math/",
    "title": "Statistics is not math...",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-11",
    "categories": [],
    "contents": "\nStatistics depends on math, like a lot of other disciplines (physics, engineering, chemistry, computer science). But just like those other disciplines, statistics is not math; math is just a tool used to solve statistical problems. Unlike those other disciplines, statistics gets lumped in with math in headlines. Whenever people use statistical analysis to solve an interesting problem, the headline reads:\n“Math can be used to solve amazing problem X”\nor\n“The Math of Y” \nHere are some examples:\nThe Mathematics of Lego - Using data on legos to estimate a distribution\nThe Mathematics of War - Using data on conflicts to estimate a distribution\nUsain Bolt can run faster with maths (Tweet) - Turns out they analyzed data on start times to come to the conclusion\nThe Mathematics of Beauty - Analysis of data relating dating profile responses and photo attractiveness\nThese are just a few off of the top of my head, but I regularly see headlines like this. I think there are a couple reasons for math being grouped with statistics: (1) many of the founders of statistics were mathematicians first (but not all of them) (2) many statisticians still identify themselves as mathematicians, and (3) in some cases statistics and statisticians define themselves pretty narrowly. \nWith respect to (3), consider the following list of disciplines:\nBiostatistics\nData science\nMachine learning\nNatural language processing\nSignal processing\nBusiness analytics\nEconometrics\nText mining\nSocial science statistics\nProcess control\nAll of these disciplines could easily be classified as “applied statistics”. But how many folks in each of those disciplines would classify themselves as statisticians? More importantly, how many would be claimed by statisticians? \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-10-evolution-evolved/",
    "title": "Evolution, Evolved",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-10",
    "categories": [],
    "contents": "\nEvolution, Evolved\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-09-sunday-data-statistics-link-roundup-4-8/",
    "title": "Sunday data/statistics link roundup (4/8)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-09",
    "categories": [],
    "contents": "\nThis is a great article about the illusion of progress in machine learning. In part, I think it explains why the Leekasso (just using the top 10) isn’t a totally silly idea. I also love how he talks about sources of uncertainty in real prediction problems that aren’t part of the classical models when developing prediction algorithms. I think that this is a hugely underrated component of building an accurate classifier - just finding the quirks particular to a type of data. Via @chlalanne.\nAn interesting post from Michael Eisen on a serious abuse of statistical ideas in the New York Times. The professor of genetics quoted in the story apparently wasn’t aware of the birthday problem. Lack of statistical literacy, even among scientists, is becoming critical. I would love it if the Kahn academy (or some enterprising students) would come up with a set of videos that just explained a bunch of basic statistical concepts - skipping all the hard math and focusing on the ideas. \n TechCrunch finally caught up to our Mayo vs. Prometheus coverage. This decision is going to affect more than just personalized medicine. Speaking of the decision, stay tuned for more on that topic from the folks over here at Simply Statistics. \nHow much is a megabyte? I love this question. They asked people on the street how much data was in a megabyte. The answers were pretty far ranging looks like. This question is hyper-critical for scientists in the new era, but the better question might be, “How much is a terabyte?”\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-09-what-is-a-major-revision/",
    "title": "What is a major revision?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-09",
    "categories": [],
    "contents": "\nI posted a little while ago on a proposal for a fast statistics journal. It generated a bunch of comments and even a really nice follow up post with some great ideas. Since then I’ve gotten reviews back on a couple of papers and I think I realized one of the key issues that is driving me nuts about the current publishing model. It boils down to one simple question: \nWhat is a major revision? \nI often get reviews back that suggest “major revisions” in one or many of the following categories:\nMore/different simulations\nNew simulations\nRe-organization of content\nRe-writing language\nAsking for more references\nAsking me to include a new method\nAsking me to implement someone else’s method for comparison\n\nI don’t consider any of these major revisions. Personally, I have stopped asking for them as major revisions. In my opinion, major revisions should be reserved for issues with the manuscript that suggest that it may be reporting incorrect results. Examples include:\n\n\n\nNo simulations\n\n\nNo real data\n\n\nThe math/computations look incorrect\n\n\nThe software didn’t work when I tried it\n\n\nThe methods/algorithms are unreadable and can’t be followed\n\n\nThe first list is actually a list of minor/non-essential revisions in my opinion. They may <em>improve<\/em> my paper, but they won&#8217;t confirm that it is correct or not. I find that they are often subjective and are up to the whims of referees. In my own personal refereeing I am making an effort to remove subjective major revisions and only include issues that are critical to evaluate the correctness of a manuscript. I also try to divorce the issues of whether an idea is interesting or not from whether an idea is correct or not. \n\n\n\n\n\n\nI’d be curious to know what other peoples’ definitions of major/minor revisions are?\n\n_\n_\n_\n_\n_\n_\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-05-study-says-dnas-power-to-predict-illness-is-limited/",
    "title": "Study Says DNA’s Power to Predict Illness Is Limited",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-05",
    "categories": [],
    "contents": "\nStudy Says DNA’s Power to Predict Illness Is Limited\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-04-epigenetics-marked-for-success/",
    "title": "Epigenetics: Marked for success",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-04",
    "categories": [],
    "contents": "\nEpigenetics: Marked for success\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-04-03-enar-meeting/",
    "title": "ENAR Meeting",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-04-03",
    "categories": [],
    "contents": "\nThis is the ENAR meeting so posting will be intermittent. If you’re at the meeting I’ll be talking at 1:45 today in the Columbia B room in a session on climate change and health. I hear Rafa is roaming the halls too so make sure you say hi if you see him.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-30-new-u-s-research-will-aim-at-flood-of-digital-data/",
    "title": "New U.S. Research Will Aim at Flood of Digital Data",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-30",
    "categories": [],
    "contents": "\nNew U.S. Research Will Aim at Flood of Digital Data\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-30-r-2-15-0-is-released/",
    "title": "R 2.15.0 is released",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-30",
    "categories": [],
    "contents": "\nR 2.15.0 is released\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:55:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-29-big-data-meeting-at-aaas/",
    "title": "Big Data Meeting at AAAS",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-29",
    "categories": [],
    "contents": "\nThe White House Office of Science and Technology Policy is hosting a meeting that will discuss several new federal efforts relating to big data. The meeting is is today from 2-3:45pm and there will be live webcast.\nParticipants include\nJohn Holdren, Assistant to the President and Director, White House Office of Science and Technology Policy\nSubra Suresh, Director, National Science Foundation\nFrancis Collins, Director, National Institutes of Health\nMarcia McNutt, Director, United States Geological Survey\nWilliam Brinkman, Director, Department of Energy Office of Science\nZach Lemnios, Assistant Secretary of Defense for Research & Engineering, Department of Defense\nKaigham “Ken” Gabriel, Deputy Director, Defense Advanced Research Projects Agency\nDaphne Koller, Stanford University (machine learning and applications in biology and education)\nJames Manyika, McKinsey & Company (Co-author of major McKinsey report on Big Data)\nLucila Ohno-Machado, UC San Diego (NIH’s “Integrating Data for Analysis, Anonymization, and Sharing” initiative)\nAlex Szalay, Johns Hopkins University (Big Data for astronomy)\n\nUpdate: Some more information from the White House itself.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-29-roche-raises-illumina-bid-to-51-seeking-faster-deal/",
    "title": "Roche Raises Illumina Bid to $51, Seeking Faster Deal",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-29",
    "categories": [],
    "contents": "\nRoche Raises Illumina Bid to $51, Seeking Faster Deal\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-27-justices-send-back-gene-case/",
    "title": "Justices Send Back Gene Case",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-27",
    "categories": [],
    "contents": "\nJustices Send Back Gene Case\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-26-r-and-the-little-data-scientists-predicament/",
    "title": "R and the little data scientist's predicament",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-26",
    "categories": [],
    "contents": "\nI just read this fascinating post on _why, apparently a bit of a cult hero among enthusiasts of the Ruby programming language. One of the most interesting bits was The Little Coder’s Predicament, which boiled down essentially says that computer programming languages have grown too complex - so children/newbies can’t get the instant gratification when they start programming. He suggested a simplified “gateway language” that would get kids fired up about programming, because with a simple line of code or two they could make the computer do things like play some music or make a video. \nI feel like there is a similar ramp up with data scientists. To be able to do anything cool/inspiring with data you need to know (a) a little statistics, (b) a little bit about a programming language, and (c) quite a bit about syntax. \nWouldn’t it be cool if there was an R package that solved the little data scientist’s predicament? The package would have to have at least some of these properties:\nIt would have to be easy to load data sets, one line of not complicated code. You could write an interface for RCurl/read.table/download.file for a defined set of APIs/data sets so the command would be something like: load(“education-data”) and it would load a bunch of data on education. It would handle all the messiness of scraping the web, formatting data, etc. in the background. \nIt would have to have a lot of really easy visualization functions. Right now, if you want to make pretty plots with ggplot(), plot(), etc. in R, you need to know all the syntax for pch, cex, col, etc. The plotting function should handle all this behind the scenes and make super pretty pictures. \nIt would be awesome if the functions would include some sort of dynamic graphics (with svgAnnotation or a wrapper for D3.js). Again, the syntax would have to be really accessible/not too much to learn. \nThat alone would be a huge start. In just 2 lines kids could load and visualize cool data in a pretty way they could show their parents/friends. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-26-supreme-court-vacates-ruling-on-brca-gene-patent/",
    "title": "Supreme court vacates ruling on BRCA gene patent!",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-26",
    "categories": [],
    "contents": "\nAs Reeves alluded to in his post about the Mayo personalized medicine case, the Supreme Court just vacated the lower court’s ruling in Association for Molecular Pathology v. Myriad Genetics (No. 11-725). The case has been sent back down to the Federal Circuit for reconsideration in light of the Court’s decision in Mayo.  This means that the Supreme Court thought the two cases were sufficiently similar that the lower courts should take another look using the new direction from Mayo.\n It’s looking more and more like the Supreme Court is strongly opposed to personalized medicine patents. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-25-some-thoughts-from-keith-baggerly-on-the-recently/",
    "title": "Some thoughts from Keith Baggerly on the recently released IOM report on translational omics",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-25",
    "categories": [],
    "contents": "\nShortly after the Duke trial scandal broke, the Institute of Medicine convened a committee to write a report on translational omics. Several statisticians (including one of our interviewees) either served on the committee or provided key testimony. The report came out yesterday.  Nature, Nature Medicine, and Science had posts about the release. Keith Baggerly sent an email with his thoughts and he gave me permission to post it here. He starts by pointing out that the Science piece has a key new observation:\n\nThe NCI’s Lisa McShane, who spent months herself trying to validate Duke results, says the IOM committee “did a really fine job” in laying out the issues. NCI now plans to require that its cooperative groups who want to use omics tests follow a checklist similar to that in the IOM report. NCI has not yet decided whether it should add new requirements for omics tests to its peer review process for investigator-initiated grants. But “our hope is that this report will heighten everyone’s awareness,” McShane says. \n\nSome further thoughts from Keith:\n\nFirst, the report helps clarify the regulatory landscape: if omics-based tests (which the FDA views as medical devices) will direct patient therapy, FDA approval in the form of an Investigational Device Exemption (IDE) is required. This is in keeping with increased guidance FDA has been providing over the past year and a half dealing with companion diagnostics. It seems likely that several of the problems identified with the Duke trials would have been caught by an FDA review, particularly if the agency already had cause for concern, such as a letter to the editor identifying analytical shortcomings. \n Second, the report recommends the publication of the full data, code, and metadata used to construct the omics assays prior to their use to guide patient therapy. Had such data and code been available earlier, this would have greatly reduced the amount of effort required for others (including us) to check and potentially extend on the underlying results.\nThird, the report emphasizes, repeatedly, that the test must be fully specified (“locked down”) before it is validated, let alone used to guide patient therapy. Quite a bit of effort is given to providing an explicit definition of locked down, in part (we suspect) because both Lisa McShane (NCI) and Robert Becker (FDA) provided testimony that incomplete specification was a problem their agencies encountered frequently. Such specification would have prevented problems such as that identified by the NCI for the Lung Metagene Score (LMS) in 2010, which led the NCI to remove the LMS evaluation as a goal of the Phase III cooperative group trial CALGB-30506.\n Finally, the very existence of the report is recognition that reproducibility is an important problem for the omics-test community. This is a necessary step towards fixing the problem.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-25-sunday-data-statistics-link-roundup-3-25/",
    "title": "Sunday data/statistics link roundup (3/25)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-25",
    "categories": [],
    "contents": "\nThe psychologist whose experiment didn’t replicate then went off on the scientists who did the replication experiment is at it again. I don’t see a clear argument about the facts of the matter in his post, just more name calling. This seems to be a case study in what not to do when your study doesn’t replicate. More on “conceptual replication” in there too. \nBerkeley is running a data science course with instructors Jeff Hammerbacher and Mike Franklin, I looked through the notes and it looks pretty amazing. Stay tuned for more info about my applied statistics class which starts this week. \nA cool article about Factual, one of the companies whose sole mission in life is to collect and distribute data. We’ve linked to them before. We are so out ahead of the Times on this one…\nThis isn’t statistics related, but I love this post about Jeff Bezos. If we all indulged our inner 11 year old a little more, it wouldn’t be a bad thing. \nIf you haven’t had a chance to read Reeves guest post on the Mayo Supreme Court decision yet, you should, it is really interesting. A fascinating intersection of law and statistics is going on in the personalized medicine world right now. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-23-this-graph-shows-that-president-obamas-proposed-budget/",
    "title": "This graph shows that President Obama's proposed budget treats the NIH even worse than G.W. Bush - Sign the petition to increase NIH funding!",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-23",
    "categories": [],
    "contents": "\nThe NIH provides financial support for a large percentage of biological and medical research in the United States. This funding supports a large number of US jobs, creates new knowledge, and improves healthcare for everyone. So I am signing this petition: \n\n\nNIH funding is essential to our national research enterprise, to our local economies, to the retention and careers of talented and well-educated people, to the survival of our medical educational system, to our rapidly fading worldwide dominance in biomedical research, to job creation and preservation, to national economic viability, and to our national academic infrastructure. \n\n\nThe current administration is proposing a flat $30.7 billion FY 2013 NIH budget. The graph below (left) shows how small the NIH budget is in comparison to the Defense and Medicare budgets in absolute terms. The difference between the administration’s proposal and the petition’s proposal ($33 billion) are barely noticeable. \nThe graph on the right shows how in 2003 growth in the NIH budget fell dramatically while medicare and military spending kept growing. However, despite the decrease in rate, the NIH budget did continue to increase under Bush. If we follow Bush’s post 2003 rate (dashed line), the 2013 budget will be about what the petition asks for: $33 billion.  \n\n\nIf you agree that the relatively modest increase in the NIH budget is worth the incredibly valuable biological, medical, and economic benefits this funding will provide, please consider signing the petition before April 15 \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-22-big-data-for-the-rest-of-us-in-one-start-up/",
    "title": "Big Data for the Rest of Us, in One Start-Up",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-22",
    "categories": [],
    "contents": "\nBig Data for the Rest of Us, in One Start-Up\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-21-more-commentary-on-mayo-v-prometheus/",
    "title": "More commentary on Mayo v. Prometheus",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-21",
    "categories": [],
    "contents": "\nSome more commentary on Mayo v. Prometheus via the Patently-O blog.\nA summary of the various briefs and history of the case can be found at the SCOTUS blog.\nSome actual news coverage of the decision.\nThe decision is well-worth reading, if you’re that kind of nerd. Here, the Court uses the phrase “law of nature” a bit more loosely than perhaps I would use it. On the one hand, something like E=mc^2 might be considered a law of nature, but on the other hand I would consider the observation that certain blood metabolites are correlated with the occurrence of patient side effects as, well, a correlation. Einstein is referred to quite a few times in the opinion, no doubt in part because he himself worked in a patent office (and also discovered a few interesting laws of nature).\nIf one were to set aside the desire to do inference, then one could argue that in a given sample of people (random or not), any correlation observed within that sample is a “law of nature”, at least within that sample. Then if I draw a different sample and observe a different correlation, is that a different law of nature? Well, it might depend on whether it’s statistically significantly different.\nIn the end, maybe it doesn’t matter, because no law of nature is patentable, no matter how many there are. I do find it interesting that the Court considered, in some sense, the possibility of statistical variation.\nThe Court also noted that simply ordering a bunch of steps together did not make a procedure patentable, if the things that were put together were things that doctors (or people in the profession) were already doing. The question becomes, if you take away the statistical correlation in the patent, is there anything left? No, because doctors were already treating patients with immune-mediated gastrointestinal disorders and those patients were already being tested for blood metabolites. \nThis section of the decision caught my eye because it sounded a lot like the work of an applied statistician. Much of applied statistics involves taking methods and techniques that are already well known (lasso, anyone?) and applying them in new and interesting ways to new and interesting data. It seems taking a bunch of well-known process/techniques and putting them together is not patentable, even if it is interesting. I don’t think I have a problem with that, but then again, getting patents aren’t my main goal.\nActual lawyers will be able to tell whether this case is significant. However, it seems there are many statistical correlations out there that are waiting to be turned into medical treatments. For example, take the Duke clinical trials saga. I don’t think it’s the case that none of these are patentable, because there still is the option of adding an “inventive concept” on top. However, it seems the simple algorthmic approach of “If X do this, and if Y do that” isn’t going to fly.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-20-laws-of-nature-and-the-law-of-patents-supreme-court/",
    "title": "Laws of Nature and the Law of Patents: Supreme Court Rejects Patents for Correlations",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-20",
    "categories": [],
    "contents": "\n\nThis is a guest post by Reeves Anderson, an associate at Arnold and Porter LLP. Reeves Anderson is a member of the Appellate and Supreme Court practice group at Arnold & Porter LLP in Washington, D.C.  The views expressed herein are those of the author alone and not of Arnold & Porter LLP or any of the firm’s clients. Stay tuned for follow-up posts by the Simply Statistics crowd on the implications of this ruling for statistics in general and personalized medicine in particular. \n\n\nWith the country’s attention focused on next week’s arguments over the constitutionality of President Obama’s health care law, the Supreme Court slipped in an important decision today concerning personalized medicine patents.  In Mayo Collaborative Services v. Prometheus Laboratories, the Court unanimously struck down medical diagnostic patents that concerned the use of thiopurine drugs in the treatment of autoimmune diseases.  Prometheus’s patents, which provided that doctors should increase or decrease a treatment dosage depending on metabolite correlations, was ineligible for patent protection, the Court held, because the patents “simply stated a law of nature.” \n\n\nAs Jeff aptly described the issue in December, Prometheus’s patents sought to control a treatment process centered “on the basis of a statistical correlation.”  Specifically, when a patient ingests a thiopurine drug, metabolites form in the patient’s bloodstream.  Because the production of metabolites varies among patients, the same dosage of thiopurine causes different effects in different patients.  This variation makes it difficult for doctors to determine optimal treatment for a particular patient.  Too high of a dosage risks harmful side effects, whereas too low would be therapeutically ineffective. \n\n\nBut measurement of a patient’s metabolite levels—in particular, 6-thioguanine and its nucleotides (6-TG) and 6-methyl-mercaptopurine (6-MMP)—is more closely correlated with the likelihood that a particular dosage of a thiopurine drug could cause harm or prove ineffective.  As the Court explained today, however, “those in the field did not know the precise correlations between metabolite levels and the likely harm or ineffectiveness.”  This is where Prometheus stepped in.  “The patent claims at issue here set forth processes embodying researchers’ findings that identified those correlations with some precision.”  Prometheus contended that blood concentrations of 6-TG or of 6-MMP above 400 and 7,000 picomoles per 8x108 red blood cells, respectively, could be toxic, while a concentration of 6-TG metabolite less than 230 pmol per 8x108 red blood cells is likely too low to be effective. \n\n\nPrometheus utilized this correlation by patenting a three-step method by which one (i) administers a drug providing 6-TG to a patient with an autoimmune disease; (ii) determines the level of 6-TG in the patient; and (iii) the administrator then can determine whether the thiopurine dosage should be adjusted accordingly.  Significantly, Prometheus’s patents did not include a treatment protocol and thus applied regardless of whether a doctor actually altered his treatment decision in light of the test—in other words, even if the doctor thought the correlations were wrong, irrelevant, or inapplicable to a particular patient.  And in fact, Mayo Clinic, the party challenging Prometheus’s patents, believed Prometheus’s correlations were wrong.  (Mayo’s toxicity levels were 450 and 5700 pmol per 8x108 red blood cells for 6-TG and 6-MMP, respectively.  At oral argument on December 7, 2011, Mayo insisted that its numbers were “more accurate” than Prometheus’s.) \n\n\nTurning to the legal issues, both parties agreed that the correlations were “laws of nature,” which, by themselves, are not patentable.  As the Supreme Court has explained repeatedly, laws of nature, like natural phenomena and abstract ideas, are “manifestations of … nature, free to all men and reserved exclusively to none.”  This principle reflects a concern that patent law ought not inhibit further discovery and innovation by tying up the “basic tools of scientific and technological work.” \n\n\nIn contrast, the application of a law of nature is patentable.  The question for the Court, then, was whether Prometheus’s patent claims “add enough to their statements of correlations to allow the process they describe to qualify as patent-eligible processes that apply natural laws.” \n\n\nThe Court’s answer was no.  Distilled down, Prometheus’s “three steps simply tell doctors to gather data from which they may draw an inference in light of the correlations.”  The Court determined that Prometheus’s method simply informed the relevant audience (doctors treating patients with autoimmune diseases) about a law of nature, and that the additional steps of “administering” a drug and “determining” metabolite levels were “well-understood, routine, conventional activity already engaged in by the scientific community.”  “[T]he effect is simply to tell doctors to apply the law somehow when treating their patients.”   \n\n\nAlthough I leave it to Jeff & company to assess the impact of today’s decision on the practice of personalized medicine, I have two principal observations.  First, it appears that the Court was disturbed by Mayo’s insistence that the correlations in Prometheus’s patents were wrong, and that patent protection would prevent Mayo from improving upon them.  Towards the end of the opinion, Justice Breyer wrote that the patents “threaten to inhibit the development of more refined treatment recommendations (like that embodied in Mayo’s test), that combine Prometheus’s correlations with later discovered features of metabolites, human physiology or individual patient characteristics.”  The worry of stifling future innovation applies to every patent, but the Court seemed especially attuned to that concern here, perhaps due in part to Mayo’s insistence that its “better” test could not be used to help patients. \n\n\nSecond, Mayo argued that a decision in its favor would reduce the costs of challenging similar patents that purported to “apply” a natural law.  Mayo’s argument was in response to the position of the U.S. Government, which participated in the case as amicus curiae (“friend of the court”).  The Government urged the Court not to rule on the threshold issue of whether Prometheus’s patents applied a law of nature, but rather to strike down the patents because they lacked “novelty” or were “obvious in light of prior art.”  The questions of novelty and obviousness, Mayo argued, are much more fact-intensive and expensive to litigate.  Whether or not the Court agreed with Mayo’s argument, it declined to follow the Government’s advice.  To skip the threshold question, the Court concluded, “would make the ‘law of nature’ exception … a dead letter.” \n\n\nMany Supreme Court watchers will now turn their attention to another patent case that has been waiting in the wings, Association for Molecular Pathology v. Myriad Genetics, which asks the Court to decide whether human genes are patentable.  Predictions anyone?\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-20-supreme-court-unanimously-rules-against-personalized/",
    "title": "Supreme court unanimously rules against personalized medicine patent!",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-20",
    "categories": [],
    "contents": "\nJust a few minutes ago the Supreme Court released their decision in the Mayo case, see here for the Simply Statistics summary of the case. The court ruled unanimously that the personalized medicine test could not be patented. Such a strong ruling likely has major implications going forward for the field of personalized medicine. At the end of the day, this decision was based on an interpretation of statistical correlation. Stay tuned for a special in-depth analysis in the next couple of days that will get into the details of the ruling and the implications for personalized medicine. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-19-interview-with-amy-heineike-director-of-mathematics/",
    "title": "Interview with Amy Heineike - Director of Mathematics at Quid",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-19",
    "categories": [],
    "contents": "\n\n\n<div>\n  <strong>Amy Heineike<\/strong>\n<\/div>\n\n<div>\n  <strong><img src=\"http://media.tumblr.com/tumblr_m1588osxOV1r08wvg.jpg\" /><\/strong>\n<\/div>\n\n<div>\n  <strong><br /><\/strong>Amy Heineike is the Director of Mathematics at <a href=\"http://quid.com/\" target=\"_blank\">Quid<\/a>, a startup that seeks to understand technology development and dissemination through data analysis. She was the first employee at Quid, where she helped develop their technology early on. She has been recognized as one of the <a href=\"http://thephenomlist.com/lists/8/people/32\" target=\"_blank\">top Big Data Scientists<\/a>. As a part of our ongoing <a href=\"http://simplystatistics.tumblr.com/interviews\" target=\"_blank\">interview series<\/a> talked to Amy about data science, Quid, and how statisticians can get involved in the tech scene. \n<\/div>\n\n<div>\n  <strong><br /><\/strong>\n<\/div>\n\n<div>\n  <strong>Which term applies to you: data scientist, statistician, computer scientist, or something else?<\/strong>\n<\/div>\n\n\n\n\n\n\nData Scientist fits better than any, because it captures the mix of analytics, engineering and product management that is my current day to day.  \n\n\n\n\n\n\n\n\nWhen I started with Quid I was focused on R&D - developing the first prototypes of what are now our core analytics technologies, and working to define and QA new data streams.  This required the analysis of lots of unstructured data, like news articles and patent filings, as well as the end visualisation and communication of the results.  \n\n\n\n\n\n\n\n\nAfter we raised VC funding last year I switched to building our data science and engineering teams out.  These days I jump from conversations with the team about ideas for new analysis, to defining refinements to our data model, to questions about scalable architecture and filling out pivotal tracker tickets.  The core challenge is translating the vision for the product back to the team so they can build it.\n\n\n\n \n\n\n<div>\n  <div>\n    <strong> How did you end up at Quid?<\/strong>\n  <\/div>\n<\/div>\n\n\n\n\n\n\nIn my previous work I’d been building models to improve our understanding of complex human systems - in particular the complex interaction of cities and their transportation networks in order to evaluate the economic impacts of, Crossrail, a new train line across London, and the implications of social networks on public policy.  Through this work it became clear that data was the biggest constraint - I became fascinated by a quest to find usable data for these questions - and thats what led me to Silicon Valley.  I knew the founders of Quid from University, and approached them with the idea of analysing their data according to ideas I’d had - especially around network analysis - and the initial work we collaborated on became core to the founding techology of Quid.\n\n\n\n\n\n\n<div>\n  <div>\n  <\/div>\n<\/div>\n\n\n<div>\n  <div>\n    <strong>Who were really good mentors to you? What were the qualities that helped you? <\/strong>\n  <\/div>\n  \n  <div>\n  <\/div>\n<\/div>\n\n\n\nI’ve been fortunate to work with some brilliant people in my career so far.  While I still worked in London I worked closely with two behavioural economists - Paul Ormerod, who’s written some fantastic books on the subject (mostly recently Why Things Fail), and Bridget Rosewell, until recently the Chief Economist to the Greater London Authority (the city government for London).  At Quid I’ve had a very productive collaboration with Sean Gourley, our CTO.\n\n\n\n\n\n\n\n\nOne unifying characteristic of these three is their ability to communicate complex ideas in a powerful way to a broad audience.  Its an incredibly important skill, a core part of analytics work is taking the results to where they are needed which is often beyond those who know the technical details, to those who care about the implications first.\n\n\n\n\n\n\n \n\n\n<strong>How does Quid determine relationships between organizations and develop insight based on data? <\/strong>\n\n\n\n\n\n\nThe core questions our clients ask us are around how technology is changing and how this impacts their business.  Thats a really fascinating and huge question that requires not just discovering a document with the answer in it, but organizing lots and lots of pieces of data to paint a picture of the emergent change.  What we can offer is not only being able to find a snapshot of that, but also being able to track how it changes over time.\n\n\n\n\n\n\n\n\nWe organize the data firstly through the insight that much disruptive technology emerges in organizations, and that the events that occur between and to organizations are a fantastic way to signal both the traction of technologies and to observe strategic decision making by key actors.\n\n\n\n\n\n\n\n\nThe first kind of relationship thats important is of the transactional type, who is acquiring, funding or partnering with who, and the second is an estimate of the technological clustering of organizations, what trends do particular organizations represent.  Both of these can be discovered through documents about them, including in government filings, press releases and news, but requires analysis of unstructured natural language.  \n\n\n\n\n\n \n\n\nWe’ve experimented with some very engaging visualisations of the results, and have had particular success with network visualisations, which are a very powerful way of allowing people to interact with a large amount of data in a quite playful way.  You can see some of our analyses in the press links at http://quid.com/in-the-news.php\n\n\n\n\n\n\n<div>\n  <div>\n    <strong>What skills do you think are most important for statisticians/data scientists moving into the tech industry?<\/strong>\n  <\/div>\n<\/div>\n\n\n\n\n\n\nTechnical statistical chops are the foundation. You need to be able to take a dataset and discover and communicate what’s interesting about it for your users.  To turn this into a product requires understanding how to turn one-off analysis into something reliable enough to run day after day, even as the data evolves and grows, and as different users experience different aspects of it.  A key part of that is being willing to engage with questions about where the data comes from (how it can be collected, stored, processed and QAed on an ongoing basis), how the analytics will be run (how will it be tested, distributed and scaled) and how people interact with it (through visualisations, UI features or static presentations?).  \n\n\n\n\n\n\n\n\nFor your ideas to become great products, you need to become part of a great team though!  One of the reasons that such a broad set of skills are associated with Data Science is that there are a lot of pieces that have to come together for it to all work out - and it really takes a team to pull it off.  Generally speaking, the earlier stage the company that you join, the broader the range of skills you need, and the more scrappy you need to be about getting involved in whatever needs to be done.  Later stage teams, and big tech companies may have roles that are purer statistics.\n\n\n\n\n\n\n \n\n\n<div>\n  <div>\n    <strong>Do you have any advice for grad students in statistics/biostatistics on how to get involved in the start-up community or how to find a job at a start-up? <\/strong>\n  <\/div>\n<\/div>\n\n\n\n\n\n\nThere is a real opportunity for people who have good statistical and computational skills to get into the startup and tech scenes now.  Many people in Data Science roles have statistics and biostatistics backgrounds, so you shouldn’t find it hard to find kindred spirits.\n\n\n\n\n\n\nWe&#8217;ve always been especially impressed with people who have built software in a group and shared or distributed that software in some way.  Getting involved in an open source project, working with version control in a team, or sharing your code on github are all good ways to start on this.\n\n\n\n\n\n\n\n\n\nIts really important to be able to show that you want to build products though.  Imagine the clients or users of the company and see if you get excited about building something that they will use.  Reach out to people in the tech scene, explore who’s posting jobs - and then be able to explain to them what it is you’ve done and why its relevant, and be able to think about their business and how you’d want to help contribute towards it.  Many companies offer internships, which could be a good way to contribute for a short period and find out if its a good fit for you.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-18-sunday-data-statistics-link-roundup-3-18/",
    "title": "Sunday data/statistics link roundup (3/18)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-18",
    "categories": [],
    "contents": "\nA really interesting proposal by Rafa (in Spanish - we’ll get on him to write a translation) for the University of Puerto Rico. The post concerns changing the focus from simply teaching to creating knowledge and the potential benefits to both the university and to Puerto Rico. It also has a really nice summary of the benefits that the university system in the United States has produced. Definitely worth a read. The comments are also interesting, it looks like Rafa’s post is pretty controversial…\nAn interesting article suggesting that the Challenger Space Shuttle disaster was at least in part due to bad data visualization. Via @DatainColour\nThe Snyderome is getting a lot of attention in genomics circles. He used as many new technologies as he could to measure a huge amount of molecular information about his body over time. I am really on board with the excitement about measurement technologies, but this poses a huge challenge for statistics and and statistical literacy. If this kind of thing becomes commonplace, the potential for false positives and ghost diagnoses is huge without a really good framework for uncertainty. Via Peter S. \nMore news about the Nike API. Now that is how to unveil some data! \nAdd the Nike API to the list of potential statistics projects for students. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-16-the-unreasonable-effectiveness-of-data-a-talk/",
    "title": "Peter Norvig on the \"Unreasonable Effectiveness of Data\"",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-16",
    "categories": [],
    "contents": "\n“The Unreasonable Effectiveness of Data”, a talk by Peter Norvig of Google. Sometimes, more data is more better. (Thanks to John C. for the link.)\n\n(Source: http://www.youtube.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-14-a-proposal-for-a-really-fast-statistics-journal/",
    "title": "A proposal for a really fast statistics journal",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-14",
    "categories": [],
    "contents": "\nI know we need a new journal like we need a good poke in the eye. But I got fired up by the recent discussion of open science (by Paul Krugman and others) and the seriously misguided Research Works Act- that aimed to make it illegal to deposit published papers funded by the government in Pubmed central or other open access databases.\n\nI also realized that I spend a huge amount of time/effort on the following things: (1) waiting for reviews (typically months), (2) addressing reviewer comments that are unrelated to the accuracy of my work - just adding citations to referees papers or doing additional simulations, and (3) resubmitting rejected papers to new journals - this is a huge time suck since I have to reformat, etc. Furthermore, If I want my papers to be published open-access I also realized I have to pay at minimum $1,000 per paper. \n\n\nSo I thought up my criteria for an ideal statistics journal. It would be accurate, have fast review times, and not discriminate based on how interesting an idea is. I have found that my most interesting ideas are the hardest ones to get published.  This journal would:\n\n\nBe open-access and free to publish your papers there. You own the copyright on your work. \n\n\nThe criteria for publication would be: (1) it has to do with statistics, computation, or data analysis, (2) is the work is technically correct. \n\n\nWe would accept manuals, reports of new statistical software, and full length research articles. \n\n\nThere would be no page limits/figure limits. \n\n\nThe journal would be published exclusively online. \n\n\nWe would guarantee reviews within 1 week and publication immediately upon review if criteria (1) and (2) are satisfied\n\n\nPapers would receive a star rating from the editor - 0-5 stars. There would be a place for readers to also review articles\n\n\nAll articles would be published with a tweet/like button so they can be easily distributed\n\n\n\n\n\nTo achieve such a fast review time, here is how it would work. We would have a large group of Associate Editors (hopefully 30 or more). When a paper was received, it would be assigned to an AE. The AEs would agree to referee papers within 2 days. They would use a form like this:\n\n\n\n\n\n\nReview of: Jeff’s Paper\n\n\nTechnically Correct: Yes\n\n\nAbout statistics/computation/data analysis: Yes\n\n\nNumber of Stars: 3 stars\n\n\n\n3 Strengths of Paper (1 required): \n\n\nThis paper revolutionizes statistics \n\n\n\n3 Weakness of Paper (1 required): \n\n\nThe proof that this paper revolutionizes statistics is pretty weak\n\n    <li>\n      because he only includes one example.\n    <\/li>\n  <\/ul><\/blockquote> \n\n  <div>\n  <\/div>\n\n  <div>\n    That&#8217;s it, super quick, super simple, so it wouldn&#8217;t be hard to referee. As long as the answers to the first two questions were yes, it would be published. \n  <\/div>\n\n  <div>\n  <\/div>\n\n  <div>\n    So now here&#8217;s my questions: \n  <\/div>\n\n  <div>\n  <\/div>\n\n  <div>\n    <ol>\n      <li>\n        Would you ever consider submitting a paper to such a journal?\n      <\/li>\n      <li>\n        Would you be willing to be one of the AEs for such a journal? \n      <\/li>\n      <li>\n        Is there anything you would change? \n      <\/li>\n    <\/ol>\n  <\/div>\n\n  <div>\n  <\/div><\/div>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-13-frighteningly-ambitious-startup-ideas/",
    "title": "Frighteningly Ambitious Startup Ideas",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-13",
    "categories": [],
    "contents": "\nFrighteningly Ambitious Startup Ideas\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-12-answers-in-medicine-sometimes-lie-in-luck/",
    "title": "Answers in Medicine Sometimes Lie in Luck",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-12",
    "categories": [],
    "contents": "\nAnswers in Medicine Sometimes Lie in Luck\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-12-sunday-data-statistics-link-roundup-3-11/",
    "title": "Sunday Data/Statistics Link Roundup (3/11)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-12",
    "categories": [],
    "contents": "\nThis is the big one. ESPN has opened up access to their API! It looks like there may only be access to some of the data for the general public though, does anyone know more? \nLooks like ESPN isn’t the only sports-related organization in the API mood, Nike plans to open up an API too. It would be great if they had better access to individual, downloadable data. \nVia Leonid K.: a highly influential psychology study failed to replicate in a study published in PLoS One. The author of the original study went off on the author of the paper, on PLoS One, and on the reporter who broke the story (including personal attacks!). It looks like the authors of the PLoS One paper actually did a more careful study than the original authors to me. The authors of the PLoS One paper, the reporter, and the editor of PLoS One all replied in a much more reasonable way. See this excellent summary for all the details. Here are a few choice quotes from the comments: \n\n1. But there’s a long tradition in social psychology of experiments as parables,\n2. I’d love to write a really long response, but let’s just say: priming methods like these fail to replicate all the time (frequently in my own studies), and the news that one of Bargh’s studies failed to replicate is not surprising to me at all.\n3. This distinction between direct and conceptual replication helps to explain why a psychologist isn’t particularly concerned whether Bargh’s finding replicates or not.\n\n\nD.  Reproducible != Replicable in scientific research. But Roger’s perspective on reproducible research still seems appropriate here. \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-11-cost-of-gene-sequencing-falls-raising-hopes-for/",
    "title": "Cost of Gene Sequencing Falls, Raising Hopes for Medical Advances",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-11",
    "categories": [],
    "contents": "\nCost of Gene Sequencing Falls, Raising Hopes for Medical Advances\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-10-ibms-watson-gets-wall-street-job-after-jeopardy-win/",
    "title": "IBM’s Watson Gets Wall Street Job After ‘Jeopardy’ Win",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-10",
    "categories": [],
    "contents": "\nIBM’s Watson Gets Wall Street Job After ‘Jeopardy’ Win\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-09-mission-control-built-for-cities/",
    "title": "Mission Control, Built for Cities",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-09",
    "categories": [],
    "contents": "\nMission Control, Built for Cities\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-08-a-plot-of-my-citations-in-google-scholar-vs-web-of/",
    "title": "A plot of my citations in Google Scholar vs. Web of Science",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-08",
    "categories": [],
    "contents": "\nThere has been some discussion about whether Google Scholar or one of the proprietary software companies numbers are better for citation counts. I personally think Google Scholar is better for a number of reasons:\nHigher numbers, but consistently/adjustably higher \nIt’s free and the data are openly available. \nIt covers more ground (patents, theses, etc.) to give a better idea of global impact\nIt’s easier to use\nI haven’t seen a plot yet relating Web of Science citations to Google Scholar citations, so I made one for my papers.\n\nGS has about 41% more citations per paper than Web of Science. That is consistent with what other people have found. It also looks reasonably linearish. I wonder what other people’s plots would look like? \nHere is the R code I used to generate the plot (the names are Pubmed IDs for the papers):\n\nlibrary(ggplot2)\nnames = c(16141318,16357033,16928955,17597765,17907809,19033188,19276151,19924215,20560929,20701754,20838408, 21186247,21685414,21747377,21931541,22031444,22087737,22096506,22257669) \ny = c(287,122,84,39,120,53,4,52,6,33,57,0,0,4,1,5,0,2,0)\nx = c(200,92,48,31,79,29,4,51,2,18,44,0,0,1,0,2,0,1,0)\nYear = c(2005,2006,2007,2007,2007,2008,2009,2009,2011,2010,2010,2011,2012,2011,2011,2011,2011,2011,2012)\n\n\nq <- qplot(x,y,xlim=c(-20,300),ylim=c(-20,300),xlab=”Web of Knowledge”,ylab=”Google Scholar”) + geom_point(aes(colour=Year),size=5) + geom_line(aes(x = y, y = y),size=2)\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-07-r-a-fisher-is-the-most-influential-scientist-ever/",
    "title": "R.A. Fisher is the most influential scientist ever",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-07",
    "categories": [],
    "contents": "\nYou can now see profiles of famous scientists on Google Scholar citations. Here are links to a few of them (via Ben L.). Von Neumann, Einstein, Newton, Feynman\nBut their impact on science pales in comparison (with the possible exception of Newton) to the impact of one statistician: R.A. Fisher. Many of the concepts he developed are so common and are considered so standard, that he is never cited/credited. Here are some examples of things he invented along with a conservative number of citations they would have received calculated via Google Scholar*. \nP-values - 3 million citations\nAnalysis of variance (ANOVA) - 1.57 million citations\nMaximum likelihood estimation - 1.54 million citations\nFisher’s linear discriminant 62,400 citations\nRandomization/permutation tests 37,940 citations\nGenetic linkage analysis 298,000 citations\nFisher information 57,000 citations\nFisher’s exact test 237,000 citations\nA couple of notes:\nThese are seriously conservative estimates, since I only searched for a few variants on some key words\nThese numbers are BIG, there isn’t another scientist in the ballpark. The guy who wrote the “most highly cited paper” got 228,441 citations on GS. His next most cited paper? 3,000 citations. Fisher has at least 5 papers with more citations than his best one. \nThis page says Bert Vogelstein has the most citations of any person over the last 30 years. If you add up the number of citations to his top 8 papers on GS, you get 57,418. About as many as to the Fisher information matrix. \nI think this really speaks to a couple of things. One is that Fisher invented some of the most critical concepts in statistics. The other is the breadth of impact of statistical ideas across a range of disciplines. In any case, I would be hard pressed to think of another scientist who has influenced a greater range or depth of scientists with their work. \nCalculations of citations #####################\nAs described in a previous post\n# of GS results for “Analysis of Variance” + # for “ANOVA” - “Analysis of Variance”\n# of GS results for “maximum likelihood”\n# of GS results for “linear discriminant”\n# of GS results for “permutation test” + # for ”permutation tests” - “permutation test”\n# of GS results for “linkage analysis”\n# of GS results for “fisher information” + # for “information matrix” - “fisher information”\n# of GS results for “fisher’s exact test” + # for “fisher exact test” - “fisher’s exact test”\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-06-are-banks-being-sidelined-by-retailers-data/",
    "title": "Are banks being sidelined by retailers' data collection?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-06",
    "categories": [],
    "contents": "\nAre banks being sidelined by retailers’ data collection?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-05-characteristics-of-my-favorite-statistics-talks/",
    "title": "Characteristics of my favorite statistics talks",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-05",
    "categories": [],
    "contents": "\nI’ve been going to/giving statistics talks for a few years now. I think everyone in our field has an opinion on the best structure/content/delivery of a talk. I am one of those people that has a pretty specific idea of what makes an amazing talk. Here are a few of the things I think are key, I try to do them and have learned many of these things from other people who I’ve seen speak. I’d love to hear what other people think. \nStructure\nI don’t like outline slides. I think they take up space but don’t add to most talks. Instead I love it when talks start with a specific, concrete, unsolved problem. In my favorite talks, this problem is usually scientific/applied. Although I have also seen great theoretical talks where a person starts with a key and unsolved theoretical problem. \nI like it when the statistical model is defined to solve the problem in the beginning, so it is easy to see the connection between the model and the purpose of the model. \nI love it when talks end by showing how they solved the problem they described at the very beginning of the talk. \nContent\nI like it when people assume I’m pretty ignorant about their problem (I usually am) and explain everything in very simple language. I think some people worry about their research looking too trivial. I have almost never come away from a talk thinking that, but I frequently leave talks confused because the background material wasn’t clear. \nI like it when talks cover enough technical detail so I can follow the basic algorithm, but not so much that I get lost in notation. I also struggle when talks go off on tangents, describing too many subproblems, rather than focusing on the main problem in the talk and just mentioning subproblems succinctly. \nI like it when proposed methods are compared to the obvious straw man and one legitimate competitor (if it exists) on a realistic simulation/data set where the answer is known. \nI love it when people give talks on work that isn’t totally finished. This type of talk is scary for two reasons: (1) you can be scooped and (2) you might not have all the answers. But I find that unfinished work leads to way more discussion/ideas than a talk about work that has been published and is “complete”. \nDelivery\nI like it when a talk runs short. I have never been disappointed when a talk ended 10-15 min early. On the other hand, when a talk is long, I almost always lose focus and don’t follow the last part. I’d love it if we moved to 30 minute seminars with more questions. \nI like it when speakers have prepared their slides and they have a clear flow and don’t get bogged down in transitions. For this reason, I don’t mind it when people give the same talk a bunch of places. I usually find that the talk is very polished.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-04-dealbook-roche-extends-deadline-for-illumina-offer/",
    "title": "DealBook: Roche Extends Deadline for Illumina Offer",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-04",
    "categories": [],
    "contents": "\nDealBook: Roche Extends Deadline for Illumina Offer\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-04-sunday-data-statistics-link-roundup-3-4/",
    "title": "Sunday data/statistics link roundup (3/4)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-04",
    "categories": [],
    "contents": "\nA cool article on Github by the folks at Wired. I’m starting to think the fact that I’m not on Github is a serious dent in my nerd cred. \nDatawrapper - a less intensive, but less flexible open source data visualization creator. I have seen a few of these types of services starting to pop up. I think that some statistics training should be mandatory before people use them. \nAn interesting blog post with the provocative title, “Why bother publishing in a journal” The story he describes works best if you have a lot of people who are interested in reading what you put on the internet. \nA post on stackexchange comparing the machine learning and statistics cultures. \nStackoverflow is a great place to look for R answers. It is the R mailing list, minus the flames…\nRoger’s posts on Beijing air pollution are worth another read if you missed them. Particularly this one, where he computes the cigarette equivalent of the air pollution levels. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-03-true-innovation/",
    "title": "True Innovation",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-03",
    "categories": [],
    "contents": "\nTrue Innovation\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-02-an-essay-on-why-programmers-need-to-learn-statistics/",
    "title": "An essay on why programmers need to learn statistics",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-02",
    "categories": [],
    "contents": "\nThis is awesome. There are a few places with some strong language, but overall I think the message is pretty powerful. Via Tariq K. I agree with Tariq, one of the gems is:\n\nIf you want to measure something, then don’t measure other sh**. \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-02-confronting-a-law-of-limits/",
    "title": "Confronting a Law Of Limits",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-02",
    "categories": [],
    "contents": "\nConfronting a Law Of Limits\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-03-01-a-cool-profile-of-a-human-rights-statistician/",
    "title": "A cool profile of a human rights statistician",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-03-01",
    "categories": [],
    "contents": "\nVia AL Daily, this dude collects data and analyzes it to put war criminals away. The idea of using statistics to quantify mass testimony is interesting. \n\nWith statistical methods and the right kind of data, he can make what we know tell us what we don’t know. He has shown human rights groups, truth commissions, and international courts how to take a collection of thousands of testimonies and extract from them the magnitude and pattern of violence — to lift the fog of war.\n\nSo how does he do it? With an idea from statistical ecology. This is a bit of a long quote but describes the key bit.\n\nWorking on the Guatemalan data, Ball found the answer. He called Fritz Scheuren, a statistician with a long history of involvement in human rights projects. Scheuren reminded Ball that a solution to exactly this problem had been invented in the 19th century to count wildlife. “If you want to find out how many fish are in the pond, you can drain the pond and count them,” Scheuren explained, “but they’ll all be dead. Or you can fish, tag the fish you catch, and throw them back. Then you go another day and fish again. You count how many fish you caught the first day, and the second day, and the number of overlaps.”\nThe number of overlaps is key. It tells you how representative a sample is. From the overlap, you can calculate how many fish are in the whole pond. (The actual formula is this: Multiply the number of fish caught the first day by the number caught the second day. Divide the total by the overlap. That’s roughly how many fish are really in the pond.) It gets more accurate if you can fish not just twice, but many more times — then you can measure the overlap between every pair of days.\nGuatemala had three different collections of human rights testimonies about what had happened during the country’s long, bloody civil war: from the U.N. truth commission, the Catholic Church’s truth commission, and the International Center for Research on Human Rights, an organization that worked with Guatemala’s human rights groups. Working for the official truth commission, Ball used the count-the-fish method, called multiple systems estimation (MSE), to compare all three databases. He found that over the time covered by the commission’s mandate, from 1978 to 1996, 132,000 people were killed (not counting those disappeared), and that government forces committed 95.4 percent of the killings. He was also able to calculate killings by the ethnicity of the victim. Between 1981 and 1983, 8 percent of the nonindigenous population of the Ixil region was assassinated; in the Rabinal region, the figure was around 2 percent. In both those regions, though, more than 40 percent of the Mayan population was assassinated.\n\nCool right? The article is worth a read. If you are inspired, check out Data Without Borders. \n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-29-gulf-on-open-access-to-federally-financed-research/",
    "title": "Gulf on Open Access to Federally Financed Research",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-29",
    "categories": [],
    "contents": "\nGulf on Open Access to Federally Financed Research\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-29-statistics-project-ideas-for-students/",
    "title": "Statistics project ideas for students",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-29",
    "categories": [],
    "contents": "\nHere are a few ideas that might make for interesting student projects at all levels (from high-school to graduate school). I’d welcome ideas/suggestions/additions to the list as well. All of these ideas depend on free or scraped data, which means that anyone can work on them. I’ve given a ballpark difficulty for each project to give people some idea.\nHappy data crunching!\nData Collection/Synthesis\nCreating a webpage that explains conceptual statistical issues like randomization, margin of error, overfitting, cross-validation, concepts in data visualization, sampling. The webpage should not use any math at all and should explain the concepts so a general audience could understand. Bonus points if you make short 30 second animated youtube clips that explain the concepts. (Difficulty: Lowish; Effort: Highish)\nBuilding an aggregator for statistics papers across disciplines that can be the central resource for statisticians. Journals ranging from PLoS Genetics to Neuroimage now routinely publish statistical papers. But there is no one central resource that aggregates all the statistics papers published across disciplines. Such a resource would be hugely useful to statisticians. You could build it using blogging software like WordPress so articles could be tagged/you could put the resource in your RSS feeder. (Difficulty: Lowish; Effort: Mediumish)\nData Analyses\nScrape the LivingSocial/Groupon sites for the daily deals and develop a prediction of how successful the deal will be based on location/price/type of deal. You could use either the RCurl R package or the XML R package to scrape the data. (Difficulty: Mediumish; Effort: Mediumish)\nYou could use the data from your city (here are a few cities with open data) to: (a) identify the best and worst neighborhoods to live in based on different metrics like how many parks are within walking distance, crime statistics, etc. (b) identify concrete measures your city could take to improve different quality of life metrics like those described above - say where should the city put a park, or (c) see if you can predict when/where crimes will occur (like these guys did). (Difficulty: Mediumish; Effort: Highish)\nDownload data on state of the union speeches from here and use the tm package in R to analyze the patterns of word use over time (Difficulty: Lowish; Effort: Lowish)\nUse this data set from Donors Choose to determine the characteristics that make the funding of projects more likely. You could send your results to the Donors Choose folks to help them improve the funding rate for their projects. (Difficulty: Mediumish; Effort: Mediumish) \nWhich basketball player would you want on your team? Here is a really simple analysis done by Rafa. But it doesn’t take into account things like defense. If you want to take on this project, you should take a look at this Denis Rodman analysis which is the gold standard. (Difficulty: Mediumish; Effort: Highish).\nData visualization\nCreating an R package that wraps the svgAnnotation package. This package can be used to create dynamic graphics in R, but is still a bit too flexible for most people to use. Writing some wrapper functions that simplify the interface would be potentially high impact. Maybe something like svgPlot() to create simple, dynamic graphics with only a few options (Difficulty: Mediumish; Effort: Mediumish). \nThe same as project 1 but for D3.js. The impact could potentially be a bit higher, since the graphics are a bit more professional, but the level of difficulty and effort would also both be higher. (Difficulty: Highish; Effort: Highish)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-29-the-case-for-open-computer-programs/",
    "title": "The case for open computer programs",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-29",
    "categories": [],
    "contents": "\nThe case for open computer programs\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-28-duke-taking-new-steps-to-safeguard-research-integrity/",
    "title": "Duke Taking New Steps to Safeguard Research Integrity",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-28",
    "categories": [],
    "contents": "\nDuke Taking New Steps to Safeguard Research Integrity\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-27-graham-dodds-security-analysis-moneyball/",
    "title": "Graham &#038; Dodd's Security Analysis: Moneyball for...Money",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-27",
    "categories": [],
    "contents": "\nThe last time I posted something about finance I got schooled by people who actually know stuff. So let me just say that I don’t claim to be an expert in this area, but I do have an interest in it and try to keep up the best I can.\nOne book I picked up a little while ago was Security Analysis by Benjamin Graham and David Dodd. This is the “bible of value investing” and so I mostly wanted to see what all the hubbub was about. In my mind, the hubbub is well-deserved. Given that it was originally written in 1934, the book has stood the test of time (the book has been updated a number of times since then). It’s quite readable and, I guess, still relevant to modern-day investing. In the 6th edition the out-of-date stuff has been relegated to an appendix. It also contains little essays (of varying quality) by modern-day value investing heros like Seth Klarman and Glenn Greenberg. It’s a heavy book though and I’m wishing I’d got it on the Kindle.\nIt occurred to me that with all the interest in data and analytics today, Security Analysis reads a lot like the Moneyball of investing. The two books make the same general point: find things that are underpriced/underappreciated and buy them when no one’s looking. Then profit!\nOne of the basic points made early on is that roughly speaking, you can’t judge a security by its cover. You need to look at the data. How novel! For example, at the time bonds were considered safe because they were bonds, while stocks (equity) were considered risky because they were stocks. There are technical reasons why this is true, but a careful look at the data might reveal that the bonds of one company are risky while the stock is safe, depending on the price at which they are trading. The question to ask for either type of security is what’s the chance of losing money? In order to answer that question you need to estimate the intrinsic value of the company. For that, you need data.\n\nThe functions of security analysis may be described under three headings: descriptive, selective, and critical. In its more obvious form, descriptive analysis consists of marshalling the important facts relating to the issue [security] and presenting them in a coherent, readily intelligible manner…. A more penetrating type of description seeks to reveal the strong and weak points in the position of an issue, compare its exhibit with that of others of similar character, and appraise the factors which are likely to influence its future performance. Analysis of this kind is applicable to almost every corporate issue, and it may be regarded as an adjunct not only to investment but also to intelligent speculation in that it provides an organized factual basis for the application of judgment.\n\nBack in Graham & Dodd’s day it must have been quite a bit harder to get the data. Many financial reports that are routinely published today by public companies were not available back then. Today, we are awash in easily accessible financial data and, one might argue as a result of that, there are fewer opportunities to make money. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-27-the-duke-saga-starter-set/",
    "title": "The Duke Saga Starter Set",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-27",
    "categories": [],
    "contents": "\nA few of our recent posts relate to the Duke trial saga. For those that want to learn more, Baggerly and Coombes have put together a “starter set”. It includes\na video of one of their talks\nthe 60 Minutes episode and clip\nslides from a recent presentation with some new details\ntheir  Annals of Applied Statistics paper\n5. a editorial they wrote for Clinical Chemistry about what information should be required to support  clinical “omics” publications (gated)\n6. links to the IOM session recordings and slides\nEnjoy!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-25-waterbillwoman-pestered-city-for-years-over-faulty/",
    "title": "'WaterBillWoman' pestered city for years over faulty bills",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-25",
    "categories": [],
    "contents": "\n‘WaterBillWoman’ pestered city for years over faulty bills\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-23-monitoring-your-health-with-mobile-devices/",
    "title": "Monitoring Your Health With Mobile Devices",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-23",
    "categories": [],
    "contents": "\nMonitoring Your Health With Mobile Devices\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-23-prediction-the-lasso-vs-just-using-the-top-10/",
    "title": "Prediction: the Lasso vs. just using the top 10 predictors",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-23",
    "categories": [],
    "contents": "\nOne incredibly popular tool for the analysis of high-dimensional data is the lasso. The lasso is commonly used in cases when you have many more predictors than independent samples (the n « p) problem. It is also often used in the context of prediction.\nSuppose you have an outcome Y and several predictors X1,…,XM, the lasso fits a model:\nY = B + B1 X1 + B2 X2 + … + BM XM + E\nsubject to a constraint on the sum of the absolute value of the B coefficients. The result is that: (1) some of the coefficients get set to zero, and those variables drop out of the model, (2) other coefficients are “shrunk” toward zero. Dropping some variables is good because there are a lot of potentially unimportant variables. Shrinking coefficients may be good, since the big coefficients might be just the ones that were really big by random chance (this is related to Andrew Gelman’s type M errors).\nI work in genomics, where n«p problems come up all the time. Whenever I use the lasso or when I read papers where the lasso is used for prediction, I always think: “How does this compare to just using the top 10 most significant predictors?” I have asked this out loud enough that some people around here started calling it the “Leekasso” to poke fun at me. So I’m going to call it that in a thinly veiled attempt to avoid Stigler’s law of eponymy (actually Rafa points out that using this name is a perfect example of this law, since this feature selection approach has been proposed before at least once).\nHere is how the Leekasso works. You fit each of the models:\nY = B + BkXk + E\ntake the 10 variables with the smallest p-values from testing the kcoefficients, then fit a linear model with just those 10 coefficients. You never use 9 or 11, the Leekasso is always 10.\nFor fun I did an experiment to compare the accuracy of the Leekasso and the Lasso.\nHere is the setup:\nI simulated 500 variables and 100 samples for each study, each N(0,1)\nI created an outcome that was 0 for the first 50 samples, 1 for the last 50\nI set a certain number of variables (between 5 and 50) to be associated with the outcome using the model with independent effects (this is an important choice, more later in the post)\nI tried different levels of signal to the truly predictive features\nI generated two data sets (training and test) from the exact same model for each scenario\nI fit the Lasso using the lars package, choosing the shrinkage parameter as the value that minimized the cross-validation MSE in the training set\nI fit the Leekasso and the Lasso on the training sets and evaluated accuracy on the test sets.\nThe R code for this analysis is available here and the resulting data is here.\nThe results show that for all configurations, using the top 10 has a higher out of sample prediction accuracy than the lasso. A larger version of the plot is here.\n\nInterestingly, this is true even when there are fewer than 10 real features in the data or when there are many more than 10 real features ((remember the Leekasso always picks 10).\nSome thoughts on this analysis:\nThis is only test-set prediction accuracy, it says nothing about selecting the “right” features for prediction.\nThe Leekasso took about 0.03 seconds to fit and test per data set compared to about 5.61 seconds for the Lasso.\nThe data generating model is the model underlying the top 10, so it isn’t surprising it has higher performance. Note that I simulated from the model: Xi = b0i + b1iY + e, this is the model commonly assumed in differential expression analysis (genomics) or voxel-wise analysis (fMRI). Alternatively I could have simulated from the model: Y = B + B1 X1 + B2 X2 + … + BM XM + E, where most of the coefficients are zero. In this case, the Lasso would outperform the top 10 (data not shown). This is a key, and possibly obvious, issue raised by this simulation. When doing prediction differences in the true “causal” model matter a lot. So if we believe the “top 10 model” holds in many high-dimensional settings, then it may be the case that regularization approaches don’t work well for prediction and vice versa.\nI think what may be happening is that the Lasso is overshrinking the parameter estimates, in other words, you give up too much bias for a gain in variance. Alan Dabney and John Storey have a really nice paper discussing shrinkage in the context of genomic prediction that I think is related.\n**\n**\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-22-air-pollution-linked-to-heart-and-brain-risks/",
    "title": "Air Pollution Linked to Heart and Brain Risks",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-22",
    "categories": [],
    "contents": "\nAir Pollution Linked to Heart and Brain Risks\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-22-professional-statisticians-agree-the-knicks-should/",
    "title": "Professional statisticians agree: the knicks should start Steve Novak over Carmelo Anthony",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-22",
    "categories": [],
    "contents": "\nA week ago, Nate Silver tweeted this:\n\nSince Lin became starting PG, Knicks have outscored opponents by 63 with Novak on the floor. Been outscored by 8 when he isn’t.\n\nIn a previous post we showed the plot below. Note that Carmelo Anthony is in ball hog territory. Novak plays the same position as Anthony but is a three point specialist. His career three point FG% of 42% (253-603) puts him 10th all time! It seems that with Lin in the lineup he is getting more open shots and helping his team. Should the Knicks start Novak? \nHat tip to David Santiago.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-21-interracial-couples-who-make-the-most-money/",
    "title": "Interracial Couples Who Make the Most Money",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-21",
    "categories": [],
    "contents": "\nInterracial Couples Who Make the Most Money\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-21-scientists-find-new-dangers-in-tiny-but-pervasive/",
    "title": "Scientists Find New Dangers in Tiny but Pervasive Particles in Air Pollution",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-21",
    "categories": [],
    "contents": "\nScientists Find New Dangers in Tiny but Pervasive Particles in Air Pollution\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-20-60-lives-30-kidneys-all-linked/",
    "title": "60 Lives, 30 Kidneys, All Linked",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-20",
    "categories": [],
    "contents": "\n60 Lives, 30 Kidneys, All Linked\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-20-company-unveils-dna-sequencing-device-meant-to-be/",
    "title": "Company Unveils DNA Sequencing Device Meant to Be Portable, Disposable and Cheap",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-20",
    "categories": [],
    "contents": "\nCompany Unveils DNA Sequencing Device Meant to Be Portable, Disposable and Cheap\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-20-i-dont-think-it-means-what-espn-thinks-it-means/",
    "title": "I don't think it means what ESPN thinks it means",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-20",
    "categories": [],
    "contents": "\n\nGiven ESPN’s recent headline difficulties it seems like they might want a headline editor or something…\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-16-how-companies-learn-your-secrets/",
    "title": "How Companies Learn Your Secrets",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-16",
    "categories": [],
    "contents": "\nHow Companies Learn Your Secrets\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-16-i-b-m-big-data-bigger-patterns/",
    "title": "I.B.M.: Big Data, Bigger Patterns",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-16",
    "categories": [],
    "contents": "\nI.B.M.: Big Data, Bigger Patterns\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:54:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-15-a-flat-budget-for-nih-in-2013-scienceinsider/",
    "title": "A Flat Budget for NIH in 2013 - ScienceInsider",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-15",
    "categories": [],
    "contents": "\nA Flat Budget for NIH in 2013 - ScienceInsider\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-15-harvards-stat-110-is-now-a-course-on-itunes/",
    "title": "Harvard's Stat 110 is now a course on iTunes",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-15",
    "categories": [],
    "contents": "\nBack in January we interviewed Joe Blitzstein and pointed out that he made his lectures freely available on iTunes. Now it is a course on iTunes and the format has been upgraded to work better with iPhones and iPads. Enjoy! \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-14-mathematicians-organize-boycott-of-a-publisher/",
    "title": "Mathematicians Organize Boycott of a Publisher",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-14",
    "categories": [],
    "contents": "\nMathematicians Organize Boycott of a Publisher\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-14-mortimer-spiegelman-award-call-for-nominations/",
    "title": "Mortimer Spiegelman Award: Call for Nominations. Deadline is April 1, 2012",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-14",
    "categories": [],
    "contents": "\nThe Statistics Section of the American Public Health Associationinvites nominations for the 2012 Mortimer Spiegelman Award honoring astatistician aged 40 or younger who has made outstanding contributionsto health statistics, especially public health statistics.\nThe award was established in 1970 and is presented annually at theAPHA meeting. The award serves three purposes: to honor theoutstanding achievements of both the recipient and Spiegelman, toencourage further involvement in public health of the finest youngstatisticians, and to increase awareness of APHA and the StatisticsSection in the academic statistical community. More details about theaward including the list of the past recipients and more informationabout the Statistics Section of APHA may be found here.\nTo be eligible for the 2012 Spiegelman Award, a candidate must havebeen born in 1972 or later.  Please send electronic versions of thenominating letter and the candidate’s CV to the 2012 Spiegelman AwardCommittee Chair, Rafael A. Irizarry rafa@jhu.edu.\nPlease state in the nominating letter the candidate’s birthday. Thenominator should include one or two paragraphs in the nominatingletter that describe how the nominee’s contributions relate to publichealth concerns. A maximum of three supporting letters per nominationcan be provided. Nominations for the 2012 Award must be submitted byApril 1, 2012.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-13-at-msnbc-a-professor-as-tv-host/",
    "title": "At MSNBC, a Professor as TV Host",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-13",
    "categories": [],
    "contents": "\nAt MSNBC, a Professor as TV Host\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-13-duke-clinical-trials-saga-on-60-minutes-first/",
    "title": "Untitled",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-13",
    "categories": [],
    "contents": "\nDuke clinical trials saga on 60 Minutes. First, the back-to-back shot of Keith and Kevin is priceless. Second, I’ve never seen a cleaner desk in my life.\n\n(Source: http://cnettv.cnet.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-13-the-duke-clinical-trials-saga-what-really-happened/",
    "title": "The Duke Clinical Trials Saga - What Really Happened",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-13",
    "categories": [],
    "contents": "\nThe Duke Clinical Trials Saga: What Really Happened\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-12T08:07:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-12-sunday-data-statistics-link-roundup-2-12/",
    "title": "Sunday Data/Statistics Link Roundup (2/12)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-12",
    "categories": [],
    "contents": "\nAn awesome alternative to D3.js - R’s svgAnnotation package. Here’s the paper in JSS. I feel like this is one step away from gaining broad use in the statistics community - it still feels a little complicated building the graphics, but there is plenty of flexibility there. I feel like a great project for a student at any level would be writing some easy wrapper functions for these functions. \nHow to run R on your Android device. This is very cool - can’t wait to start running simulations on my Nexus S.\nInteractive word clouds via John C. and why word clouds may be dangerous via Jason D. \nTrends in APIs - there are more of them! Go get your free data. \nA really interesting paper by Gary King on how to get a paper by exactly replicating, then building on or discussing, the results of a previous publication. \n25 minute seminars - I love this post by Rafa, probably because my attention span is so short. But I think 25-30 minute talks are optimal for me to learn something, but not start to zone out…\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-12-the-age-of-big-data/",
    "title": "The Age of Big Data",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-12",
    "categories": [],
    "contents": "\nThe Age of Big Data\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-11-data-says-jeremy-lin-is-for-real/",
    "title": "Data says Jeremy Lin is for real",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-11",
    "categories": [],
    "contents": "\nNate Silver makes a table of all NBA players that have had four games in a row with 20+ points, 6+ assists, 50%+ shooting. The list is short (and it doesn’t include Kobe).  \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-11-peter-thiel-on-peer-review-science/",
    "title": "Peter Thiel on Peer Review/Science",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-11",
    "categories": [],
    "contents": "\nPeter Theil gives his take on science funding/peer review:\n\nMy libertarian views are qualified because I do think things worked better in the 1950s and 60s, but it’s an interesting question as to what went wrong with DARPA. It’s not like it has been defunded, so why has DARPA been doing so much less for the economy than it did forty or fifty years ago? Parts of it have become politicized. You can’t just write checks to the thirty smartest scientists in the United States. Instead there are bureaucratic processes, and I think the politicization of science—where a lot of scientists have to write grant applications, be subject to peer review, and have to get all these people to buy in—all this has been toxic, because the skills that make a great scientist and the skills that make a great politician are radically different. There are very few people who are both great scientists and great politicians. So a conservative account of what happened with science in the 20thcentury is that we had a decentralized, non-governmental approach all the way through the 1930s and early 1940s. At that point, the government could accelerate and push things tremendously, but only at the price of politicizing it over a series of decades. Today we have a hundred times more scientists than we did in 1920, but their productivity per capita is less that it used to be.\n\nThiel has a history of making controversial comments, and I don’t always agree with him, but I think that his point about the politicization of the grant process is interesting. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-10-duke-saga-on-60-minutes-this-sunday/",
    "title": "Duke Saga on 60 Minutes this Sunday",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-10",
    "categories": [],
    "contents": "\nThis Sunday February 12, the news magazine 60 Minutes will have a feature on the Duke Clinical Trials saga. Will Dr. Potti himself make an appearance? This is from the 60 Minutes web site:\n\nDeception at Duke - Scott Pelley reports on a Duke University oncologist whose supervisor says he manipulated the data in his study of a breakthrough cancer therapy. Kyra Darnton is the producer.\n\nThe word on the street is that the segment will also feature statisticians Keith Baggerly and Kevin Coombes of the M.D. Anderson Cancer Center.\nAnd that makes two posts this week about people at M.D. Anderson. What’s going on here?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-09-an-example-of-how-sending-a-paper-to-a-statistics/",
    "title": "An example of how sending a paper to a statistics journal can get you scooped",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-09",
    "categories": [],
    "contents": "\nIn a previous post I complained about statistics journals taking way too long rejecting papers. Today I am complaining because even when everything goes right —better than above average review time (for statistics), useful and insightful comments from reviewers— we can come out losing.\nIn May 2011 we submitted a paper on removing GC bias from RNAseq data to Biostatistics. It was published on December 27. However, we were scooped by this BMC Bioinformatics paper published ten days earlier despite being submitted three months later and accepted 11 days after ours. The competing paper has already earned the “highly accessed” distinction. The two papers, both statistics papers, are very similar, yet I am afraid more people will read the one that was finished second but published first.\nNote that Biostatistics is one of the fastest stat journals out there. I don’t blame the journal at all here. We statisticians have to change our culture when it comes to reviews.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-08-dealbook-illumina-formally-rejects-roches-takeover/",
    "title": "DealBook: Illumina Formally Rejects Roche's Takeover Bid",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-08",
    "categories": [],
    "contents": "\nDealBook: Illumina Formally Rejects Roche’s Takeover Bid\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-08-statisticians-and-clinicians-collaborations-based-on/",
    "title": "Statisticians and Clinicians: Collaborations Based on Mutual Respect",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-08",
    "categories": [],
    "contents": "\nStatisticians and Clinicians: Collaborations Based on Mutual Respect\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-07-wolfram-a-search-engine-finds-answers-within-itself/",
    "title": "Wolfram, a Search Engine, Finds Answers Within Itself",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-07",
    "categories": [],
    "contents": "\nWolfram, a Search Engine, Finds Answers Within Itself\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-06-an-r-script-for-estimating-future-inflation-via-the/",
    "title": "An R script for estimating future inflation via the Treasury market",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-06",
    "categories": [],
    "contents": "\nOne factor that is critical for any financial planning is estimating what future inflation will be. For example, if you’re saving money in an instrument that gains 3% per year, and inflation is estimated to be 4% per year, well then you’re losing money in real terms.\nThere are a variety of ways to estimate the rate of future inflation. You could, for example, use past rates as an estimate of future rates. However, the Treasury market provides an estimate of what the market thinks annual inflation will be over the next 5, 10, 20, and 30 years.\nBasically, the Treasury issue two types of securities: nominal securities that pay a nominal interest rate (fixed percentage of your principal), and inflation-indexed securities (TIPS) that pay an interest rate that is applied to your principal adjusted by the consumer price index (CPI). As the CPI goes up and down, the payments for inflation-indexed securities go up and down (although they can’t go negative so you always get your principal back). As these securities trade throughout the day, their respective market-based interest rates go up and down continuously. The difference between the nominal interest rate and the real interest rate for a fixed period of time (5, 10, 20, years)  can be used as a rough estimate of annual inflation over that time period.\nThe Treasury publishes data for its auctions everyday on the yield for both nominal and inflation-indexed securities. There is an XML feed for nominal yields and for real yields. Using these, I used the XML R package and wrote an R script to scrape the data and calculate the inflation estimate.  \nAs of today, the market’s estimate of annual inflation is:\n5-year Inflation: 1.88%\n10-year Inflation: 2.18%\n30-year Inflation: 2.38%\n\nBasically, you just call the ‘inflation()’ function with no arguments and it produces the above print out.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-06-sunday-data-statistics-link-roundup-2-5/",
    "title": "Sunday Data/Statistics Link Roundup (2/5)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-06",
    "categories": [],
    "contents": "\nCool app, you can write out an equation on the screen and it translates the equation to latex. Via Andrew G.\nYet another D3 tutorial. Stay tuned for some cool stuff on this front here at Simply Stats in the near future. Via Vishal.\nOur favorite Greek statistician in the news again. \nHow measurement of academic output harms science. Related: is submitting scientific papers too time consuming? Stay tuned for more on this topic this week. Via Michael E. \nOne from the archives: Data visualization and art. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-03-why-dont-we-hear-more-about-adrian-dantley-on-espn/",
    "title": "Why don't we hear more about Adrian Dantley on ESPN? This graph makes me think he was as good an offensive player as Michael Jordan.",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-03",
    "categories": [],
    "contents": "\nIn my last post I complained about efficiency not being discussed enough by NBA announcers and commentators. I pointed out that some of the best scorers have relatively low FG% or TS%. However, via the comments it was pointed out that top scorers need to take more difficult shots and thus are expected to have lower efficiency. The plot below (made with this R script) seems to confirm this (click image to enlarge) . The dashed line is from regression and the colors represent guards (green), forwards (orange) and centers (purple).\n\nAmong this group TS% does trend down with points per game and centers tend to have higher TS%. Forwards and guards are not very different. However, the plot confirms that some of the supposed all time greats are more ball hogs than good scorers. \nA couple of  further observations. First, Adrian Dantley was way better than I thought. Why isn’t he more famous? Second, Kobe is no Jordan. Also note Jordan played several seasons past his prime which lowered his career averages. So I added points for five of these players using only data from their prime years (ages 24-29). Here Jordan really stands out. But so does Dantley! \n\npd - Note that these plots say nothing about defense, rebounding, or passing. This in-depth analysis makes a convincing argument that Dennis Rodman is one of the most valuable players of all time.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-02-clevelands-2001-plan-for-redefining-statistics-as/",
    "title": "Cleveland's (?) 2001 plan for redefining statistics as \"data science\"\n",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-02",
    "categories": [],
    "contents": "\nThis plan has been making the rounds on Twitter and is being attributed to William Cleveland in 2001 (thanks to Kasper for the link). I’m not sure of the provenance of the document but it has some really interesting ideas and is worth reading in its entirety. I actually think that many Biostatistics departments follow the proposed distribution of effort pretty closely. \nOne of the most interesting sections is the discussion of computing (emphasis mine): \n\nData analysis projects today rely on databases, computer and network hardware, and computer and network software. A collection of models and methods for data analysis will be used only if the collection is implemented in a computing environment that makes the models and methods sufﬁciently efﬁcient to use. In choosing competing models and methods, analysts will trade effectiveness for efﬁciency of use.\n…..\nThis suggests that statisticians should look to computing for knowledge today, just as data science looked to mathematics in the past.\n\nI also found the theory section worth a read and figure it will definitely lead to some discussion: \n\nMathematics is an important knowledge base for theory. It is far too important to take for granted by requiring the same body of mathematics for all. Students should study mathematics on an as-needed basis.\n….\nNot all theory is mathematical. In fact, the most fundamental theories of data science are distinctly nonmathematical. For example, the fundamentals of the Bayesian theory of inductive inference involve nonmathematical ideas about combining information from the data and information external to the data. Basic ideas are conveniently expressed by simple mathematical expressions, but mathematics is surely not at issue. \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-02-01-evidence-based-music/",
    "title": "Evidence-based Music",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-02-01",
    "categories": [],
    "contents": "\nThere was recently a fascinating article published in PNAS that compared the sound quality of different types of violins. In this study, researchers assembled a collection of six violins, three of which were made by Stradivari and Guarneri del Gesu and three made by modern luthiers (i.e. 20th century). The combined value of the “old” violins was $10 million, about 100 times greater than the combined value of the “new” violins. Also, they note:\n\nNumbers of subjects and instruments were small because it is difficult to persuade the owners of fragile, enormously valuable old violins to release them for extended periods into the hands of blindfolded strangers.\n\nYeah, I’d say so.\nThey then got 21 professional violinists to try them all out wearing glasses to obscure their vision so they couldn’t see the violins. The researchers were also blinded to the type of violin as the study was being conducted.\nThe conclusions were striking:\n\nWe found that (i) the most-preferred violin was new; (ii) the least-preferred was by Stradivari; (iii) there was scant correlation between an instrument’s age and monetary value and its perceived quality; and (iv) most players seemed unable to tell whether their most-preferred instrument was new or old.\n\nFirst, I’m glad the researchers got people to actually play the instruments. I don’t think it’s sufficient to just listen to some recordings because usually the recordings are by different performers and the quality of the recording itself may vary quite a bit. Second, the study was conducted in a hotel room for its “dry acoustics”, but I think changing the venue might have changed the results. Third, even though the authors don’t declare any specific financial conflict of interest, it’s worth noting that the second author is a violinmaker who could theoretically benefit if people decide they no longer need to focus on old Italian violins.\nI was surprised, but not that surprised, at the results. As a lifelong violinist, I had always wondered whether the Strads and the Guarneris were that much better. I once played on a Guarneri (for about 30 seconds) and I think it’s fair to say that it was incredible. But I’ve also seen some amazing violins made by guys in Brooklyn and New Jersey. I’d always heard that Strads have a darker more mellow sound, which I suppose is nice, but I think these days people may prefer a brighter and bigger sound, especially for those larger modern-day concert halls. \nI hope that this study and others like it will get people to focus on which violins sound good rather than where they came from. I’m glad to see the use of data pose a challenge to another long-standing convention.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-31-this-graph-makes-me-think-kobe-is-not-that-good-he/",
    "title": "This graph makes me think Kobe is not that good, he just shoots a lot",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-31",
    "categories": [],
    "contents": "\nI find it surprising that NBA commentators rarely talk about field goal percentage. Everybody knows that the more you shoot the more you score. But players that score a lot are admired without consideration of their FG%. Of course having a high FG% is not necessarily admirable as many players only take easy shots, while top-scorers need to take difficult ones. Regardless, missing is undesirable and players that miss more than usual are not criticized enough. Iverson, for example, had a lowly career FG% of 43 yet he regularly made the allstar team. But I am not surprised he never won an NBA championship: it’s hard to win when your top scorer misses so often.\n\nExperts consider Kobe to be one of the all time greats and compare him to Jordan. They never mention that he is consistently among league leaders in missed shots. So far this year, Kobe has missed a whopping 279 times for a league leading 13.3 misses per game. In contrast, Lebron has missed 8.8 per game and has scored about the same per game. The plot above (made with this R script) shows career FG% for players considered to be superstars, top-scorers, and that have won multiple championships (red lines are 1st and 3rd quartiles). I also include Gasol, Lebron, Wade, and Dominique. Note that Kobe has the worst FG% in this group.  So how does he win 5 championships? Well perhaps Shaq and later Gasol made up for his misses. Note that the first year Kobe played without Shaq, the Lakers did not make the playoffs. Also, during Kobe’s career the Lakers’ record has been similar with and without him. Experts may compare Kobe to Jordan, but perhaps we should be comparing him to Dominique.\nUpdate: Please see Brunsloe87’s comment for a much better analysis than mine. He/she points out that it’s too simplistic to look at FG%. Instead we should look at something closer to points scored per shot taken. This rewards players, like Kobe, that draw many fouls and has a high FT%. There is a weighted statistic called true scoring % (TS%) that tries to summarize this and below I include a plot of TS% for the same players. Kobe is no Jordan but he is not as bad as Dominique either. He is somewhere in the middle. \n\nThe comment also points out that Magic didn’t shoot as much as other superstars so it’s unfair to include him. A better plot would plot TS% versus shots taken (e.g. FGA+FTA/2) but I’ll let someone with more time make that one. Anyways, this plot explains why the early 80s Lakers (Magic+Kareem) were so good.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-30-why-in-person-education-isnt-dead-yet-but-a/",
    "title": "Why in-person education isn't dead yet...but a statistician could finish it off",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-30",
    "categories": [],
    "contents": "\nA growing tend in education is to put lectures online, for free. The Kahn Academy, Stanford’s recent AI course, and Gary King’s new quantitative government course at Harvard are three of the more prominent examples. This new pedagogical format is more democratic, free, and helps people learn at their own pace. It has led some, including us here at Simply Statistics, to suggest that the future of graduate education lies in online courses. Or to forecast the end of in-class lectures. \nAll this excitement led John Cook to ask, “What do colleges sell?”. The answers he suggested were: (1) real credentials, like a degree, (2) motivation to ensure you did the work, and (3) feedback to tell you how you are doing. As John suggests, online lectures really only target motivated and self-starting learners. For graduate students, this may work (maybe), but for the vast majority of undergrads or high-school students, self-guided learning won’t work due to a lack of motivation. \nI would suggest that until the feedback, assessment,and credentialing problems have been solved, online lectures are still more edu-tainment than education. \nOf these problems, I think we are closest to solving the feedback problem with online quizes and tests to go with online lectures. What we haven’t solved are assessment and credentialing. The reason is there is no good system for verifying a person taking a quiz/test online is who they say they are. This issue has two consequences: (1) it is difficult to require that a person do online quizes/tests like we do with in-class quizes/tests and (2) it is difficult to believe credentials given to people who take courses online. \nWhat does this have to do with statistics? Well, what we need is an Completely Automated Online Test for Student Identity (COATSI). People will notice a similarity between my acronym and the acronym for CAPTCHAs, the simple online Turing tests used to prove that you are a human and not a computer. \nThe properties of a COATSI need to be:\nCompletely automated\nProvide tests that verify the identity of the student being assessed\nCan be used throughout an online quiz/test/assessment\nAre simple and easy to solve\nI can’t think of a deterministic system that can be used for this purpose. My suspicion is that a COATSI will need to be statistical. For example, one idea is to have people sign in with Facebook, then at random intervals while they are solving problems, they have to identify their friends by name. If they do this quickly/consistently enough, they are verified as the person taking the test. \nI don’t have a good solution to this problem yet; I’d love to hear more suggestions. I also think this seems like a potentially hugely important and very challenging problem for a motivated grad student or postdoc….\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-29-sunday-data-statistics-link-roundup-1-29/",
    "title": "Sunday data/statistics link roundup (1/29)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-29",
    "categories": [],
    "contents": "\nA really nice D3 tutorial. I’m 100% on board with D3, if they could figure out a way to export the graphics as pdfs, I think this would be the best visualization tool out there.\nA personalized calculator that tells you what number (of the 7 billion or so) that you are based on your birth day. I’m person 4,590,743,884. Makes me feel so special….\nAn old post of ours, on dongle communism. One of my favorite posts, it came out before we had much traffic but deserves more attention.\nThis isn’t statistics/data related but too good to pass up. From the Bones television show, malware fractals shaved into a bone. I love TV science. Thanks to Dr. J for the link.\nStats are popular…\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-27-this-simple-bar-graph-clearly-demonstrates-that-the-us/",
    "title": "This simple bar graph clearly demonstrates that the US can easily increase research funding",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-27",
    "categories": [],
    "contents": "\nSome NIH R01 paylines are down to 10%. This means only 10% of grants are being funded. The plot below highlights that all we need is a tiny litte slice from Defense, Medicare, Medicaid or Social Security to bring that back up to 20%. The plot was taken from Alex Tarrabok’s great article in the Atlantic.\nUpdate: The y-axis unit is billions of US dollars.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-26-when-should-statistics-papers-be-published-in-science/",
    "title": "When should statistics papers be published in Science and Nature?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-26",
    "categories": [],
    "contents": "\nLike many statisticians, I was amped to see a statistics paper appear in Science. Given the impact that statistics has on the scientific community, it is a shame that more statistics papers don’t appear in the glossy journals like Science or Nature. As I pointed out in the previous post, if the paper that introduced the p-value was cited every time this statistic was used, the paper would have over 3 million citations!\nBut a couple of our readers* have pointed to a response to the MIC paper published by Noah Simon and Rob Tibshirani. Simon and Tibshirani show that the MIC statistic is underpowered compared to another recently published statistic for the same purpose that came out in 2009 in the Annals of Applied Statistics. A nice summary of the discussion is provided by Florian over at his blog. \nIf the AoAS statistic came out first (by 2 years) and is more powerful (according to simulation), should the MIC statistic have appeared in Science? \nThe whole discussion reminds me of a recent blog post suggesting that journals need to pick one between groundbreaking and definitive. The post points out that groundbreaking and definitive are in many ways in opposition to each other. \nAgain, I’d suggest that statistics papers get short shrift in the glossy journals and I would like to see more. And the MIC statistic is certainly groundbreaking, but it isn’t clear that it is definitive. \nAs a comparison, a slightly different story played out with another recent high-impact statistical method, the false discovery rate (FDR). The original papers were published in statistics journals. Then when it was clear that the idea was going to be big, a more general-audience-friendly summary was published in PNAS (not Science or Nature but definitely glossy). This might be a better way for the glossy journals to know what is going to be a major development in statistics versus an exciting - but potentially less definitive - method. \nFlorian M. and John S.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-25-a-wordcloud-comparison-of-the-2011-and-2012-sotu/",
    "title": "A wordcloud comparison of the 2011 and 2012 #SOTU",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-25",
    "categories": [],
    "contents": "\nI wrote a quick (and very dirty) R script for creating a comparison cloud and a commonality cloud for President Obama’s 2011 and 2012 State of the Union speeches. The cloud on the left shows words that have different frequencies between the two speeches and the cloud on the right shows the words in common between the two speeches. Here is a higher resolution version.\n\nThe focus on jobs hasn’t changed much. But it is interesting how the 2012 speech seems to focus more on practical issues (tax, pay, manufacturing, oil) versus more emotional issues in 2011 (future, schools, laughter, success, dream).\nThe wordcloud R package does all the heavy lifting.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-25-the-end-of-in-class-lectures-is-closer-than-i-thought/",
    "title": "The end of in-class lectures is closer than I thought",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-25",
    "categories": [],
    "contents": "\nOur previous post on future of (statistics) graduate education was motivated by  he Stanford online course on Artificial Intelligence.  Here is an update on the class that had 160,000 people enroll. Some highlights: 1- Sebastian Thrun has given up his tenure at Stanford and he’s started a new online university called Udacity. 2- 248 students got a perfect score: they never got a single question wrong, over the entire course of the class. All 248 took the course online; not one was enrolled at Stanford. 3- Students from Afghanistan completed the course. What do you think are the chances these students could afford Stanford’s tuition? 4 - There were more students from Lithuania alone than there are students at Stanford altogether.\nThe class evaluations were not perfect. Here is a particularly harsh one. They also need to figure out how to evaluate online students. But I am sure there are plenty of people working on that problem. Here is an example. Regardless, this was the first such experiment and for a first try it seems like a huge success to me. As more professors try this, for example Harvard’s Gary King is conducting a similar class in Quantitative Research Methodology, it will become clearer that there is no future for in-class lectures as we know them today.\nThanks to Alex and Jeff for all the links. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-23-why-statisticians-should-join-and-launch-startups/",
    "title": "Why statisticians should join and launch startups",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-23",
    "categories": [],
    "contents": "\nThe tough economic times we live in, and the potential for big paydays, have made entrepreneurship cool. From the venture capitalist-in-chief, to the javascript coding mayor of New York, everyone is on board. No surprise there, successful startups lead to job creation which can have a major positive impact on the economy. \nThe game has been dominated for a long time by the folks over in CS. But the value of many recent startups is either based on, or can be magnified by, good data analysis. Here are a few startups that are based on data/data analysis: \nThe Climate Corporation -analyzes climate data to sell farmers weather insurance.\nFlightcaster - uses public data to predict flight delays\nQuid - uses data on startups to predict success, among other things.\n100plus - personalized health prediction startup, predicting health based on public data\nHipmunk - The main advantage of this site for travel is better data visualization and an algorithm to show you which flights have the worst “agony”.\nTo launch a startup you need just a couple of things: (1) a good, valuable source of data (there are lots of these on the web) and (2) a good idea about how to analyze them to create something useful. The second step is obviously harder than the first, but the companies above prove you can do it. Then, once it is built, you can outsource/partner with developers - web and otherwise - to implement your idea. If you can build it in R, someone can make it an app. \nThese are just a few of the startups whose value is entirely derived from data analysis. But companies from LinkedIn, to Bitly, to Amazon, to Walmart are trying to mine the data they are generating to increase value. Data is now being generated at unprecedented scale by computers, cell phones, even thremostats! With this onslaught of data, the need for people with analysis skills is becoming incredibly acute. \nStatisticians, like computer scientists before them, are poised to launch, and make major contributions to, the next generation of startups. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-22-sunday-data-statistics-link-roundup-1-21/",
    "title": "Sunday Data/Statistics Link Roundup (1/21)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-22",
    "categories": [],
    "contents": "\nIs the microarray dead? Jeremey Leipzig seems to think that statistical methods for microarrays should be. I’m not convinced, the technology has finally matured to the point we can use it for personalized medicine and we abandon it for the next hot thing? Not to Andrew for the link.\nData from 5 billion webpages available from the Common Crawl. Want to build your own search tool - or just find out whats on the web? Get your Hadoop on. Nod to Peter S. for the heads up. \nSimon and Tibhsirani criticize the greatly publicized MIC statistic. Nod to John S. for the link.\nA public/free statistics class being offered through the IQSS at Harvard. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-20-interview-with-joe-blitzstein/",
    "title": "Interview With Joe Blitzstein",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-20",
    "categories": [],
    "contents": "\n\nJoe Blitzstein\n\n\n\n\n\n\n\n\n\n\n\nJoe Blitzstein is Professor of the Practice in Statistics at Harvard University and co-director of the graduate program. He moved to Harvard after obtaining his Ph.D. with Persi Diaconis at Stanford University. Since joining the faculty at Harvard, he has been immortalized in Youtube prank videos, been awarded a “favorite professor” distinction four times, and performed interesting research on the statistical analysis of social networks. Joe was also the first person to discover our blog on Twitter. You can find more information about him on his personal website. Or check out his Stat 110 class, now available from iTunes!\n\n\n\n\n\nWhich term applies to you: data scientist/statistician/analyst?\n\nStatistician, but that should and does include working with data! Ithink statistics at its best interweaves modeling, inference,prediction, computing, exploratory data analysis (includingvisualization), and mathematical and scientific thinking. I don’tthink “data science” should be a separate field, and I’m concernedabout people working with data without having studied much statisticsand conversely, statisticians who don’t consider it important ever tolook at real data. I enjoyed the discussions by Drew Conway and onyour blog (athttp://www.drewconway.com/zia/?p=2378andhttp://simplystatistics.tumblr.com/post/11271228367/datascientist)and think the relationships between statistics, machine learning, datascience, and analytics need to be clarified.\n\nHow did you get into statistics/data science (e.g. your history)?\n\nI always enjoyed math and science, and became a math major as anundergrad Caltech partly because I love logic and probability andpartly because I couldn’t decide which science to specialize in. Oneof my favorite things about being a math major was that it felt soconnected to everything else: I could often help my friends who weredoing astronomy, biology, economics, etc. with problems, once they hadexplained enough so that I could see the essential pattern/structureof the problem. At the graduate level, there is a tendency for math tobecome more and more disconnected from the rest of science, so I wasvery happy to discover that statistics let me regain this, and havethe best of both worlds: you can apply statistical thinking and toolsto almost anything, and there are so many opportunities to do thingsthat are both beautiful and useful.\n\nWho were really good mentors to you? What were the qualities that reallyhelped you?\n\nI’ve been extremely lucky that I have had so many inspiringcolleagues, teachers, and students (far too numerous to list), so Iwill just mention three. My mother, Steffi, taught me at an early ageto love reading and knowledge, and to ask a lot of “what if?”questions. My PhD advisor, Persi Diaconis, taught me many beautifulideas in probability and combinatorics, about the importance ofstarting with a simple nontrivial example, and to ask a lot of “whocares?” questions. My colleague Carl Morris taught me a lot about howto think inferentially (Brad Efron called Carl a “natural”statistician in his interview athttp://www-stat.stanford.edu/~ckirby/brad/other/2010Significance.pdf,by which I think he meant that valid inferential thinking does notcome naturally to most people), about parametric and hierarchicalmodeling, and to ask a lot of “does that assumption make sense in thereal world?” questions.\n\nHow do you get students fired up about statistics in your classes?\n\nStatisticians know that their field is both incredibly useful in thereal world and exquisitely beautiful aesthetically. So why isn’t thatalways conveyed successfully in courses? Statistics is oftenmisconstrued as a messy menagerie of formulas and tests, rather than acoherent approach to scientific reasoning based on a few fundamentalprinciples. So I emphasize thinking and understanding rather thanmemorization, and try to make sure everything is well-motivated andmakes sense both mathematically and intuitively. I talk a lot aboutparadoxes and results which at first seem counterintuitive, sincethey’re fun to think about and insightful once you figure out what’sgoing on.\nAnd I emphasize what I call “stories,” by which I mean anapplication/interpretation that does not lose generality. As a simpleexample, if X is Binomial(m,p) and Y is Binomial(n,p) independently,then X+Y is Binomial(m+n,p). A story proof would be to interpret X asthe number of successes in m Bernoulli trials and Y as the number ofsuccesses in n different Bernoulli trials, so X+Y is the number ofsuccesses in the m+n trials. Once you’ve thought of it this way,you’ll always understand this result and never forget it. Amisconception is that this kind of proof is somehow less rigorous thanan algebraic proof; actually, rigor is determined by the logic of theargument, not by how many fancy symbols and equations one writes out.\nMy undergraduate probability course, Stat 110, is now worldwideviewable for free on iTunes U athttp://itunes.apple.com/WebObjects/MZStore.woa/wa/viewPodcast?id=495213607with 34 lecture videos and about 250 practice problems with solutions.I hope that will be a useful resource, but in any case looking throughthose materials says more about my teaching style than anything I canwrite here does.\nWhat are your main research interests these days?\nI’m especially interested in the statistics of networks, withapplications to social network analysis and in public health. There isa tremendous amount of interest in networks these days, coming from somany different fields of study, which is wonderful but I think thereneeds to be much more attention devoted to the statistical issues.Computationally, most network models are difficult to work with sincethe space of all networks is so vast, and so techniques like Markovchain Monte Carlo and sequential importance sampling become crucial;but there remains much to do in making these algorithms more efficientand in figuring out whether one has run them long enough (usually theanswer is “no” to the question of whether one has run them longenough). Inferentially, I am especially interested in how to makevalid conclusions when, as is typically the case, it is not feasibleto observe the full network. For example, respondent-driven samplingis a link-tracing scheme being used all over the world these days tostudy so-called “hard-to-reach” populations, but much remains to bedone to know how best to analyze such data; I’m working on this withmy student Sergiy Nesterko. With other students and collaborators I’mworking on various other network-related problems. Meanwhile, I’m alsofinishing up a graduate probability book with Carl Morris,“Probability for Statistical Science,” which has quite a few newproofs and perspectives on the parts of probability theory that aremost useful in statistics.\n\nYou have been immortalized in several Youtube videos. Do you think thishelped make your class more “approachable”?\n\nThere were a couple strange and funny pranks that occurred in my firstyear at Harvard. I’m used to pranks since Caltech has a long historyand culture of pranks, commemorated in several “Legends of Caltech”volumes (there’s even a movie in development about this), but pranksare quite rare at Harvard. I try to make the class approachablethrough the lectures and by making sure there is plenty of support,help, and encouragement is available from the teaching assistants andme, not through YouTube, but it’s fun having a few interestingoccasions from the history of the class commemorated there.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-19-data-journalism-awards/",
    "title": "Data Journalism Awards",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-19",
    "categories": [],
    "contents": "\nData Journalism Awards\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-19-fundamentals-of-engineering-review-question-oops/",
    "title": "Fundamentals of Engineering Review Question Oops",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-19",
    "categories": [],
    "contents": "\nThe Fundamentals of Engineering Exam is the first licensing exam for engineers. You have to pass it on your way to becoming a professional engineer (PE). I was recently shown a problem from a review manual: \n\nWhen it is operating properly, a chemical plant has a daily production rate that is normally distributed with a mean of 880 tons/day and a standard deviation of 21 tons/day. During an analysis period, the output is measured with random sampling on 50 consecutive days, and the mean output is found to be 871 tons/day. With a 95 percent confidence level, determine if the plant is operating properly. \nThere is at least a 5 percent probability that the plant is operating properly. \nThere is at least a 95 percent probability that the plant is operating properly. \nThere is at least a 5 percent probability that the plant is not operating properly. \nThere is at least a 95 percent probability that the plant is not operating properly. \n\nWhoops…seems to be a problem there. I’m glad that engineers are expected to know some statistics; hopefully the engineering students taking the exam can spot the problem…but then how do they answer? \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-17-figshare-and-dont-trust-celebrities-stating-facts/",
    "title": "figshare and don't trust celebrities stating facts",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-17",
    "categories": [],
    "contents": "\nA couple of links:\nfigshare is a site where scientists can share data sets/figures/code. One of the goals is to encourage researchers to share negative results as well. I think this is a great idea - I often find negative results and this could be a place to put them. It also uses a tagging system, like Flickr. I think this is a great idea for scientific research discovery. They give you unlimited public space and 1GB of private space. This could be big, a place to help make reproducible research efforts user-friendly. Via TechCrunch\nDon’t trust celebrities stating facts because they usually don’t know what they are talking about. I completely agree with this. Particularly because I have serious doubts about the statisteracy of most celebrities. Nod to Alex for the link (our most active link finder!).  \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-16-a-tribute-to-one-of-the-most-popular-methods-in/",
    "title": "Untitled",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-16",
    "categories": [],
    "contents": "\n[youtube http://www.youtube.com/watch?v=oPzERmPlmw8?wmode=transparent&autohide=1&egm=0&hd=1&iv\\_load\\_policy=3&modestbranding=1&rel=0&showinfo=0&showsearch=0&w=500&h=375]\nA tribute to one of the most popular methods in statistics.\n\n(Source: http://www.youtube.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-15-sunday-data-statistics-link-roundup/",
    "title": "Sunday Data/Statistics Link Roundup",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-15",
    "categories": [],
    "contents": "\nStatistics help for journalists (don’t forget to keep rating stories!) This is the kind of thing that could grow into a statisteracy page. The author also has a really nice plug for public schools. \nAn interactive graphic to determine if you are in the 1% from the New York Times (I’m not…).\nMike Bostock’s d3.js presentation, this is some really impressive visualization software. You have to change the slide numbers manually but it is totally worth it. Check out slide 10 and slide 14. This is the future of data visualization. Here is a beginners tutorial to d3.js by Mike Dewar.\nAn online diagnosis prediction start-up (Symcat) based on data analysis from two Hopkins Med students.\nFinally, a bit of a bleg. I’m going to try to make this link roundup a regular post. If you have ideas for links I should include, tweet us @simplystats or send them to Jeff’s email. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-13-academics-are-partly-to-blame-for-supporting-the-closed/",
    "title": "Academics are partly to blame for supporting the closed and expensive access system of publishing",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-13",
    "categories": [],
    "contents": "\nMichael Eisen recently published a New York Times op-ed arguing that a bill meant to protect publishers, introduced in the House of Representatives, will result in tax payers paying twice for scientific research. According to Eisen\n\nIf the bill passes, to read the results of federally funded research, most Americans would have to buy access to individual articles at a cost of $15 or $30 apiece. In other words, taxpayers who already paid for the research would have to pay again to read the results.\n\nWe agree and encourage our readers to write Congress opposing the “Research Works Act”. However, whereas many are vilifying the publishers that are lobbying for this act,  I think us academics are the main culprits keeping open access from succeeding.\nIf this bill makes it into law, I do not think that the main issue will be US taxpayers paying twice for research, but rather that access will be even more restricted to the general scientific community. Interested parties outside the US -and in developing countries in particular- should have unrestriced access to scientific knowledge. Congresswoman Carolyn Maloney gets it wrong by not realizing that giving China (and other countries) access to scientific knowledge is beneficial to science in general and consequently to everyone.  However, to maintain the high quality of research publications we currently enjoy, someone needs to pay for competent editors, copy editors, support staff, and computer servers.  Open access journals shift the costs from the readers to authors that have plenty of funds (grants, startups, etc..) to cover the charges.  By charging the authors, papers can be made available online for free. Free to everyone. Open access. PLoS has demonstrated that the open access model is viable, but a paper in PLoS Biology will run you $2,900 (see Jeff’s table). Several non-profit societies and for profit publishers, such as Nature Publishing Group, offer open access for about the same price. \nSo given all the open access options, why do gated journals survive? I think the main reason is that we -the scientific community- through appointments and promotions committees, study sections, award committees, etc. use journal prestige to evaluate publication records disregarding open access as a criteria (see Eisen’s related post on decoupling publication and assessment). Therefore, those that decide to only publish in open access journals, may hinder not only their careers, but also the careers of their students and postdocs. The other reason is that for authors, publishing gated papers is typically cheaper than open access papers, and we don’t always make the more honorable decision. \nAnother important consideration is that a substantial proportion of publication costs comes from printing paper copies. My department continues to buy print copies of several stat journals as well as some of the general science magazines. The Hopkins library, on behalf of the faculty, buys print versions of hundreds of journals. As long as we continue to create a market for paper copies, the journals will continue to allocate resources to producing them. Somebody has to pay for this, yet with online versions already being produced the print versions are superfluous.\nApart from opposing the Research Works Act as Eisen proposes, there are two more things I intend to do in 2012: 1) lobby my department to stop buying print versions and 2) lobby my study section to give special consideration to open access publications when evaluating a biosketch or a progress report.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-13-in-the-era-of-data-what-is-a-fact/",
    "title": "In the era of data what is a fact?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-13",
    "categories": [],
    "contents": "\nThe Twitter universe is abuzz about this article in the New York Times. Arthur Brisbane, who responds to reader’s comments, asks \n\nI’m looking for reader input on whether and when New York Times news reporters should challenge “facts” that are asserted by newsmakers they write about.\n\nHe goes on to give a couple of examples of qualitative facts that reporters have used in stories without questioning the veracity of the claims. As many people pointed out in the comments, this is completely absurd. Of course reporters should check facts and report when the facts in their stories, or stated by candidates, are not correct. That is the purpose of news reporting. \nBut I think the question is a little more subtle when it comes to quantitative facts and statistics. Depending on what subsets of data you look at, what summary statistics you pick, and the way you present information - you can say a lot of different things with the same data. As long as you report what you calculated, you are technically reporting a fact - but it may be deceptive. The classic example is calculating median vs. mean home prices. If Bill Gates is in your neighborhood, no matter what the other houses cost, the mean price is going to be pretty high! \nTwo concrete things can be done to deal with the malleability of facts in the data age.\nFirst, we need to require that our reporters, policy makers, politicians, and decision makers report the context of numbers they state. It is tempting to use statistics as blunt instruments, punctuating claims. Instead, we should demand that people using statistics to make a point embed them in the broader context. For example, in the case of housing prices, if a politician reports the mean home price in a neighborhood, they should be required to state that potential outliers may be driving that number up. How do we make this demand? By not believing any isolated statistics - statistics will only be believed when the source is quoted and the statistic is described.  \nBut this isn’t enough, since the context and statistics will be meaningless without raising overall statisteracy (statistical literacy, not to be confused with numeracy).  In the U.S. literacy campaigns have been promoted by library systems. Statisteracy is becoming just as critical; the same level of social pressure and assistance should be applied to individuals who don’t know basic statistics as those who don’t have basic reading skills. Statistical organizations, academic departments, and companies interested in analytics/data science/statistics all have a vested interest in raising the population statisteracy. Maybe a website dedicated to understanding the consequences of basic statistical concepts, rather than the concepts themselves?\nAnd don’t forget to keep rating health news stories!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-11-healthnewsrater/",
    "title": "Help us rate health news reporting with citizen-science powered http://www.healthnewsrater.com",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-11",
    "categories": [],
    "contents": "\nWe here at Simply Statistics are big fans of science news reporting. We read newspapers, blogs, and the news sections of scientific journals to keep up with the coolest new research. \nBut health science reporting, although exciting, can also be incredibly frustrating to read. Many articles have sensational titles, like “How using Facebook could raise your risk of cancer”. The articles go on to describe some research and interview a few scientists, then typically make fairly large claims about what the research means. This isn’t surprising - eye catching headlines are important in this era of short attention spans and information overload. \nIf just a few extra pieces of information were reported in science stories about the news, it would be much easier to evaluate whether the cancer risk was serious enough to shut down our Facebook accounts. In particular we thought any news story should report:\nA link back to the original research article where the study (or studies) being described was published. Not just a link to another news story. \nA description of the study design (was it a randomized clinical trial? a cohort study? 3 mice in a lab experiment?)\nWho funded the study - if a study involving cancer risk was sponsored by a tobacco company, that might say something about the results.\nPotential financial incentives of the authors - if the study is reporting a new drug and the authors work for a drug company, that might say something about the study too. \nThe sample size - many health studies are based on a very small sample size, only 10 or 20 people in a lab. Results from these studies are much weaker than results obtained from a large study of thousands of people. \nThe organism - Many health science news reports are based on studies performed in lab animals and may not translate to human health. For example, here is a report with the headline “Alzheimers may be transmissible, study suggests”. But if you read the story, scientists injected Alzheimer’s afflicted brain tissue from humans into mice. \nSo we created a citizen-science website for evaluating health news reporting called HealthNewsRater. It was built by Andrew Jaffe and Jeff Leek, with Andrew doing the bulk of the heavy lifting.  We would like you to help us collect data on the quality of health news reporting. When you read a health news story on the Nature website, at nytimes.com, or on a blog, we’d like you to take a second to report on the news. Just determine whether the 6 pieces of information above are reported and input the data at HealthNewsRater.\nWe calculate a score for each story based on the formula:\nHNR-Score = (5 points for a link to the original article + 1 point each for the other criteria)/2\nThe score weights the link to the original article very heavily, since this is the best source of information about the actual science underlying the story. \nIn a future post we will analyze the data we have collected, make it publicly available, and let you know which news sources are doing the best job of reporting health science. \nUpdate: If you are a web-developer with an interest in health news contact us to help make HealthNewsRater better! \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-10-do-you-own-or-rent/",
    "title": "Do you own or rent?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-10",
    "categories": [],
    "contents": "\nWhen it comes to computing, history has gone back and forth between what I would call the “owner model” and the “renter model”. The question is what’s the best approach and how do you determine that?\nBack in the day when people like John von Neumann were busy inventing the computer to work out H-bomb calculations, there was more or less a renter model in place. Computers were obviously quite expensive and so not everyone could have one. If you wanted to do your calculation, you’d walk down to the computer room, give them your punch cards with your program written out, and they’d run it for you. Sometime later you’d get some print out with the results of your program. \nA little later, with time-sharing types of machines, you could have dumb terminals login to a central server and run your calculations that way. I guess that saved you the walk to the computer room (and all the punch cards). I still remember some of these green-screen dumb terminals from my grad school days (yes, UCLA still had these monstrosities in 1999). \nWith personal computers in the 80s, you could own your own computer, so there was no need to depend on some central computer (and a connection to it) to do the work for you. As computing components got cheaper, these personal computers got more and more powerful and rivaled the servers of yore. It was difficult for me to imagine ever needing things like mainframes again except for some esoteric applications. Especially, with the development of Linux, you could have all the power of a Unix mainframe on your desk or lap (or now your palm). \nBut here we are, with Jeff buying a Chromebook. Have we just taken a step back in time? Is cloud computing and the renter model the way to go? I have to say that I was a big fan of “cloud computing” back in the day. But once Linux came around, I really didn’t think there was a need for the thin client/fat server model.\nBut it seems we are going back that way and the reason seems to be because of mobile devices. Mobile devices are now just small computers, so many people own at least two computers (a “real” computer and a phone). With multiple computers, it’s a pain to have to synchronize both the data and the applications on them. If they’re made by different manufacturers then you can’t even have the same operating system/applications on the devices. Also, no one cares about the operating system anymore, so why should it have to be managed? The cloud helps solve some of these problems, as does owning devices from the same company (as I do, Apple fanboy that I am).\nI think the all-renter model of the Chromebook is attractive, but I don’t think it’s ready for prime time just yet. Two reasons I can think of are (1) Microsoft Office and (2) slow network connections. If you want to make Jeff very unhappy, you can either (1) send him a Word document that needs to be edited in Track Changes; or (2) invite him to an international conference on some remote island. The need for a strong network connection is problematic because I’ve yet to encounter a hotel that had a fast enough connection for me to work remotely over on our computing cluster. For that reason I’m sticking with my current laptop.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-10-statistical-crime-fighter/",
    "title": "Statistical Crime Fighter",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-10",
    "categories": [],
    "contents": "\nDick Berk is using his statistical superpowers to fight crime. Seriously. Here is my favorite paragraph.\n\nDrawing from criminal databases dating to the 1960s, Berk initially modeled the Philadelphia algorithm on more than 100,000 old cases, relying on three dozen predictors, including the perpetrator’s age, gender, neighborhood, and number of prior crimes. To develop an algorithm that forecasts a particular outcome—someone committing murder, for example—Berk applied a subset of the data to “train” the computer on which qualities are associated with that outcome. “If I could use sun spots or shoe size or the size of the wristband on their wrist, I would,” Berk said. “If I give the algorithm enough predictors to get it started, it finds things that you wouldn’t anticipate.” Philadelphia’s parole officers were surprised to learn, for example, that the crime for which an offender was sentenced—whether it was murder or simple drug possession—does not predict whether he or she will commit a violent crime in the future. Far more predictive is the age at which he (yes, gender matters) committed his first crime, and the amount of time between other offenses and the latest one—the earlier the first crime and the more recent the last, the greater the chance for another offense.\n\nHat tip to Alex Nones.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-09-a-statistician-and-apple-fanboy-buys-a-chromebook-and/",
    "title": "A statistician and Apple fanboy buys a Chromebook...and loves it!",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-09",
    "categories": [],
    "contents": "\nI don’t mean to brag, but I was an early Apple Fanboy - not sure that is something to brag about now that I write it down. I convinced my advisor to go to all Macs in our lab in 2004. Since then I have been pretty dedicated to the brand, dutifully shelling out almost 2g’s every time I need a new laptop. I love the way Macs just work (until they don’t and you need a new laptop).\nBut I hate the way Apple seems to be dedicated to bleeding every last cent out of me. So I saved up my Christmas gift money (thanks Grandmas!) and bought a Chromebook. It cost me $350 and I was at least in part inspired by these clever ads.\nSo far I’m super pumped about the performance of the Chromebook. Things I love:\nAbout 10 seconds to boot from shutdown, instantly awake from sleep\nSuper long battery life - 8 hours a charge might be an underestimate\nSize - its a 12 inch laptop and just right for sitting on my lap and typing\nSince everything is cloud based, nothing to install/optimize\nIt took me a while to get used to the Browser being the operating system. When I close the last browser window, I expect to see the Desktop. Instead, a new browser window pops up. But that discomfort only lasted a short time.\nIt turns out I can do pretty much everything I do on my Macbook on the Chromebook. I can access our department’s computing cluster by turning on developer mode and opening a shell(thanks Caffo!). I can do all my word processing on google docs. Email is just gmail as usual. Scribtex for latex (Caffo again). Google Music is so awesome I wish I had started my account before I got my Chromebook. The only thing I’m really trying to settle on is a cloud-based code editor with syntax highlighting. I’m open to suggestions (Caffo?).\nI’m starting to think I could bail on Apple….\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-08-building-the-team-that-built-watson/",
    "title": "Building the Team That Built Watson",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-08",
    "categories": [],
    "contents": "\nBuilding the Team That Built Watson\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-08-make-us-a-part-of-your-day-add-simply-statistics-to/",
    "title": "Make us a part of your day - add Simply Statistics to your RSS feed",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-08",
    "categories": [],
    "contents": "\nYou can add us to your RSS feed through feedburner.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-08-sunday-data-statistics-link-roundup-2/",
    "title": "Sunday Data/Statistics Link Roundup",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-08",
    "categories": [],
    "contents": "\nA few data/statistics related links of interest:\nEric Lander Profile\nThe math of lego (should be “The statistics of lego”)\nWhere people are looking for homes.\nHans Rosling’s Ted Talk on the Developing world (an oldie but a goodie)\nElsevier is trying to make open-access illegal (not strictly statistics related, but a hugely important issue for academics who believe government funded research should be freely accessible), more here. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-08-where-do-you-get-your-data/",
    "title": "Where do you get your data?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-08",
    "categories": [],
    "contents": "\nHere’s a question I get fairly frequently from various types of people: Where do you get your data? This is sometimes followed up quickly with “Can we use some of your data?”\nMy contention is that if someone asks you these questions, start looking for the exits.\n\nThere are of course legitimate reasons why someone might ask you this question. For example, they might be interested in the source of the data to verify its quality. But too often, they are interested in getting the data because they believe it would be a good fit to a method that they have recently developed. Even if that is in fact true, there are some problems.\nBefore I go on, I need to clarify that I don’t have a problem with data sharing per se, but I usually get nervous when a person’s opening line is “Where do you get your data?” This question presumes a number of things that are usually signs of a bad collaborator:\nThe data are just numbers. My method works on numbers, and these data are numbers, so my method should work here. If it doesn’t work, then I’ll find some other numbers where it does work.\nThe data are all that are important. I’m not that interested in working with an actual scientist on an important problem that people care about, because that would be an awful lot of work and time (see here). I just care about getting the data from whomever will give it to me. I don’t care about the substantive context.\nOnce I have the data, I’m good, thank you. In other words, the scientific process is modular. Scientists generate the data and once I have it I’ll apply my method until I get something that I think makes sense. There’s no need for us to communicate. That is unless I need you to help make the data pretty and nice for me.\nThe real question that I think people should be asking is “Where do you find such great scientific collaborators?” Because it’s those great collaborators that generated the data and worked hand-in-hand with you to get intelligible results.\nNiels Keiding wrote a provocative commentary about the tendency for statisticians to ignore the substantive context of data and to use illustrative/toy examples over and over again. He argued that because of this tendency, we should not be so excited about reproducible research, because as more data become available, we will see more examples of people ignoring the science.\nI disagree that this is an argument against reproducible research, but I agree that statisticians (and others) do have a tendency to overuse datasets simply because they are “out there” (stackloss data, anyone?). However, it’s probably impossible to stop people from conducting poor science in any field, and we shouldn’t use the possibility that this might happen in statistics to prevent research from being more reproducible in general. \nBut I digress…. My main point is that people who simply ask for “the data” are probably not interested in digging down and understanding the really interesting questions. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-06-p-values-and-hypothesis-testing-get-a-bad-rap-but-we/",
    "title": "P-values and hypothesis testing get a bad rap - but we sometimes find them useful.",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-06",
    "categories": [],
    "contents": "\nThis post written by Jeff Leek and Rafa Irizarry.\nThe p-value is the most widely-known statistic. P-values are reported in a large majority of scientific publications that measure and report data. R.A. Fisher is widely credited with inventing the p-value. If he was cited every time a p-value was reported his paper would have, at the very least, 3 million citations* - making it the most highly cited paper of all time. \n\nHowever, the p-value has a large number of very vocal critics. The criticisms of p-values, and hypothesis testing more generally, range from philosophical to practical. There are even entire websites dedicated to “debunking” p-values! One issue many statisticians raise with p-values is that they are easily misinterpreted, another is that p-values are not calibrated by sample size, another is that it ignores existing information or knowledge about the parameter in question, and yet another is that very significant (small) p-values may result even when the value of the parameter of interest is scientifically uninteresting.\nWe agree with all these criticisms. Yet, in practice, we find p-values useful and, if used correctly, a powerful tool for the advancement of science. The fact that many misinterpret the p-value is not the p-value’s fault. If the statement “under the null the chance of observing something this convincing is 0.65” is correct, then why not use it? Why not explain to our collaborator that the observation they thought was so convincing can easily happen by chance in a setting that is uninteresting. In cases where p-values are small enough then the substantive experts can help decide if the parameter of interest is scientifically interesting. In general, we find p-value to be superior to our collaborators intuition of what patterns are statistically interesting and which ones are not.\nWe also find p-values provide a simple way to construct decision algorithms. For example, a government agency can define general rules based on p-values that are applied equally to products needing a specific seal of approval. If the rule proves to be to lenient or restrictive, we change the p-value cut-off appropriately. In this situation we view the p-value as part of a practical protocol, not a tool for statistical inference.\nMoreover the p-value has the following useful properties for applied statisticians:\np-values are easy to calculate, even for complicated statistics. Many statistics do not lend themselves to easy analytic calculation; but using permutation and bootstrap procedures p-values can be calculated even for very complicated statistics. \np-values are relatively easy to understand.  The statistical interpretation of the p-value remains roughly the same no matter how complicated the underlying statistic and they also bounded between 0 and 1. This also means that p-values are easy to mis-interpret - they are not posterior probabilities. But this is a difficulty with education, not a difficulty with the statistic itself. \np-values have simple, universal properties  Correct p-values are uniformly distributed under the null, regardless of how complicated the underlying statistic. \np-values are calibrated to error rates scientists care about Regardless of the underlying statistic, calling all P-values less than 0.05 significant leads to on average about 5% false positives even if the null hypothesis is always true. If this property is ignored things like publication bias can result, but again this is a problem with education and the scientific process, not with p-values. \np-values are useful for multiple testing correction. The advent of new measurement technology has shifted much of science from hypothesis driven to discovery driven making the existing multiple testing machinery useful. Using the simple, universal properties of p-values it is possible to easily calculate estimates of quantities like the false discovery rate - the rate at which discovered associations are false.\np-values are reproducible. All statistics are reproducible with enough information. Given the simplicity of calculating p-values, it is relatively easy to communicate sufficient information to reproduce them. \nWe agree there are flaws with p-values, just like there are with any statistic one might choose to calculate. In particular, we do think that confidence intervals should be reported with p-values when possible. But we believe that any other decision-making statistic would lead to other problems. One thing we are sure about is that p-values beat scientists’ intuition about chance any day. So before bashing p-values too much we should be careful because, like democracy to government, p-values may be the worst form of statistical significance calculation except all those other forms that have been tried from time to time. \n————————————————————————————————————\n* Calculated using Google Scholar using the formula:\nNumber of P-value Citations = # of papers with exact phrase “P < 0.05” + (# of papers with exact phrase “P < 0.01” and not exact phrase “P < 0.05”) +   (# of papers with exact phrase “P < 0.001” and not exact phrase “P < 0.05” or “P < 0.001”) \n= 1,320,000 + 1,030,000 + 662,500\nThis is obviously an extremely conservative estimate. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-05-why-all-academics-should-have-professional-twitter/",
    "title": "Why all #academics should have professional @twitter accounts",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-05",
    "categories": [],
    "contents": "\nI started my professional Twitter account @leekgroup about a year and half ago at the suggestion of a colleague of mine, John Storey (@storeylab). I started using the account to post updates on papers/software my group was publishing. Basically, everything I used to report on my webpage as “News”. \nI started to give talks where the title slide included my Twitter name, rather than my webpage. It frequently drew the biggest laugh in the talk, and I would get comments like, “Do you really think people care what you are thinking every moment of every day?” That is what some people use Twitter for, and no I’m not really interested in making those kind of updates. \nSo I started describing why I think Twitter is useful for academics at the beginning of talks:\nYou can integrate it directly into your website (like so), using Twitter widgets. If you have a Twitter account you just go here, get the widget for your website, and add the code to your homepage. Now you don’t have to edit HTML to make news updates, you just login to Twitter and type the update in the box.\nYou can quickly gain a much broader audience for your software/papers. In the past, I had to rely on people actually coming to my website to find my papers or seeing them in journals. Now, when I announce a paper, my followers see it and if they like it, they pass it on to their followers, etc. I have noticed that my papers are being downloaded more and by a broader audience since I joined. \nI can keep up on what other people are doing. Many statisticians have Twitter accounts that they use professionally. I follow many of them and when they publish new papers, I see them pop up, rather than having to go to all their websites. It’s like an RSS feed of papers from people I want to follow. \nYou can connect with people outside academia. Particularly in my area, I’d like the statistical tools I’m developing to be used by folks in industry who work on genomics. It’s hard to get the word out about my methods through traditional channels, but a lot of those folks are on Twitter. \nThe best part is, there is an amplification effect to this medium. So as more and more academics join and follow each other, it is easier and easier for us all to keep up with what is happening in the field. If you are intimidated by using any social media, you can get started with some really easy how-to’s like this one.\nAlright, enough advertising for Twitter, I’m going back to work. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-05-will-amazon-offer-analytics-as-a-service/",
    "title": "Will Amazon Offer Analytics as a Service?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-05",
    "categories": [],
    "contents": "\nWill Amazon Offer Analytics as a Service?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-03-baltimore-gun-offenders-and-where-academics-dont-live/",
    "title": "Baltimore gun offenders and where academics don't live",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-03",
    "categories": [],
    "contents": "\nJeff recently posted links to data from cities and states. He and I wrote R code that plots gun offender locations for Baltimore. Specifically we plot the locations that appear on this table. I added locations of the Baltimore neighborhoods where most of our Hopkins colleagues live as well as the location of the medical institutions where we work. Note the corridor with no points between the West side (Barksdale territory) and East side (Prop Joe territory). Not surprisingly, academics don’t live near the gun offenders. \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-01-02-list-of-cities-states-with-open-data-help-me-find/",
    "title": "List of cities/states with open data - help me find more!",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2012-01-02",
    "categories": [],
    "contents": "\nIt’s the beginning of 2012 and statistics/data science has never been hotter. Some of the most important data is data collected about civic organizations. If you haven’t seen Bill Gate’s TED Talk about the importance of state budgets, you should watch it now. A major key to solving a lot of our economic problems lies in understanding and using data collected about cites and states. \nU.S. cities and states are jumping on this idea and our own Baltimore was one of the earliest adopters. I thought I’d make a list of all the cities that have made an effort to make civic data public. Here are a few I’ve found:\nBaltimore\nNew York City\nSan Francisco\nSeattle\nPortland\nBoston\nSan Diego (not sure if this is official)\nChicago\nAustin\nWashington D.C.\nPhiladelphia \nNew Orleans\nThere are also open data sites for many states:\nCalifornia\nWashington\nOregon\nIllinois\nUtah\nMaine\nCivic organizations are realizing that opening their data through APIs or by hosting competitions can lead to greater transparency, good advertising, and new and useful applications. If I had one data-related wish for 2012, it would be that the critical mass of data/statistics knowledge being developed could be used with these data to help solve some of our most pressing problems. \nUpdate: Oh Canada! In the comments Ani Ruhil points to some Canadian cities/provinces with open data pages. \nVancouver\nToronto\nWindsor\nOttawa\nEdmonton\nBritish Columbia\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-28-grad-students-in-bio-statistics-do-a-postdoc/",
    "title": "Grad students in (bio)statistics - do a postdoc!",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-28",
    "categories": [],
    "contents": "\nUp until about 20 years ago, postdocs were scarce in Statistics. In contrast, during the same time period, it was rare for a Biology PhD to go straight into a tenure track position.\nDriven mostly by the availability of research funding for those working in applied areas,  postdocs are becoming much more common in our field and I think this is great. It is great for PhD students to expand their horizons during two years in which they don’t have to worry about teaching, committee meetings, or grant writing. It is also great for those of us fortunate enough to work with well-trained, independent, energetic, bright, and motivated fresh PhDs. Many of our best graduates are electing to postpone their entry into tenure track jobs in favor of postdocs. Also students from other fields, computer science and engineering in particular, are taking postdocs with statisticians. I think these are both good trends. If they continue, the result will be that, as a field, we will become more well-rounded and productive. \nThis trend has been particularly beneficial for me. Most of the postdocs I have hired have come to me with a CV worthy of a tenure track job. They have been independent and worked more as collaborators than advisees. So why pass on more $ and prestige? A PhD in Statistics/Computer Science/Engineering can be on a very specific topic and students may not gain any collaborative experience whatsoever. A postdoc at Hopkins Biostat provides a new experience in a highly collaborative environment, with access to world leaders in the biomedical sciences, and where we focus on development of applied tools. The experience can also improve a student’s visibility and job prospects, while delaying the tenure clock until they have more publications under their belts.\nAn important thing you should be aware of is that in many departments you can negotiate the start of a tenure track position. So seriously consider taking 1-2 years of almost 100% research time before commencing the grind of a tenure track job. \nI’m not the only one who thinks postdocs are a good thing for our field and for biostatistics students. The column below was written by Terry Speed in November 2003 and is reprinted with permission from the IMS Bulletin, http://bulletin.imstat.org\n\nIn Praise of Postdocs\n\n\nI don’t know what proportion of IMS members have PhDs (or an equivalent) in probability or statistics, but I’d guess it’s fairly high. I don’t know what proportion of those that do have PhDs would also have formal post-doctoral research experience, but here I’d guess it’s rather low.\n\n\nWhy? One possible reason is that for much of the last 40 years, anyone completing a PhD in prob or stat and wanting a research career, could go straight into one. Prospective employers of people with PhDs in our field—be they universities, research institutes, national labs or companies—don’t require their novices to have completed a postdoc, and most graduating PhDs are only to happy to go straight into their first job.\n\n\nThis is in sharp contrast with the biological and physical sciences, where it is rare to appoint someone to a tenure-track faculty or research scientist position without their having completed one or more postdocs.\n\n\nThee number of people doing postdocs in probability or statistics has been growing over the last 15 years. This is in part due to the arrival on the scene of institutes such as the MSRI, IMA, IPAM, NISS, NCAR, and recently the MBI and SAMSI in the US, the Newton Institute in the UK, the Fields Institute in Canada, the Institut Henri Poincaré in France, and others elsewhere around the world. In such institutes short- term postdoc positions go with their current research programs, and there are usually a smaller number continuing for longer periods.\n\n\nIt is also the case that an increasing number of senior researchers are being awarded research funds to support postdocs in prob or stat, often in the newer, applied areas such as computational biology.\n\n\nAnd finally, it is has long been the case that many countries (Germany, Sweden, Switzerland, and the US, to name a few) have national grants supporting postdoctoral research in their own or, even better, another country. I think all of this is great, and would like to see this trend continue and strengthen.\n\n\nWhy do I think postdocs are a good thing? And why do I think young probabilists and statisticians should do one, even when they can get a good job without having done so?\n\n\nFor most of us, doing a PhD means getting totally absorbed in some relatively narrow research area for 2–3 years, treating that as the most important part of science for that time, and trying to produce some of the best work in that area. This is fine, and we get a PhD for our efforts, but is it good training for a lifelong research career? While it is obviously good preparation for doing more of the same, I don’t think it is adequate for research in general. I regard the successful completion of a PhD as (at least) evidence that the person in question can do research, but it doesn’t follow that they can go on and successfully do research in new area, or in a different environment, or without close supervision.\n\n\nPostdocs give you the chance to broaden, to learn new technical skills, to become acquainted with new areas, and to absorb the culture of a new institution, all at a time when your professional responsibilities are far fewer than they would have been had you taken that first “real” job. The postdoc period can be a wonderful time in your scientific life, one which sees you blossom, building on the confidence you gained by having completed your PhD, in what is still essentially a learning environment, but one where you can follow your own interests, explore new areas, and still make mistakes. At the worst, you have delayed your entry into the workforce two or three years, and you can still keep on working in your PhD area if you wish. The number of openings for researchers in prob or stat doesn’t fluctuate so much on this time scale, so you are unlikely to be worse off than the earnings foregone. At best, you will move into a completely new area of research, one much better suited to your personal interests and skills, perhaps also better suited to market demand, but either way, one chosen with your PhD experience behind you. This can greatly enhance your long-term career prospects and more than compensate for your delayed entry into the workforce.\n\n\nStudents: the time to think about this is now [November], not just as you are about to file your dissertation. And the choice is not necessarily one between immediate security and career development: you might be able to have both. You shouldn’t shy from applying for tenure-track jobs and postdocs at the same time, and if offered the job you want, requesting (say) two years’ leave of absence to do the postdoc you want. Employers who care about your career development are unlikely to react badly to such a request.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-21-an-r-function-to-map-your-twitter-followers/",
    "title": "An R function to map your Twitter Followers",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-21",
    "categories": [],
    "contents": "\nI wrote a little function to make a personalized map of who follows you or who you follow on Twitter. The idea for this function was inspired by some plots I discussed in a previous post. I also found a lot of really useful code over at flowing data here. \nThe function uses the packages twitteR, maps, geosphere, and RColorBrewer. If you don’t have the packages installed, when you source the twitterMap code, it will try to install them for you. The code also requires you to have a working internet connection. \nOne word of warning is that if you have a large number of followers or people you follow, you may be rate limited by Twitter and unable to make the plot.\nTo make your personalized twitter map, first source the function:\n\nsource(“http://biostat.jhsph.edu/~jleek/code/twitterMap.R&#8221;)\n\nThe function has the following form: \ntwitterMap <- function(userName,userLocation=NULL,fileName=”twitterMap.pdf”,nMax = 1000,plotType=c(“followers”,”both”,”following”))\nwith arguments:\nuserName - the twitter username you want to plot\nuserLocation - an optional argument giving the location of the user, necessary when the location information you have provided Twitter isn’t sufficient for us to find latitude/longitude data\nfileName - the file where you want the plot to appear\nnMax - The maximum number of followers/following to get from Twitter, this is implemented to avoid rate limiting for people with large numbers of followers. \nplotType - if “both” both followers/following are plotted, etc. \nThen you can create a plot with both followers/following like so: \n\n twitterMap(“simplystats”)\n\nHere is what the resulting plot looks like for our Twitter Account:\n\nIf your location can’t be found or latitude longitude can’t be calculated, you may have to chose a bigger city near you. The list of cities used by twitterMap can be found like so:\n\nlibrary(maps)\n\n\ndata(world.cities)\n\n\ngrep(“Baltimore”, world.cities[,1])\n\nIf your city is in the database, this will return the row number of the world.cities data frame corresponding to your city. \n\nIf you like this function you may also like our function to determine if you are a data scientist or to analyze your Google Scholar citations page.\n\n\n\n\n\nUpdate: The bulk of the heavy lifting done by these functions is performed by Jeff Gentry’s very nice twitteR package and code put together by Nathan Yau over at FlowingData. This is really an example of standing on the shoulders of giants. \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-19-on-hard-and-soft-money/",
    "title": "On Hard and Soft Money",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-19",
    "categories": [],
    "contents": "\nAs the academic job hunting season goes into effect many will be applying to a variety of different types of departments. In statistics, there is a pretty big separation between statistics departments, which tend to be in arts & sciences colleges, and biostatistics departments, which tend to be in medical or public health institutions. A key difference between these two types of departments is the funding model.\n\nStatistics department faculty tend to be on 9- or 10-month salaries with funding primarily coming from teaching classes (research funding can be obtained for the summer months). Biostatistics departments faculty tend to have 12-month salaries with a large chunk of funding coming from research grants. Statistics departments are sometimes called “hard money” departments (i.e. tuition money is “hard”) while biostatistics departments are “soft money”. Grant money is considered “soft” because it has a tendency to go away a bit more easily. As long as students want to attend a university, there will always be tuition.\nThe biostatistics department at Johns Hopkins is a soft money department. We tend to get the bulk of our salaries from research project grants. Statisticians can play two roles on research grants: as a co-investigator/collaborator and as a principal investigator (PI). I guess that’s true of anyone, but statisticians are very commonly part of research projects as co-investigators because pretty much every research project these days will need statistical advice or methodological development. Researchers often have trouble getting their grants funded if they don’t have a statistician on board. So there’s often plenty of funding to go around for statisticians. But the real problem is getting enough time to do the research you want to do. If you’re spending all your time doing other people’s work, then sure you’re getting paid, but you’re not getting things done that will advance your career.\nIn a soft money department, I can think of two ways to go. The first is to write your own grants with you as the PI. That way you can guarantee funding for yourself to do the things you find interesting (assuming your grant is funded!). The other approach is to collaborate on a project where the work you need to do is work you would have done anyway. That can be a happy coincidence because then you don’t have to deal with the administrative burden of running a research project. But this approach relies a bit on luck and on the research environment at your institution.\nMany job candidates tell me that they are worried about working in a soft money department because if they can’t get their grants funded then they will be in some sort of trouble. In hard money departments, at least the majority of their salary is guaranteed by the teaching they do. This is true to some extent, but I contend that they are worrying about the wrong thing, mainly money.\nWhat job candidates should really be worried about is whether the department will support them in their career. Candidates should be looking for departments that mentor their junior faculty and create an environment in which it will be easy to succeed. If you’re in a department that routinely hangs their junior faculty out to dry, you can have all the hard money you want and you’ll still be unhappy. A soft money department that supports their junior faculty will make sure the right structure is in place for faculty to succeed. \nHere are some things to look out for in any department, but perhaps more so in a soft money department:\nIs there administrative support staff to help with writing grants i.e. for drafting budgets, assembling biosketches, and other paperwork?\nAre their senior faculty around who have successfully written grants and would be willing to read your grants and give you feedback?\nIs the environment there sufficient for you to do the things you want to do? For example, are their excellent collaborators for you to work with? Powerful computing support? All these things will help you get an edge over people who don’t have easy access to these resources.\nBesides having a good idea, the environment can play a key role in writing a good grant. For starters, if all your collaborators are in the same building as you, it makes it a lot easier to coordinate meetings to discuss ideas and to do the preparation. If you’re trying to work with 4 different people in 4 different institutions (maybe in different timezones), things just get a little harder and maybe you don’t get the feedback you need.\nSimilarly, if you have a strong computing infrastructure in place, then you can test it out beforehand and see what its capabilities are. If you need to purchase the same infrastructure for yourself as part of a grant, then you won’t know what it can do until you get and set it up. In our department, we are constantly buying new systems for our computing center and there are always glitches in the beginning with new equipment and new software. If you can avoid having to do this, it makes the grant a lot easier to write.\nLastly, I’ll just say that if you’re in the position of applying for tenure-track academic jobs, you’re probably not lazy. So you’re going to do your work no matter where you go. You just need to find a place where you can get things done. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-18-new-features-on-simply-statistics/",
    "title": "New features on Simply Statistics",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-18",
    "categories": [],
    "contents": "\nCheck out our Editor’s Picks and Interviews pages. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-16-in-greece-a-statistician-faces-life-in-prison-for/",
    "title": "In Greece, a statistician faces life in prison for doing his job: calculating and reporting a statistic",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-16",
    "categories": [],
    "contents": "\nIn a recent post I described the importance of government statisticians. Well, apparently in Greece it is a dangerous job, as Andreas Georgiou, the person in charge of the Greek statistics office, found out.\n\nSo far, though, his efforts have been met with resistance, strikes and a criminal investigation that could lead to life in prison for Georgiou.\n\nWhat are his efforts ?\n\nHis first priority after he was appointed was to figure out how big Greece’s deficit really was back in 2009, when the crisis began. He looked through all the data and concluded that Greece’s deficit that year was 15.8 percent of GDP — higher what had previously been reported.\nEurostat, the central authority in Brussels, praised Georgiou’s methodology and blessed the number as true. The hundreds of Greek people who work beneath Georgiou — the old guard — did not.\n\nSo in response, the “old guard” decided to vote on the summary statistic:\n\nSkordas sits on a governing board for the statistics office. His board wanted to debate and vote on the deficit number before anyone in Brussels was allowed to see it. Georgiou, the technocrat, saw that as a threat to his independence. He refused. The number is the number, he said. It’s not something to be put up for a vote.\n\nDid they perform a Bayesian analysis based on the vote?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-16-interview-with-nathan-yau-of-flowingdata/",
    "title": "Interview with Nathan Yau of FlowingData",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-16",
    "categories": [],
    "contents": "\n\nNathan Yau\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNathan Yau is a graduate student in statistics at UCLA and the author of the extremely popular data visualization blog flowingdata.com. He recently published a book Visualize This-a really nice guide to modern data visualization using R, Illustrator and Javascript - which should be on the bookshelf of any statistician working on data visualization.\n\n\n\n\n\n\n\n\n\n\n\nDo you consider yourself a statistician/data scientist/or something else?\n\nStatistician. I feel like statisticians can call them data scientists, but not the other way around. Although with data scientists there’s an implied knowledge of programming, which statisticians need to get better at.\n\nWho have been good mentors to you and what qualities have been most helpful for you?\n\nI’m visualization-focused, and I really got into the area during a summer internship at The New York Times. Before that, I mostly made graphs in R for reports. I learned a lot about telling stories with data and presenting data to a general audience, and that has stuck with me ever since.\n\n\nSimilarly, my adviser Mark Hansen has showed me how data is more free-flowing and intertwined with everything. It’s hard to describe. I mean coming into graduate school, I thought in terms of datasets and databases, but now I see it as something more organic. I think that helps me see what the data is about more clearly.\n\nHow did you get into statistics/data visualization?\nIn undergrad, an introduction to statistics (for engineering) actually pulled me in. The professor taught with so much energy, and the material sort of clicked with me. My friends who were also taking the course complained and had trouble with it, but I wanted more for some reason. I eventually switched from electrical engineering to statistics.\n\nI got into visualization during my first year in grad school. My adviser gave a presentation on visualization, but from a media arts perspective rather than a charts-and-graphs-in-R-Tufte point of view. I went home after that class, googled visualization and that was that.\n\nWhy do you think there has been an explosion of interest in data visualization?\n\nThe Web is a really visual place, so it’s easy for good visualization to spread. It’s also easier for a general audience to read a graph than it is to understand statistical concepts. And from a more analytical point of view, there’s just a growing amount of data and visualization is a good way to poke around.\n\nOther than R, what tools should students learn to improve their data visualizations?\n\nFor static graphics, I use Illustrator all the time to bring storytelling into the mix or to just provide some polish. For interactive graphics on the Web, it’s all about JavaScript nowadays. D3, Raphael.js, and Processing.js are all good libraries to get started.\n\nDo you think the rise of infographics has led to a “watering down” of data visualization?\nSo I actually just wrote <a href=\"http://flowingdata.com/2011/12/08/on-low-quality-infographics\" target=\"_blank\">a post<\/a> along these lines. It&#8217;s true that there a lot of low-quality infographics, but I don&#8217;t think that takes away from visualization at all. It makes good work more obvious. I think the flood of infographics is a good indicator of people&#8217;s eagerness to read data.<\/div> \n<div class=\"im\">\n  <strong>How did you decide to write your book &#8220;Visualize This&#8221;?<\/strong>\n<\/div>\n\n<div class=\"im\">\n<\/div>\n\n<div class=\"im\">\n  Pretty simple. I get emails and comments all the time when I post graphics on FlowingData that ask how something was done. There aren&#8217;t many resources that show people how to do that. There are books that describe what makes good graphics but don&#8217;t say anything about how to actually go about doing it, and there are programming books for say, R, but are too technical for most and aren&#8217;t visualization-centric. I wanted to write a book that I wish I had in the early days.\n<\/div>\n\n<div class=\"im\">\n  <strong>Any final thoughts on statistics, data and visualization? <\/strong>\n<\/div>\n\n<div class=\"im\">\n  <strong><br /><\/strong>Keep an open mind. Oftentimes, statisticians seem to box themselves into positions of analysis and reports. Statistics is an applied field though, and now more than ever, there are opportunities to work anywhere there is data, which is practically everywhere.\n<\/div>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-14-dear-editors-associate-editors-referees-please-reject/",
    "title": "Dear editors/associate editors/referees, Please reject my papers quickly",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-14",
    "categories": [],
    "contents": "\nThe review times for most journals in our field are ridiculous. Check out Figure 1 here. A careful review takes time, but not six months. Let’s be honest, those papers are sitting on desks for the great majority of those six months. But here is what really kills me: waiting six months for a review basically saying the paper is not of sufficient interest to the readership of the journal. That decision you can come to in half a day. If you don’t have time, don’t accept the responsibility to review a paper.\nI like sharing my work with my statistician colleagues, but the Biology journals never  do this to me. When my paper is not of sufficient interest, these journals reject me in days not months. I sometimes work on topics that are fast pace and many of my competitors are not statisticians. If I have to wait six months for each rejection, I can’t compete. By the time the top three applied statistics journals reject the paper, more than a year goes by and the paper is no longer novel. Meanwhile I can go through Nature Methods, Genome Research, and Bioinformatics in less than 3 months.\nNick Jewell once shared an idea that I really liked. It goes something like this. Journals in our field will accept every paper that is correct. The editorial board, with the help of referees, assigns each paper into one of five categories A, B, C, D, E based on novelty, importance, etc… If you don’t like the category you are assigned, you can try your luck elsewhere. But before you go, note that the paper’s category can improve after publication based on readership feedback. While we wait for this idea to get implemented, I please ask that if you get one of my papers and you don’t like it, reject it quickly. You can write this review: “This paper rubbed me the wrong way and I heard you like being rejected fast so that’s all I am going to say.” Your comments and critiques are valuable, but not worth the six month wait. \nps -  I have to admit that the newer journals have not been bad to me in this regard. Unfortunately, for the sake of my students/postdocs going into the job market and my untenured jr colleagues, I feel I have to try the established top journals first as they still impress more on a CV.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-14-smoking-is-a-choice-breathing-is-not/",
    "title": "Smoking is a choice, breathing is not.",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-14",
    "categories": [],
    "contents": "\nOver the last week or so I’ve been posting about the air pollution levels in Beijing, China. The twitter feed from the US Embassy there makes it easy to track the hourly levels of fine particulate matter (PM2.5) and you can use this R code to make a graph of the data.\nOne problem with talking about particulate matter levels is that the units are a bit abstract. We usually talk in terms of micrograms per cubic meter (mcg/m^3), which is a certain mass of particles per volume of air. The 24-hour national ambient air quality standard for fine PM in the US is 35 mcg/m^3. But what does that mean in reality?\nC. Arden Pope III and colleagues recently wrote an interesting paper in Environmental Health Perspectives on the dose-response relationship between particles and lung cancer and cardiovascular disease. They combined data from air pollution studies and smoking studies to estimate the dose-response curve for a very large range of PM levels. Ambient air pollution, not surprisingly, is on the low-end of PM exposure, followed by second hand smoke, followed by active smoking. One challenge they faced is putting everything on the same scale in terms of PM exposure so that the different studies could be compared.\nHere are the important details: On average, actively smoking a cigarette generates a dose of about 12 milligrams (mg) of particulate matter. Daily inhalation rates obviously depend on your size, age, physical activity, health, and other factors, but in adults they range from about 13 to 23 cubic meters of air per day. For convenience, I’ll just take the midpoint of that range, which is 18 cubic meters per day.\nIf your city’s fine PM levels were compliant with the US national standard of 35 mcg/m^3, then in the worst case scenario you’d be breathing in about 630 micrograms of particles per day, which is about 0.05 cigarettes (1 cigarette every 20 days). Sounds like it’s not too bad, but keep in mind that most of the increase in risk from smoking is seen in the low range of the dose-response curve (although this is obviously very low).\nIf we move now to Beijing, where 24-hour average levels can easily reach up to 300 mcg/m^3 (and indoor levels can reach 200 mcg/m^3), then we’re talking about a daily dose of almost half a cigarette. Now, half a cigarette might still seem like not that much, but keep in mind that pretty much everyone is exposed: old and young, sick and healthy_._ Not everyone gets the same dose because of variation in inhalation rates, but even the low end of the range gives you about 0.3 cigarettes. \nBeijing is hardly alone here, as a number of studies in Asian cities show comparable levels of fine PM. I’ve redone my previous plot of PM2.5 in Beijing in terms of number cigarettes per day. Here’s the last 2 months in Beijing (for an average adult).\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-12-the-supreme-courts-interpretation-of-statistical/",
    "title": "The Supreme Court's interpretation of statistical correlation may determine the future of personalized medicine",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-12",
    "categories": [],
    "contents": "\nSummary/Background\nThe Supreme Court heard oral arguments last week in the case Mayo Collaborative Services vs. Prometheus Laboratories (No 10-1150). At issue is a patent Prometheus Laboratories holds for making decisions about the treatment of disease on the basis of a measurement of a specific, naturally occurring molecule and a corresponding calculation. The specific language at issue is a little technical, but the key claim from the patent under dispute is:\n\nA method of optimizing therapeutic efficacy for treatment of an immune-mediated gastrointestinal disorder, comprising:\nadministering a drug providing 6-thioguanine to a subject having saidimmune-mediated gastrointestinal disorder; and\ndetermining the level of 6-thioguanine in said subject having said immune-mediated gastrointestinal disorder,\nwherein the level of 6-thioguanine less than about 230 pmol per 8x10^8 red blood cells indicates a need to increase the amount of said drug subsequently administered to said subject and\nwherein the level of 6-thioguanine greater than about 400 pmol per 8x10^8 red blood cells indicates a need to decrease the amount of said drug subsequently administered to said subject.\n\nSo basically the patent is on a decision made about treatment on the basis of a statistical correlation. When the levels of a specific molecule (6-thioguanine) are too high, then the dose of a drug (thiopurine) should be decreased, if they are too low then the dose of the drug should be increased. Here (and throughout the post) correlation is interpreted more loosely as a relationship between two variables; rather than the strict definition as the linear relationship between two quantitative variables.\nThis correlation between levels of 6-thioguanine and patient response was first reported by a group of academics in a paper in 1996. Prometheus developed a diagnostic test based on this correlation. Doctors (including those at the Mayo clinic) would draw blood, send it to Prometheus, who would calculate the levels of 6-thioguanine and report them back.\nAccording to Mayo’s brief, some Doctors at the Mayo, who used this test, decided it was possible to improve on the test. So they developed their own diagnostic test, based on a different measurement of 6-thioguanine (6-TGN) and reported different information including:\n\nA blood reading greater than 235 picomoles of 6-TGN is a “target therapeutic range,” and a reading greater than 250 picomoles of 6-TGN is associated with remission in adult patients; and\nA blood reading greater than 450 picomoles of 6-TGN indicates possible adverse health effects, but in some instances levels over 700 are associated with remission without significant toxicity, while a “clearly defined toxic level” has not been established; and\nA blood reading greater than 5700 picomoles of 6-MMP is possibly toxic to the liver.\n\nThey subsequently created their own proprietary test and started to market that test. At which point Prometheus sued the Mayo Clinic for infringement. The most recent decision on the case was made by a federal circuit court who upheld Prometheus’ claim. A useful summary is here.\nThe arguments for the two sides are summarized in the briefs for each side; for Mayo:\n\nWhether 35 U.S.C. § 101 is satisfied by a patent claim that covers observed correlations between blood test results and patient health, so that the patent effectively preempts use of the naturally occurring correlations, simply because well-known methods used to administer prescription drugs and test blood may involve “transformations” of body chemistry.\n\nand for Prometheus:\n\nWhether the Federal Circuit correctly held that concrete methods for improving the treatment of patients suffering from autoimmune diseases by using individualized metabolite measurements to inform the calibration of the patient’s dosages of synthetic thiopurines are patentable processes under 35 U.S.C. §101.\n\nBasically, Prometheus claims that the patent covers cases where doctors observe a specific data point and make a decision about a specific drug on the basis of that data point and a known correlation with patient outcomes. Mayo, on the other hand, says that since the correlation between the data and the outcome are naturally occurring processes, they can not be patented.\nIn the oral arguments, the attorney for Mayo makes the claim that the test is only patentable if Prometheus specifies a specific level for 6-thioguanine and a specific treatment associated with that level (see page 21-24 of the transcript). He then goes on to suggest that the Mayo would then be free to pick another level and another treatment option for their diagnostic test. Justice Breyer disagrees even with this specific option (see page 38 of the transcript and his fertilizer example). He has made this view known before in his dissent to the dismissal of the Labcorp writ of certori (a very similar case focusing on whether a correlation can be patented).\nBrief summary: Prometheus is trying to patent a correlation between a molecule’s level and treatment decisions. Mayo is claiming this is a natural process and can’t be patented.\nImplications for Personalized Medicine (a statistician’s perspective)\nI believe this case has major potential consequences for the entire field of personalized medicine. The fundamental idea of personalized medicine is that treatment decisions for individual patients will be tailored on the basis of data collected about them and statistical calculations made on the basis of that data (i.e. correlations, or more complicated statistical functions).\nAccording to my interpretation, if the Supreme Court rules in favor of Mayo in a broad sense, then this suggests that decisions about treatment made on the basis of data and correlation are not broadly patentable. In both the Labcorp dissent and the oral arguments for the Prometheus case, Justice Breyer argues that the process described by the patents:\n\n…instructs the user to (1) obtain test results and (2) think about them.\n\nHe suggests that these are natural correlations and hence can not be patented, just the way a formula like E = mc^2 can not be patented. The distinction seems to be subtle, where E=mc^2 is a formula that exactly describes a property of nature, the observed correlation is an empirical estimate of a parameter calculated on the basis of noisy data.\nFrom a statistical perspective, there is little difference between calculating a correlation and calculating something more complicated, like the Oncotype DXsignature. Both return a score that can be used to determine treatment or other health care decisions. In some sense, they are both “natural phenomena” - one is just more complicated to calculate than the other. So it is not surprising that Genomic Health, the developers of Oncotype, have filed an amicus in favor of Prometheus.\nOnce a score is calculated, regardless of the level of complication in calculating that score, the personalized decision still comes down to a decision made by a doctor on the basis of a number.So if the court broadly decides in favor of Mayo, from a statistical perspective, this would seemingly pre-empt patenting any personalized medicine decision made on the basis of observing data and making a calculation.\nUnlike traditional medical procedures like surgery, or treatment with a drug, these procedures are based on data and statistics. But in the same way, a very specific set of operations and decisions is taken with the goal of improving patient health. If these procedures are broadly ruled as simply “natural phenomena”, it suggests that the development of personalized decision making strategies is not, itself, patentable. This decision would also have implications for other companies that use data and statistics to make money, like software giant SAP, which has also filed anamicus brief in support of the federal circuit court opinion (and hence Prometheus).\nA large component of medical treatment in the future will likely be made on the basis of data and statistical calculations on those data - that is the goal of personalized medicine. So the Supreme Court’s decision about the patentability of correlation has seemingly huge implications for any decision made on the basis of data and statistical calculations.Regardless of the outcome, this case lends even further weight to the idea that statistical literacy is critical, including for Supreme Court justices.\nSimply Statistics will be following this case closely; look for more in depth analysis in future blog posts.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-09-interview-w-mario-marazzi-puerto-rico-institute-of/",
    "title": "Interview w/ Mario Marazzi, Puerto Rico Institute of Statistics Director, on the importance of Government Statisticians",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-09",
    "categories": [],
    "contents": "\n\n[Desplace hacia abajo para traducción al español]\n\n\nIn my opinion, the importance of government statisticians is underappreciated. In the US, agencies such as the CDC, the Census Bureau, and the Bureau of Labor Statistics employ statisticians to help collect and analyze data that contribute to important policy decisions. How many students will enroll in public schools this year? Is there a type II diabetes epidemic? Is unemployment rising? How many homeless people are in Los Angeles? The answers to these questions can guide policy and spending decisions and they can’t be answered without the help of the government statisticians that collect and analyze relevant data.\n\n\nUntil recently the Puerto Rican government had no formal mechanisms for collecting data. Puerto Rico, an unincorporated territory of the United States, has many serious economic and social problems .  With a very high murder rate, less than 50% of the working-age population in the labor force, an economy that continues to worsen after 5 years of recession , and a substantial traffic problem , Puerto Rico can certainly benefit from sound government statistics to better guide policy-making.  Better measurement, information and knowledge can only improve the situation.\n\n\nIn 2007, the Puerto Rico Institute of Statistics was founded. Mario Marazzi, who obtained his PhD in Economics from Cornell University, left a prestigious job at the Federal Reserve to become the first Executive Director of the Institute.  Given the complicated political landscape in Puerto Rico, Mario made an admirable sacrifice for his home country. He was kind enough to answer some questions for Simply Statistics:\n\n\nWhat is the biggest success story of the Institute?\n\n\nI would say that our biggest success story has been to revive the idea that high-quality statistics are critical for the success of any organization in Puerto Rico.  For too long, statistics were neglected and even abused in Puerto Rico.  There is now a palpable sense in Puerto Rico that it is important to devote resources and time to ensure that data are produced with care.\n\n\nWe have also undertaken a number of critical statistical projects since our inauguration in 2007.  For instance, the Institute completed the revision to Puerto Rico’s Consumer Price Index, after identifying that official inflation had been overestimated by more than double for 15 years.  The Institute revised Puerto Rico’s Mortality Statistics, after detecting the use of an inconsistent selection methodology for the cause of death, as well as discovering thousands of deaths that had not been previously included in the official data.  We also undertook Puerto Rico’s first-ever Science and Technology Survey that allowed us to measure the economic impact of Research and Development activities in Puerto Rico.\n\n\nWhat discovery, made from collecting data in Puerto Rico, has most surprised you?\n\n\nWe performed a study on migration patterns during the last decade.  From anecdotal evidence, it was fairly clear that in the last five years there had been an elevated level of migration out of Puerto Rico.  Nevertheless, the data revealed a few stunning conclusions.  For five consecutive years, about 1 percent of Puerto Rico’s population simply left Puerto Rico every year, even after taking into account the people who migrated to Puerto Rico.  The demographic consequences were significant: migration had been accelerating the aging of Puerto Rico’s population, and people who left Puerto Rico had a greater level of educational achievement than those who arrived.  In fact, for the first-time ever in recorded history, Puerto Rico’s population actually declined between the 2000 and 2010 Census.  Despite declining fertility rates, it is now clear migration was the cause of the overall population decrease.\n\n\nAre government agencies usually willing to cooperate with the Institute? If not, what resources does the Institute have available to make them comply?\n\n\nFrequently, statistical functions are not very high on policymakers’ lists of priorities.  As a result, government statisticians are usually content to collaborate with the Institute, since we can bring resources to help solve the common problems they face.\n\n\nAt times, some agencies can be reluctant to undertake the changes needed to produce high-quality statistics.  In these instances, the Institute is endowed with the authority by law to move the process along, through statistical policy mandates approved by the Board of Directors of the Institute. \n\n\nIf there is a particular agency that excels at collecting and sharing data, can others learn from them?\n\n\nDefinitely, we encourage agencies to share their best practices with one another.  To facilitate this process, the Institute has the responsibility of organizing the Puerto Rico Statistical Coordination Committee, where representatives from each agency can share practical experiences, and enhance interagency coordination.\n\n\nDo you think Puerto Rico needs more statisticians?\n\n\nAbsolutely.  Some of our brightest minds in statistics work outside of Puerto Rico, both in Universities and in the Federal Government.  Puerto Rico needs an injection of human resources to bring its statistical system up to global standards.\n\n\nWhat can academic statisticians do to help institutes such as yours?\n\n\nAcademic statisticians are instrumental to furthering the mission of the Institute.  Governments produce statistics in a wide array of disciplines.  Each area can have very specific and unique methodologies.  It is impossible for one to be an expert in every methodology. \n\n\nAs a result, the Institute depends on the collaboration of academic statisticians that can bring to bear their expertise in specific fields.  For example, academic biostatisticians can help identify needed improvements to existing methodologies in health statistics.  Index theorists can train government statisticians in the latest index methodologies.  Computational statisticians can analyze large data sets to help us explain the otherwise unexplained behavior of the data. \n\n\nWe also host several Puerto Rico datasets on the Institute’s website, which were provided by professors from a number of different fields.  \n\n\nEntrevista con Mario Marazzi (version en español)\n\n\nEn mi opinión, la importancia de los estadísticos que trabajan para el gobierno se subestima.En los EEUU, agencias como el Center for Disease Control, el Census Bureau y el Bureau of Labor Statistics emplean estadísticos para ayudar a recopilar y analizar datos que contribuyen a importantes decisiones de política pública. Por ejemplo, ¿cuántos estudiantes se matricularán en las escuelas públicas este año? ¿Hay una epidemia de diabetes tipo II?  ¿El desempleo está aumentando? ¿Cuántos deambulantes viven en Los Ángeles?  Las respuestas a estas preguntas ayudan determinar las decisiones presupuestarias y de política pública y no se pueden contestar sin la ayuda de los estadísticos del gobierno que recogen y analizan los datos pertinentes.\n\n\nHasta hace poco el gobierno de Puerto Rico no tenía mecanismos formales de recolección de datos. Puerto Rico, un territorio no incorporado de Estados Unidos, tiene muchos problemas socioeconómicos. Con una tasa de asesinatos muy alta, menos de 50% de la población con edad de trabajar en la fuerza laboral, una economía que sigue empeorando después de 5 años de recesión y problemas serios de tráfico, Puerto Rico se beneficiaría de estadísticas gubernamentales de alta calidad para mejor guíar la formulación de política pública. Mejores medidas, información y conocimientos sólo pueden mejorar la situación.\n\n\nEn 2007, se inaguró el Institute de Estadísticas de Puerto Rico. Mario Marazzi, quien obtuvo su doctorado en Economía de la Universidad de Cornell, dejó un trabajo prestigioso en Federal Reserve para convertirse en el primer Director Ejecutivo del Instituto.\n\n\nTomando en cuenta el complicado panorama político en Puerto Rico, Mario hizo un sacrificio admirable por su país y cordialmente aceptó contestar unas preguntas para nuestro blog:\n\n\n¿Cuál ha side el mayor éxito del Instituto?\n\n\nYo diría que nuestro mayor éxito ha sido revivir la idea de que las estadísticas de alta calidad son cruciales para el éxito de cualquier organización en Puerto Rico.  Por mucho tiempo, las estadísticas fueron descuidadas e incluso abusadas en Puerto Rico. En la actualidad existe una sensación palpable en Puerto Rico que es importante dedicar recursos y tiempo para asegurarse de que los datos se produzcan con cuidado.\n\n\nTambién, desde nuestra inauguración en 2007, hemos realizado una serie de proyectos críticos de estadística.  Por ejemplo, el Instituto concluyó la revisión del Índice de Precios al Consumidor de Puerto Rico, después de identificar que la inflación oficial había sido sobreestimada por más del doble durante 15 años. El Instituto revisó las Estadísticas de Mortalidad de Puerto Rico, después de detectar el uso de una metodología de selección inconsistente para determinar la causa de muerte y tras descubrir miles de muertes que no habían sido incluidos en los datos oficiales.  Además, realizamos por primera vez en Puerto Rico la primera Encuesta de Ciencia y Tecnología que nos permitió medir el impacto económico de las actividades de investigación y desarrollo en Puerto Rico.\n\n\n¿Cuál descubrimiento, realizado a partir de la recopilación de datos en Puerto Rico, más te ha sorprendido?\n\n\nNosotros realizamos un estudio sobre los patrones de migración durante la última década. A partir de la evidencia anecdótica, era bastante claro que durante los últimos cinco años ha habido un nivel elevado de emigración de Puerto Rico. Sin embargo, los datos revelaron algunas conclusiones sorprendentes. Durante cinco años consecutivos, 1 por ciento de la población de Puerto Rico se ha ido de Puerto Rico todos los años, incluso después de tomar en cuenta la gente que emigró a Puerto Rico. Las consecuencias demográficas eran importantes: la migración ha acelerado el envejecimiento de la población de Puerto Rico y las personas que se fueron de Puerto Rico tienen un mayor nivel de preparación escolar que los que llegaron. De hecho, por primera vez en la historia, la población de Puerto Rico disminuyó entre el Censo de 2000 y el del 2010.  A pesar de tasas de fecundidad que disminuyen, ahora está claro que la migración es la causa principal de la reducción de población.\n\n\n¿Por lo general, las agencias gubernamentales están dispuestas a cooperar con el Instituto?  Si no, ¿qué recursos tiene disponible el Instituto para obligarlos?\n\n\nFrecuentemente, las estadísticas no aparecen muy altas en las listas de prioridades de los políticos. Como resultado, los estadísticos del gobierno por lo general están contentos de colaborar con el Instituto, ya que nosotros podemos aportar recursos para ayudar a resolver los problemas comunes a que se enfrentan.\n\n\nA veces, algunas agencias pueden mostrarse reacios a emprender los cambios necesarios para producir estadísticas de alta calidad. En estos casos, el Instituto posee la autoridad legal de acelerar el proceso, a través de mandatos aprobados por el Consejo de Administración del Instituto.\n\n\nSi hay un organismo en particular que se destaca en la recopilación y el intercambio de datos, ¿otros pueden aprender de ellos?\n\n\nDefinitivamente.  Nosotros animamos a las agencias a compartir sus mejores prácticas con otros. Para facilitar este proceso, el Instituto tiene la responsabilidad de organizar el Comité de Coordinación Estadística de Puerto Rico, donde representantes de cada agencia pueden compartir experiencias prácticas y mejorar la coordinación interinstitucional.\n\n\n ¿Cree usted que Puerto Rico necesita más estadísticos?\n\n\nPor supuesto. Algunas de nuestras mentes más brillantes en estadísticas trabajan fuera de Puerto Rico, tanto en las universidades como en el Gobierno Federal. Puerto Rico necesita una inyección de recursos humanos para que su sistema estadístico llegue a los estándares mundiales.\n\n\n¿Qué pueden hacer los estadísticos académicos hacer ayudar a instituciones como la suya?\n\n\nLos estadísticos académicos son fundamentales para promover la misión del Instituto. Los gobiernos generan las estadísticas en una amplia gama de disciplinas. Cada área puede tener metodologías muy específicas y únicas. Es imposible que uno sea un experto en cada metodología.\n\n\nComo resultado, el Instituto cuenta con la colaboración de estadísticos académicos que pueden ejercer sus conocimientos en campos específicos. Por ejemplo, los bioestadísticos académicos pueden ayudar a identificar las mejoras necesarias a las metodologías existentes en el contexto de la salud pública.  Los “Index theorists” pueden entrenar a los estadísticos del gobierno en las últimas metodologías de índice. Los estadísticos computacionales pueden analizar grandes “datasets” que nos ayudan explicar comportamientos de otra manera  no explicados de los datos.\n\n\nTambién organizamos varios datasets de Puerto Rico en la página web del Instituto, que fueron proporcionados por profesores en varios campos diferentes.\n\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-08-plotting-beijingair-data/",
    "title": "Plotting BeijingAir Data",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-08",
    "categories": [],
    "contents": "\nHere’s a bit of R code for scraping the BejingAir Twitter feed and plotting the hourly PM2.5 values for the past 24 hours. The script defaults to the past 24 hours but you can modify that by simply changing the value for the variable ‘n’. \nYou can just grab the code from this R script. Note that you need to use the latest version of the ‘twitteR’ package because the data structure has changed from previous versions.\nUsing a modified version of the code in the script, I made a plot of the 24-hour average PM2.5 levels in Beijing over the last 2 months or so. The dashed line shows the US national ambient air quality standard for 24-hour average PM2.5. Note that the plot below is 24-hour averages so it is comparable to the US standard and also looks (somewhat) less extreme than the hourly values.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-07-clean-air-a-luxury-in-beijings-pollution-zone/",
    "title": "Clean Air A 'Luxury' In Beijing's Pollution Zone",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-07",
    "categories": [],
    "contents": "\nClean Air A ‘Luxury’ In Beijing’s Pollution Zone\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-07-outrage-grows-over-air-pollution-and-chinas-response/",
    "title": "Outrage Grows Over Air Pollution and China’s Response",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-07",
    "categories": [],
    "contents": "\nOutrage Grows Over Air Pollution and China’s Response\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-06-beijing-air-contd/",
    "title": "Beijing Air (cont'd)",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-06",
    "categories": [],
    "contents": "\nFollowing up a bit on my previous post on air pollution in Beijing, China, my brother forwarded me a link to some work conducted by Steven Q. Andrews on comparing particulate matter (PM) air pollution in China versus Europe and the US. China does not officially release fine PM measurements (PM2.5) and furthermore does not have an official standard for that metric. In the US, PM standards are generally focused on PM2.5 now as opposed to PM10 (which includes coarse thoracic particles). Apparently, China is proposing a standard for PM2.5 but it has not yet been implemented.\nThe main issue seems to be that China has a somewhat different opinion about what it means to be a “bad” pollution day. In the US, the daily average national ambient air quality standard for PM2.5 is 35 mcg/m^3, whereas the proposed standard in China is 75 mcg/m^3. The WHO recommends PM2.5 levels be below 25 mcg/m^3. In China, days under 35 would be considered “excellent” and days under 75 would be considered “good”.\nIt’s a bit difficult to understand what this means here because in the US we so rarely see days where the daily average is above 75 mcg/m^3. In fact, for the period 1999-2008, if you look across the entire PM2.5 monitoring network for the US, you see that 99% of days fell below the level of 75 mcg/m^3. So seeing a day like that would be quite a rare event indeed.\nThe Chinese government has consistently claimed that air pollution has improved over time. But Andrews notes\n\n…these so-called improvements are due to irregularities in the monitoring and reporting of air quality – and not to less polluted air. Most importantly, the government changed monitoring station locations twice. In 2006, it shut down the two most polluted stations and then, in 2008, began monitoring outside the city, beyond the sixth ring road, which is 15 to 20 kilometres from Beijing’s centre.\n\nAndrews has previously published on inconsistencies between Beijing’s claims of “blue sky days” and the actual monitoring of PM in a paper in Environmental Research Letters. That paper showed an unusually high number of measurements falling just below the cutoff for a “blue sky day”. The reason for this pattern is not clear but it raises questions about the quality of the official monitoring data.\nChina has a novel propagandistic approach to air pollution regulation, which is to separate the data from the interpretation. So a day that has PM2.5 levels at 75 mcg/m^3 is called “good” and as long as you have a lot of “good” or “excellent” days, then you are set. The problem is that you can call something a “blue sky day” or whatever you want, but people still have to suffer the real consequences of high PM days. It’s hard to “relabel” increased asthma attacks, irritated respiratory tracts, and myocardial infarctions. \nAndrews notes\n\nAs the China Daily recently wrote: “All of the residents in the city are aware of the poor air quality, so it does not make sense to conceal it for fear of criticism.”\n\nMaybe the best way to conceal the air pollution is to actually get rid of it?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-06-who-can-resist-biostatistics-ryan-gosling/",
    "title": "Who can resist Biostatistics Ryan Gosling?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-06",
    "categories": [],
    "contents": "\nWho can resist Biostatistics Ryan Gosling?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:53:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-05-online-learning-personalized/",
    "title": "Online Learning, Personalized",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-05",
    "categories": [],
    "contents": "\nOnline Learning, Personalized\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-05-preventing-errors-through-reproducibility/",
    "title": "Preventing Errors through Reproducibility",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-05",
    "categories": [],
    "contents": "\nChecklist mania has hit clinical medicine thanks to people like Peter Pronovost and many others. The basic idea is that simple and short checklists along with changes to clinical culture can prevent major errors from occurring in medical practice. One particular success story is Pronovost’s central line checklist which dramatically reduced bloodstream infections in hospital intensive care units.  \nThere are three important points about the checklist. First, it neatly summarizes information, bringing the latest evidence directly to clinical practice. It is easy to follow because it is short. Second, it serves to slow you down from whatever you’re doing. Before you cut someone open for surgery, you stop for a second and run the checklist. Third, it is a kind of equalizer that subtly changes the culture: everyone has to follow the checklist, no exceptions. A number of studies have now shown that when clinical units follow checklists, infection rates go down and hospital stays are shorter compared to units using standard procedures. \nHere’s a question: What would it take to convince you that an article’s results were reproducible, short of going in and reproducing the results yourself? I recently raised this question in a talk I gave at the Applied Mathematics Perspectives conference. At the time I didn’t get any responses, but I’ve had some time to think about it since then.\nI think most people are thinking of this issue along the lines of “The only way I can confirm that an analysis is reproducible is to reproduce it myself”. In order for that to work, everyone needs to have the data and code available to them so that they can do their own independent reproduction. Such a scenario would be sufficient (and perhaps ideal) to claim reproducibility, but is it strictly necessary? For example, if I reproduced a published analysis, would that satisfy you that the work was reproducible, or would you have to independently reproduce the results for yourself? If you had to choose someone to reproduce an analysis for you (not including yourself), who would it be?\nThis idea is embedded in the reproducible research policy at Biostatistics, but of course we make the data and code available too. There, a (hopefully) trusted third party (the Associate Editor for Reproducibility) reproduces the analysis and confirms that the code was runnable (at least at that moment in time). \nIt’s important to point out that reproducible research is not only about correctness and prevention of errors. It’s also about making research results available to others so that they may more easily build on the work. However, preventing errors is an important part and the question is then what is the best way to do that? Can we generate a reproducibility checklist?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-03-citizen-science-makes-statistical-literacy-critical/",
    "title": "Citizen science makes statistical literacy critical",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-03",
    "categories": [],
    "contents": "\nIn today’s Wall Street Journal, Amy Marcus has a piece on the Citizen Science movement, focusing on citizen science in health in particular. I am fully in support of this enthusiasm and a big fan of citizen science - if done properly. There have already been some pretty big success stories. As more companies like Fitbit and 23andMe spring up, it is really easy to collect data about yourself (right Chris?). At the same time organizations like Patients Like Me make it possible for people with specific diseases or experiences to self-organize. \nBut the thing that struck me the most in reading the article is the importance of statistical literacy for citizen scientists, reporters, and anyone reading these articles. For example the article says:\n\nThe questions that most people have about their DNA—such as what health risks they face and how to prevent them—aren’t always in sync with the approach taken by pharmaceutical and academic researchers, who don’t usually share any potentially life-saving findings with the patients.\n\nI think its pretty unlikely that any organization would hide life-saving findings from the public. My impression from reading the article is that this statement refers to keeping results blinded from patients/doctors during an experiment or clinical trial. Blinding is a critical component of clinical trials, which reduces many potential sources of bias in the results of a study. Obviously, once the trial/study has ended (or been stopped early because a treatment is effective) then the results are quickly disseminated.\nSeveral key statistical issues are then raised in bullet-point form without discussion: \n\nAmateurs may not collect data rigorously, they say, and may draw conclusions from sample sizes that are too small to yield statistically reliable results. \nHaving individuals collect their own data poses other issues. Patients may enter data only when they are motivated, or feeling well, rendering the data useless. In traditional studies, both doctors and patients are typically kept blind as to who is getting a drug and who is taking a placebo, so as not to skew how either group perceives the patients’ progress.\n\nThe article goes on to describe an anecdotal example of citizen science - which suffers from a key statistical problem (small sample size):\n\nLast year, Ms. Swan helped to run a small trial to test what type of vitamin B people with a certain gene should take to lower their levels of homocysteine, an amino acid connected to heart-disease risk. (The gene affects the body’s ability to metabolize B vitamins.)\nSeven people—one in Japan and six, including herself, in her local area—paid around $300 each to buy two forms of vitamin B and Centrum, which they took in two-week periods followed by two-week “wash-out” periods with no vitamins at all.\n\nThe article points out the issue:\n\nThe scientists clapped politely at the end of Ms. Swan’s presentation, but during the question-and-answer session, one stood up and said that the data was not statistically significant—and it could be harmful if patients built their own regimens based on the results.\n\nBut doesn’t carefully explain the importance of sample size, suggesting instead that the only reason why you need more people is “insure better accuracy”. \nIt strikes me that statistical literacy is critical if the citizen science movement is going to go forward. Ideas like experimental design, randomization, blinding, placebos, and sample size need to be in the toolbox of any practicing citizen scientist. \nOne major drawback is that there are very few places where the general public can learn about statistics. Mostly statistics is taught in university courses. Resources like the Kahn Academy and the Cartoon Guide to Statistics exist, but are only really useful if you are self motivated and have some idea of math/statistics to begin with. \nSince knowledge of basic statistical concepts is quickly becoming indispensable for citizen science or even basic life choices like deciding on healthcare options, do we need “adult statistical literacy courses”? These courses could focus on the basics of experimental design and how to understand results in stories about science in the popular press. It feels like it might be time to add a basic understanding of statistics and data to reading/writing/arithmetic as critical life skills. I’m not the only one who thinks so.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-03-new-s-e-c-tactics-yield-actions-against-hedge-funds/",
    "title": "New S.E.C. Tactics Yield Actions Against Hedge Funds",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-03",
    "categories": [],
    "contents": "\nNew S.E.C. Tactics Yield Actions Against Hedge Funds\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-03-reverse-scooping/",
    "title": "Reverse scooping",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-03",
    "categories": [],
    "contents": "\nI would like to define a new term: reverse scooping is when someone publishes your idea after you, and doesn’t cite you. It has happened to me a few times. What does one do? I usually send a polite message to the authors with a link to my related paper(s). These emails are usually ignored, but not always. Most times I don’t think it is malicious though. In fact, I almost reverse scooped a colleague recently.  People arrive at the same idea a few months (or years) later and there is just too much literature to keep track-off. And remember the culprit authors were not the only ones that missed your paper, the referees and associate editor missed it as well. One thing I have learned is that if you want to claim an idea, try to include it in the title or abstract as very few papers get read cover-to-cover.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-03-the-worlds-has-changed-from-analogue-to-digital-and/",
    "title": "The worlds has changed from analogue to digital and it's time mathematical education makes the change too.",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-03",
    "categories": [],
    "contents": "\nThe worlds has changed from analogue to digital and it’s time mathematical education makes the change too.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-02-reproducible-research-in-computational-science/",
    "title": "Reproducible Research in Computational Science",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-02",
    "categories": [],
    "contents": "\nFirst of all, thanks to Rafa for scooping me with my own article. Not sure if that’s reverse scooping or recursive scooping or….\nThe latest issue of Science has a special section on Data Replication and Reproducibility. As part of the section I wrote a brief commentary on the need for reproducible research in computational science. Science has a pretty tight word limit for it’s commentaries and so it was unfortunately necessary to omit a number of relevant topics.\nThe editorial introducing the special section, as well as a separate editorial in the same issue, seem to emphasize the errors/fraud angle. This might be because Science has once or twice been at the center of instances of scientific fraud. But as I’ve said previously (and a point I tried to make in the commentary), reproducibility is not needed soley to prevent fraud, although that is an important objective. Another important objective is getting ideas across and disseminating knowledge. I think this second objective often gets lost because there’s a sense that knowledge dissemination already happens and that it’s the errors that are new and interesting. While the errors are perhaps new, there is a problem of ideas not getting across as quickly as they could because of a lack of code and/or data. The lack of published code/data is arguably holding up the advancement of science (if not Science).\nOne important idea I wanted to get across was that we can ramp up to achieve the ideal scenario, if getting there immediately is not possible. People often get hung up on making the data available but I think a substantial step could be made by simply making code available. Why doesn’t every journal just require it? We don’t have to start with a grand strategy involving funding agencies and large consortia. We can start modestly and make useful improvements. \nA final interesting question that came up as the issue was going to press was whether I was talking about “reproducibility” or “replication”. As I made clear in the commentary, I define “replication” as independent people going out and collecting new data and “reproducibility” as independent people analyzing the same data. Apparently, others have the reverse definitions for the two words. The confusion is unfortunate because one idea has a centuries long history whereas the importance of the other idea has only recently become relevant. I’m going to stick to my guns here but we’ll have to see how the language evolves.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-01-beijing-air/",
    "title": "Beijing Air",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\nIf you’re interested in know what the air quality looks like in Beijing China, the US Embassy there has a particulate matter monitor on its roof that tweets the level of fine particulate matter (PM2.5) every hour (see @BeijingAir). In case you’re not used to staring at PM2.5 values all the time, let me provide some context.\nThe US National Ambient Air Quality Standard for the 24-hour average PM2.5 level is 35 mcg/m^3. The twitter feed shows hourly values, so you can’t compare it directly to the US NAAQS (you’d have to take the average of 24 values), but the levels are nevertheless pretty high.\nFor example, here’s the hourly time series plot of one 24-hour period in March of 2010:\n\nThe red and blue lines show the average and maximum 24-hour value for Wake County, NC for the period 2000-2006 (I made this plot when I was giving a talk in Raleigh).\nSo, things could be worse here in the US, but remember that there’s no real evidence of a threshold for PM2.5, so even levels here are potentially harmful. But if you’re traveling to China anytime soon, might want to bring a respirator.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-01-dna-sequencing-caught-in-deluge-of-data/",
    "title": "DNA Sequencing Caught in Deluge of Data",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\nDNA Sequencing Caught in Deluge of Data\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-12-01-rogers-perspective-on-reproducible-research-published/",
    "title": "Roger's perspective on reproducible research published in Science",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-30-selling-the-power-of-statistics/",
    "title": "Selling the Power of Statistics",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-30",
    "categories": [],
    "contents": "\nA few weeks ago we learned that Warren Buffett is a big IBM fan (a $10 billion fan, that is). Having heard that I went over to the IBM web site to see what they’re doing these days. For starters, they’re not selling computers anymore! At least not the kind that I would use. One of the big things they do now is “Business Analytics and Optimization” (i.e. statistics), which is one of the reasons they bought SPSS and then later Algorithmics.\nRoaming around the IBM web site, I found this little video on how IBM is involved with tennis matches like the US Open. It’s the usual promo video: a bit cheesy, but pretty interesting too. For example, they provide all the players an automatically generated post-game “match analysis DVD” that has summaries of all the data from their match with corresponding video.\nIt occurred to me that one of the challenges that a company like IBM faces is selling the “power of analytics” to other companies. They need to make these promo videos because, I guess, some companies are not convinced they need this whole analytics thing (or at least not from IBM). They probably need to do methods and software development too, but getting the deal in the first place is at least as important.\nIn contrast, here at Johns Hopkins, my experience has been that we don’t really need to sell the “power of statistics” to anyone. For the most part, researchers around here seem to be already “sold”. They understand that they are collecting a ton of data and they’re going to need statisticians to help them understand it. Maybe Hopkins is the exception, but I doubt it.\nGood for us, I suppose, for now. But there is a danger that we take this kind of monopoly position for granted. Companies like IBM hire the same people we do (including one grad school classmate) and there’s no reason why they couldn’t become direct competitors. We need to continuously show that we can make sense of data in novel ways. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-29-contributions-to-the-r-source/",
    "title": "Contributions to the R source",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-29",
    "categories": [],
    "contents": "\nOne of the nice things about tracking the R subversion repository using git instead of subversion is you can do\ngit shortlog -s -n\nwhich gives you\n19855  ripley\n  6302  maechler\n  5299  hornik\n  2263  pd\n  1153  murdoch\n   813  iacus\n   716  luke\n   661  jmc\n   614  leisch\n   472  ihaka\n   403  murrell\n   286  urbaneks\n   284  rgentlem\n   269  apache\n   253  bates\n   249  tlumley\n   164  duncan\n    92  r\n    43  root\n    40  paul\n    40  falcon\n    39  lyndon\n    34  thomas\n    33  deepayan\n    26  martyn\n    18  plummer\n    15  (no author)\n    14  guido\n     3  ligges\n     1  mike\n\nThese data are since 1997 so for Brian Ripley, that’s 3.6 commits per day for the last 15 years. \nI think that number 1 position will be out of reach for a while. \nBy the way, I highly recommend to anyone tracking subversion repositories that they use git to do it. You get all of the advantages of git and there are essentially no downsides.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-28-reproducible-research-and-turkey/",
    "title": "Reproducible Research and Turkey",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-28",
    "categories": [],
    "contents": "\nOver the Thanksgiving recent break I naturally started thinking about reproducible research in between salting the turkey and making the turkey stock. Clearly, these things are all related. \n\nI sometimes get the sense that many people see reproducibility as essentially binary. A published paper is either reproducible, as in you can compute every single last numerical result to within epsilon precision, or it’s not. My feeling is that there is a spectrum of reproducibility when it comes to published scientific findings. Some papers are more reproducible than others. And that’s where cooking comes in.\nI do a bit of cooking and I am a shameless consumer of food blogs/web sites. There seems pretty solid agreement (and my own experience essentially confirms) that the more you can make yourself and not have to rely on other people doing the cooking, the better. For example, for Thanksgiving, you could theoretically buy yourself a pre-roasted turkey that’s ready to eat. My brother tells me this is what homesick Americans do in China because so few people have an oven (I suppose you could steam a turkey?). Or you could buy an un-cooked turkey that is “flavor injected”. Or you could buy a normal turkey and brine/salt it yourself. Or you could get yourself one of those heritage turkeys. Or you could raise your own turkeys…. I think in all of these cases, the turkey would definitely be edible and maybe even tasty. But some would probably be more tasty than others. \nAnd that’s the point. There’s a spectrum when it comes to cooking and some methods result in better food than others. Similarly, when it comes to published research there is a spectrum of what authors can make available to reproduce their work. On the one hand, you have just the paper itself, which reveals quite a bit of information (i.e. the scientific question, the general approach) but usually too few details to actually reproduce (or even replicate) anything. Some authors might release the code, which allows you to study the algorithms and maybe apply them to your own work. Some might release the code and the data so that you can actually reproduce the published findings. Some might make a nice R package/vignette so that you barely have to lift a finger. Each case is better than the previous, but that’s not to say that I would only accept the last/best case. Some reproducibility is better than none.\nThat said, I don’t think we should shoot low. Ideally, we would have the best case, which would allow for full reproducibility and rapid dissemination of ideas. But while we wait for that best case scenario, it couldn’t hurt to have a few steps in between.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-27-apple-this-is-ridiculous-you-gotta-upgrade-to/",
    "title": "Apple this is ridiculous - you gotta upgrade to upgrade!?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-27",
    "categories": [],
    "contents": "\nSo along with a few folks here around Hopkins we have been kicking around the idea of developing an app for the iPhone/Android. I’ll leave the details out for now (other than to say stay tuned!).\nBut to start developing an app for the iPhone, you need a version of Xcode, Apple’s development environment. The latest version of Xcode is version 4, which can only be installed with the latest version of Mac OS X Lion (10.7, I think) and above. So I dutifully went off to download Lion. Except, whoops! You can only download Lion from the Mac App store.\nNow this wouldn’t be a problem, if you didn’t need OS X Snow Leopard (10.6 and above) to access the App store. Turns out I only have version 10.5 (must be OS X Housecat or something). I did a little searching and it looks like the only way I can get Lion is if I buy Snow Leopard first and upgrade to upgrade!\nIt isn’t the money so much (although it does suck to pay $60 for $30 worth of software), but the time and inconvenience this causes. Apple has done this a couple of times to me in the past with operating systems needing to be upgraded so I can buy things from iTunes. But this is getting out of hand….maybe I need to consider the alternatives.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-23-an-r-function-to-analyze-your-google-scholar-citations/",
    "title": "An R function to analyze your Google Scholar Citations page",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-23",
    "categories": [],
    "contents": "\nGoogle scholar has now made Google Scholar Citations profiles available to anyone. You can read about these profiles and set one up for yourself here.\nI asked John Muschelli and Andrew Jaffeto write me a function that would download my Google Scholar Citations data so I could play with it. Then they got all crazy on it and wrote a couple of really neat functions. All cool/interesting components of these functions are their ideas and any bugs were introduced by me when I was trying to fiddle with the code at the end.\nSo how does it work? Here is the code. You can source the functions like so:\nsource(“http://biostat.jhsph.edu/~jleek/code/googleCite.r&#8221;)\nThis will install the following packages if you don’t have them: wordcloud, tm, sendmailR, RColorBrewer. Then you need to find the url of a google scholar citation page. Here is Rafa Irizarry’s:\nhttp://scholar.google.com/citations?user=nFW-2Q8AAAAJ\nYou can then call the googleCite function like this:\nout = googleCite(“http://scholar.google.com/citations?user=nFW-2Q8AAAAJ;,pdfname=”rafa_wordcloud.pdf;)\nor search by name like this:\nout = searchCite(“Rafa Irizarry”,pdfname=”rafa_wordcloud.pdf”)\nThe function will download all of Rafa’s citation data and put it in the matrix out. It will also make wordclouds of (a) the co-authors on his papers and (b) the titles of his papers and save them in the pdf file specified (There is an option to turn off plotting if you want). Here is what Rafa’s clouds look like:\n\nWe have also written a little function to calculate many of the popular citation indices. You can call it on the output like so:\ngcSummary(out)\nWhen you download citation data, an email with the data table will also be sent to Simply Statistics so we can collect information on who is using the function and perform population-level analyses.\nIf you liked this function you might also be interesting in our R function to determine if you are a data scientist, or in some of the other stuff going on over at Simply Statistics.\nEnjoy!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-22-data-scientist-vs-statistician/",
    "title": "Data Scientist vs. Statistician",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-22",
    "categories": [],
    "contents": "\nThere’s in interesting discussion over at reddit on the difference between a data scientist and a statistician. My crude summary of the discussion seems to be that by and large they are the same but the phrase “data scientist” is just the hip new name for statistician that will probably sound stupid 5 years from now.\nMy question is why isn’t “statistician” hip? The comments don’t seem to address that much (although a few go in that direction). There a few interesting comments about computing. For example from ByteMining:\n\nStatisticians typically don’t care about performance or coding style as long as it gets a result. A loop within a loop within a loop is all the same as an O(1) lookup.\n\nAnother more down-to-earth comment comes from marshallp:\n\nThere is a real distinction between data scientist and statistician\nthe statistician spent years banging his/her head against blackboards full of math notation to get a modestly paid job\nthe data scientist gets s—loads of cash after having learnt a scripting language and an api\nMore people should be encouraged into data science and not pointless years of stats classes\n\nNot sure I fully agree but I see where he’s coming from!\n[Note: See also our post on how determine whether you are a data scientist.]\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-20-ozone-rules/",
    "title": "Ozone rules",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-20",
    "categories": [],
    "contents": "\nA recent article in the New York Times describes the backstory behind the decision to not revise the ozone national ambient air quality standard. This article highlights the reality of balancing the need to set air pollution regulation to protect public health and the desire to get re-elected. Not having ever served in politics (does being elected to the faculty senate count?) I can’t comment on the political aspect. But I wanted to highlight some of the scientific evidence that goes into developing these standards. \n\nA bit of background: the Clean Air Act of 1970 and its subsequent amendments requires that national ambient air quality standards be set to protect public health with “an adequate margin of safety”. Ozone (usually referred to as smog in the press) is one of the pollutants for which standards are set, along with particulate matter, nitrogen oxides, sulfur dioxide, carbon monoxide, and airborne lead. Importantly, the Clean Air Act requires that the EPA to set standards based on the best available scientific evidence.\nThe ozone standard was re-evaluated years ago under the (second) Bush administration. At the time, the EPA staff recommended a daily standard of between 60 and 70 ppb as providing an adequate margin of safety. Roughly speaking, if the standard is 70 ppb, this means that states cannot have levels of ozone higher than 70 ppb on any given day (that’s not exactly true but the real standard is a mouthful). Stephen Johnson, EPA administrator at the time, set the standard at 75 ppb, citing in part the lack of evidence showing a link between ozone and health at low levels.\nWe’ve conducted epidemiological analyses that show that ozone is associated with mortality even at levels far below 60 ppb (See Figure 2). Note, this paper was not published in time to make into the previous EPA review. The study suggests that if a threshold exists below which ozone has no health effect, it is probably at a level lower than the current standard, possibly nearing natural background levels. Detecting thresholds at very low levels is challenging because you start running out of data quickly. But other studies that have attempted to do this have found results similar to ours.\nThe bottom line is pollution levels below current air quality standards should not be misinterpreted as safe for human health.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-20-show-em-the-data/",
    "title": "Show 'em the data!",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-20",
    "categories": [],
    "contents": "\n\n\nIn a previouspostI argued that students entering college should be shown job prospect by major data. This week I found out the American Bar Association might make it a requirement for law school accreditation.\n\n\nHat tip to Willmai Rivera.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-18-interview-with-h-ctor-corrada-bravo/",
    "title": "Interview with Héctor Corrada Bravo",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-18",
    "categories": [],
    "contents": "\nHéctor Corrada Bravo\n\nHéctor Corrada Bravo is an assistant professor in the Department of Computer Science and the Center for Bioinformatics and Computational Biology at the University of Maryland, College Park. He moved to College Park after finishing his Ph.D. in computer science at the University of Wisconsin and a postdoc in biostatistics at the Johns Hopkins Bloomberg School of Public Health. He has done outstanding work at the intersection of molecular biology, computer science, and statistics. For more info check out his webpage.\n\nWhich term applies to you: statistician/data scientist/computerscientist/machine learner?\nI want to understand interesting phenomena (in my case mostly inbiology and medicine) and I believe that our ability to collect a large number of relevantmeasurements and infer characteristics of these phenomena can drivescientific discovery and commercial innovation in the near future.Perhaps that makes me a data scientist and means that depending on thetask at hand one or more of the other terms apply.\nA lot of the distinctions many people make between these terms arevacuous and unnecessary, but some are nonetheless useful to thinkabout. For example, both statisticians and machine learners [sic] knowhow to create statistical algorithms that compute interesting and informative objects using measurements (perhaps) obtained through some stochastic or partially observedprocess. These objects could be genomic tools for cancer screening, orstatistics that better reflect the relative impact of baseball playerson team success. \nBoth fields also give us ways to evaluate and characterize these objects.However, there are times when these objects are tools that fulfill animmediately utilitarian purpose and thinking like an engineer might(as many people in Machine Learning do) is the right approach.Other times, these objects are there to help us get insights about ourworld and thinking in ways that many statisticians do is the rightapproach.  You need both of these ways of thinking to do interestingscience and dogmatically avoiding either of them is a terrible idea.\nHow did you get into statistics/data science (i.e. your history)?\nI got interested in Artificial Intelligence at one point, and foundthat my mathematics background was nicely suited to work on this. OnceI got into it, thinking about statistics and how to analyze andinterpret data was natural and necessary. I started working with twowonderful advisors at Wisconsin, Raghu Ramakrishnan (CS) and Grace Wahba (Statistics)that helped shape the way I approach problems from different anglesand with different goals. The last piece was discovering thatcomputational biology is a fantastic setting in which to apply anddevise these methods to answer really interesting questions.\nWhat is the problem currently driving you?\nI’ve been working on cancer epigenetics to find specific genomicmeasurements for which increased stochasticity appears to be generalacross multiple cancer types. Right now, I’m really wondering how farinto the clinic can these discoveries be taken, if at all. Forexample, can we build tools that use these genomic measurements toimprove cancer screening?\nHow do you see CS/statistics merging in the future?\nI think that future got here some time ago, but is about to get muchmore interesting.\nHere is one example: Computer Science is about creating and analyzingalgorithms and building the systems that can implement them. Some ofwhat many computer scientists have done looks at problems concerning how tokeep, find and ship around information (Operating Systems, Networks,Databases, etc.). Many times these have been driven by very specificneeds, e.g., commercial transactions in databases. In some ways,companies have moved from from asking how do I use data to keep trackof my activities to how do I use data to decide which activities to doand how to do them. Statistical tools should be used to answer thesequestions, and systems built by computer scientists have statisticalalgorithms at their core.\nBeyond R, what are some really useful computational tools forstatisticians to know about?\nI think a computational tool that everyone can benefit a lot fromunderstanding better is algorithm design and analysis. This doesn’thave to be at a particularly deep level, but just getting a sense ofhow long a particular process might take, and how to devise a different way of doing it that might make it more efficient is really useful. I’ve been toying with the idea of creating a CS course called (something like) “Highlights of continuousmathematics for computer science” that reminds everyone of the coolstuff that one learns in math now that we can appreciate their usefulness. Similarily, I thinkstatistics students can benefit from “Highlights of discretemathematics for statisticians”.\nNow a request for comments below from you and readers: (5a) Beyond R,what are some really useful statistical tools for computer scientiststo know about?\nReview times in statistics journals are long, should statisticiansmove to conference papers?\nI don’t think so. Long review times (anything more than 3 weeks) arereally not necessary. We tend to publish in journals with fairly quickreview times that produce (for the most part) really useful andinsightful reviews. \nI was recently talking to senior members in my field who were tellingme stories about the “old times” when CS was moving from mainlypublishing in journals to now mainly publishing in conferences. Butnow, people working in collaborative projects (like computational biology) work in fieldsthat primarily publish in journals, so the field needs to be able toproperly evaluate their impact and productivity. There is no perfectsystem. \nFor instance, review requests in fields where conferences are the mainpublication venue come in waves (dictated by conference schedule).Reviewers have a lot of papers to go over in a relatively short timewhich makes their job of providing really helpful and fair reviews notso easy. So, in that respect, the journal system can be better. The one thing that is universally true is that you don’t need long review times.\nPrevious Interviews: Daniela Witten, Chris Barr, Victoria Stodden\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-17-google-scholar-pages/",
    "title": "Google Scholar Pages",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-17",
    "categories": [],
    "contents": "\nIf you want to get to know more about what we’re working on, you can check out our Google Scholar pages:\nJeff Leek\nRafael Irizarry\nRoger Peng\nI’ve only been using it for a day but I’m pretty impressed by how much it picked up. My only problem so far is having to merge different versions of the same paper.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-17-the-history-of-nonlinear-principal-components/",
    "title": "History of Nonlinear Principal Components Analysis",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-17",
    "categories": [],
    "contents": "\n[youtube http://www.youtube.com/watch?v=V-hFORcBj44?wmode=transparent&autohide=1&egm=0&hd=1&iv\\_load\\_policy=3&modestbranding=1&rel=0&showinfo=0&showsearch=0&w=500&h=375]\nThe History of Nonlinear Principal Components Analysis, a lecture given by Jan de Leeuw. For those that have ~45 minutes to spare, it’s a very nice talk given in Jan’s characteristic style.\n\n(Source: http://www.youtube.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-12T08:10:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-16-amazon-ec2-is-42-on-top-500-supercomputer-list/",
    "title": "Amazon EC2 is #42 on Top 500 supercomputer list",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-16",
    "categories": [],
    "contents": "\nAmazon EC2 is #42 on Top 500 supercomputer list\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-16-ok-cupid-data-on-infochimps-anybody-got-1k-for-data/",
    "title": "OK Cupid data on Infochimps - anybody got $1k for data?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-16",
    "categories": [],
    "contents": "\nOK Cupid is an online dating site that has grown its visibility in part through a pretty awesome blog called OK Trends, where they have analyzed their online dating data to, for example, show you what kind of profile picture works best. Now, they have compiled data from their personality survey and made it available online through Infochimps. We have talked about Infochimps before, it is basically a site for distributing/selling data. Unfortunately, the OK Cupid data costs $1000. I can think of some cool analyses we could do with this data, but unfortunately the price is a little steep for me. Anybody got a grand they want to give me to buy some data? \nRelated Posts: Jeff on APIs, Jeff on Data sources, Roger on Private health insurers to release data\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-16-preparing-for-tenure-track-job-interviews/",
    "title": "Preparing for tenure track job interviews",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-16",
    "categories": [],
    "contents": "\nIf you are in the job market you will soon be receiving (or already received) an invitation for an interview. So how should you prepare?  You have two goals. The first is to make a good impression. Here are some tips:\n\nDuring your talk, do NOT go over your allotted time. Practice your talk at least twice. Both times in front of a live audiences that asks questions. \nKnow you audience. If it’s a “math-y” department, give a more “math-y” talk. If it’s an applied department, give a more applied talk. But (sorry for the cliché) be yourself. Don’t pretend to be interested in something you are not. I remember one candidate that pretended to be interested in applications and it back fired badly during the talk.  \nLearn about the faculty’s research interests. This will help during the one-on-one interviews.\n Be ready to answer the question “what do you want to teach?” and “where do you see yourself in five years?”\n\nI can’t think of any department where it is necessary to wear a suit (correct me if I’m wrong in the comments). In some places you might feel uncomfortable wearing a suit while those interviewing you are in shorts and t-shirt. But do dress up. Show them you care. \n\nSecond, and just as important, you want to figure out if you like the department you are visiting. Do you want to spend the next 5, 10, 50 years there?  Make sure to find out as much as you can to answer this question. Some questions are more appropriate for junior faculty, the more sensitive ones for the chair. Here are some example questions I would ask:\nWhat are the expectations for promotion? Would you promote someone publishing exclusively in Nature? Somebody publishing exclusively in Annals of Statistics? Is being a PI on an R01 a requirement for tenure? \nWhat are the expectations for teaching/service/collaboration? How are teaching and committee service assignments made?   \nHow did you connect with your collaborators? How are these connections made?\nWhat percent of my salary am I expected to cover? Is it possible to do this by being a co-investigator?\nWhere do you live? How are the schools? How is the commute?  \nHow many graduate students does the department have? How are graduate students funded? If I want someone to work with me, do I have to cover their stipend/tuition?\nSpecific questions for the junior Faculty:\nAre the expectations for promotion made clear to you? Do you get feedback on your progress? Do the senior faculty mentor you? Do the senior faculty get along? What do you like most about the department? What can be improved? In the last 10 years, what percent of junior faculty get promoted?\nQuestions for the chair:\nWhat percent of my salary am I expected to cover? How soon? Is their bridge funding? What is a standard startup package? Can you describe the promotion process in detail? What space is available for postdocs? (for hard money place) I love teaching, but can I buy out teaching with grants? \nI am sure I missed stuff, so please comment away….\nUpdate: I can’t believe I forgot computing! Make sure to ask about computing support. This varies a lot from place to place. Some departments share amazing systems. Ask how costs are shared? How is the IT staff? Is R supported? In others you might have to buy your own hardware. Get all the details.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-15-first-100-posts/",
    "title": "First 100 Posts",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-15",
    "categories": [],
    "contents": "\nIn honor of us passing the 100 post milestone, I’ve collected a few of our more interesting posts from the past 3 months for those who have not been avid followers from Day 1. Enjoy!\nFirst Post: Data Science = Hot Career Choice\nAdvice for stats students on the academic job market\nGetting responses from busy people\n25 minute seminars\nThe future of graduate education\nAn R function to determine if you are a data scientist\nComputing on the language\nFinding good collaborators\nInterview with Daniela Witten\nIs statistics too darn hard?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-14-new-oreilly-book-on-parallel-r-computation/",
    "title": "New O'Reilly book on parallel R computation",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-14",
    "categories": [],
    "contents": "\nNew O’Reilly book on parallel R computation\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-14-the-cost-of-a-u-s-college-education/",
    "title": "The Cost of a U.S. College Education",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-14",
    "categories": [],
    "contents": "\nAs a follow up to my previous post on expected salaries by majors I want to share the following graph:\n\nSo why is the cost of higher education going up at a faster rate than most everything else? Economists please correct me if I’m wrong, but it must be that demand grew right? Universities are non-profits so they didn’t necessarily have to respond by increasing offers. But apparently they did. So if the proportion of the population going to college grew, why is there a shortage of STEM majors? I think it’s because the proportion of the population that can complete such a degree has not changed since 1985 and most of those people were already going to college. If this is right, then it implies that to make more offers, the universities had to grow majors with higher graduation rates. The graph below (taken from here) seems to confirm this:\n\nUnfortunately, in 1985 there was no dearth of psychologists, visual and performing artists, and journalists. So we should not be surprised that the increase in their numbers resulted in graduates from these fields having a harder time finding employment (see bottom of this table). Meanwhile, the US has 2 million job openingsthat can’t be filled, many in vocational careers. So why aren’t more students opting for technical training with good job prospects?In thisNYTimes article, Motoko Rich explains that\n\nIn European countries like Germany, Denmark and Switzerland, vocational programs have long been viable choices for a significant portion of teenagers. Yet in the United States, technical courses have often been viewed as the ugly stepchildren of education, backwaters for underachieving or difficult students.\n\nIt’s hard not to think that universities have benefited from the social stigma associated with vocational degrees. In any case, as I said in mymy previous post, I am not interested in telling people what to study, but universities should show students the data.\n\n\n\n",
    "preview": "http://marginalrevolution.com/wp-content/uploads/2011/11/EducationTabarrok-300x296.png",
    "last_modified": "2021-11-11T13:52:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-11-cooperation-between-referees-and-authors-increases-peer/",
    "title": "Cooperation between Referees and Authors Increases Peer Review Accuracy",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-11",
    "categories": [],
    "contents": "\nJeff Leek and colleagues just published an article in PLoS ONE on the differences between anonymous (closed) and non-anonymous (open) peer review of research articles. They developed a “peer review game” as a model system to track authors’ and reviewers’ behavior over time under open and closed systems.\nUnder the open system, it was possible for authors to see who was reviewing their work. They found that under the open system authors and reviewers tended to cooperate by reviewing each others’ work. Interestingly, they say\n\nIt was not immediately clear that cooperation between referees and authors would increase reviewing accuracy. Intuitively, one might expect that players who cooperate would always accept each others solutions - regardless of whether they were correct. However, we observed that when a submitter and reviewer acted cooperatively, reviewing accuracy actually increased by 11%.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-10-expected-salary-by-major/",
    "title": "Expected Salary by Major",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-10",
    "categories": [],
    "contents": "\nIn thisrecent editorialabout the Occupy Wall Street movement, Richard Kim profiles a protestor that despite having a master’s degree can’t find a job. This particular protestorquit his job as a school teacher three years ago and took out a $35K student loan to obtain a master’s degree in puppetry from the University of Connecticut. I wonder if, before taking his money, UConn showed this person data on job prospects for their puppetry graduates. More generally,I wonder if any university shows their idealist 18 year old freshmen such data.\n\nGeorgetown’s Center for Education and the Workforce has an informativeinteractive webpagethat students can use to find out by-major salary information. I scraped data from thisWall Street Journal webpagewhich also provides, for each major, unemployment rates, salary quartiles, and its rank in popularity. I used these data to compute expected salaries by multiplying median salary by percent of employment. The graph above shows expected salary versus popularity rank (1=most popular) for the 50 most popular majors (Go here for a complete table and here is the raw data and code). I also included Physics (the 70-th). I used different colors to represent four categories: engineering, math/stat/computers, physical sciences, and the rest. As a baseline I added a horizontal line representing the average salary for a truck driver: $65K, a job currently withplenty of openings. Different font sizes are used only to make names fit.A couple of observations stand out. First, only one of the top 10 most popular majors,Computer Science,has a higher expected salary than truck drivers. Second, Psychology, the fifth most popular major, has an expected salary of $40K and, as seen in the table, an unemployment rate of 6.1%; almost three times worse than nursing.\nA few editorial remarks:1)I understand that being a truck driver is very hard and that there is little room for career development. 2) I am not advocating that people pick majors based on future salaries. 3) I think college freshmen deserve to know the data given how much money they fork over to us. 4) The graph is for bachelor’s degrees, not graduate education. The CEW website includes data for graduate degrees. Note that Biology shoots way up with a graduate degree. 5) For those interested in a PhD in Statistics I recommend you major in Math with a minor in a liberal arts subject, such as English, while taking as many programming classes as you can. We all know Math is the base for everything statisticians do, but why English? Students interested in academia tend to underestimate the importance of writing and communicating.\nRelated articles:ThisNY Times article describes how/why students are leaving the sciences. Here, Alex Tabarrok describes big changes in the balance of majors between 1985 and today and herehe shares his thoughts on Richard Kim’s editorial. Matt Yglesias explains thatunemploymentis rising across the board. Finally, Peter Orszag share his views on how a changing world is changing the value of a college degree.\nHat tip to David Santiago for sending various of these links and Harris Jaffee for help with scrapping.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:39-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-09-statisticians-on-twitter-help-me-find-more/",
    "title": "Statisticians on Twitter...help me find more!",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-09",
    "categories": [],
    "contents": "\nIn honor of our blog finally dragging itself into the 21st century and jumping onto Twitter/Facebook, I have been compiling a list of statistical people on Twitter. I couldn’t figure out an easy way to find statisticians in one go (which could be because I don’t have Twitter skills). \nSo here is my very informal list of statisticians I found in a half hour of searching. I know I missed a ton of people; let me know who I missed so I can update!\n@leekgroup - Jeff Leek (What, you thought I’d list someone else first?)\n@rdpeng - Roger Peng\n@rafalab - Rafael Irizarry\n@storeylab - John Storey\n@bcaffo - Brian Caffo\n@sherrirose - Sherri Rose\n@raphg - Raphael Gottardo\n@airoldilab - Edo Airoldi\n@stat110 - Joe Blitzstein\n@tylermccormick - Tyler McCormick\n@statpumpkin - Chris Volinsky\n@fivethirtyeight - Nate Silver\n@flowingdata - Nathan Yau\n@kinggary - Gary King\n@StatModeling - Andrew Gelman\n@AmstatNews - Amstat News\n@hadleywickham - Hadley Wickham\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-08-coarse-pm-and-measurement-error-paper/",
    "title": "Coarse PM and measurement error paper",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-08",
    "categories": [],
    "contents": "\nHoward Chang, a former PhD student of mine now at Emory, just published a paper on a measurement error model for estimating the health effects of coarse particulate matter (PM). This is a cool paper that deals with the problem that coarse PM tends to be very spatially heterogeneous. Coarse PM is a bit of a hot topic now because there is currently no national ambient air quality standard for coarse PM specifically. There is a standard for fine PM, but compared to fine PM,  the scientific evidence for health effects of coarse PM is relatively less developed. \nWhen you want to assign a coarse PM exposure level to people in a county (assuming you don’t have personal monitoring) there is a fair amount of uncertainty about the assignment because of the spatial variability. This is in contrast to pollutants like fine PM or ozone which tend to be more spatially smooth. Standard approaches essentially ignore the uncertainty which may lead to some bias in estimates of the health effects.\nHoward developed a measurement error model that uses observations from multiple monitors to estimate the spatial variability and correct for it in time series regression models estimating the health effects of coarse PM. Another nice thing about his approach is that it avoids any complex spatial-temporal modeling to do the correction.\nRelated Posts: Jeff on “Cool papers” and “Dissecting the genomics of trauma”\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-07-is-statistics-too-darn-hard/",
    "title": "Is Statistics too darn hard?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-07",
    "categories": [],
    "contents": "\nIn this NY Times article, Christopher Drew points out that many students planning engineering and science majors end up switching to other subjects or fail to get any degree. He argues that this is partly due todo the difficulty of classes. In a previous post we lamented the anemic growth in math and statistics majors in comparison to other majors. I do not think we should make our classes easier just to keep these students. But we can certainly do a better job of motivating the material and teaching it more interesting. After having fun in high school science classes, students entering college are faced with the reality that the first college science classes can be abstract and technical. But in Statistics we certainly can be teaching the practical aspects first. Learning the abstractions is so much easier and enjoyable when you understand the practical problem behind the math. And in Statistics there is always a practical aspect behind the math. The statistics class I took in college was so dry and removed from reality that I can see why it would turn students away from the subject. So, if you are teaching undergrad (or grads) I highly recommend the Stat labs text book by Deb Nolan and Terry Speed that teaches Mathematical Statistics through applications. If you know of other good books please post in the comments? Also, if you know of similar books for other science, technology, engineering, and math (STEM) subjects please share as well.\nRelated Pots: Jeff on “The 5 most critical statistical concepts”, Rafa on “The future of graduate education”, Jeff on “Graduate student data analysis inspired by a high-school teacher”\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:37-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-06-reproducible-research-notes-from-the-field/",
    "title": "Reproducible research: Notes from the field",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-06",
    "categories": [],
    "contents": "\nOver the past year, I’ve been doing a lot of talking about reproducible research. Talking to people, talking on panel discussions, and talking about some of my own work. It seems to me that interest in the topic has exploded recently, in part due to some recent scandals, such as the Duke clinical trials fiasco.\nIf you are unfamiliar with the term “reproducible research”, the basic idea is that authors of published research should make available the necessary materials so that others may reproduce to a very high degree of similarity the published findings. If that definitions seems imprecise, well that’s because it is.\n\nI think reproducibility becomes easier to define in the context of a specific field or application. Reproducibility often comes up in the context of computational science. In computational science fields, often much of the work is done on the computer using often very large amounts of data. In other words, the analysis of the data is of comparable difficulty as the collection of the data (maybe even more complicated). Then the notion of reproducibility typically comes down to the idea of making the analytic data and the computer code available to others. That way, knowledgeable people can run your code on your data and presumably get your results. If others do not get your results, then that may be a sign of a problem, or perhaps a misunderstanding. In either case, a resolution needs to be found. Reproducibility is key to science much the way it is key to programming. When bugs are found in software, being able to reproduce the bug is an important step to fixing it. Anyone learning to program in C knows the pain of dealing with a memory-related bug, which will often exhibit seemingly random and non-reproducible behavior.\nMy discussions with others about the need for reproducibility in science often range far and wide. One reason is that many people have very different ideas what (a) what is reproducibility and (b) why we need it. Here is my take on various issues.\nReproducibility is not replication. There’s often honest confusion between the notion of reproducibility and what I would call a “full replication”. A full replication doesn’t analyze the same dataset, but rather involves an independent investigator collecting an independent dataset conducting an independent analysis. Full replication has been a fundamental component of science for a long time now and will continue to be the primary yardstick for measuring the plausibility of scientific claims. I think most would agree that full replication is preferable, but often it is simply not possible.\nReproducibility is not needed solely to prevent fraud. I’ve heard many people emphasize reproducibility as a means to prevent fraud. Journal editors seem to think this is the main reason for demanding reproducibility. It is_ one_ reason, but to be honest, I’m not sure it’s all that useful for detecting fraud. If someone truly wants to commit fraud, then it’s possible to make the fraud reproducible. If I just generate a bunch of numbers and claim it as data that I collected, any analysis from that dataset can be reproducible. While demanding reproducibility may be useful for ferreting out certain types of fraud, it’s not a general solution and it’s not the primary reason we need it. \nReproducibility is not as easy as it sounds. Making one’s research reproducible is hard. It’s especially hard when you try to do it after the research has been done. In that case it’s more like an audit, and I’m guessing for most people the word “audit” is NOT synonymous with “fun”. Even if you set out to make your work reproducible from the get go, it’s easy to miss things. Code can get lost (even with a version control system) and metadata can slip through the cracks. Even when you’ve done everything right, computers and software can change. Virtual machines like Amazon EC2 and others seem to have some potential. The single most useful tool that I have found is a good version control system, like git. \nAt this point, anything would be better than nothing. Right now, I think the bar for reproducibility is quite low in the sense that most published work is not reproducible. Even if data are available, often the code that analyzed the data is not available. So if you’re publishing research and you want to make it at least partially reproducible, just put what you can out there. On the web, on github, in a data repository, wherever you can. If you can’t publish the data, make your code available. Even that is better than nothing. In fact, I find reading someone’s code to be very informative and often questions can arise without looking at the data. Until we have a better infrastructure for distributing reproducible research, we will have to make do with what we have. But if we all start putting stuff out there, the conversation will turn from “Why should I make stuff available?” to “Why wouldn’t I make stuff available?”\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-05-new-ways-to-follow-simply-statistics/",
    "title": "New ways to follow Simply Statistics",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-05",
    "categories": [],
    "contents": "\nIn case you prefer to follow Simply Statistics using some other platforms, we’ve added two new features. First, we have an official Twitter feed that you can follow. We also have a new Facebook page that you can like. Please follow us and join the discussion!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-04-interview-with-victoria-stodden/",
    "title": "Interview with Victoria Stodden",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-04",
    "categories": [],
    "contents": "\nVictoria Stodden\n\nVictoria Stodden is an assistant professor of statistics at Columbia University in New York City. She moved to Columbia after getting her Ph.D. at Stanford University. Victoria has made major contributions to the area of reproducible research and has been appointed to the NSF’s Advisory Committee for Infrastructure. She is the recent recipient of an NSF grant for “Policy Design for Reproducibility and Data Sharing in Computational Science”\n\nWhich term applies to you: data scientist/statistician/analyst (or something else)?\nDefinitely statistician. My PhD is from the stats department at Stanford University.\nHow did you get into statistics/data science (e.g. your history)?\nSince my undergrad days I’ve been motivated by problems in what’s called ‘social welfare economics.’ I interpret that as studying how people can best reach their potential, particularly how the policy environment affects outcomes. This includes the study of regulatory design, economic growth, access to knowledge, development, and empowerment. My undergraduate degree was in economics, and I thought I would carry on with a PhD in economics as well. I realized that folks with my interests were mostly doing empirical work so I thought I should prepare myself with the best training I could in statistics. Hence I chose to do a PhD in statistics to augment my data analysis capabilities as much as I could since I envisioned myself immersed in empirical research in the future.\nWhat is the problem currently driving you?\nRight now I’m working on the problem of reproducibility in our body of published computational science. This ties into my interests because of the critical role of knowledge and reasoning in advancing social welfare. Scientific research is becoming heavily computational and as a result the empirical work scientists do is becoming more complex and yet less tacit: the myriad decisions made in data filtering, analysis, and modeling are all recordable in code. In computational research there are so many details in the scientific process it is nearly impossible to communicate them effectively in the traditional scientific paper – rendering our published computational results unverifiable, if there isn’t access to the code and data that generated them.\nAccess to the code and data permits readers to check whether the descriptions in the paper correspond to the published results, and allows people to understand why independent implementations of the methods in the paper might produce differing results. It also puts the tools of scientific reasoning into people’s hands – this is new. For much of scientific research today all you need is an internet connection to download the reasoning associated with a particular result. Wide availability of the data and code is still largely a dream, but one the scientific community is moving towards.\nWho were really good mentors to you? What were the qualities that really helped you?\nMy advisor, David Donoho, is an enormous influence. He is the clearest scientific thinker I have ever been exposed to. I’ve been so very lucky with the people who have come into my life. Through his example, Dave is the one who has had the most impact on how I think about and prioritize problems and how I understand our role as statisticians and scientific thinkers. He’s given me an example of how to do this and it’s hard to underestimate his influence in my life.\nWhat do you think are the barriers to reproducible research?\nAt this point, incentives. There are many concrete barriers, which I talk about in my papers and talks (available on my website http://stodden.net), but they all stem from misaligned incentives. If you think about it, scientists do lots of things they don’t particularly like in the interest of research communication and scientific integrity. I don’t know any computational scientist who really loves writing up their findings into publishable articles for example, but they do. This is because the right incentives exist. A big part of the work I am doing concerns the scientific reward structure.  For example, my work on the Reproducible Research Standard is an effort to realign the intellectual property rules scientists are subject to, to be closer to our scientific norms. Scientific norms create the incentive structure for the production of scientific research, providing rewards for doing things people might not do otherwise. For example, scientists have a long established norm of giving up all intellectual property rights over their work in exchange for attribution, which is the currency of success. It’s the same for sharing the code and data that underlies published results – not part of the scientific incentive and reward structure today but becoming so, through adjusting a variety of other factors like finding agency policy, journal publication policy, and expectations at the institutional level.\nWhat have been some success stories in reproducible research?\nI can’t help but point to my advisor, David Donoho. An example he gives is his release of http://www-stat.stanford.edu/~wavelab - the first implementation of wavelet routines in MATLAB, before MATLAB included their own wavelet toolbox.  The release of the Wavelab code was a factor that he believes made him one of the top 5 highly cited authors in Mathematics in 2000.\nHiring and promotion committees seem to be starting to recognize the difference between candidates that recognize the importance of reproducibility and clear scientific communication, compared to others who seem to be wholly innocent of these issues.\nThere is a nascent community of scientific software developers that is achieving remarkable success.  I co-organized a workshop this summer bringing some of these folks together, see http://www.stodden.net/AMP2011. There are some wonderful projects underway to assist in reproducibility, from workflow tracking to project portability to unique identifiers for results reproducible in the cloud. Fascinating stuff.\nCan you tell us a little about the legal ramifications of distributing code/data?\nSure. Many aspects of our current intellectual property laws are quite detrimental to the sharing of code and data. I’ll discuss the two most impactful ones. Copyright creates exclusive rights vested in the author for original expressions of ideas – and it’s a default. What this means is that your expression of your idea – your code, your writing, figures you create – are by default copyright to you. So for your lifetime and 70+ years after that, you (or your estate) need to give permission for the reproduction and re-use of the work – this is exactly counter to scientific norms or independent verification and building on others’ findings. The Reproducible Research Standard is a suite of licenses that permit scientists to set the terms of use of their code, data, and paper according to scientific norms: use freely but attribute. I have written more about this here: http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4720221\nIn 1980 Congress passed the Bayh-Dole Act, which was designed to create incentives for access to federally funded scientific discoveries by securing ownership rights for universities with regard to inventions by their researchers. The idea was that these inventions could then by patented and licensed by the university, making the otherwise unavailable technology available for commercial development. Notice that Bayh-Dole was passed on the eve of the computer revolution and Congress could not have foreseen the future importance of code to scientific investigation and its subsequent susceptibility to patentability. The patentability of scientific code now creates incentives to keep the code hidden: to avoid creating prior art in order to maximize the chance of obtaining the patent, and to keep hidden from potential competitors any information that might be involved in commercialization. Bayh-Dole has created new incentives for computational scientists – that of startups and commercialization – that must be reconciled with traditional scientific norms of openness.\nRelated Posts: Jeff’s interviews with Daniela Witten and Chris Barr. Roger’s talk on reproducibility \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-03-free-access-publishing-is-awesome-but-expensive-how/",
    "title": "Free access publishing is awesome...but expensive. How do we pay for it?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-03",
    "categories": [],
    "contents": "\nI am a huge fan of open access journals. I think open access is good both for moral reasons (science should be freely available) and for more selfish ones (I want people to be able to read my work). If given the choice, I would publish all of my work in journals that distribute results freely.\nBut it turns out that for most open/free access systems, the publishing charges are paid by the scientists publishing in the journals. I did a quick scan and compiled this little table of how much it costs to publish a paper in different journals (here is a bigger table):\nPLoS One $1,350.00\nPLoS Biology: $2,900.00\nBMJ Open $1,937.28\nBioinformatics (Open Access Option) $3,000.00\nGenome Biology (Open Access Option) $2,500.00\nBiostatistics (Open Access Option) $3,000.00\nThe first thing I noticed is that it is minimum about $1,500 to get a paper published open access. That may not seem like a lot of money and most journals offer discounts to people who can’t pay. But it still adds up, this last year my group has published 7 papers. If I paid for all of them to be published open access, that would be at minimum $10,500! That is half the salary of a graduate student researcher for an entire year. For a senior scientist that may be no problem, but for early career scientists, or scientists with limited access to resources, it is a big challenge.\nPublishers who are solely dedicated to open access (PLoS, BMJ Open, etc.) seem to have on average lower publication charges than journals who only offer open access as an option. I think part of this is that the journals that aren’t open access in general have to make up some of the profits they lose by making the articles free. I certainly don’t begrudge the journals the costs. They have to maintain the websites, format the articles, and run the peer review process. That all costs money.\nA modest proposal\nWhat I wonder is if there was a better place for that money to come from? Here is one proposal (hat tip to Rafa): academic and other libraries pay a ton of money for subscriptions to journals like Nature and Science. They also are required to pay for journals in a large range of disciplines. What if, instead of investing this money in subscriptions for their university, academic libraries pitched in and subsidized the publication costs of open/free access?\nIf all university libraries pitched in, the cost for any individual library would be relatively small. It would probably be less than paying for subscriptions to hundreds of journals. At the same time, it would be an investment that would benefit not only the researchers at their school, but also the broader scientific community by keeping research open. Then neither the people publishing the work, nor the people reading it would be on the hook for the bill.\nThis approach is the route taken by ArXiv, a free database of unpublished papers. These papers haven’t been peer reviewed, so they don’t always carry the same weight as papers published in peer-reviewed journals. But there are a lot of really good and important papers in the database - it is an almost universally accepted pre-print server.\nThe other nice thing about ArXiv is that you don’t pay for article processing, the papers are published as is. The papers don’t look quite as pretty as they do in Nature/Science or even PLoS, but it is also much cheaper. The only costs associated with making this a full fledged peer-reviewed journal would be refereeing (which scientists do for free anyway) and editorial responsibilities (again mostly volunteer by scientists).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-02-guest-post-smart-thoughts-on-the-adhd-200-data/",
    "title": "Guest Post: SMART thoughts on the ADHD 200 Data Analysis Competition",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-02",
    "categories": [],
    "contents": "\nNote: This is a guest post by our colleagues Brian Caffo, Ani Eloyan, Fang Han, Han Liu,John Muschelli, Mary Beth Nebel, Tuo Zhao and Ciprian Crainiceanu. They won the ADHD 200 imaging data analysis competition. There has been some controversy around the results because one team obtained a higher score without using any of the imaging data. Our colleagues have put together a very clear discussion of the issues raised by the competition so we are publishing it here to contribute to the discussion. Questions about this post should be directed to the Hopkins team leader Brian Caffo  \nBackground\nBelow we share some thoughts about the ADHD 200 competition, a landmark competition using functional and structural brain imaging data to predict ADHD status. \n \n\nNote, we’re calling these “SMART thoughts” to draw attention to our working group, “Statistical Methods and Applications for Research in Technology” (www.smart-stats.org), though hopefully the acronym applies in the non-intended sense as well.\n Our team was declared the official winners of the competition. However, a team from the University of Alberta scored a higher number of competition points, but was disqualified for not having used imaging data. We have been in email contact with a representative of that team and have enjoyed the discussion. We found those team members to be gracious and to embody an energy and scientific spirit that are refreshing to encounter.  We mentioned our sympathy to them, in that the process seemed unfair, especially given the vagueness of what qualifies as use of the imaging data. More on this thought below.   This brings us to the point of this note, concern over the narrative surrounding the competition based on our reading of web pages, social media and water cooler discussions. We are foremost concerned with the unwarranted conclusion that because the team with the highest competition point total did not use imaging data, the overall scientific validity of using (f)MRI imaging data to study ADHD is now in greater doubt.  We stipulate that, like many others, we are skeptical of the utility of MRI data for tasks such as ADHD diagnoses. We are not arguing against such skepticism.  Instead we are arguing against using the competition results as if they were strong evidence for such skepticism. We raise four points to argue against overreacting to the competition outcome with respect to the use of structural and functional MRI in the study of ADHD.\nPoint 1. The competition points are not an accurate measure of performance and scientific value.\nBecause the majority of the training, and hence presumably the test, sets in the competition were typically developing, the competition points favored specificity.  In addition, a correct label of TD yielded 1 point, while a correct ADHD diagnosis with incorrect subtype yielded .5 points. These facts suggest a classifier that declares everyone as TD as a starting point. For example, if 60% of the 197 test subjects are controls, this algorithm would yield 118 competition points, better than all but a few entrants. In fact, if 64.5% or higher of the test set is TD, this algorithm wins over Alberta (and hence everyone else).In addition, competition points are variables possessing randomness.  It is human nature to interpret the anecdotal rankings of competitions as definitive evidence of superiority. This works fine as long as rankings are reasonably deterministic. But is riddled with logical flaws when rankings are stochastic. Variability in rankings has a huge effect on the result of competitions, especially when highly tuned prediction methods from expert teams are compared. Indeed, in such cases the confidence intervals of the AUCs (or other competition criteria) overlap. The 5th or 10th place team may actually have had the most scientifically informative algorithm.\nPoint 2. Biologically valueless predictors were important.\nMost importantly, contributing location (aka site), was a key determinant of prediction performance. Site is a proxy for many things: the demographics of the ADHD population in the site’s PI’s studies, the policies by which a PI chose to include data, scanner type, IQ measure, missing data patterns, data quality and so on. In addition to site, missing data existence and data quality also held potentially important information about prediction, despite being (biologically) unrelated to ADHD. The likely causality, if existent, would point in the reverse direction (i.e. that presence of ADHD would result in a greater propensity for missing data and lower data quality, perhaps due to movement in the scanner).This is a general fact regarding prediction algorithms, which do not intrinsically account for causal directions or biological significance.\nPoint 3. The majority of the imaging data is not prognostic.\nLikely every entrant, and the competition organizers, were aware that the majority of the imaging data is not useful for predicting ADHD. (Here we use the term “imaging data” loosely, meaning raw and/or processed data.)   In addition, the imaging data are noisy. Therefore, use of these data introduced tens of billions of unnecessary numbers to predict 197 diagnoses. As such, even if extremely important variables are embedded in the imaging data, (non-trivial) use of all of the imaging data could degrade performance, regardless of the ultimate value of the data. To put this in other words, suppose all entrants were offered an additional 10 billion numbers, say genomic data, known to be noisy and, in aggregate, not predictive of disease. However, suppose that some unknown function of a small collection of variables was very meaningful for prediction, as is presumably the case with genomic data. If the competition did not require its use, a reasonable strategy would be to avoid using these data altogether. Thus, in a scientific sense, we are sympathetic to the organizers’ choice to eliminate the Alberta team, since a primary motivation of the competition was to encourage a large set of eyes to sift through a large collection of very noisy imaging data. Of course, as stated above, we believe that what constitutes a sufficient use of the imaging data is too vague to be an adequate rule to eliminate a team in a competition. Thus our scientifically motivated support of the organizers conflicts with our procedural dispute of the decision made to eliminate the Alberta team.\nPoint 4. Accurate prediction of a response is neither necessary nor sufficient for a covariate to be biologically meaningful.\nAccurate prediction of a response is an extremely high bar for a variable of interest. Consider drug development for ADHD. A drug does not have to demonstrate that its application to a collection of symptomatic individuals would predict with high accuracy a later abatement of symptoms.  Instead, a successful drug would have to demonstrate a mild averaged improvement over a placebo or standard therapy when randomized. As an example, consider randomly administering such a drug to 50 of 100 subjects who have ADHD at baseline.  Suppose data are collected at 6 and 12 months. Further suppose that 8 out of 50 of those receiving the drug had no ADHD symptoms at 12 months, while 1 out of 50 of those receiving placebo had no ADHD symptoms at 12 months. The Fisher’s exact test P-value is .03, by the way.  The statistical evidence points to the drug being effective. Knowledge of drug status, however, would do little to improve prediction accuracy. That is, given a new data set of subjects with ADHD at baseline and knowledge of drug status, the most accurate classification for every subject would be to guess that they will continue to have ADHD symptoms at 12 months.  Of course, our confidence in that prediction would be slightly lower for those having received the drug.However, consider using ADHD status at 6 months as a predictor. This would be enormously effective at locating those subjects who have an abatement of symptoms whether they received the drug or not. In this thought experiment, one predictor (symptoms at 6 months) is highly predictive, but not meaningful (it simply suggests that Y is a good predictor of Y), while the other (presence of drug at baseline) is only mildly predictive, but is statistically and biologically significant.As another example, consider the ADHD200 data set. Suppose that a small structural region is highly impacted in an unknown subclass of ADHD. Some kind of investigation of morphometry or volumetrics might detect an association with disease status. The association would likely be weak, given absence of a-priori knowledge of this region or the subclass. This weak association would not be useful in a prediction algorithm. However, digging into this association could potentially inform the biological basis of the disease and further refine the ADHD phenotype.Thus, we argue that it is important to differentiate the ultimate goals of obtaining high prediction accuracy with that of biological discovery of complex mechanisms in the presence of high dimensional data. \nConclusions\nWe urge caution in over-interpretation of the scientific impact of the University of Alberta’s strongest performance in the competition.  Ultimately, what Alberta’s having the highest point total established is that they are fantastic people to talk to if you want to achieve high prediction accuracy. (Looking over their work, this appears to have already been established prior to the competition :-).It was not established that brain structure or resting state function, as measured by MRI, is a blind alley in the scientific exploration of ADHD.\nRelated Posts: Roger on “Caffo + Ninjas = Awesome”, Rafa on the “Self Assessment Trap”, Roger on “Private health insurers to release data”\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-02-i-gave-a-talk-on-reproducible-research-back-in/",
    "title": "Reproducible Research Talk in Vancouver",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-02",
    "categories": [],
    "contents": "\n[youtube http://www.youtube.com/watch?v=aH8dpcirW1U?wmode=transparent&autohide=1&egm=0&hd=1&iv\\_load\\_policy=3&modestbranding=1&rel=0&showinfo=0&showsearch=0&w=500&h=375]\nI gave a talk on reproducible research back in July at the Applied Mathematics Perspectives workshop in Vancouver, BC.\nIn addition to the YouTube version, there’s also a Silverlight version where you can actually see the slides while I’m talking.\n\n(Source: http://www.youtube.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-12T08:08:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-02-we-need-better-marketing/",
    "title": "We need better marketing",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-02",
    "categories": [],
    "contents": "\nIn this post Alex Tabarrok argues that not enough people are obtaining “degrees that pay” and that college has been oversold. It struck me that the number of students studying Visual and Performing Arts has more than doubled since 1985. Yet for Math and Statistics there has been no increase at all! We need to do a better job at marketing. The great majority (if not all) of the people I know with Statistics degrees have found a job related to Statistics. With a Master’s, salary can be as high as $110K.So to those interested in Visual and Performing Arts that are good with numbers I suggest you hedge your bets: do a double major and consider Statistics. My brother, a successful musician, majored in Math. He uses his math skills to supplement his income by playing poker with other musicians.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:33-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-11-01-advice-on-promotion-letters-bleg/",
    "title": "Advice on promotion letters bleg",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-11-01",
    "categories": [],
    "contents": "\nThis fall I have been asked to write seven promotion letters. Writing these takes me at least 2 hours. If it’s someone I don’t know it takes me longer because I have to read some of their papers. Earlier this year, I wrote one for a Biology department that took me at least 6 hours. So how many are too many? Should I set a limit? Advice and opinions in the comments would be greatly appreciated.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-29-the-5-most-critical-statistical-concepts/",
    "title": "The 5 Most Critical Statistical Concepts",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-29",
    "categories": [],
    "contents": "\nIt seems like everywhere we look, data is being generated - from politics, to biology, to publishing, to social networks. There are also diverse new computational tools, like GPGPU and cloud computing, that expand the statistical toolbox. Statistical theory is more advanced than its ever been, with exciting work in a range of areas. \nWith all the excitement going on around statistics, there is also increasing diversity. It is increasingly hard to define “statistician” since the definition ranges from very mathematical to very applied. An obvious question is: what are the most critical skills needed by statisticians? \n\nSo just for fun, I made up my list of the top 5 most critical skills for a statistician by my own definition. They are by necessity very general (I only gave myself 5). \nThe ability to manipulate/organize/work with data on computers - whether it is with excel, R, SAS, or Stata, to be a statistician you have to be able to work with data. \nA knowledge of exploratory data analysis - how to make plots, how to discover patterns with visualizations, how to explore assumptions\nScientific/contextual knowledge - at least enough to be able to abstract and formulate problems. This is what separates statisticians from mathematicians. \nSkills to distinguish true from false patterns - whether with p-values, posterior probabilities, meaningful summary statistics, cross-validation or any other means. \nThe ability to communicate results to people without math skills - a key component of being a statistician is knowing how to explain math/plots/analyses.\nWhat are your top 5? What order would you rank them in? Even though these are so general, I almost threw regression in there because of how often it pops up in various forms. \nRelated Posts: Rafa on graduate education and What is a Statistician? Roger on “Do we really need applied statistics journals?”\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:31-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-27-computing-on-the-language/",
    "title": "Computing on the Language",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-27",
    "categories": [],
    "contents": "\nAnd now for something a bit more esoteric….\nI recently wrote a function to deal with a strange problem. Writing the function ended up being a fun challenge related to computing on the R language itself.\nHere’s the problem: Write a function that takes any number of R objects as arguments and returns a list whose names are derived from the names of the R objects.\n\nPerhaps an example provides a better description. Suppose the function is called ‘makeList’. Then \nx <- 1y <- 2z <- \"hello\"makeList(x, y, z)\n\nreturns\nlist(x = 1, y = 2, z = \"hello\")\n\nIt originally seemed straightforward to me, but it turned out to be very much not straightforward. \nNote that a function like this is probably most useful during interactive sessions, as opposed to programming.\nI challenge you to take a whirl at writing the function, you know, in all that spare time you have. I’ll provide my solution in a future post.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-26-visualizing-yahoo-email/",
    "title": "Visualizing Yahoo Email",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-26",
    "categories": [],
    "contents": "\nHere is a cool page where yahoo shows you the email it is processing in real time. It includes a visualization of the most popular words in emails at a given time. A pretty neat tool and definitely good for procrastination, but I’m not sure what else it is good for…\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-24-archetypal-athletes/",
    "title": "Archetypal Athletes",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-24",
    "categories": [],
    "contents": "\nHere is a cool paper on the ArXiv about archetypal athletes. The basic idea is to look at a large number of variables for each player and identify multivariate outliers or extremes. These outliers are the archetypes talked about in the title. \nAccording to his analysis the author claims the best players (for different reasons, i.e. different archetypes) in the NBA in 2009/2010 were:  Taj Gibson, Anthony Morrow, and Kevin Durant. The best soccer players were Wayne Rooney, Leonel Messi, and Christiano Ronaldo.\nThanks to Andrew Jaffe for pointing out the article. \nRelated Posts: Jeff on “Innovation and Overconfidence”, Rafa on “Once in a lifetime collapse”\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-24-web-scraping/",
    "title": "Web-scraping",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-24",
    "categories": [],
    "contents": "\nThe internet is the greatest source of publicly available data. One of the key skills to being able to obtain data from the web is “web-scraping”, where you use a piece of software to run through a website and collect information. \nThis technique can be used for collecting data from databases or to collect data that is scattered across a website. Here is a very cool little exercise in web-scraping that can be used as an example of the things that are possible. \nRelated Posts: Jeff on APIs, Data Sources, Regex, and The Open Data Movement.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-22-graduate-student-data-analysis-inspired-by-a/",
    "title": "Graduate student data analysis inspired by a high-school teacher",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-22",
    "categories": [],
    "contents": "\nI love watching TED talks. One of my absolute favorites is the talk by Dan Meyer on how math class needs a makeover. Dan also has one of the more fascinating blogs I have read. He talks about math education, primarily K-12 education.  His posts on curriculum design, assessment , work ethic, and homework are really, really good. In fact, just go read all his author choices. You won’t regret it. \nThe best quote from the talk is:\n\nAsk yourselves, what problem have you solved, ever, that was worth solving, where you knew knew all of the given information in advance? Where you didn’t have a surplus of information and have to filter it out, or you didn’t have insufficient information and have to go find some?\n\n\nMany of the data analyses I have done in classes/assigned in class have focused on a problem with exactly the right information with relatively little extraneous data or missing information. But I have been slowly evolving these problems; as an example here is a data analysis project that we developed last year for the qualifying exam at JHU. This project is what I consider a first step toward a “less helpful” project model. \nThe project was inspired by this blog post at marginal revolution which Rafa suggested. As with the homework problem Dan dissects in his talk, there are layers to this problem:\nUnderstanding the question\nDownloading and filtering the data\nExploratory analysis\nFitting models/interpreting results\nSynthesis and writing the results up\nReproducibility of the R code\nFor this analysis, I was pretty specific with 1. Understanding the question:\n\n\n(1) The association between enrollment and the percent of students scoring “Advanced” on the MSA in Reading and Math in the 5th grade. \n\n\n(2) The change in the number of students scoring “Advanced” in Reading and Math from one year to the next (at minimum consider the change from 2009-2010) versus enrollment. \n\n\n(3) Potential reasons for results like those in Table 1.  \n\n\n\nAlthough I didn’t mention the key idea from the Marginal Revolution post. I think for a qualifying exam, this level of specificity is necessary, but for an in-class project I think I would have removed this information so students had to “discover the question” themselves. \n\n\nI was also pretty specific with the data source suggesting the Maryland Education department’s website. However, several students went above and beyond and found other data sources, or downloaded more data than I suggested. In the future, I think I will leave this off too. My google/data finding skills don’t hold a candle to those of my students. \n\n\nSteps 3-5 were summed up with the statement: \n\n\n\nYour project is to analyze data from the MSA and write a short letter either in favor of or against spending money to decrease school sizes.\n\n\n\nThis is one part of the exam I’m happy with. It is sufficiently vague to let the students come to their own conclusions. It also suggests that the students should draw conclusions and support them with statistical analyses. One of the major difficulties I have struggled with in teaching this class is getting students to state a conclusion as a result of their analysis and to quantify how uncertain they are about that decision. In my mind, this is different from just the uncertainty associated with a single parameter estimate. \n\n\nIt  was surprising how much requiring reproducibility helped students focus their analyses. I think because they had to organize/collect their code which, helped them organize their analysis. Also, there was a strong correlation between reproducibility and quality of the written reports.\n\n\nGoing forward I have a couple of ideas of how I would change my data analysis projects:\n\nBe less helpful - be less clear about the problem statement, data sources, etc. I definitely want students to get more practice formulating problems. \nFocus on writing/synthesis - my students are typically very good at fitting models, but sometimes struggle with putting together the “story” of an analysis. \nStress much less about whether specific methods will work well on the data analyses I suggest. One of the more helpful things I think these messy problems produce is a chance to figure out what works and what doesn’t on real world problems. \nRelated Posts: Rafa on the future of graduate education, Roger on applied statistics journals.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-21-interview-with-chris-barr/",
    "title": "Interview With Chris Barr",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-21",
    "categories": [],
    "contents": "\n\nChris Barr\n\n\nChris Barr is an assistant professor of biostatistics at the Harvard School of Public Health in Boston. He moved to Boston after getting his Ph.D. at UCLA and then doing a postdoc at Johns Hopkins Bloomberg School of Public Health. Chris has done important work in environmental biostatistics and is also the co-founder of OpenIntro, a very cool open-source (and free!) educational resource for statistics.  \n\n\n\n Which term applies to you: data scientist/statistician/analyst?\n\n\nI’m a “statistician” by training. One day, I hope to graduate to “scientist”. The distinction, in my mind, is that a scientist can bring real insight to a tough problem, even when the circumstances take them far beyond their training.\n\n\n Statisticians get a head start on becoming scientists. Like chemists and economists and all the rest, we were trained to think hard as independent researchers. Unlike other specialists, however, we are given the opportunity, from a young age, to see all types of different problems posed from a wide range of perspectives.\n\n\nHow did you get into statistics/data science (e.g. your history)?\n\n\nI studied economics in college, and I had planned to pursue a doctorate in the same field. One day a senior professor of statistics asked me about my future, and in response to my stated ambition, said: “Whatever an economist can do, a statistician can do better.” I started looking at graduate programs in statistics and noticed UCLA’s curriculum. It was equal parts theory, application, and computing, and that sounded like how I wanted to spend my next few years. I couldn’t have been luckier. The program and the people were fantastic.\n\n\nWhat is the problem currently driving you?\n\n\nI’m working on so many projects, it’s difficult to single out just one. Our work on smoking bans (joint with Diez, Wang, Samet, and Dominici) has been super exciting. It is a great example about how careful modeling can really make a big difference. I’m also soloing a methods paper on residual analysis for point process models that is bolstered by a simple idea from physics. When I’m not working on research, I spend as much time as I can on OpenIntro.\n\n\nWhat is your favorite paper/idea you have had? Why?\n\n\n I get excited about a lot of the problems and ideas. I like the small teams (one, two, or three authors) that generally take on theory and methods problems; I also like the long stretches of thinking time that go along with those papers. That said, big science papers, where I get to team up with smart folks from disciplines and destinations far and wide, really get me fired up. Last, but not least, I really value the work we do on open source education and reproducible research. That work probably has the greatest potential for introducing me to people, internationally and in small local communities, that I’d never know otherwise.\n\n\nWho were really good mentors to you? What were the qualities that really helped you?\n\n\nIdentifying key mentors is such a tough challenge, so I’ll adhere to a self-imposed constraint by picking just one: Rick Schoenberg. Rick was my doctoral advisor, and has probably had the single greatest impact on my understanding of what it means to be a scientist and colleague. I could tell you a dozen stories about the simple kindness and encouragement that Rick offered. Most importantly, Rick was positive and professional in every interaction we ever had. He was diligent, but relaxed. He offered structure and autonomy. He was all the things a student needs, and none of the things that make students want to read those xkcd comics. Now that I’m starting to make my own way, I’m grateful to Rick for his continuing friendship and collaboration.\n\n\nI know you asked about mentors, but if I could mention somebody who, even though not my mentor, has taught me a ton, it would be David Diez. David was my classmate at UCLA and colleague at Harvard. We are also cofounders of OpenIntro. David is probably the hardest working person I know. He is also the most patient and clear thinking. These qualities, like Rick’s, are often hard to find in oneself and can never be too abundant.\n\n\n What is OpenIntro?\n\n\nOpenIntro is part of the growing movement in open source education. Our goal, with the help of community involvement, is to improve the quality and reduce the cost of educational materials at the introductory level. Founded by two statisticians (Diez, Barr), our early activities have generated a full length textbook (OpenIntro Statistics: Diez, Barr, Cetinkaya-Rundel) that is available for free in PDF and at cost ($9.02) in paperback. People can also use openintro.org to manage their course materials for free, whether they are using our book or not. The software, developed almost entire by David Diez, makes it easy for people to post lecture notes, assignments, and other resources. Additionally, it gives people access to our online question bank and quiz utility. Last but not least, we are sponsoring a student project competition. The first round will be this semester, and interested people can visit openintro.org/stat/comp for additional information. We are little fish, but with the help of our friends (openintro.org/about.php) and involvement from the community, we hope to do a good thing.\n\n\nHow did you get the idea for OpenIntro?\n\n\n \n\n\n Regarding the book and webpage - David and I had both started writing a book on our own; David was keen on an introductory text, and I was working on one about statistical computing. We each realized that trying to solo a textbook while finishing a PhD was nearly impossible, so we teamed up. As the project began to grow, we were very lucky to be joined by Mine Cetinkaya-Rundel, who became our co-author on the text and has since played a big role in developing the kinds of teaching supplements that instructors find so useful (labs and lecture notes to name a few). Working with the people at OpenIntro has been a blast, and a bucket full of nights and weekends later, here we are!\n\n\n Regarding making everything free - David and I started the OpenIntro project during the peak of the global financial crisis. With kids going to college while their parents’ house was being foreclosed, it seemed timely to help out the best way we knew how. Three years later, as I write this, the daily news is running headline stories about the Occupy Wall Street movement featuring hard times for young people in America and around the world. Maybe “free” will always be timely.\n\n\nFor More Information\n\n\nCheck out Chris’ webpage, his really nice publications including this one on the public health benefits of cap and trade, and the OpenIntro project website. Keep your eye open for the paper on cigarette bans Chris mentions in the interview, it is sure to be good. \n\n\nRelated Posts: Jeff’s interview with Daniela Witten, Rafa on the future of graduate education, Roger on colors in R.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-21-the-self-assessment-trap/",
    "title": "The self-assessment trap",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-21",
    "categories": [],
    "contents": "\nSeveral months ago I was sitting next to my colleague Ben Langmead at the Genome Informatics meeting. Various talks were presented on short read alignments and every single performance table showed the speaker’s method as #1 and Ben’s Bowtie as #2 among a crowded field of lesser methods. It was fun to make fun of Ben for getting beat every time, but the reality was that all I could conclude was that Bowtie was best and speakers were falling into the the self-assessment trap: each speaker had tweaked the assessment to make their method look best. This practice is pervasive in Statistics where easy-to-tweak Monte Carlo simulations are commonly used to assess performance. In a recent paper, a team at IBM described how the problem in the systems biology literature is pervasive as well. Co-author Gustavo Stolovitzky Stolovitsky is a co-developer of the DREAM challenge in which the assessments are fixed and developers are asked to submit. About 7 years ago we developed affycomp, a comparison webtool for microarray preprocessing methods. I encourage others involved in fields where methods are constantly being compared to develop such tools. It’s a lot of work, but journals are usually friendly to papers describing the results of such competitions.\nRelated Posts:  Roger on colors in R, Jeff on battling bad science\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-20-anthropology-of-the-tribe-of-statisticians/",
    "title": "Anthropology of the Tribe of Statisticians",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-20",
    "categories": [],
    "contents": "\nFrom the BBC a pretty fascinating radio piece.\n\n…in the same way that a telescope enables you to see things that are too far away to see with the naked eye, a microscope enables you to see things that are too small to see with the naked eye, statistics enables you to see things in masses of data which are too complex for you to see with the naked eye. \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-20-caffos-theorem/",
    "title": "Caffo's Theorem",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-20",
    "categories": [],
    "contents": "\nBrian Caffo from the comments:\n\nPersonal theorem: the application of statistics in any new field will be labeled “Technical sounding word” + ics. Examples: Sabermetrics, analytics, econometrics, neuroinformatics, bioinformatics, informatics, chemeometrics.\nIt’s like how adding mayonnaise to anything turns it in to salad (eg: egg salad, tuna salad, ham salad, pasta salad, …)\nI’d like to be the first to propose the statistical study of turning things in salad. So called mayonaisics.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-20-finding-good-collaborators/",
    "title": "Finding good collaborators",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-20",
    "categories": [],
    "contents": "\nThe job of the statistician is almost entirely about collaboration. Sure, there’s theoretical work that we can do by ourselves, but most of the impact that we have on science comes from our work with scientists in other fields. Collaboration is also what makes the field of statistics so much fun.\nSo one question I get a lot from people is “how do you find good collaborations”? Or, put another way, how do you find good collaborators? It turns out this distinction is more important than it might seem.\n\nMy approach to developing collaborations has evolved over time and I consider myself fairly lucky to have developed a few very productive and very enjoyable collaborations. These days my strategy for finding good collaborations is to look for good collaborators. I personally find it important to work with people that I like as well as respect as scientists, because a good collaboration is going to involve a lot of personal interaction. A place like Johns Hopkins has no shortage of very intelligent and very productive researchers that are doing interesting things, but that doesn’t mean you want to work with all of them.\nHere’s what I’ve been telling people lately about finding collaborations, which is a mish-mash of a lot of advice I’ve gotten over the years.\nFind people you can work with. I sometimes see situations where a statistician will want to work with someone because he/she is working on an important problem. Of course, you want to be working on a problem that interests you, but it’s only partly about the specific project. It’s very much about the person. If you can’t develop a strong working relationship with a collaborator, both sides will suffer. If you don’t feel comfortable asking (stupid) questions, pointing out problems, or making suggestions, then chances are the science won’t be as good as it could be. \nIt’s going to take some time. I sometimes half-jokingly tell people that good collaborations are what you’re left with after getting rid of all your bad ones. Part of the reasoning here is that you actually may not know what kinds of people you are most comfortable working with. So it takes time and a series of interactions to learn these things about yourself and to see what works and doesn’t work. Of course, you can’t take forever, particularly in academic settings where the tenure clock might be ticking, but you also can’t rush things either. One rule I heard once was that a collaboration is worth doing if it will likely end up with a published paper. That’s a decent rule of thumb, but see my next comment.\nIt’s going to take some time. Developing good collaborations will usually take some time, even if you’ve found the right person. You might need to learn the science, get up to speed on the latest methods/techniques, learn the jargon, etc. So it might be a while before you can start having intelligent conversations about the subject matter. Then it takes time to understand how the key scientific questions translate to statistical problems. Then it takes time to figure out how to develop new methods to address these statistical problems. So a good collaboration is a serious long-term investment which has some risk of not working out.  There may not be a lot of papers initially, but the idea is to make the early investment so that truly excellent papers can be published later.\nWork with people who are getting things done. Nothing is more frustrating than collaborating on a project with someone who isn’t that interested in bringing it to a close (i.e. a published paper, completed software package). Sometimes there isn’t a strong incentive for the collaborator to finish (i.e she/he is already tenured) and other times things just fall by the wayside. So finding a collaborator who is continuously getting things done is key. One way to determine this is to check out their CV. Is there a steady stream of productivity? Papers in good journals? Software used by lots of other people? Grants? Web site that’s not in total disrepair?\nYou’re not like everyone else. One thing that surprised me was discovering that just because someone you know works well with a specific person doesn’t mean that you will work well with that person. This sounds obvious in retrospect, but there were a few situations where a collaborator was recommended to me by a source that I trusted completely, and yet the collaboration didn’t work out. The bottom line is to trust your mentors and friends, but realize that differences in personality and scientific interests may determine a different set of collaborators with whom you work well.\nThese are just a few of my thoughts on finding good collaborators. I’d be interested in hearing others’ thoughts and experiences along these lines.\nRelated Posts: Rafa on authorship conventions, finish and publish\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:26-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-19-do-we-really-need-applied-statistics-journals/",
    "title": "Do we really need applied statistics journals?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-19",
    "categories": [],
    "contents": "\nAll statisticians in academia are constantly confronted with the question of where to publish their papers. Sometimes it’s obvious: A theoretical paper might go to the Annals of Statistics or_JASA Theory & Methods_ or Biometrika. A more “methods-y” paper might go to JASA or JRSS-B or_Biometrics_ or maybe even Biostatistics (where all three of us are or have been associate editors).\nBut where should the applied papers go? I think this is an increasingly large category of papers being produced by statisticians. These are papers that do not necessarily develop a brand new method or uncover any new theory, but apply statistical methods to an interesting dataset in a not-so-obvious way. Some papers might combine a set of existing methods that have never been combined before in order to solve an important scientific problem.\nWell, there are some official applied statistics journals: JASA Applications & Case Studies or JRSS-C or Annals of Applied Statistics. At least they have the word “application” or “applied” in their title. But the question we should be asking is if a paper is published in one of those journals, will it reach the right audience?\nWhat is the audience for an applied stat paper? Perhaps it depends on the subject matter. If the application is biology, then maybe biologists. If it’s an air pollution and health application, maybe environmental epidemiologists. My point is that the key audience is probably not a bunch of other statisticians.\nThe fundamental conundrum of applied stat papers comes down to this question:If your application of statistical methods is truly addressing an important scientific question, then shouldn’t the scientists in the relevant field want to hear about it? If the answer is yes, then we have two options: Force other scientists to read our applied stat journals, or publish our papers in their journals. There doesn’t seem to be much momentum for the former, but the latter is already being done rather frequently.\nAcross a variety of fields we see statisticians making direct contributions to science by publishing in non-statistics journals. Some examples are this recent paper in Nature Genetics or a paper I published a few years ago in the Journal of the American Medical Association. I think there are two key features that these papers (and many others like them) have in common:\nThere was an important scientific question addressed. The first paper investigates variability of methylated regions of the genome and its relation to cancer tissue and the second paper addresses the problem of whether ambient coarse particles have an acute health effect. In both cases, scientists in the respective substantive areas were interested in the problem and so it was natural to publish the “answer” in their journals. The problem was well-suited to be addressed by statisticians. Both papers involved large and complex datasets for which training in data analysis and statistics was important. In the analysis of coarse particles and hospitalizations, we used a national database of air pollution concentrations and obtained health status data from Medicare. Linking these two databases together and conducting the analysis required enormous computational effort and statistical sophistication. While I doubt we were the only people who could have done that analysis, we were very well-positioned to do so.\nSo when statisticians are confronted by a scientific problems that are both (1) important and (2) well-suited for statisticians, what should we do? My feeling is we should skip the applied statistics journals and bring the message straight to the people who want/need to hear it.\nThere are two problems that come to mind immediately. First, sometimes the paper ends up being so statistically technical that a scientific journal won’t accept it. And of course, in academia, there is the sticky problem of how do you get promoted in a statistics department when your CV is filled with papers in non-statistics journals. This entry is already long enough so I’ll address these issues in a future post.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-18-caffo-ninjas-awesome/",
    "title": "Caffo + Ninjas = Awesome",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-18",
    "categories": [],
    "contents": "\nOur colleague Brian Caffo and his team of statistics ninjas won the “Imaging-Based Diagnostic Classification Contest” as part of the ADHD 200 Global Competition. From the prize citation:\n\nThe method developed by the team from Johns Hopkins University excelled in its specificity, or its ability to identify typically developing children (TDC) without falsely classifying them as ADHD-positive. They correctly classified 94% of TDC, showing that a diagnostic imaging methodology can be developed with a very low risk of false positives, a fantastic result. Their method was not as effective in terms of sensitivity, or its ability to identify true positive ADHD diagnoses. They only identified 21% of cases; however, among those cases, they discerned the subtypes of ADHD with 89.47% accuracy. Other teams demonstrated that there is ample room to improve sensitivity scores. \n\nCongratulations to Brian and his team!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-18-spectacular-plots-made-entirely-in-r/",
    "title": "Spectacular Plots Made Entirely in R",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-18",
    "categories": [],
    "contents": "\nWhen doing data analysis, I often create a set of plots quickly just to explore the data and see what the general trends are. Later I go back and fiddle with the plots to make them look pretty for publication. But some people have taken this to the next level. Here are two plots made entirely in R:\n\n\nThe descriptions of how they were created are here and here.\nRelated: Check out Roger’s post on R colors and my post on APIs\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-17-colors-in-r/",
    "title": "Colors in R",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-17",
    "categories": [],
    "contents": "\nOne of my favorite R packages that I use all the time is the RColorBrewer package. The package has been around for a while now and is written/maintained by Erich Neuwirth. The guts of the package are based on Cynthia Brewer’s very cool work on the use of color in cartography (check out the colorbrewer web site).\nAs a side note, I think the ability to manipulate colors in plots/graphs/maps is one of R’s many great strengths. My personal experience is that getting the right color scheme can make a difference in how data are perceived in a plot.\n\nRColorBrewer basically provides one function, brewer.pal, that generates different types of color palettes. There are three types of palettes: sequential, diverging, and qualitative. Roughly speaking, sequential palettes are for continuous data where low is less important and high is more important, diverging palettes are for continuous data where both low and high are important (i.e. deviation from some reference point), and qualitative palettes are for categorical data where there is no logical order (i.e. male/female).\nTo use the brewer.pal function, it’s often useful to combine it with another R function, colorRampPalette. This function is built into R and is part of the grDevices package. It takes a palette of colors and interpolates between the colors to give you an entire spectrum. Think of a painter’s palette with 4 or 5 color blotches on it, and then think of the painter taking a brush and blending the colors together. That’s what colorRampPalette does. So brewer.pal gives you the colors and colorRampPalette mashes them together. It’s a happy combination.\nSo, how do we use these functions? My basic approach is to first set the palette depending on the type of data. Suppose we have continuous sequential data and we want the “Blue-Purple” palette\ncolors <- brewer.pal(4, \"BuPu\")\n\nHere, I’ve taken 4 colors from the “BuPu” palette, so there are now 4 blotches on my palette. To interpolate these colors, I can call colorRampPalette, which actually returns a function.\npal <- colorRampPalette(colors)\n\nNow, pal is a function that takes a positive integer argument and returns that number of colors from the palette. So for example\n> pal(5)\n[1] \"#EDF8FB\" \"#C1D7E9\" \"#9FB1D4\" \"#8B80BB\" \"#88419D\"\n\nI got 5 different colors from the palette, with their red, green, and blue values coded in hexadecimal. If I wanted 20 colors I could have called pal(20).\nThe pal function is useful in other functions like image or wireframe (in the lattice package). In both of those functions, the ‘col’ argument can be given a set of colors generated by the pal function. For example, you could call\ndata(volcano)\nimage(volcano, col = pal(30))\n\nand you would plot the ‘volcano’ data using 30 colors from the “BuPu” palette.\nIf you’re wondering what all the different palettes are and what colors are in them, here’s a handy reference:\n\nOr you can just call\ndisplay.brewer.all()\nThere’s been a lot of interesting work done on colors in R and this is just scratching the surface. I’ll probably return to this subject in a future post.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-17-competing-through-data-three-experts-offer-their-game/",
    "title": "Competing through data: Three experts offer their game plan",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-17",
    "categories": [],
    "contents": "\nCompeting through data: Three experts offer their game plan\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-16-where-would-we-be-without-dennis-ritchie/",
    "title": "Where would we be without Dennis Ritchie?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-16",
    "categories": [],
    "contents": "\n\nMost have probably seen this already since it happend a few days ago, but Dennis Ritchie died. It just blows my mind how influential his work was — developing the C language, Unix — and how so many pieces of technology bear his fingerprints. \nMy first encounter with K&R was in college when I learned C programming in the “Data Structures and Programming Techniques” class at Yale (taught by Stan “the man” Eisenstadt). Looking back, his book seems fairly easy to read and understand, but I must have cursed that book a million times when I took that course!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-14-interview-with-daniela-witten/",
    "title": "Interview With Daniela Witten",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-14",
    "categories": [],
    "contents": "\nNote: This is the first in a series of posts where we will be interviewing junior, up-and-coming statisticians/data scientists. Our goal is to build visibility for people who are at the early stages of their careers.\nDaniela Witten\n\nDaniela is an assistant professor of Biostatistics at the University of Washington in Seattle. She moved to Seattle after getting her Ph.D. at Stanford. Daniela has been developing exciting new statistical methods for analyzing high dimensional data and is a recipient of the NIH Director’s Early Independence Award.\nWhich term applies to you: data scientist/statistician/data analyst?\nStatistician! We have to own the term. Some of us have a tendency to try to sugarcoat what we do. But I say that I’m a statistician with pride! It means that I have been rigorously trained, that I have a broadly applicable skill set, and that I’m always open to new and interesting problems. Also, I sometimes get surprised reactions from people at cocktail parties, which is funny.\nTo the extent that there is a stigma associated with being a statistician, we statisticians need to face the problem and overcome it. The future of our field depends on it.\nHow did you get into statistics/data science?\nI definitely did not set out to become a statistician. Before I got to college, I was planning to study foreign languages. Like most undergrads, I changed my mind, and eventually I majored in biology and math. I spent a summer in college doing experimental biology, but quickly discovered that I had neither the hand-eye coordination nor the patience for lab work. When I was nearing the end of college, I wasn’t sure what was next. I wanted to go to grad school, but I didn’t want to commit to one particular area of study for the next five years and potentially for my entire career.\nI was lucky to be at Stanford and to stumble upon the Stat department there. Initially, statistics appealed to me because it was a good way to combine my interests in math and biology from the safety of a computer terminal instead of a lab bench. After spending more time in the department, I realized that if I studied statistics, I could develop a broad skill set that could be applied to a variety of areas, from cancer research to movie recommendations to the stock market.\nWhat is the problem currently driving you?\nMy research involves the development of statistical methods for the analysis of very large data sets. Recently, I’ve been interested in better understanding networks and their applications to biology. In the past few years there has been a lot of work in the statistical community on network estimation, or graphical modeling. In parallel, biologists have been interested in taking network-based approaches to understanding large-scale biological data sets. There is a real need for these two areas of research to be brought closer together, so that statisticians can develop useful tools for rigorous network-based analysis of biological data sets.\nFor example, the standard approach for analyzing a gene expression data set with samples from two classes (like cancer and normal tissue) involves testing each gene for differential expression between the two classes, for instance using a two-sample t-statistic. But we know that an individual gene does not drive the differences between cancer and normal tissue; rather, sets of genes work together in pathways in order to have an effect on the phenotype. Instead of testing individual genes for differential expression, can we develop an approach to identify aspects of the gene network that are perturbed in cancer?\nWhat are the top 3 skills you look for in a student who works with you?\nI look for a student who is intellectually curious, self-motivated, and a good personality fit. Intellectual curiosity is a prerequisite for grad school, self-motivation is needed to make it through the 2 years of PhD level coursework and 3 years of research that make up a typical Stat/Biostat PhD, and a good personality fit is needed because grad school is long and sometimes frustrating ( but ultimately very rewarding), and it’s important to have an advisor who can be a friend along the way!\nWho were really good mentors to you? What were the qualities that really helped you?\nMy PhD advisor, Rob Tibshirani, has been a great mentor. In addition to being a top statistician, he is also an enthusiastic advisor, a tireless advocate for his students, and a loyal friend. I learned from him the value of good collaborations and of simple solutions to complicated problems. I also learned that it is important to maintain a relaxed attitude and to occasionally play pranks on students.\nFor more information:\nCheck out her website. Or read her really nice papers on penalized classification and penalized matrix decompositions.\n\n\n\n",
    "preview": "http://www.biostat.washington.edu/~dwitten/DanielaWittenSmall.jpg",
    "last_modified": "2021-11-11T13:52:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-13-moneyball-for-academic-institutes/",
    "title": "Moneyball for Academic Institutes",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-13",
    "categories": [],
    "contents": "\nA way that universities grow in research fields for which they have no department is by creating institutes. Millions of dollars are invested to promote collaboration between existing faculty interested in the new field. But do they work? Does the university get their investment back? Through the years I have noticed that many institutes are nothing more than a webpage and others are so successful they practically become self-sustained entities. This paper (published in STM) led by John Hogenesch, uses data from papers and grants to evaluate an institute at Penn. Among other things, they present a method that uses network analysis to objectively evaluate the effect of the institute on collaboration. The findings are fascinating. \nThe use of data to evaluate academics is becoming more and more popular, especially among administrators. Is this a good thing? I am not sure yet, but statisticians better get involved before a biased analyses gets some of us fired.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-12-benfords-law/",
    "title": "Benford's law",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-12",
    "categories": [],
    "contents": "\nAm I the only one who didn’t know about Benford’s law? It says that for many datasets, the probability that the first digit of a random element is d is given by P(d)= log_10 (1 + 1/d). This post by Jialan Wang explores financial report data and, using Benford’s law, notices that something fishy is going on… \nHat tip to David Santiago.\nUpdate: A link has been fixed. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-11-authorship-conventions/",
    "title": "Authorship conventions",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-11",
    "categories": [],
    "contents": "\nThe main role of academics is the creation of knowledge. In science, publications are the main venue by which we share our accomplishments, our ideas. Not surprisingly, publications are heavily weighted in hires and promotions. But with multiple author papers how do we know how much each author contributed?  Here are some related links from Science and Nature and below I share some thoughts specific to Applied Statistics.\nIt is common for theoretical statisticians to publish solo papers. For these it is clear who takes the credit for the idea. In contrast, applied statisticians typically include various authors. Examples include the postdoc that did most the work, the graduate student that helped, the programmer that wrote associated software, and the biologists that created the data. So what position do we assign ourself so that those that evaluate us know our role? Many of us working with lab scientists have adopted their convention: the main knowledge creator, usually the lab head, goes last and is the corresponding author. Here are examples from Jeff, Hongkai, Ingo, and myself. Through conversations with senior Biostatistics and Statistics faculty I have been surprised to learn that many are not aware of this. In some cases they went as far as advising junior faculty to publish more first author papers. This is somewhat concerning because junior faculty could be faced with study sections (where our grants are evaluated) that look for last author papers. Study section is not going to change so I am hoping this post will help educate the statistical community about the meaning of last author papers for those of us working in genomics and other lab-science related fields. Here is a summary of authorship conventions in these fields:\nLast and corresponding author is associated with the major contributor of ideas and leadership. This is the most desirable position.\nFirst author is associated with the person who did most the implementation and computing work. Very good for a postdoc or jr faculty. Excellent for a graduate student.\nFirst and corresponding is sometimes used when the person not only had the ideas, but also did half or more of the work. This is rare.\nBig collaborative projects will have two or more corresponding authors and two or more “first” authors. I included an example above.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-11-errors-in-biomedical-computing/",
    "title": "Errors in Biomedical Computing",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-11",
    "categories": [],
    "contents": "\nBiomedical Computation Review has a nice summary (in which I am quoted briefly) by Kristin Sainani about the many different types of errors in computational research, including the infamous Duke incident and some other recent examples. The reproducible research policy at Biostatistics is described as an example for how the publication process might need to change to prevent errors from persisting (or occurring).\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-11-government-data-collection-vortex/",
    "title": "Government data collection vortex",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-11",
    "categories": [],
    "contents": "\nGovernment data collection vortex\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-11-terences-stuff-speaking-reading-writing/",
    "title": "Terence’s Stuff: Speaking, reading, writing",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-11",
    "categories": [],
    "contents": "\nTerence’s Stuff: Speaking, reading, writing\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-10-datascientist/",
    "title": "An R function to determine if you are a data scientist",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-10",
    "categories": [],
    "contents": "\n“Data scientist” is one of the buzzwords in the running for rebranding applied statistics mixed with some computing. David Champagne, over at Revolution Analytics, described the skills for being a data scientist with a Venn Diagram. Just for fun, I wrote a little R function for determining where you land on the data science Venn Diagram. Here is an example of a plot the function makes using the Simply Statistics bloggers as examples. \n\nThe code can be found here. You will need the png and klaR R packages to run the script. You also need to either download the file datascience.png or be connected to the internet. \nHere is the function definition:\ndataScientist(names=c(“D. Scientist”),skills=matrix(rep(1/3,3),nrow=1), addSS=TRUE, just=NULL)\nnames = a character vector of the names of the people to plot\naddSS = if TRUE will add the blog authors to the plot\njust = whether to write the name on the right or the left of the point, just = “left” prints on the left and just =”right” prints on the right. If just=NULL, then all names will print to the right. \nskills = a matrix with one row for each person you are plotting, the first column corresponds to “hacking”, the second column is “substantive expertise”, and the third column is “math and statistics knowledge”\nSo how do you define your skills? Here is how it works:\nIf you are an academic\nYou calculate your skills by adding papers in journals. The classification scheme is the following:\nHacking = sum of papers in journals that are primarily dedicated to software/computation/methods for very specific problems. Examples are: Bioinformatics, Journal of Statistical Software, IEEE Computing in Science and Engineering, or a software article in Genome Biology.\nSubstantive  = sum of papers in journals that primarily publish scientific results such as JAMA, New England Journal of Medicine, Cell, Sleep, Circulation\nMath and Statistics = sum of papers in primarily statistical journals including Biostatistics, Biometrics, JASA, JRSSB, Annals of Statistics\nSome journals are general, like Nature, Science, the Nature sub-journals, PNAS, and PLoS One. For papers in those journals, assess which of the areas the paper falls in by determining the main contribution of the paper in terms of the non-academic classification below. \nIf you are a non-academic\nSince papers aren’t involved, determine the percent of your time you spend on the following things:\nHacking = downloading/transferring data, cleaning data, writing software, combining previously used software\nSubstantive = time you spend learning about the scientific problem, discussing with scientists, working in the lab/field.\nMath and Statistics = time you spend formalizing a problem in mathematical notation, time you spend developing new mathematical/statistical theory, time you spend developing general method.\nEnjoy!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-10-excuse-our-mess/",
    "title": "Excuse our mess...",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-10",
    "categories": [],
    "contents": "\n…we are in the process of changing themes. The spammers got to us in the notes. I tried to fix the html and that didn’t go so well. New theme up shortly. \nUpdate: Done! We are back in business - minus the spammers. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-09-a-nice-presentation-on-regex-in-r/",
    "title": "A nice presentation on regex in R",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-09",
    "categories": [],
    "contents": "\nOver at Recology here is a nice presentation on regular expressions. I found this on the R bloggers site. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-09-apis/",
    "title": "APIs!",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-09",
    "categories": [],
    "contents": "\nApplication programming interfaces (APIs) are tools that are built by companies/governments/organizations to allow software engineers to interact with their websites. One of the main uses of these APIs is to allow software engineers to build apps on top of Facebook/Twitter/etc. Many APIs are really helpful for statisticians/data scientists as well. Using APIs, it is generally very easy to collect large amounts of interesting data. Here are some examples of APIs (you may need to sign up for accounts to get access to some of these). They vary in how easy/useful it is to obtain data from them. If people know of other good ones, I’d love to see them in the comments. \nWeb 2.0\nTwitter and associated R package\nGoogle analytics\nBlogger\nIndeed\nGroupon\nPublishing\nNew York Times\nArXiv\nPubmed\nPLoS\nMendeley\nGovernment\nFedSpending \nDepartment of Education\nCDC\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-09-hello-world-2/",
    "title": "Hello world!",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-09",
    "categories": [],
    "contents": "\nWelcome to WordPress.com. After you read this, you should delete and write your own post, with a new title above. Or hit Add New on the left (of the admin dashboard) to start a fresh post.\nHere are some suggestions for your first post.\nYou can find new ideas for what to blog about by reading the Daily Post.\nAdd PressThis to your browser. It creates a new blog post for you about any interesting  page you read on the web.\nMake some changes to this page, and then hit preview on the right. You can always preview any post or edit it before you share it to the world.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-08-single-screen-productivity/",
    "title": "Single Screen Productivity",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-08",
    "categories": [],
    "contents": "\nHere’s a claim for which I have absolutely no data: I believe I am more productive with a smaller screen/monitor. I have a 13” MacBook Air that I occasionally hook up to a 21-inch external monitor. Sometimes, when I want to read a document I’ll hook up the external monitor so that I can see a whole page at a time. Other times, when I’m using R, I’ll have the graphics window on the external and then the R console and Emacs on the main screen.\nBut my feeling is that when I’ve got more monitor real estate I’m less productive. I think it’s because I have the freedom to open more windows and to have more things going on. When I’ve got my laptop, I can only really afford to have 1 or 2 windows open. So I’m more focused on whatever I’m supposed to be doing. I also think this is one of the (small) reasons that people like things like the iPad. It’s a single application/single window device.\nA quick Google search will find some pretty crazy multiple-monitor setups out there. For some of them you’d think they were head of security at Los Angeles International Airport or something. And most people I know would scoff at the idea of working solely on your laptop while in the office. Partially, it’s an ergonomic issue. But maybe they just need an external monitor that’s 13 inches? I think I have one sitting in my basement somewhere….\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-07-r-workshop-reading-in-large-data-frames/",
    "title": "R Workshop: Reading in Large Data Frames",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-07",
    "categories": [],
    "contents": "\n One question I get a lot about how to read large data frames into R. There are some useful tricks that can save you both time and memory when reading large data frames but I find that many people are not aware of them. Of course, your ability to read data is limited by your available memory. I usually do a rough calculation along the lines of\n# rows * # columns * 8 bytes / 2^20\nThis gives you the number of megabytes of the data frame (roughly speaking, it could be less). If this number is more than half the amount of memory on your computer, then you might run into trouble.\n\nFirst, read the help page for ‘read.table’. It contains many hints for how to read in large tables. Of course, help pages tend to be a little confusing so I’ll try to distill the relevant details here.\nThe following options to ‘read.table()’ can affect R’s ability to read large tables:\ncolClasses\nThis option takes a vector whose length is equal to the number of columns in year table. Specifying this option instead of using the default can make ‘read.table’ run MUCH faster, often twice as fast. In order to use this option, you have to know the of each column in your data frame. If all of the columns are “numeric”, for example, then you can just set ‘colClasses = “numeric”’. If the columns are all different classes, or perhaps you just don’t know, then you can have R do some of the work for you.\nYou can read in just a few rows of the table and then create a vector of classes from just the few rows. For example, if I have a file called “datatable.txt”, I can read in the first 100 rows and determine the column classes from that:\ntab5rows <- read.table(\"datatable.txt\", header = TRUE, nrows = 100)\nclasses <- sapply(tab5rows, class)\ntabAll <- read.table(\"datatable.txt\", header = TRUE, colClasses = classes)\n\nAlways try to use ‘colClasses’, it will make a very big difference. In particular, if one of the column classes is “character”, “integer”, “numeric”, or “logical”, then things will be optimal (because those are the basic classes).\nnrows\nSpecifying the ‘nrows’ argument doesn’t necessary make things go faster but it can help a lot with memory usage. R doesn’t know how many rows it’s going to read in so it first makes a guess, and then when it runs out of room it allocates more memory. The constant allocations can take a lot of time, and if R overestimates the amount of memory it needs, your computer might run out of memory. Of course, you may not know how many rows your table has. The easiest way to find this out is to use the ‘wc’ command in Unix. So if you run ‘wc datafile.txt’ in Unix, then it will report to you the number of lines in the file (the first number). You can then pass this number to the ‘nrows’ argument of ‘read.table()’. If you can’t use ‘wc’ for some reason, but you know that there are definitely less than, say, N rows, then you can specify ‘nrows = N’ and things will still be okay. A mild overestimate for ‘nrows’ is better than none at all.\ncomment.char\nIf your file has no comments in it (e.g. lines starting with ‘#’), then setting ‘comment.char = “”’ will sometimes make ‘read.table()’ run faster. In my experience, the difference is not dramatic.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-06-a-really-cool-paper-on-the-hot-hand-in-sports/",
    "title": "A Really Cool Paper on the \"Hot Hand\" in Sports",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-06",
    "categories": [],
    "contents": "\nI just found this really cool paper on the phenomenon of the “hot hand” in sports. The idea behind the “hot hand” (also called the “clustering illusion”) is that success breeds success. In other words, when you are successful (you win games, you make free throws, you get hits) you will continue to be successful. In sports, it has frequently been observed that events are close to independent, meaning that the “hot hand” is just an illusion. \n\nIn the paper, the authors downloaded all the data on NBA free throws for the 2005/2006 through the 2009/2010 seasons. They cleaned up the data, then analyzed changes in conditional probability. Their analysis suggested that free throw success was not an independent event. They go on to explain: \n\nHowever, while statistical traces of this phenomenon are observed in the data, an open question still remains: are these non random patterns a result of “success breeds success” and “failure breeds failure” mechanisms or simply “better” and “worse” periods? Although free throws data is not adequate to answer this question in a definite way, we speculate based on it, that the latter is the dominant cause behind the appearance of the “hot hand” phenomenon in the data.\n\nThe things I like about the paper are that they explain things very simply, use a lot of real data they obtained themselves, and are very careful in their conclusions. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-06-prezi/",
    "title": "Prezi",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-06",
    "categories": [],
    "contents": "\nAndrew Jaffe pointed me to prezi.com. It looks like a new way of making presentations. Andrew made an example here in just a couple of minutes. Here is one about Coca-Cola.\nThings I like: \nI go to a lot of Beamer/Powerpoint talks, these presentations at least look different and could be interesting. \nIt is cool how it is easy to arrange slides in a non-linear order and potentially avoid clicking forward a few slides then back a few slides\nI also like how the “global picture” of the talk can be shown in a display. \nThings I’m not worried about:\nAll the zooming and panning might start to drive people nuts, like slide transitions in powerpoint. \nThere is serious potential for confusing presentations, organization is already a problem with some talks. \nThere is potential for people to spend too much time on making the prezi look cool and less on content. \nUpdate: From the comments Abhijit points out that David Smith put together a presentation on the R ecosystem using Prezi. Check it out here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-06-r-workshop/",
    "title": "R Workshop",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-06",
    "categories": [],
    "contents": "\nI am going to start a continuing “R Workshop” series of posts with R tips and tricks. If you have questions you’d like answered or were wondering about certain aspects, please leave them in the comments.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-05-submitting-scientific-papers-is-too-time-consuming/",
    "title": "Submitting scientific papers is too time consuming",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-05",
    "categories": [],
    "contents": "\nAs an academic who does a lot of research for a living, I spend a lot of my time writing and submitting papers. Before my time, this process involved sending multiple physical copies of a paper by snail mail to the editorial office. New technology has changed this process. Now to submit a paper you generally have to: (1) find a Microsoft Word or Latex template for the journal and use it for your paper and (2) upload the manuscript and figures (usually separately). This is a big improvement over snail mail submission! But it still takes a huge amount of time. Some simple changes would give academics back huge blocks of time to focus on teaching and research.\nJust to give an idea of how complicated the current system is here is an outline of what it takes to submit a paper.\nTo complete step (1) you go to the webpage of the journal you are submitting to, find their template files, and wrestle your content into the template. Sometimes this requires finding additional files which are not on the website of the journal you are submitting too. It always requires a large amount of tweaking the text and content to fit the template.\nTo complete step (2) you have to go the webpage of the journal and start an account with their content management system. There are frequently different requirements for usernames and passwords, leading to proliferation of both. Then you have to upload the files and fill out between 5-7 web forms with information about the authors, information about the paper, information about the funding, information about human subjects research, etc. If the files aren’t in the right format you may have to reformat them before they will be accepted. Some journals even have editorial assistants who will go over your submission and find problems that have to be resolved before your paper can even be reviewed.\nThis whole process can take anywhere from one to ten hours, depending on the journal. If you have to revise your paper for that journal, you have to go through the process again. If your paper is rejected, then you have to start all over with a new template and a new content management system at a new journal.\nIt seems like a much simpler system would be for people to submit their papers in pdf/word format with all the figures embedded. If the paper is accepted to a journal, then of course you might need to reformat the submission to make it easier for typesetters to reformat your article. But that could happen just one time, once a paper is accepted.\nThis seems like a small thing. But suppose you submit a paper between 10 and 15 times a year (very common for academics in my field). Suppose it takes on average 3 hours to submit a paper. That is 3 x 10 = 30 hours a year, almost an entire workweek, just dealing with reformatting papers!\nIn the comments, I’d love to hear about the best/worst experiences you have had submitting papers. Where is good? Where is bad?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-04-cool-papers/",
    "title": "Cool papers",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-04",
    "categories": [],
    "contents": "\nHere is a paper where they scraped Twitter data over a year and showed how the the tweets corresponded with sleep patterns and diurnal rhythms. The coolest part of this paper is that these two guys just went out and collected the data for free. I wish they had focused on more interesting questions though, it seems like you could do a lot with data like this. \nSince flu season is upon us, here is an interesting paper where the authors used data on friendship networks and class structure in a high school to study flu transmission. They show targeted treatment isn’t as effective as people had thought when using random mixing models. \nThis one is a little less statistical. Over the last few years there were some pretty high profile papers that suggested that over-expressing just one protein could double or triple the lifetime of flies or worms. Obviously, that is a pretty crazy/interesting result. But in this paper some of those results are called into question. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-04-defining-data-science/",
    "title": "Defining data science",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-04",
    "categories": [],
    "contents": "\nRebranding of statistics as a field seems to be a popular topic these days and “data science” is one of the potential rebranding options. This article over at Revolutions is a nice summary of where the term comes from and what it means. This quote seems pretty accurate:\n\nMy own take is that Data Science is a valuable rebranding of computer science and applied statistics skills.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-03-innovation-and-overconfidence/",
    "title": "Innovation and overconfidence",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-03",
    "categories": [],
    "contents": "\nI posted a while ago on how overconfidence may be a good thing. I just read this fascinating article by Neal Stephenson (via aldaily.com) about innovation starvation. The article focuses a lot on how science fiction inspires people to work on big/hard/impossible problems in science. Its a great read for the nerds in the audience. But one quote stuck out for me:\n\nMost people who work in corporations or academia have witnessed something like the following: A number of engineers are sitting together in a room, bouncing ideas off each other. Out of the discussion emerges a new concept that seems promising. Then some laptop-wielding person in the corner, having performed a quick Google search, announces that this “new” idea is, in fact, an old one—or at least vaguely similar—and has already been tried. Either it failed, or it succeeded. If it failed, then no manager who wants to keep his or her job will approve spending money trying to revive it. If it succeeded, then it’s patented and entry to the market is presumed to be unattainable, since the first people who thought of it will have “first-mover advantage” and will have created “barriers to entry.” The number of seemingly promising ideas that have been crushed in this way must number in the millions.\n\nThis has to be the single biggest killer of ideas for me. I come up with an idea, google it, find something that is close, and think well it has already been done so I will skip it. I wonder how many of those ideas would have actually turned into something interesting if I had just had a little more overconfidence and skipped the googling? \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-03-oracleworld-claims-and-sensations/",
    "title": "OracleWorld Claims and Sensations",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-03",
    "categories": [],
    "contents": "\nLarry Ellison, the CEO of Oracle, like most technology CEOs, has a tendency for the over-the-top sales pitch. But it’s fun to keep track of what these companies are up to just to see what they think the trends are. It seems clear that companies like IBM, Oracle, and HP, which focus substantially on the enterprise (or try to), think the future is data data data. One piece of evidence is the list of companies that they’ve acquired recently.\nEllison claims that they’ve developed a new computer that integrates hardware with software to produce an overall faster machine. Why do we need this kind of integration? Well, for data analysis, of course!\nI was intrigued by this line from the article:\n\nOn Sunday Mr. Ellison mentioned a machine that he claimed would do data analysis 18 to 23 times faster than could be done on existing machines using Oracle databases. The machine would be able to compute both standard Oracle structured data as well as unstructured data like e-mails, he said.\n\nIt’s always a bit hard in these types of articles to figure out what they mean by “data analysis”, but even still, there’s an important idea here. \nAlex Szalay talks about the need to “bring the computation to the data”. This comes from his experience working with ridiculous amounts of data from the Sloan Digital Sky Survey. There, the traditional model of pulling the data on to your computer, running some analyses, and then producing results just does not work. But the opposite is often reasonable. If the data are sitting in an Oracle/Microsoft/etc. database, you bring the analysis to the database and operate on the data there. Presumably, the analysis program is smaller than the dataset, or this doesn’t quite work.\nSo if Oracle’s magic computer is real, it and others like it could be important as we start bringing more computations to the data.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-02-department-of-analytics-anyone/",
    "title": "Department of Analytics, anyone?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-02",
    "categories": [],
    "contents": "\nThis article following up on the Moneyball PR demonstrates one of the reasons why statistics might be doomed:\n\nJulia Rozovsky is a Yale M.B.A. student who studied economics and math as an undergraduate, a background that prepared her for a traditional — and lucrative — consulting career. Instead, partly as a result of reading “Moneyball” and finding like-minded people, she pointed herself toward work in analytics.\n\nWhy can’t they call it statistics?? The message, of course, is statistics is boring. Analytics is awesome. We probably need to start changing the names of our departments.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-02-karls-take-on-meetings/",
    "title": "Karl's take on meetings",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-02",
    "categories": [],
    "contents": "\nKarl’s take on meetings\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-10-01-bits-big-data-sorting-reality-from-the-hype/",
    "title": "Bits: Big Data: Sorting Reality From the Hype",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-10-01",
    "categories": [],
    "contents": "\nBits: Big Data: Sorting Reality From the Hype\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-30-battling-bad-science/",
    "title": "Battling Bad Science",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-30",
    "categories": [],
    "contents": "\nHere is a pretty awesome TED talk by epidemiologist Ben Goldacre where he highlights how science can be used to deceive/mislead. It’s sort of like epidemiology 101 in 15 minutes. \nThis seems like a highly topical talk. Over on his blog, Steven Salzberg has pointed out that Dr. Oz has recently been engaging in some of these shady practices on his show. Too bad he didn’t check out the video first. \nIn the comments section of the TED talk, one viewer points out that Dr. Goldacre doesn’t talk about the role of the FDA and other regulatory agencies. I think that regulatory agencies are under-appreciated and deserve credit for addressing many of these potential problems in the conduct of clinical trials. \nMaybe there should be an agency regulating how science is reported in the news? \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-29-kindle-fire-and-machine-learning/",
    "title": "Kindle Fire and Machine Learning",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-29",
    "categories": [],
    "contents": "\nAmazon released it’s new iPad competitor, the Kindle Fire, today. A quick read through the description shows it has some interesting features, including a custom-built web browser called Silk. One innovation that they claim is that the browser works in conjunction with Amazon’s EC2 cloud computing platform to speed up the web-surfing experience by doing some computing on your end and some on their end. Seems cool, if it really does make things faster.\nAlso there’s this interesting bit:\n\n\nMachine Learning\nFinally, Silk leverages the collaborative filtering techniques and machine learning algorithms Amazon has built over the last 15 years to power features such as “customers who bought this also bought…” As Silk serves up millions of page views every day, it learns more about the individual sites it renders and where users go next. By observing the aggregate traffic patterns on various web sites, it refines its heuristics, allowing for accurate predictions of the next page request. For example, Silk might observe that 85 percent of visitors to a leading news site next click on that site’s top headline. With that knowledge, EC2 and Silk together make intelligent decisions about pre-pushing content to the Kindle Fire. As a result, the next page a Kindle Fire customer is likely to visit will already be available locally in the device cache, enabling instant rendering to the screen.\n\n\nThat seems like a logical thing for Amazon to do. While the idea of pre-fetching pages is not particularly new, I haven’t yet heard of the idea of doing data analysis on web pages to predict which things to pre-fetch. One issue this raises in my mind, is that in order to do this, Amazon needs to combine information across browsers, which means your surfing habits will become part of one large mega-dataset. Is that what we want?\nOn the one hand, Amazon already does some form of this by keeping track of what you buy. But keeping track of every web page you goto and what links you click on seems like a much wider scope.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-29-once-in-a-lifetime-collapse/",
    "title": "Once in a lifetime collapse",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-29",
    "categories": [],
    "contents": "\n\n\nBaseball Prospectus uses Monte Carlo simulation to predict which teams will make the postseason. According to this page, on Sept 1st, the probability of the Red Sox making the playoffs was 99.5%. They were ahead of the Tampa Bay Rays by 9 games. Before last night’s game, in September, the Red Sox had lost 19 of 26 games and were tied with the Rays for the wild card (the last spot for the playoffs). To make this event even more improbable, The Red Sox were up by one in the ninth with two outs and no one on for the last place Orioles. In this situation the team that’s winning, wins more than 95% of the time. The Rays were in exactly the same situation as the Orioles, losing to the first place Yankees (well, their subs). So guess what happened? The Red Sox lost, the Rays won. But perhaps the most amazing event is that these two games, both lasting much more than usual (one due to rain the other to extra innings) ended within seconds of each other. \nUpdate: Nate Silver beat me to it. And has much more!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-29-why-does-obama-need-statisticians/",
    "title": "Why does Obama need statisticians?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-29",
    "categories": [],
    "contents": "\nIt’s worth following up a little on why the Obama campaign is recruiting statisticians (note to Karen: I am not looking for a new job!). Here’s the blurb for the position of “Statistical Modeling Analyst”:\n\nThe Obama for America Analytics Department analyzes the campaign’s data to guide election strategy and develop quantitative, actionable insights that drive our decision-making. Our team’s products help direct work on the ground, online and on the air. We are a multi-disciplinary team of statisticians, mathematicians, software developers, general analysts and organizers - all striving for a single goal: re-electing President Obama. We are looking for staff at all levels to join our department from now through Election Day 2012 at our Chicago, IL headquarters.\nStatistical Modeling Analysts are charged with predicting electoral outcomes using statistical models. These models will be instrumental in helping the campaign determine how to most effectively use its resources.\n\nI wonder if there’s a bonus for predicting the correct outcome, win or lose?\nThe Obama campaign didn’t invent the idea of heavy data analysis in campaigns, but they seem to be heavy adopters. There are 3 openings in the “Analytics” category as of today.\nNow, can someone tell me why they don’t just call it simply “Statistics”?\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-28-obama-recruiting-analysts-who-know-r/",
    "title": "Obama recruiting analysts who know R",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-28",
    "categories": [],
    "contents": "\nObama recruiting analysts who know R\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-28-the-future-of-graduate-education/",
    "title": "The future of graduate education",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-28",
    "categories": [],
    "contents": "\nStanford is offering a free online course and more than 100,000 students have registered. This got the blogosphere talking about the future of universities. Matt Yglesias thinks that “colleges are the next newspaper and are destined for some very uncomfortable adjustments”. Tyler Cowen reminded us that since 2003 he has been saying that professors are becoming obsolete. His main point is that thanks to the internet, the need for lecturers will greatly diminish. He goes on to predict that\n\nthe market was moving towards superstar teachers, who teach hundreds at a time or even thousands online. Today, we have the Khan Academy, a huge increase in online education, electronic textbooks and peer grading systems and highly successful superstar teachers with Michael Sandel and his popular course Justice, serving as example number one.\n\nI think this is particularly true for stat and biostat graduate programs, especially in hard money environments.\n\nA typical Statistics department will admit five to ten PhD students. In most departments we teach probability theory, statistical theory, and applied statistics. Highly paid professors teach these three courses for these five to ten students, which means that the university ends up spending hundreds of thousands of dollars on them.  Where does this money come from? From those that teach hundreds at a time. The stat 101 courses are full of tuition paying students. These students are subsidizing the teaching of our graduate courses. In hard money institutions, they are also subsidizing some of the research conducted by the professors that teach the small graduate courses. Note that 75% of their salaries are covered by the University, yet they are expected to spend much less than 75% of their time preparing and teaching these relatively tiny classes. The leftover time they spend on research for which they have no external funding. This isn’t a bad thing as a lot of good theoretical and basic knowledge has been created this way. However, outside pressure to lower tuition costs has University administrators looking for ways to save and graduate education might be a target. “If you want to teach a class, fill it up with 50 students. If you want to do research, get a grant. ” the administrator might say.\nNote that, for example, the stat theory class is pretty much the same every year and across universities. So we can pick a couple of superstar stat theory teachers and have them lead an online course for all the stat and biostat graduate students in the world. Then each department hires an energetic instructor, paying him/her 1/4 what they pay a tenured professor, to sit in a room discussing the online lectures with the five to ten PhD students in the program. Currently there are no incentives for the tenured professor to teach well, but the instructor would be rewarded solely by their teaching performance.  Not only does this scheme cut costs, but it can also increase revenue as faculty will have more time to write grant proposals, etc..\nSo, with teaching out of the equation, why even have departments? Well, for now the internet can’t substitute the one-on-one interactions needed during PhD thesis supervision. As long as NIH and NSF are around, research faculty will be around. The apprenticeship system that has worked for centuries will survive the uncomfortable adjustments that are coming. Special topic seminars will also survive as faculty will use them as part of their research agenda. Rotations, similar to those implemented in Biology programs, can serve as match makers between professors and students. But classroom teaching is due for some “uncomfortable adjustments”.\nI agree with Tyler Cowen and Matt Yglesias: the number of cushy professors jobs per department will drop dramatically in the future, especially in hard money institutions. So let’s get ready. Maybe Biostat departments should start planning for the future now. Harvard, Seattle, Michigan, Emory, etc.. want to teach stat theory with us?\nPS -  I suspect this all applies to liberal arts and hard science graduate programs.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-28-the-open-data-movement/",
    "title": "The Open Data Movement",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-28",
    "categories": [],
    "contents": "\nI’m not sure which of the categories this infographic on open data falls into, but I find it pretty exciting anyway. It shows the rise of APIs and how data are increasingly open. It seems like APIs are all over the place in the web development community, but less so in health statistics. Although, from the comments, John M. posts places to find free government data including some health data: \n\n1) CDC’s National Center for Health Statistics, http://www.cdc.gov/nchs/2) NHANES (National and Health and Nutrition Examination Survey)  http://www.cdc.gov/nchs/nhanes.htm3) National Health Interview Survey: http://www.cdc.gov/nchs/nhis.htm4) World Health Organization: www.who.gov5) US Census Bureau: www.uscensus.gov6) Emory maintains a repository of links related to stats/biostat including online databases \nhttp://www.sph.emory.edu/cms/departments_centers/bios/resources.html#govlist\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-28-the-p-0-05-journal/",
    "title": "The p>0.05 journal",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-28",
    "categories": [],
    "contents": "\nI want to start a journal called “P>0.05”. This journal will publish all the negative results in science. These would also be stored in a database. Think of all the great things we could do with this. We could, for example, plot p-value histograms for different disciplines. I bet most would have a flat distribution. We could also do it by specific association. A paper comes out saying chocolate is linked to weaker bones? Check the histogram and keep eating chocolate. Any publishers interested? \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-27-some-cool-papers/",
    "title": "Some cool papers",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-27",
    "categories": [],
    "contents": "\nA cool article on the regulator’s dilemma. It turns out what is the best risk profile to prevent one bank from failing is not the best risk profile to prevent all banks from failing. \nPersistence of web resources for computational biology. I think this one is particularly relevant for academic statisticians since a lot of academic software/packages are developed by graduate students. Once they move on, a large chunk of “institutional knowledge” is lost. \nAre private schools better than public schools? A quote from the paper: “Indeed when comparing the average score in the two types of schools after adjusting for the enrollment effects, we find quite surprisingly that public schools perform better on average.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-26-25-minute-seminars/",
    "title": "25 minute seminars",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-26",
    "categories": [],
    "contents": "\nMost Statistics and Biostatistics departments have weekly seminars. We usually invite outside speakers to share their knowledge via a 50 minute powerpoint (or beamer) presentation. This gives us the opportunity to meet colleagues from other Universities and pick their brains in small group meetings. This is all great. But, giving a good one hour seminar is hard. Really hard. Few people can pull it off. I propose to the statistical community that we cut the seminars to 25 minutes with 35 minutes for questions and further discussion. We can make exceptions of course. But in general, I think we would all benefit from shorter seminars. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-26-by-poring-over-statistics-ignored-by-conventional/",
    "title": "By poring over statistics ignored by conventional scouts, - 05.12.03 - SI Vault",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-26",
    "categories": [],
    "contents": "\nBy poring over statistics ignored by conventional scouts, - 05.12.03 - SI Vault\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-26-unoriginal-genius/",
    "title": "Unoriginal genius",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-26",
    "categories": [],
    "contents": "\n\n“The world is full of texts, more or less interesting; I do not wish to add any more”\n\nThis quote is from an article in the Chronicle Review. I highly recommend reading the article, particularly check out the section on the author’s “Uncreative writing” class at UPenn. The article is about how there is a trend in literature toward combining/using other people’s words to create new content. \n\n\n\nThe prominent literary critic Marjorie Perloff has recently begun using the term “unoriginal genius” to describe this tendency emerging in literature. Her idea is that, because of changes brought on by technology and the Internet, our notion of the genius—a romantic, isolated figure—is outdated. An updated notion of genius would have to center around one’s mastery of information and its dissemination. Perloff has coined another term, “moving information,” to signify both the act of pushing language around as well as the act of being emotionally moved by that process. She posits that today’s writer resembles more a programmer than a tortured genius, brilliantly conceptualizing, constructing, executing, and maintaining a writing machine.\n\nIt is fascinating to see this happening in the world of literature; a similar trend seems to be happening in statistics. A ton of exciting and interesting work is done by people combining known ideas and tools and applying them to new problems. I wonder if we need a new definition of “creative”? \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:52:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-24-how-do-you-spend-your-day/",
    "title": "How do you spend your day?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-24",
    "categories": [],
    "contents": "\nI’ve seen visualizations of how people spend their time a couple of places. Here is a good one over at Flowing Data. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-23-dongle-communism/",
    "title": "Dongle communism",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-23",
    "categories": [],
    "contents": "\nIf you have a mac and give talks or teach, chances are you have embarrassed yourself by forgetting your dongle. Our lab meetings and classes were constantly delayed due to missing dongles. Communism solved this problem. We bought 10 dongles, sprinkled them around the department, and declared all dongles public property. All dongles, not just the 10. No longer do we have to ask to borrow dongles because they have no owner. Please join the revolution. ps -I think this should apply to pens too!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-23-getting-email-responses-from-busy-people/",
    "title": "Getting email responses from busy people",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-23",
    "categories": [],
    "contents": "\nI’ve had the good fortune of working with some really smart and successful people during my career. As a young person, one problem with working with really successful people is that they get a ton of email. Some only see the subject lines on their phone before deleting them. \nI’ve picked up a few tricks for getting email responses from important/successful people:  \nThe SI Rules\nTry to send no more than one email a day. \nEmails should be 3 sentences or less. Better if you can get the whole email in the subject line. \nIf you need information, ask yes or no questions whenever possible. Never ask a question that requires a full sentence response.\nWhen something is time sensitive, state the action you will take if you don’t get a response by a time you specify. \nBe as specific as you can while conforming to the length requirements. \nBonus: include obvious keywords people can use to search for your email. \nAnecdotally, SI emails have a 10-fold higher response probability. The rules are designed around the fact that busy people who get lots of email love checking things off their list. SI emails are easy to check off! That will make them happy and get you a response. \nIt takes more work on your end when writing an SI email. You often need to think more carefully about what to ask, how to phrase it succinctly, and how to minimize the number of emails you write. A surprising side effect of applying SI principles is that I often figure out answers to my questions on my own. I have to decide which questions to include in my SI emails and they have to be yes/no answers, so I end up taking care of simple questions on my own. \nHere are examples of SI emails just to get you started: \nExample 1\nSubject: Is my response to reviewer 2 ok with you?\nBody: I’ve attached the paper/responses to referees.\nExample 2\nSubject: Can you send my letter of recommendation to john.doe@someplace.com?\nBody:\nKeywords = recommendation, Jeff, John Doe.\nExample 3\nSubject: I revised the draft to include your suggestions about simulations and language\nRevisions attached. Let me know if you have any problems, otherwise I’ll submit Monday at 2pm. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-22-most-popular-infographics/",
    "title": "Most popular infographics",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-22",
    "categories": [],
    "contents": "\nThanks to Karl Broman via Andrew Gelman.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-22-statistix/",
    "title": "StatistiX",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-22",
    "categories": [],
    "contents": "\nI think our field would attract more students if we changed the name to something ending with X or K. I’ve joked about this for years, but someone has actually done it (kind of):\nhttp://www.bitlifesciences.com/AnalytiX2012/\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-22-the-killer-app-for-peer-review/",
    "title": "The Killer App for Peer Review",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-22",
    "categories": [],
    "contents": "\nA little while ago, over at Genomes Unzipped, Joe Pickrell asked, “Why publish science in peer reviewed journals?” He points out the flaws with the current peer review system and suggests how we can do better. What he suggests is missing is the killer app for peer review. \nWell, PLoS has now developed an API, where you can easily access tons of data on the papers published in those journals including downloads, citations, number of social bookmarks, and mentions in major science blogs. Along with Mendeley a free reference manager, they have launched an competition to build cool apps with their free data. \nSeems like with the right statistical analysis/cool features a recommender system for say, PLoS One could have most of the features suggested by Joe in his article. One idea would be an RSS-feed based on an idea like the Pandora music sharing service. You input a couple of papers you like from the journal, then it creates an RSS feed with papers similar to that paper. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-21-small-ball-is-a-bad-strategy/",
    "title": "Small ball is a bad strategy",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-21",
    "categories": [],
    "contents": "\nBill James pointed this out a long time ago. If you don’t know Bill James, you should look him up. I consider him to be one of the most influential statisticians of all times. This post relates to one of his first conjectures: sacrificing outs for runs, referred to as small ball, is a bad strategy. \nESPN’s Gamecast, a webtool that gives you pitch-by-pitch updates of baseball games, also gives you a pitch-by-pitch “probability” of wining. Gamecast confirms the conjecure with data. How do they calculate this “probability”? I am pretty sure it is based only on historical data. No modeling. For example, if the away team is up 4-2 in the bottom of the 7th with no outs and runners on 1st and 2nd, they look at all the instances exactly like this one that have ever happened in the digitally recorded history of baseball and report the proportion of times the home team wins. Well in this situation this proportion is 45%. If the next batter successfully bunts, moving the runners over, this proportion drops to 41%.  Furthermore, if after the successful bunt, the run from third scores on a sacrifice fly, the proportion drops again from 41%  to 39%. The extra out hurts you more than the extra run helps you. That was Bill James’ intuition: you only have three outs so the last thing you want to do is give 33% away. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-20-finish-and-publish/",
    "title": "Finish and publish",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-20",
    "categories": [],
    "contents": "\nRoger pointed us to this Amstat news profile of statisticians including one on Francesca Dominici. Francesca has used her statistics skills to become a top environmental scientist. She had this advice for young [academic] statisticians:\n\nFirst, I would say find a good mentor in or outside the department. Prioritize, manage your time, and identify the projects you would like to lead. Focus the most productive time of day on those projects. Take ownership of projects. The biggest danger is getting pulled in very different directions; focus on one main project. Finish everything you start. Always publish. Even if it is not revolutionary, publish.\n\nI think this is great advice. And I want to add to the last two sentences. If you are smart and it took you time to figure out the solution to a problem you find interesting, chances are others will want to read about it. So follow Francesca’s advice: finish and publish.  Remember Voltaire’s  quote “perfection is the enemy of the good”.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-20-macarthur-fellow-shwetak-patel/",
    "title": "MacArthur Fellow Shwetak Patel",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-20",
    "categories": [],
    "contents": "\nThe new MacArthur Fellows list is out and, as usual, they are an interesting bunch. One person that I thought was worth pointing out is Shwetak Patel. I had the privilege of meeting Shwetak at a National Research Council meeting on sustainability and computer science. Basically, he’s working on devices that you can install in your home to monitor your resource usage. He’s already spun-off a startup company to make/sell some of these devices. \nIn the writeup for the award, they mention\n\nWhen coupled with a machine learning algorithm that analyzes patterns of activity and the signature noise produced by each appliance, the sensors enable users to measure and disaggregate their energy and water consumption and to detect inefficiencies more effectively.\n\nNow that’s statistics at work!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-20-private-health-insurers-to-release-data/",
    "title": "Private health insurers to release data",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-20",
    "categories": [],
    "contents": "\nIt looks like four major private health insurance companies will be releasing data for use by academic researchers. They will create a non-profit institute called the Health Care Cost Institute and deposit the data there. Researchers can request the data from the institute by (I’m guessing) writing a short proposal.\nHealth insurance billing claims data might not sound all that exciting, but they are a gold mine of very interesting information about population health. In my group, we use billing claims from Medicare Part A to explore the relationships between ambient air pollutants and hospital admissions for various cardiovascular and respiratory diseases. The advantage of using a database like Medicare is that the population is very large (about 48 million people) and highly relevant. Furthermore, the data are just sitting there, already collected. The disadvantage is that you get relatively little information about those people. For example, you can’t find out what a particular Medicare enrollee’s blood pressure is on a given day. Also, it requires some pretty sophisticated data analysis skills to go through these large databases and extract the information you need to address a scientific question. But this “disadvantage” is what allows statisticians to play an important role in making scientific discoveries.\nIt’s not clear what kind of information will be made available from the private insurers—it looks like it’s mostly geared towards doing economic/cost analysis. However, I’m guessing that there will be a host of other uses for the data that will be revealed as time goes on. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-20-statistician-profiles/",
    "title": "Statistician Profiles",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-20",
    "categories": [],
    "contents": "\nJust in case you forgot to renew your subscription to Amstat News, there’s a nice little profile of statisticians (including my good colleague Francesca Dominici) in the latest issue explaining how they ended up where they are.\nI remember a few years ago I was at a dinner for our MPH program and the director at the time, Ron Brookmeyer, told all the students to ask the faculty how they ended up in public health. The implication, of course, was that the route was likely to be highly nonlinear. It was definitely that way for me.\nStatisticians in particular, I think, have the ability to lead interesting careers simply because we have the ability to operate in a variety of substantive fields. I started out developing point process models for predicting wildfire occurrence. Perhaps to the chagrin of my advisor, I’m not doing much point process modeling now, but rather am working in environmental health doing quite a bit of air pollution epidemiology.\nSo ask a statistician how they ended up where they are. It’ll probably be an interesting story.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-19-data-sources/",
    "title": "Data Sources",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-19",
    "categories": [],
    "contents": "\nHere are places you can get data sets to analyze (for class projects, fun and profit!)\nData Market\nInfochimps\nData.gov\nFactual.com\nI’m sure there are a ton more…would love to hear from people. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-19-meetings/",
    "title": "Meetings",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-19",
    "categories": [],
    "contents": "\nIn this TED talk Jason Fried explains why work doesn’t happen at work. He describes the evils of meetings. Meetings are particularly disruptive for applied statisticians, especially for those of us that hack data files, explore data for systematic errors, get inspiration from visual inspection, and thoroughly test our code. Why? Before I become productive I go through a ramp-up/boot-up stage. Scripts need to be found, data loaded into memory, and most importantly, my brains needs to re-familiarize itself with the data and the essence of the problem at hand. I need a similar ramp up for writing as well. It usually takes me between 15 to 60 minutes before I am in full-productivity mode. But once I am in “the zone”, I become very focused and I can stay in this mode for hours. There is nothing worse than interrupting this state of mind to go to a meeting. I lose much more than the hour I spend at the meeting. A short way to explain this is that having 10 separate hours to work is basically nothing, while having 10 hours in the zone is when I get stuff done.\n\nOf course not all meetings are a waste of time. Academic leaders and administrators need to consult and get advice before making important decisions. I find lab meetings very stimulating and, generally, productive: we unstick the stuck and realign the derailed. But before you go and set up a standing meeting consider this calculation: a weekly one hour meeting with 20 people translates into 1 hour x 20 people x 52 weeks/year = 1040 person hours of potentially lost production per year. Assuming 40 hour weeks, that translates into six months. How many grants, papers, and lectures can we produce in six months? And this does not take into account the non-linear effect described above. Jason Fried suggest you cancel your next meeting, notice that nothing bad happens and enjoy the extra hour of work.\nI know many others that are like me in this regard and for you I have these recommendations: 1- avoid unnecessary meetings, especially if you are already in full-productivity mode. Don’t be afraid to use this as an excuse to cancel.  If you are in a soft $ institution, remember who pays your salary.  2- Try to bunch all the necessary meetings all together into one day. 3- Separate at least one day a week to stay home and work for 10 hours straight. Jason Fried also recommends that every work place declare a day in which no one talks. No meetings, no chit-chat, no friendly banter, etc… No talk Thursdays anyone? \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-18-ideas-data-blogs-i-read/",
    "title": "Ideas/Data blogs I read",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-18",
    "categories": [],
    "contents": "\nR bloggers - good R blogs aggregator\nFlowing Data - interesting data visualizations\nMarginal Revolution - an econ blog with lots of interesting ideas\nRevolutions - another news about R blog\nSteven Salzberg’s blog\nAndrew Gelman’s blog\nI’m sure there are a ton more good blogs like this out there. Any suggestions of what I should be reading? \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-16-google-fusion-tables/",
    "title": "Google Fusion Tables",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-16",
    "categories": [],
    "contents": "\n\nThanks to Hilary Parker for pointing out Google Fusion Tables. The coolest thing here, from my self-centered spatial statistics point of view, is that it automatically geocodes locations for you. So you can upload a spreadsheet of addresses and it will map them for you on Google Maps.\nUnfortunately, there doesn’t seem to be an easy way to extract the latitude/longitude values, but I’m hoping that’s just a quick hack away….\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-15-another-academic-job-market-option-liberal-arts/",
    "title": "Another academic job market option: liberal arts colleges",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-15",
    "categories": [],
    "contents": "\nLiberal arts colleges are option that falls close to the 75% hard/25% soft option described by Rafa in his advice for folks on the job market. At these schools the teaching load may be even a little heavier than schools like Berkeley/Duke; the students will usually be exclusively undergraduates. Examples of this kind of place are Pomona College, Carleton College, Grinnell College, etc. The teaching load is the focus at places like this, but research plays an increasingly major role for academic faculty. In a recent Nature editorial, Amy Cheng Vollmer produces an interesting analogy for the differences in responsibilities. \n\n“It’s like comparing the winter Olympics to the summer Olympics,” says Vollmer, who frequently gives talks on career issues. “It’s not easier, it’s different”\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-15-communicating-uncertainty-visually/",
    "title": "Communicating uncertainty visually",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-15",
    "categories": [],
    "contents": "\nFrom a cool review about communicating risk to people without statistical/probabilistic training.\n\nDespite the burgeoning interest in infographics, there is limited experimental evidence on how different types of visualizations are processed and understood, although the effectiveness of some graphics clearly depends on the relative numeracy of an audience. \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-15-when-overconfidence-is-good/",
    "title": "When overconfidence is good",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-15",
    "categories": [],
    "contents": "\nA paper came out in the latest issue of Nature called the “Evolution of Confidence”. The authors describe a simple model where two participants are competing for a resource. They can either both claim the resource, only one can claim the resource, or neither can. If the ratio of the value of the resource over the cost of competition is good enough, then it makes sense to be overconfident about your abilities to obtain it. \nThe amazing thing about this paper is that it explains a really old idea “why are people overconfident” with really simple models and simulations (done in R!). Based on my own experience, I feel like they may be on to something. You can’t get a paper in Nature if you don’t send it there…\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-14-dissecting-the-genomics-of-trauma/",
    "title": "Dissecting the genomics of trauma",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-14",
    "categories": [],
    "contents": "\nToday the results of a study I’ve been involved with for a long time (read: since my early graduate school days) came out in PLoS Medicine (also Princeton News coverage, Eurekalert press release).\nWe looked at gene expression profiles - how much each of your 20,000 genes is turned on or turned off - in patients who had experienced blunt force trauma. Using these profiles we were able to distinguish very early on which of the patients were going to have positive or negative health trajectories. The idea was to compare patients to themselves and see how much their genomic profiles deviated from the earliest measurements.\nI’m excited about this paper for a couple of reasons: (1) like we say in the paper, “Trauma is the number one killer of individuals 1-44y of age in the United States”, (2) the communicating author and joint first authors, Keyur Desai and Chuen Seng Tan, on the paper were statisticians, highlighting the important role statistics played in the scientific process. \nUpdate:  If you want to check out the data/analyze them yourself, there is a website explaining how to access the data & code here. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-12-advice-for-stats-students-on-the-academic-job-market/",
    "title": "Advice for stats students on the academic job market",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-12",
    "categories": [],
    "contents": "\nJob hunting season is upon us. Openings are already being posted here, here, and here. So you should have your CV, research statement, and web page ready. I highly recommend having a web page.  It doesn’t have to be fancy. Here, here, and here are some good ones ranging from simple to a bit over the top. Minimum requirements are a list of publications and a link to a CV. If you have written software, link to that as well.\nThe earlier you submit the better. Don’t wait for your letters. Keep in mind two things: 1) departments have a limit of how many people they can invite and 2) admissions committee members get tired after reading 200+ CVs. \nIf you are seeking an academic job your CV should focus on the following: PhD granting institution, advisor (including postdoc advisor if you have one), and papers. Be careful not to drown out these most important features with superflous entries. For papers, Include three sections: 1-published, 2-under review, and 3-under preparation. For 2, include the journal names and if possible have tech reports available on your web page. For 3, be ready to give updates during the interview. If you have papers for which you are co-first author be sure to highlight that fact somehow. \nSo what are the different types of jobs?  Before listing the options I should explain the concept of hard versus soft money. Revenue in academia comes from tuition (in public schools the state kicks in some extra $), external funding (e.g. NIH grants), services (e.g. patient care), and philanthropy (endowment). The money that comes from tuition, services, and philanthropy is referred to as hard money. Every year roughly the same amount is available and the way its split among departments rarely changes. When it does, it’s because your chair has either lost or won a long hard-fought zero-sum battle. Research money comes from NIH, NSF, DoD, etc.. and one has to write grants to raise funding (which pay part or all of your salary). These days about 10% of grant applications are funded, so it is certainly not guaranteed. Although at the school level the law of large numbers kicks in, at the individual level it certainly doesn’t. Note that the break down of revenue varies widely from institution to institution. Liberal arts colleges are almost 100% hard money while research institutes are almost 100% soft money.\nSo to simplify, your salary will come from teaching (tuition) and research (grants). The percentages will vary depending on the department. Here are four types of jobs:\nSoft money university positions: examples are Hopkins and Harvard Biostat. A typical breakdown is 75% soft/25% hard. To earn the hard money you will have to teach, but not that much. In my dept we teach 48 classroom hours a year (equivalent to one one-semester class). To earn the soft money you have to write, and eventually get, grants. As a statistician you don’t necessarily have to write your own grants, you can partner up with other scientists that need help. And there are many! Salaries are typically higher in these positions. Stress levels are also higher given the uncertainty of funding. I personally like this as it keeps me motivated, focused, and forces me to work on problems important enough to receive NIH funding.\n1a) Some schools of medicine have Biostatistics units that are 100% soft money. One does not have to teach, but, unless you have a joint appointment, you won’t have access to grad students.  Still these are tenure track jobs. Although at 100% soft what does tenure mean? The Oncology Biostat division at Hopkins is an example. I should mention at MD Anderson, one only needs to raise 50% of ones salary and the other 50% is earned via service (statistical consulting to the institution). I imagine there are other places like this, as well as institutions that use endowments to provide some hard money. \nHard money positions: examples are Berkeley and Stanford Stat. A typical break down is 75% hard/25% soft. You get paid a 9 month salary.  If you want to get paid in the summer and pay students, you need a grant. Here you typically teach two classes a semester but many places let you “buy out” of  teaching if you can get grants to pay your salary. Some tension exists when chairs decide who teaches the big undergrand courses (lots of grunt work) and who teaches the small seminar classes where you talk about your own work.\nResearch associate positions: examples are jobs in schools of medicine in departments other than Stat/Biostat. These positions are typically 100% soft and are created because someone at the institution has a grant to pay for you. These are usually not tenure track positons and you rarely have to teach. You also have less independence since you have to work on the grant that funds you.\nIndustry: typically 100% hard. There are plenty of for-profit companies where one can have fruitful research careers. AT & T, Google, IBM, Microsoft, and Genentech are all examples of companies with great research groups. Note that S, the language that R is based on, was born in Bell Labs. And one of the co-creators of R now does his research at Genentech. Salaries are typically higher in Industry and cafeteria food can be quite awesome. The drawbacks are no access to students and lack of independence (although not always!).\nUpdate: I reader points out that I forgot:\nGovernment jobs: The FDA and NIH are examples of agencies that have research positions. The NCI’s Biometric Research Branch is an example. I would classify these as 100% hard. But it is different than other hard money places in that you have to justify your budget every so often. Service, collaborative, and independent research is expected.  A drawback is that you don’t have access to students although you can get joint appointments. At Hopkins we have a couple of NCI researchers with joint appointments. \nOk, that is it for now. Sometime in December we will blog about job interviews. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-11-the-duke-saga/",
    "title": "The Duke Saga",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-11",
    "categories": [],
    "contents": "\nFor those of you that don’t know about the saga involving genomic signatures, I highly recommend reading this very good summary published in The Economist. Baggerly and Coombes are two statisticians that can confidently say they have made an impact on clinical research and actually saved lives. A paper by this pair describing the details was published in the Annals of Applied Statistics as most of the Biology journals refused to publish their letters to the editor. Baggerly is also a fantastic public speaker as seen in this video and this one. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-10-what-is-a-statistician/",
    "title": "What is a Statistician?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-10",
    "categories": [],
    "contents": "\n\nThis Column was written by Terry Speed in 2006 and is reprinted with permission from the IMS Bulletin, http://bulletin.imstat.org\n\nIn the generation of my teachers, say from 1935 to 1960, relatively few statisticians were trained for the profession. The majority seemed to come from mathematics, without any specialized statistical training. There was also a sizeable minority coming from other areas, such as astronomy (I can think of one prominent example), chemistry or chemical engineering (three), economics (several), history (one), medicine (several), physics (two), and psychology (several). In those days, PhD programs in statistics were few and far between, and many, perhaps most people moved into statistics because they were interested in the subject, or were responding to a perceived need. They learned the subject on the job, either in government, industry or academia. I also think statistics benefited disproportionately from the minority coming from outside mathematics and statistics, but that may be a personal bias.\n\n\nThis diversity of backgrounds seems to have diminished from the mid-1960s. Almost all of my colleagues in statistics over the last 40 years had some graduate training in statistics. Typically they had a PhD in statistics, probability or mathematics, the last two with some exposure to statistics. A few had masters degrees or diplomas in statistics. My experience probably reflects that of most of you.\n\n\nBy the 1960s our subject had become professional, there was a ticket of entry into it — a PhD or equivalent — and many graduate programs handing them out. I know many statistics departments now include people with joint appointments, for example in the biological, engineering or social sciences, but I have the impression that the majority are people who trained in statistics and moved ‘away’ through their interest in applications there, rather than people from these other areas who were embraced by the statisticians. As is to be expected, there are plenty of exceptions.\n\n\nWhy am I presenting this made-up history of the recent origins of statisticians? Because I have the sense that the situation which has prevailed for about 40 years is changing again. I see a steady trickle, which I predict will grow substantially, of people not trained in statistics moving into our profession. Many have noticed, and I have previously remarked on, the current shortage of bright young people going into our subject. We probably all know universities, institutes or industries trying hard to recruit statisticians, and coming up empty handed. On the other hand, there has been substantial growth in areas which, while not generally regarded as mainstream statistics, might well have been, had things gone differently. My unoriginal observation is that some people from these areas are starting to see statistics as a worthwhile career, not beating but joining us. Computer science, machine learning, image analysis, information theory and bioinformatics, to name a few, have all provided future statisticians to statistics departments around the world in recent years, and I think there will be much more of this.\n\n\nRecently there was a call for applications for the new United Kingdom EPSRC Statistics Mobility Fellowships, whose aim is “to attract new researchers into the statistics discipline at an early stage in their career”. Is this “mobility” a good idea? In my view, unquestionably yes. Not only do we need an influx of talent to swell our numbers, we also need it to broaden and enrich our subject, so that much of the related activity we now see taking place outside of statistics, and threatening its future, comes inside. In his highly stimulating polemic “Statistical Modelling: The Two Cultures” published in Statistical Science just 5 years ago (16:199–231, 2001), my late colleague Leo Breiman argued that “the focus in the statistical community on data models has:\n\nled to irrelevant theory and questionable scientific conclusions; \nkept statisticians from using more suitable algorithmic models; \nprevented statisticians from working on exciting new problems.”\n\nHis view was that “we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.”\n\n\nOne, perhaps over-optimistic, view is that the reform that Leo so desired will come automatically as mainstream statistics is joined by “outsiders” from fields like those mentioned above. Are there risks in this trend? There must be. We want statistics broadened and enriched; we don’t want to see it fragmented, trivialized, or otherwise weakened. We need our theorists working hard to incorporate all these new ideas into our long-standing big picture, we need the newcomers to become familiar with the best we have to offer, and we all need to work together in answering the questions of all the people outside our discipline needing our involvement.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-09-data-visualization-and-art/",
    "title": "Data visualization and art",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-09",
    "categories": [],
    "contents": "\nMark Hansen is easily one of my favorite statisticians today. He is a Professor of Statistics at UCLA and his collaborations with artists have brought data visualization to a whole new place, one that is both informative and moving. \nHere is a video of his project with Ben Rubin called Listening Post. The installation grabs conversations from unrestricted chat rooms and processes them in real-time to create interesting “themes” or “movements”. I believe this one is called “I am” and the video is taken from the Whitney Museum of American Art.\n[youtube http://www.youtube.com/watch?v=dD36IajCz6A&w=420&h=345]\nHere some pretty cool time-lapse photography of the installation of Listening Post at the San Jose Museum of Art\n[youtube http://www.youtube.com/watch?v=cClHQU6Fqro]\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-08-any-other-team-wins-the-world-series-good-for/",
    "title": "Untitled",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-08",
    "categories": [],
    "contents": "\n[youtube http://www.youtube.com/watch?v=\\_tvh5edD22c?wmode=transparent&autohide=1&egm=0&hd=1&iv\\_load_policy=3&modestbranding=1&rel=0&showinfo=0&showsearch=0&w=500&h=375]\n“Any other team wins the World Series, good for them…if we win, with this team … we’ll have changed the game.”\nMoneyball! Maybe the start of the era of data. Plus it is a feel good baseball movie where a statistician is the hero. I haven’t been this stoked for a movie in a long time.\n\n(Source: http://www.youtube.com/)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-08-data-analysis-companies-getting-gobbled-up/",
    "title": "Data analysis companies getting gobbled up",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-08",
    "categories": [],
    "contents": "\nCompanies that specialize in data analysis, or essentially, statistics, are getting gobbled up by larger companies. IBM bought SPSS, then later Algorithmics. MSCI bought RiskMetrics. HP bought Autonomy. Who’s next? SAS?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-08-data-science-hot-career-choice/",
    "title": "Data Science = Hot Career Choice",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-08",
    "categories": [],
    "contents": "\nNot only are data analytics companies getting scooped up left and right, “data science” is blowing up as a career. Data science is sort of an amorphous term, like any hot topic (e.g., cloud computing). Regardless, people who can crunch numbers and find patterns are in high-demand, and I’m not the only one saying so.\nDon’t believe the hype? Search for “data” on the career site of Amazon, Google, Facebook, Groupon, Livingsocial, Square, ….\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-07-awesome-stat-ed-links/",
    "title": "Awesome Stat Ed Links",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-07",
    "categories": [],
    "contents": "\nOpenintro - A free online introduction to stats textbook, even the latex is free! One of the authors is Chris Barr, a former postdoc at Hopkins.\nThe undergraduate guide to R - A free intro to R at a super-beginners level, the most popular (and free) statistical programming language. Written by an undergrad at Princeton.  \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-07-build-your-own-pre-cog/",
    "title": "Build your own pre-cog",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-07",
    "categories": [],
    "contents": "\nOkay, this is not really about pre-cog, but just a pointer to some data that might be of interest to people. A number of cities post their crime data online, ready for scraping and data analysis. For example, the Baltimore Sun has a Google map of homicides in the city of Baltimore. There’s also some data for Oakland.\nLooking at the map is fun, but not particularly useful from a data analysis standpoint. However, with a little fiddling (and some knowledge of XML), you can pull the data from the map and use it for data analysis.\nWhy not build your own model to predict crime?\nI’ll just add that the model used in the pre-cog program was published in the Journal of the American Statistical Association in this article.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-07-first-things-first/",
    "title": "First things first",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-07",
    "categories": [],
    "contents": "\nAbout us:\nWe are three professors who are fired up about the new era where data is abundant and statisticians are scientists. \nAbout this blog:\nWe’ll be posting ideas we find interesting, contributing to discussion of science/popular writing, and linking to articles that inspire us. \nWhy “Simply Statistics”:\nWe needed a title. Plus, we like the idea of using simple statistics to solve real, important problems. We aren’t fans of unnecessary complication - that just leads to lies, damn lies and something else. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-07-pre-cog-and-stats/",
    "title": "Pre-cog and stats",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-07",
    "categories": [],
    "contents": "\nA cool article here on a group predicting the place/time when crime is going to happen. It looks like they are using a Poisson process. They liken it to predicting the after shocks of an earthquake. More details on the math behind the pre-cog software can be found here. I wonder what their prediction accuracy is?  Thanks to Rafa for pointing the link out. \n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-07-seek-simplicity-and-distrust-it/",
    "title": "Seek Simplicity and Distrust It",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-07",
    "categories": [],
    "contents": "\nSeek simplicity and distrust it.\nA. N. Whitehead\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-12T08:09:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-07-where-are-the-case-studies/",
    "title": "Where are the Case Studies?",
    "description": {},
    "author": [
      {
        "name": "Admin",
        "url": {}
      }
    ],
    "date": "2011-09-07",
    "categories": [],
    "contents": "\nMany case studies I find interesting don’t appear in JASA Applications and Case Studies or other applied statistics journals for that matter. Some because the technical skill needed to satisfy reviewers is not sufficiently impressive, others because they lack mathematical rigor. But perhaps the main reason for this disconnect is that many interesting case studies are developed by people outside our field or outside academia.\nIn this blog we will try to introduce readers to some of these case studies. I’ll start it off by pointing readers to Nate Silver’s FiveThirtyEight blog. Mr. Silver (yes, Mr. not Prof. nor Dr.) is one of my favorite statisticians. He first became famous for PECOTA; a system that uses data and statistics to predict the performance of baseball players. In FiveThirtyEight he uses a rather sophisticated meta-analysis approach to predicting election outcomes.\nFor example, for the 2008 election he used data from the primaries to calibrate pollsters and then properly weighed these pollsters’ predictions to give a more precise estimate of election results. He predicted Obama would win 349 to 189 with a 6.1% difference in the popular vote. The actual result was 365 to 173 with a difference of 7.2%. His website included graphs that very clearly illustrated the uncertainty of his prediction. These were updated daily and I had a ton of fun visiting his blog at least once a day. I also learned quite a bit, used his data in class, and gained insights that I have used in my own projects.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2011-09-01-examplepost/",
    "title": "Example post",
    "description": {},
    "author": [
      {
        "name": "Jeff Leek",
        "url": {}
      }
    ],
    "date": "2011-09-01",
    "categories": [],
    "contents": "\nWrite your text here in Markdown. Be aware that our blog runs with Jekyll\nDo codeblocks like this https://help.github.com/articles/creating-and-highlighting-code-blocks/\nPut all images in the public/ directory or point to them on a website where they are permanent\n\n\n\n",
    "preview": {},
    "last_modified": "2021-11-11T13:51:43-05:00",
    "input_file": {}
  }
]
